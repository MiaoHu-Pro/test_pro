Ticket-no,Summary,Project,IssueType,Status,Sub-task,IssuelinksCount,Issuelinks,Description,Created time,Updated time,attachment,resolution,priority
SPARK-20901,Feature parity for ORC with Parquet,SPARK,Improvement,Open,[],68,"[<JIRA IssueLink: id='12514670'>, <JIRA IssueLink: id='12517221'>, <JIRA IssueLink: id='12542254'>, <JIRA IssueLink: id='12504725'>, <JIRA IssueLink: id='12504718'>, <JIRA IssueLink: id='12504729'>, <JIRA IssueLink: id='12504719'>, <JIRA IssueLink: id='12504722'>, <JIRA IssueLink: id='12504720'>, <JIRA IssueLink: id='12504727'>, <JIRA IssueLink: id='12504724'>, <JIRA IssueLink: id='12504732'>, <JIRA IssueLink: id='12504736'>, <JIRA IssueLink: id='12504737'>, <JIRA IssueLink: id='12504731'>, <JIRA IssueLink: id='12509266'>, <JIRA IssueLink: id='12516722'>, <JIRA IssueLink: id='12517225'>, <JIRA IssueLink: id='12513790'>, <JIRA IssueLink: id='12513915'>, <JIRA IssueLink: id='12516082'>, <JIRA IssueLink: id='12517306'>, <JIRA IssueLink: id='12517717'>, <JIRA IssueLink: id='12517824'>, <JIRA IssueLink: id='12521645'>, <JIRA IssueLink: id='12523813'>, <JIRA IssueLink: id='12524102'>, <JIRA IssueLink: id='12526272'>, <JIRA IssueLink: id='12536621'>, <JIRA IssueLink: id='12526887'>, <JIRA IssueLink: id='12534751'>, <JIRA IssueLink: id='12535919'>, <JIRA IssueLink: id='12541851'>, <JIRA IssueLink: id='12543159'>, <JIRA IssueLink: id='12543211'>, <JIRA IssueLink: id='12550590'>, <JIRA IssueLink: id='12550847'>, <JIRA IssueLink: id='12504728'>, <JIRA IssueLink: id='12517584'>, <JIRA IssueLink: id='12518075'>, <JIRA IssueLink: id='12541292'>, <JIRA IssueLink: id='12512469'>, <JIRA IssueLink: id='12504716'>, <JIRA IssueLink: id='12504721'>, <JIRA IssueLink: id='12522271'>, <JIRA IssueLink: id='12504738'>, <JIRA IssueLink: id='12504734'>, <JIRA IssueLink: id='12504730'>, <JIRA IssueLink: id='12512283'>, <JIRA IssueLink: id='12512969'>, <JIRA IssueLink: id='12527400'>, <JIRA IssueLink: id='12536618'>, <JIRA IssueLink: id='12609647'>, <JIRA IssueLink: id='12504723'>, <JIRA IssueLink: id='12512236'>, <JIRA IssueLink: id='12525682'>, <JIRA IssueLink: id='12525929'>, <JIRA IssueLink: id='12527360'>, <JIRA IssueLink: id='12532821'>, <JIRA IssueLink: id='12524275'>, <JIRA IssueLink: id='12526349'>, <JIRA IssueLink: id='12527130'>, <JIRA IssueLink: id='12521382'>, <JIRA IssueLink: id='12519071'>, <JIRA IssueLink: id='12526915'>, <JIRA IssueLink: id='12526212'>, <JIRA IssueLink: id='12514737'>, <JIRA IssueLink: id='12512338'>]",This issue aims to track the feature parity for ORC with Parquet.,2017-05-26T17:53:47.554+0000,2021-02-27T18:02:07.634+0000,,Major
SPARK-28266,data duplication when `path` serde property is present,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12624745'>, <JIRA IssueLink: id='12564776'>]","Spark duplicates returned datasets when `path` serde is present in a parquet table. 

Confirmed versions affected: Spark 2.2, Spark 2.3, Spark 2.4.

Confirmed unaffected versions: Spark 2.1 and earlier (tested with Spark 1.6 at least).

Reproducer:

{code:python}
>>> spark.sql(""create table ruslan_test.test55 as select 1 as id"")
DataFrame[]

>>> spark.table(""ruslan_test.test55"").explain()

== Physical Plan ==
HiveTableScan [id#16], HiveTableRelation `ruslan_test`.`test55`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#16]

>>> spark.table(""ruslan_test.test55"").count()
1

{code}

(all is good at this point, now exist session and run in Hive for example - )

{code:sql}
ALTER TABLE ruslan_test.test55 SET SERDEPROPERTIES ( 'path'='hdfs://epsdatalake/hivewarehouse/ruslan_test.db/test55' )
{code}

So LOCATION and serde `path` property would point to the same location.
Now see count returns two records instead of one:

{code:python}
>>> spark.table(""ruslan_test.test55"").count()
2

>>> spark.table(""ruslan_test.test55"").explain()
== Physical Plan ==
*(1) FileScan parquet ruslan_test.test55[id#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://epsdatalake/hivewarehouse/ruslan_test.db/test55, hdfs://epsdatalake/hive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>
>>>

{code}

Also notice that the presence of `path` serde property makes TABLE location 
show up twice - 
{quote}
InMemoryFileIndex[hdfs://epsdatalake/hivewarehouse/ruslan_test.db/test55, hdfs://epsdatalake/hive..., 
{quote}

We have some applications that create parquet tables in Hive with `path` serde property
and it makes data duplicate in query results. 

Hive, Impala etc and Spark version 2.1 and earlier read such tables fine, but not Spark 2.2 and later releases.
",2019-07-05T23:29:43.781+0000,2021-10-18T18:10:34.061+0000,Fixed,Major
ORC-409,Changes for extending MemoryManagerImpl,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12544244'>],"Orc memory manager uses MemoryMX bean to get max heap usage and assumes 50% (configurable) is available for orc writers. This works well if container model where container size is close to Xmx. In LLAP for example, a single container runs multiple executors and a single task take up 50% of heap which is typically much more that the task executors own memory limit. 

Example: 128Gb container + 32 executors (assuming no cache) means there is 4GB available per executor. But a single task that opens multiple orc writers in the default case will assume 64GB memory is available for orc when in fact it should only use 2GB from memory per executor.

 

WriterOption provides a mechanism to plug-in a custom memory manager for such scenarios which is greate. But if the custom memory manager want to extend MemoryManagerImpl class, MemoryManagerImpl should use getTotalMemoryPool() everywhere so that custom memory manager can just Override getTotalMemoryPool() method and rest of the code from MemoryManagerImpl. ",2018-09-28T00:49:49.964+0000,2019-01-02T19:12:45.029+0000,Fixed,Major
TEZ-1788,Test allowing vertex level disabling of speculation,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12407261'>, <JIRA IssueLink: id='12407268'>]",TEZ-1233 allows programmatic ability to set configs on a per vertex basis. Verify that this works for enabling/disabling speculation per vertex.,2014-11-18T19:39:19.689+0000,2016-05-18T04:57:43.471+0000,Fixed,Major
CALCITE-3914,Improve SubstitutionVisitor to consider RexCall of type PLUS and TIMES for canonicalization ,CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12585549'>],,2020-04-10T22:05:35.839+0000,2020-05-24T11:59:43.729+0000,Fixed,Major
SPARK-14423,Handle jar conflict issue when uploading to distributed cache,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12496424'>],"Currently with the introduction of assembly-free deployment of Spark, by default yarn#client will upload all the jars in assembly to HDFS staging folder. If the jars in assembly and specified with \--jars have the same name, this will introduce exception while downloading these jars and make the application fail to run.

Here is the exception when running example with {{run-example}}:

{noformat}
16/04/06 10:29:48 INFO Client: Application report for application_1459907402325_0004 (state: FAILED)
16/04/06 10:29:48 INFO Client:
	 client token: N/A
	 diagnostics: Application application_1459907402325_0004 failed 2 times due to AM Container for appattempt_1459907402325_0004_000002 exited with  exitCode: -1000
For more detailed output, check application tracking page:http://hw12100.local:8088/proxy/application_1459907402325_0004/Then, click on links to logs of each attempt.
Diagnostics: Resource hdfs://localhost:8020/user/sshao/.sparkStaging/application_1459907402325_0004/avro-mapred-1.7.7-hadoop2.jar changed on src filesystem (expected 1459909780508, was 1459909782590
java.io.IOException: Resource hdfs://localhost:8020/user/sshao/.sparkStaging/application_1459907402325_0004/avro-mapred-1.7.7-hadoop2.jar changed on src filesystem (expected 1459909780508, was 1459909782590
	at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:61)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:357)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:356)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

The problem is that this jar {{avro-mapred-1.7.7-hadoop2.jar}} both existed in assembly and example folder.

We should handle this situation, since now spark example is failed to run under yarn mode.",2016-04-06T03:48:11.876+0000,2020-05-17T18:16:57.017+0000,Fixed,Major
TEZ-3881,"expose kill reason, as opposed to just a single kill count, in vertex progress",TEZ,Bug,Open,[],2,"[<JIRA IssueLink: id='12523514'>, <JIRA IssueLink: id='12523515'>]","A followup from TEZ-3880 that would provide more information to the callers to decide what to display and how. An API change, so we can do it in phase 2.

cc [~EricWohlstadter] ",2018-01-04T21:33:24.288+0000,2018-01-04T21:35:04.079+0000,,Major
SPARK-20328,"HadoopRDDs create a MapReduce JobConf, but are not MapReduce jobs",SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12500771'>, <JIRA IssueLink: id='12500790'>]","In order to obtain {{InputSplit}} information, {{HadoopRDD}} creates a MapReduce {{JobConf}} out of the Hadoop {{Configuration}}: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L138

Semantically, this is a problem because a HadoopRDD does not represent a Hadoop MapReduce job.  Practically, this is a problem because this line: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L194 results in this MapReduce-specific security code being called: https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java#L130, which assumes the MapReduce master is configured (e.g. via {{yarn.resourcemanager.*}}).  If it isn't, an exception is thrown.

So I'm seeing this exception thrown as I'm trying to add Kerberos support for the Spark Mesos scheduler:

{code}
Exception in thread ""main"" java.io.IOException: Can't get Master Kerberos principal for use as renewer
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:116)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)
{code}

I have a workaround where I set a YARN-specific configuration variable to trick {{TokenCache}} into thinking YARN is configured, but this is obviously suboptimal.

The proper fix to this would likely require significant {{hadoop}} refactoring to make split information available without going through {{JobConf}}, so I'm not yet sure what the best course of action is.",2017-04-13T21:21:53.732+0000,2019-05-21T04:12:09.226+0000,Incomplete,Major
TEZ-1510,TezConfiguration should not add tez-site.xml as a default resource. ,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12398258'>],"Currently on the first construction of a TezConfiguration, tez-site.xml gets added a static resource for all future Configuration objects.",2014-08-27T20:20:15.444+0000,2014-10-04T07:40:53.626+0000,Fixed,Blocker
ZOOKEEPER-1897,ZK Shell/Cli not processing commands,ZOOKEEPER,Bug,Resolved,[],5,"[<JIRA IssueLink: id='12430164'>, <JIRA IssueLink: id='12430165'>, <JIRA IssueLink: id='12387345'>, <JIRA IssueLink: id='12504137'>, <JIRA IssueLink: id='12397890'>]","When running zookeeper 3.4.5 I was able to run commands using zkCli such as 

zkCli.sh -server 127.0.0.1:2182 ls /
zkCli.sh -server 127.0.0.1:2182 get /blah

After upgrading to 3.4.6 these commands no longer work.

I think issue https://issues.apache.org/jira/browse/ZOOKEEPER-1535 was the reason the commands were running in previous versions.

It looks like the client exits when a command is present.

{code:title=ZooKeeperMain.java}
    void run() throws KeeperException, IOException, InterruptedException {
        if (cl.getCommand() == null) {
            System.out.println(""Welcome to ZooKeeper!"");

            boolean jlinemissing = false;
            // only use jline if it's in the classpath
            try {
                Class consoleC = Class.forName(""jline.ConsoleReader"");
                Class completorC =
                    Class.forName(""org.apache.zookeeper.JLineZNodeCompletor"");

                System.out.println(""JLine support is enabled"");

                Object console =
                    consoleC.getConstructor().newInstance();

                Object completor =
                    completorC.getConstructor(ZooKeeper.class).newInstance(zk);
                Method addCompletor = consoleC.getMethod(""addCompletor"",
                        Class.forName(""jline.Completor""));
                addCompletor.invoke(console, completor);

                String line;
                Method readLine = consoleC.getMethod(""readLine"", String.class);
                while ((line = (String)readLine.invoke(console, getPrompt())) != null) {
                    executeLine(line);
                }
            } catch (ClassNotFoundException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (NoSuchMethodException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (InvocationTargetException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (IllegalAccessException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            } catch (InstantiationException e) {
                LOG.debug(""Unable to start jline"", e);
                jlinemissing = true;
            }

            if (jlinemissing) {
                System.out.println(""JLine support is disabled"");
                BufferedReader br =
                    new BufferedReader(new InputStreamReader(System.in));

                String line;
                while ((line = br.readLine()) != null) {
                    executeLine(line);
                }
            }
        }
    }
{code}",2014-03-18T23:10:22.623+0000,2017-05-20T23:07:11.238+0000,Fixed,Major
TEZ-3529,Tez UI: Add 'All Queries' table in the landing page along 'All DAGs' page,TEZ,Bug,Closed,[],4,"[<JIRA IssueLink: id='12485894'>, <JIRA IssueLink: id='12485897'>, <JIRA IssueLink: id='12487146'>, <JIRA IssueLink: id='12491534'>]","Landing page must have two tabs - All DAGs & All Queries

Following search functionalities must be supported:
- Search for Hive query ID
- Search for user who submitted the query",2016-11-07T13:47:09.504+0000,2017-08-22T00:02:18.886+0000,Fixed,Major
SPARK-2688,Need a way to run multiple data pipeline concurrently,SPARK,New Feature,Resolved,[],5,"[<JIRA IssueLink: id='12392441'>, <JIRA IssueLink: id='12406966'>, <JIRA IssueLink: id='12406965'>, <JIRA IssueLink: id='12392440'>, <JIRA IssueLink: id='12394572'>]","Suppose we want to do the following data processing: 
{code}
rdd1 -> rdd2 -> rdd3
           | -> rdd4
           | -> rdd5
           \ -> rdd6
{code}
where -> represents a transformation. rdd3 to rrdd6 are all derived from an intermediate rdd2. We use foreach(fn) with a dummy function to trigger the execution. However, rdd.foreach(fn) only trigger pipeline rdd1 -> rdd2 -> rdd3. To make things worse, when we call rdd4.foreach(), rdd2 will be recomputed. This is very inefficient. Ideally, we should be able to trigger the execution the whole graph and reuse rdd2, but there doesn't seem to be a way doing so. Tez already realized the importance of this (TEZ-391), so I think Spark should provide this too.

This is required for Hive to support multi-insert queries. HIVE-7292.",2014-07-25T13:14:25.549+0000,2015-12-03T14:00:30.798+0000,Won't Fix,Major
HAMA-149,Refactoring the Hbase APIs  ,HAMA,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12323374'>],"> I would wait for HBASE-880 which will change the client API
> in a way that I think would be easier to create the abstractions
> you are looking at.
> ---
> Jim Kellerman, Powerset (Live Search, Microsoft Corporation)
> - Hide quoted text -
> > -----Original Message-----
> > From: edward@udanax.org [mailto:edward@udanax.org] On Behalf Of Edward J.
> > Yoon
> > Sent: Tuesday, February 03, 2009 1:01 AM
> > To: hbase-dev@hadoop.apache.org; hama-dev@incubator.apache.org
> > Subject: Hbase client
> >
> > I would like to have abstraction layers for the HTable and RowResult
> > because My project uses the two classes to implement Graph and AdjList
> > (or Matrix and Vector). w/o abstraction layers, the codes are becoming
> > very messy.
> >
> > What do you think?
> > --
> > Best Regards, Edward J. Yoon @ NHN, corp.
> > edwardyoon@apache.org
> > http://blog.udanax.org",2009-02-10T03:01:16.168+0000,2013-05-02T02:29:20.548+0000,Won't Fix,Major
INFRA-16464,Enable Precommit hook for new Hadoop subproject: HDDS,INFRA,Improvement,Closed,[],1,[<JIRA IssueLink: id='12532917'>],"We would like to enable automatic precommit builds for all the patches in HDDS project (which is a new subproject of Hadoop).

The Precommit-Admin is checking the changes by a saved filter in jira: https://issues.apache.org/jira/issues/?filter=12323182

Would you be so kind to edit the query and add HDDS project to the JQL of the filter?

https://issues.apache.org/jira/issues/?filter=12323182

Thanks a lot
",2018-04-30T11:31:29.157+0000,2018-08-26T04:24:31.872+0000,Fixed,Major
FLUME-1208,Hbase sink should be built only in Hadoop-1.0  profile,FLUME,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12355202'>, <JIRA IssueLink: id='12352344'>, <JIRA IssueLink: id='12352345'>, <JIRA IssueLink: id='12359088'>]","Until HBASE-6020 is resolved, there is no Hbase jar which we can use to build against hbase compiled against hadoop-23(hadoop-1.0 and hadoop-23 though are compile compatible, produce different jars).

As a result, we should build the hbase sink only in the hadoop-1.0 profile. We need to modify the master pom, dist pom and hbase sink pom for this.",2012-05-16T19:26:33.331+0000,2012-10-17T19:38:09.262+0000,Won't Fix,Major
TEZ-3715,Differentiate between TaskAttempt submission and TaskAttempt started,TEZ,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12503363'>, <JIRA IssueLink: id='12505228'>]",,2017-05-09T04:22:26.445+0000,2017-08-22T00:03:27.918+0000,Fixed,Major
SPARK-26437,"Decimal data becomes bigint to query, unable to query",SPARK,Bug,Resolved,[],5,"[<JIRA IssueLink: id='12646565'>, <JIRA IssueLink: id='12550847'>, <JIRA IssueLink: id='12550844'>, <JIRA IssueLink: id='12550843'>, <JIRA IssueLink: id='12550845'>]","this is my sql:

create table tmp.tmp_test_6387_1224_spark  stored  as ORCFile  as select 0.00 as a

select a from tmp.tmp_test_6387_1224_spark

CREATE TABLE `tmp.tmp_test_6387_1224_spark`(

 {color:#f79232} `a` decimal(2,2)){color}

ROW FORMAT SERDE

  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'

STORED AS INPUTFORMAT

  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'

OUTPUTFORMAT

  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'

When I query this table（use hive or sparksql，the exception is same）, I throw the following exception information

*Caused by: java.io.EOFException: Reading BigInteger past EOF from compressed stream Stream for column 1 kind DATA position: 0 length: 0 range: 0 offset: 0 limit: 0*

        *at org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readBigInteger(SerializationUtils.java:176)*

        *at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$DecimalTreeReader.next(TreeReaderFactory.java:1264)*

        *at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StructTreeReader.next(TreeReaderFactory.java:2004)*

        *at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:1039)*

 ",2018-12-25T06:19:16.708+0000,2022-08-29T06:48:01.553+0000,Fixed,Major
MNG-6324,multithread build: DefaultProjectDependenciesResolver.resolve waits for a lock indenfinetly causing the build to hang,MNG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12522631'>, <JIRA IssueLink: id='12522612'>]","This issue have given me a very bad day about 2 weeks ago...and after I was able to get going - I've just dropped my {{-T9}} switch...
but today I've seen it again in travis-ci runs also...and again dropping {{-T n}} fixed it.

So I've checked it: {{3.3.9}} and {{3.5.0}} are not affected - seems like the problem is introduced in {{3.5.2}}.

It might be possible that this is something which is specific to Hive..please let me know if that's the matter.

steps to reproduce:
{code}
git clone https://github.com/apache/hive
cd hive
mvn install -Pitests -Dmaven.repo.local=$PWD/_empty_repo -T9 -pl itests/qtest -Dtest=TestCliDriver#*[udf_power] -DinitScript=asd.sql -am -q
{code}

the above should normally take around 8-10 minutes; if you have a caching repository configured nearby...

environment info:

{code}
00:02:06.741 Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T09:58:13+02:00)
00:02:06.742 Maven home: /home/jenkins/ws/workspace/hive-mvn-checker/MAVEN_VERSION/3.5.2/jdk/j8/node/Sustwork/apache-maven-3.5.2
00:02:06.743 Java version: 1.8.0_144, vendor: Oracle Corporation
00:02:06.743 Java home: /home/jenkins/ws/tools/hudson.model.JDK/j8/jre
00:02:06.744 Default locale: en_US, platform encoding: UTF-8
00:02:06.744 OS name: ""linux"", version: ""4.4.0-101-generic"", arch: ""amd64"", family: ""unix""
{code}

relevant stacktrace parts (other threads are idle):
{code}

@@@ a thread in this states was always present ; when I've seen the lockup
""BuilderThread 8"" #91 prio=5 os_prio=0 tid=0x00007fafcc691000 nid=0x5f2b waiting on condition [0x00007faf99b6c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.eclipse.aether.connector.basic.PartialFile$LockFile.lock(PartialFile.java:113)
	at org.eclipse.aether.connector.basic.PartialFile$LockFile.<init>(PartialFile.java:69)
	at org.eclipse.aether.connector.basic.PartialFile$Factory.newInstance(PartialFile.java:278)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector$GetTaskRunner.runTask(BasicRepositoryConnector.java:438)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:360)
	at org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableErrorForwarder.java:75)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector$DirectExecutor.execute(BasicRepositoryConnector.java:583)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector.get(BasicRepositoryConnector.java:259)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:498)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:399)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:224)
	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:338)
	at org.apache.maven.project.DefaultProjectDependenciesResolver.resolve(DefaultProjectDependenciesResolver.java:202)
	at org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.getDependencies(LifecycleDependencyResolver.java:223)
	at org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.resolveProjectDependencies(LifecycleDependencyResolver.java:145)
	at org.apache.maven.lifecycle.internal.MojoExecutor.ensureDependenciesAreResolved(MojoExecutor.java:246)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:200)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:200)
	at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:196)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

@@@ this is not always present
""BuilderThread 4"" #61 prio=5 os_prio=0 tid=0x00007fafcc554000 nid=0x5ef9 waiting on condition [0x00007faf9a8bc000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.eclipse.aether.connector.basic.PartialFile$LockFile.lock(PartialFile.java:113)
	at org.eclipse.aether.connector.basic.PartialFile$LockFile.<init>(PartialFile.java:69)
	at org.eclipse.aether.connector.basic.PartialFile$Factory.newInstance(PartialFile.java:278)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector$GetTaskRunner.runTask(BasicRepositoryConnector.java:438)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:360)
	at org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableErrorForwarder.java:75)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector$DirectExecutor.execute(BasicRepositoryConnector.java:583)
	at org.eclipse.aether.connector.basic.BasicRepositoryConnector.get(BasicRepositoryConnector.java:259)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:498)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:399)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:224)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifact(DefaultArtifactResolver.java:201)
	at org.apache.maven.repository.internal.DefaultArtifactDescriptorReader.loadPom(DefaultArtifactDescriptorReader.java:261)
	at org.apache.maven.repository.internal.DefaultArtifactDescriptorReader.readArtifactDescriptor(DefaultArtifactDescriptorReader.java:192)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.resolveCachedArtifactDescriptor(DefaultDependencyCollector.java:539)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.getArtifactDescriptorResult(DefaultDependencyCollector.java:522)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:411)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:365)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:353)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.doRecurse(DefaultDependencyCollector.java:507)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:460)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:365)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:353)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.doRecurse(DefaultDependencyCollector.java:507)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:460)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:365)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:353)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.doRecurse(DefaultDependencyCollector.java:507)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:460)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.processDependency(DefaultDependencyCollector.java:365)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:353)
	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.collectDependencies(DefaultDependencyCollector.java:256)
	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.collectDependencies(DefaultRepositorySystem.java:282)
	at org.apache.maven.project.DefaultProjectDependenciesResolver.resolve(DefaultProjectDependenciesResolver.java:169)
	at org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.getDependencies(LifecycleDependencyResolver.java:223)
	at org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.resolveProjectDependencies(LifecycleDependencyResolver.java:145)
	at org.apache.maven.lifecycle.internal.MojoExecutor.ensureDependenciesAreResolved(MojoExecutor.java:246)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:200)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:200)
	at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call(MultiThreadedBuilder.java:196)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""main"" #1 prio=5 os_prio=0 tid=0x00007fafcc00a800 nid=0x5e80 waiting on condition [0x00007fafd2ebf000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081d95340> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ExecutorCompletionService.take(ExecutorCompletionService.java:193)
	at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder.multiThreadedProjectTaskSegmentBuild(MultiThreadedBuilder.java:140)
	at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder.build(MultiThreadedBuilder.java:101)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:955)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:290)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)

{code}
",2017-12-19T16:49:49.974+0000,2017-12-19T19:06:06.739+0000,Duplicate,Major
IMPALA-9827,"Support reading ""full-ACID"" Parquet tables",IMPALA,Improvement,Open,[],2,"[<JIRA IssueLink: id='12590407'>, <JIRA IssueLink: id='12590406'>]","We should use the support for reading transactional tables added in IMPALA-9042 to support Parquet acid, when the spec for that is nailed now by HIVE-8123.",2020-06-04T19:34:32.645+0000,2020-12-21T20:42:12.324+0000,,Major
HCATALOG-484,HCatalog should use ql.metadata Table and Partition classes,HCATALOG,Bug,Closed,[],3,"[<JIRA IssueLink: id='12357368'>, <JIRA IssueLink: id='12357172'>, <JIRA IssueLink: id='12357171'>]","Hive has two sets of classes for representing tables and partitions. For example:

* {{import org.apache.hadoop.hive.ql.metadata.Table}}
* {{import org.apache.hadoop.hive.metastore.api.Table}}

The {{metadata.api}} versions represent exactly what's stored in the metastore, while the {{ql.metadata}} versions add some business logic, such as providing serde-reported columns.

HCatalog should always use the {{ql.metadata}} versions of Table and Partition classes except when communicating with the metastore, or serializing these objects into the job conf.",2012-08-29T23:27:54.867+0000,2013-02-15T21:32:48.904+0000,Fixed,Major
TEZ-2242,Refactor ShuffleVertexManager code,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12412131'>],,2015-03-27T07:13:23.250+0000,2015-06-30T04:53:04.229+0000,Fixed,Major
HCATALOG-356,Compilation failure due to API change in HIVE-2908,HCATALOG,Task,Closed,[],1,[<JIRA IssueLink: id='12350120'>],"HIVE-2908(http://svn.apache.org/viewvc?view=revision&revision=1308427) that went into hive 0.9 has API changes to DropTableDesc and this breaks hcatalog trunk (0.5) build. 

",2012-04-02T23:56:16.583+0000,2013-02-15T21:32:50.072+0000,Fixed,Major
RANGER-3373,Ranger Hbase plugin not compatible with Hbase 2.3.4,RANGER,Bug,Open,[],1,[<JIRA IssueLink: id='12621351'>],"Ranger is incompatible with Hbase 2.3.4 because AccessControlLists class has been changed to PermissionStorage

https://issues.apache.org/jira/browse/HBASE-22084",2021-08-18T10:35:34.713+0000,2022-04-12T09:38:54.788+0000,,Major
SPARK-28461,Pad Decimal numbers with trailing zeros to the scale of the column,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12566025'>],"PostgreSQL:
{code:sql}
postgres=# select cast(1 as decimal(38, 18));
       numeric
----------------------
 1.000000000000000000
(1 row)
{code}

Spark SQL:
{code:sql}
spark-sql> select cast(1 as decimal(38, 18));
1
spark-sql>
{code}

Hive fix this issue by HIVE-12063.",2019-07-21T00:34:48.734+0000,2019-12-02T00:02:59.583+0000,Fixed,Major
TEZ-714,OutputCommitters should not run in the main AM dispatcher thread,TEZ,Improvement,Closed,[],6,"[<JIRA IssueLink: id='12385651'>, <JIRA IssueLink: id='12423549'>, <JIRA IssueLink: id='12423562'>, <JIRA IssueLink: id='12409721'>, <JIRA IssueLink: id='12385650'>, <JIRA IssueLink: id='12397467'>]","Follow up jira from TEZ-41.
1) If there's multiple OutputCommitters on a Vertex, they can be run in parallel.
2) Running an OutputCommitter in the main thread blocks all other event handling, w.r.t the DAG, and causes the event queue to back up.
3) This should also cover shared commits that happen in the DAG.",2014-01-09T03:12:41.353+0000,2015-06-30T04:53:06.697+0000,Fixed,Critical
ZOOKEEPER-1576,Zookeeper cluster - failed to connect to cluster if one of the provided IPs causes java.net.UnknownHostException,ZOOKEEPER,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12374395'>, <JIRA IssueLink: id='12550394'>, <JIRA IssueLink: id='12520682'>]","Using a cluster of three 3.4.3 zookeeper servers.
All the servers are up, but on the client machine, the firewall is blocking one of the  servers.
The following exception is happening, and the client is not connected to any of the other cluster members.

The exception:Nov 02, 2012 9:54:32 PM com.netflix.curator.framework.imps.CuratorFrameworkImpl logError
SEVERE: Background exception was not retry-able or retry gave up
java.net.UnknownHostException: scnrmq003.myworkday.com
at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$1.lookupAllHostAddr(Unknown Source)
at java.net.InetAddress.getAddressesFromNameService(Unknown Source)
at java.net.InetAddress.getAllByName0(Unknown Source)
at java.net.InetAddress.getAllByName(Unknown Source)
at java.net.InetAddress.getAllByName(Unknown Source)
at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:440)
at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:375)

The code at the org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:60) is :
public StaticHostProvider(Collection<InetSocketAddress> serverAddresses) throws UnknownHostException {
for (InetSocketAddress address : serverAddresses) {
InetAddress resolvedAddresses[] = InetAddress.getAllByName(address
.getHostName());
for (InetAddress resolvedAddress : resolvedAddresses) { this.serverAddresses.add(new InetSocketAddress(resolvedAddress .getHostAddress(), address.getPort())); }
}
......

The for-loop is not trying to resolve the rest of the servers on the list if there is an UnknownHostException at the InetAddress.getAllByName(address.getHostName()); 
and it fails the client connection creation.


I was expecting the connection will be created for the other members of the cluster. 
Also, InetAddress is a blocking command, and if it takes very long time,  (longer than the defined timeout) - that also should allow us to continue to try and connect to the other servers on the list.
Assuming this will be fixed, and we will get connection to the current available servers, I think the zookeeper should continue to retry to connect to the not-connected server of the cluster, so it will be able to use it later when it is back.
If one of the servers on the list is not available during the connection creation, then it should be retried every x time despite the fact that we 

",2012-11-07T11:11:10.403+0000,2018-12-19T11:43:05.880+0000,Fixed,Major
SAMZA-117,Jobs that end due to failed container count should be marked as failed rather than finished,SAMZA,Bug,Open,[],1,[<JIRA IssueLink: id='12387611'>],"Currently if a job ends because of too many failed containers within a specific time, the YARN job correctly ends but is marked as ""finished.""  It would be more accurate to consider these jobs failed and set their status as such.",2014-01-06T20:57:22.915+0000,2014-05-07T03:13:27.864+0000,,Major
FLUME-2312,Add utility for adorning HTTP contexts in Jetty,FLUME,Bug,Resolved,[],1,[<JIRA IssueLink: id='12382498'>],The idea is to accomplish the same as HBASE-10473 to disallow TRACE and OPTIONS,2014-02-05T23:41:07.735+0000,2014-05-04T04:18:27.430+0000,Fixed,Major
KUDU-3201,Support gzipped HMS notifications (GzipJSONMessageEncoder),KUDU,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12600858'>, <JIRA IssueLink: id='12600859'>, <JIRA IssueLink: id='12600857'>]","In HIVE-20679 a new message encoder was added to Hive that gzips and based64 encodes notification messages. In HIVE-21307 the default encoder was changed to use the gzip encoder, GzipJSONMessageEncoder.

Though Hive 4 is not released, failures as a result of this new message format were reported in IMPALA-8751. We should add support for the gzip format to prevent future HMS sync issues.",2020-10-07T15:09:12.106+0000,2020-10-09T21:23:30.212+0000,Fixed,Major
TEZ-1372,Fix preWarm to work after recent API changes,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12393465'>, <JIRA IssueLink: id='12393745'>]",,2014-08-04T00:48:39.660+0000,2014-09-06T01:35:44.401+0000,Fixed,Blocker
LENS-1158,Thread names with hive2 build are growing huge,LENS,Bug,Closed,[],1,[<JIRA IssueLink: id='12467813'>],"One of the log looks the following:

27 May 2016 04:10:24 [bf1c4dce-f578-4237-8035-b3ba6846319e] [cbfb48cc-e1eb-4b58-9db4-b5b09d15a55d 36caf230-8e8a-4b4d-a0ef-857ec94c0996 7e180c99-0d54-42a0-86e7-d9f88e7e5942 f302b519-9397-4b82-8f50-c1fc93da58d8 097a39a7-b948-4804-ad77-f6beab75a017 1461e1a1-cb77-4c7a-8b20-85969dbcbc9e 656edc21-6050-4174-aa2b-d6ea9d1f71de 276d7420-c9c3-43b2-94f1-e26282b9d24c grizzly-http-server-32] ERROR hive.ql.metadata.Table - Unable to get field from serde:....",2016-05-27T05:10:47.406+0000,2016-08-22T08:35:33.657+0000,Fixed,Major
YETUS-1197,NPE in DocletEnvironmentProcessor ,YETUS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12646387'>],"In HBASE-25983 we tried to build javadoc with JDK11 and finally we end with a NPE.

{noformat}
[ERROR] java.lang.NullPointerException
[ERROR] 	at org.apache.yetus.audience.tools.DocletEnvironmentProcessor$DocEnvImpl.excluded(DocletEnvironmentProcessor.java:60)
[ERROR] 	at org.apache.yetus.audience.tools.DocletEnvironmentProcessor$DocEnvImpl.isIncluded(DocletEnvironmentProcessor.java:50)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.toolkit.util.Utils.isIncluded(Utils.java:2685)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.formats.html.HtmlDocletWriter.getModuleLink(HtmlDocletWriter.java:629)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.formats.html.ClassWriterImpl.getHeader(ClassWriterImpl.java:104)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:138)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.toolkit.builders.ClassBuilder.build(ClassBuilder.java:120)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.formats.html.HtmlDoclet.generateClassFiles(HtmlDoclet.java:263)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.toolkit.AbstractDoclet.generateClassFiles(AbstractDoclet.java:273)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.toolkit.AbstractDoclet.startGeneration(AbstractDoclet.java:207)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.doclets.toolkit.AbstractDoclet.run(AbstractDoclet.java:114)
[ERROR] 	at jdk.javadoc/jdk.javadoc.doclet.StandardDoclet.run(StandardDoclet.java:72)
[ERROR] 	at org.apache.yetus.audience.tools.ExcludePrivateAnnotationsStandardDoclet.run(ExcludePrivateAnnotationsStandardDoclet.java:58)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.tool.Start.parseAndExecute(Start.java:588)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.tool.Start.begin(Start.java:432)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.tool.Start.begin(Start.java:345)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.tool.Main.execute(Main.java:63)
[ERROR] 	at jdk.javadoc/jdk.javadoc.internal.tool.Main.main(Main.java:52)
{noformat}

It is this line

https://github.com/apache/yetus/blob/bfc2193567614ca3818dc37705eae09cac281ee9/audience-annotations-component/audience-annotations/src/main/java/org/apache/yetus/audience/tools/DocletEnvironmentProcessor.java#L60

{code}
    private boolean excluded(final Element e) {
      // Exclude private and limited private types
      if (e.getAnnotation(InterfaceAudience.Private.class) != null) { // <==== this line
        return true;
      }
      ...
    }
{code}

Seems the Element is null. I checked the code in OpenJDK

https://github.com/adoptium/jdk11u/blob/faf1f86f95117a5e2a0d52b67647dee893311545/src/jdk.javadoc/share/classes/jdk/javadoc/internal/tool/ElementsTable.java#L328

It does check whether the Element passed in is null, so I think we should also add null check here.",2022-08-24T13:05:40.874+0000,2022-08-29T16:01:40.212+0000,Fixed,Major
SPARK-22575,Making Spark Thrift Server clean up its cache,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12535472'>, <JIRA IssueLink: id='12535470'>]","Currently, Spark Thrift Server accumulates data in its appcache, even for old queries. This fills up the disk (using over 100GB per worker node) within days, and the only way to clear it is to restart the Thrift Server application. Even deleting the files directly isn't a solution, as Spark then complains about FileNotFound.

I asked about this on [Stack Overflow|https://stackoverflow.com/questions/46893123/how-can-i-make-spark-thrift-server-clean-up-its-cache] a few weeks ago, but it does not seem to be currently doable by configuration.

Am I missing some configuration option, or some other factor here?

Otherwise, can anyone point me to the code that handles this, so maybe I can try my hand at a fix?

Thanks!",2017-11-21T16:40:53.237+0000,2021-05-25T01:54:43.740+0000,Incomplete,Minor
IMPALA-462,ALTER DATABASE support to rename or move database,IMPALA,New Feature,Reopened,[],3,"[<JIRA IssueLink: id='12605457'>, <JIRA IssueLink: id='12605456'>, <JIRA IssueLink: id='12603177'>]","I suggest adding an ALTER DATABASE statement, for completeness and future expansion.
Currently, Hive has ALTER DATABASE that AFAICT only allows a SET clause to change properties.

One logical syntax / use case for an Impala ALTER DATABASE would be:

ALTER DATABASE old_name RENAME TO new_name;

(OK to disallow for the DEFAULT database or the currently USEd database.)",2013-07-12T22:36:53.000+0000,2020-12-23T22:27:25.534+0000,,Minor
TEZ-1347,Consolidate MRHelpers,TEZ,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12393864'>, <JIRA IssueLink: id='12393863'>, <JIRA IssueLink: id='12394122'>]","- Remove methods which don't belong in MRHelpers and potentially move them to TezHelpers.
- Get rid of methods which we don't expect/want users to use.
- Get rid of multiple variants of the same method, if these exist.
- Investigate other cleanup in MRHelpers.",2014-07-30T23:52:57.952+0000,2014-09-06T01:35:24.556+0000,Fixed,Blocker
RANGER-1715,Enhance Ranger Hive Plugin to support authorization on Hive replication Tasks,RANGER,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12557964'>, <JIRA IssueLink: id='12526398'>]","Hive has enhanced it authorization for Replication Task https://issues.apache.org/jira/browse/HIVE-17005. The proposal from Ranger side is to have  ""Admin"" permission in RangerHive privilege model and command REPL DUMP and REPL LOAD should be authorized for the users with ""Admin"" privilege on  Database / Table level.
For REPL STATUS command, the user should have SELECT privilege on the Database/ Table Level.",2017-07-28T01:21:37.846+0000,2019-08-05T19:58:15.759+0000,Fixed,Major
FLINK-15534,YARNSessionCapacitySchedulerITCase#perJobYarnClusterWithParallelism failed due to NPE,FLINK,Bug,Closed,[],5,"[<JIRA IssueLink: id='12609258'>, <JIRA IssueLink: id='12589258'>, <JIRA IssueLink: id='12609218'>, <JIRA IssueLink: id='12610251'>, <JIRA IssueLink: id='12579502'>]","As titled, travis run fails with below error:
{code}
07:29:22.417 [ERROR] perJobYarnClusterWithParallelism(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase)  Time elapsed: 16.263 s  <<< ERROR!
java.lang.NullPointerException: 
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)

	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.perJobYarnClusterWithParallelism(YARNSessionCapacitySchedulerITCase.java:405)
Caused by: org.apache.hadoop.ipc.RemoteException: 
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)

	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.perJobYarnClusterWithParallelism(YARNSessionCapacitySchedulerITCase.java:405)
{code}

https://api.travis-ci.org/v3/job/634588108/log.txt",2020-01-09T08:06:08.162+0000,2021-03-10T07:52:56.145+0000,Won't Fix,Blocker
TEZ-3290,Set full task attempt id string in configuration object,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12468455'>],"MRInput sets dag index, vertex index, task index, task attempt index and application id string in configuration object. Constructing full task attempt identifier (applicationId_dagIndex_vertexIndex_taskIndex_taskAttempIndex) from the individual pieces if error prone because of conversion from string to int conversions. Also YARN does not provide an API to construct ApplicationID object from string. It will be much easier if tez MRInputHelpers set the full taskAttemptID string in configuration object.",2016-06-03T21:50:23.535+0000,2016-07-09T19:16:02.918+0000,Fixed,Major
INFRA-12019,Provide information on how to push Docker images for the HBase project,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12468265'>],"Since INFRA-8441, the HBase project now has an actual need to push Docker images created as part of ongoing efforts in HBASE-12721. While I'm not an HBase committer, can details of how to go about pushing images to apache/hbase be sent along, perhaps to [~stack] ?",2016-06-01T23:11:52.145+0000,2017-06-17T07:04:27.869+0000,Fixed,Major
WHIRR-662,remove local TestDFSIO implementation,WHIRR,Bug,Open,[],2,"[<JIRA IssueLink: id='12358036'>, <JIRA IssueLink: id='12358037'>]","Assuming I've got my versions right, that 1.0.3 is after 0.20.2, then MAPREDUCE-1832 is fixed, so after WHIRR-661 is in the local copy of TestDFSIO can be pulled. ",2012-09-20T10:43:21.383+0000,2013-05-02T02:29:56.283+0000,,Minor
CALCITE-2798,"Optimizer should remove ORDER BY in sub-query, provided it has no LIMIT or OFFSET",CALCITE,Improvement,Closed,[],5,"[<JIRA IssueLink: id='12558073'>, <JIRA IssueLink: id='12595169'>, <JIRA IssueLink: id='12577491'>, <JIRA IssueLink: id='12626614'>, <JIRA IssueLink: id='12587609'>]","The following SQL performs sort twice, however inner sort can be eliminated
{code}select * from (
  select * from ""emps"" 
order by ""emps"".""deptno""
) order by 1 desc{code}

The same goes for (window calculation will sort on its own)

{code}select row_number() over (order by ""emps"".""deptno"")  from (
  select * from ""emps"" 
order by ""emps"".""deptno"" desc
){code}

The same goes for SetOp (union, minus):
{code}select * from (
  select * from ""emps"" 
order by ""emps"".""deptno""
) union select * from (
  select * from ""emps"" 
order by ""emps"".""deptno"" desc
){code}

There might be other cases like that (e.g. Aggregate, Join, Exchange, SortExchange)",2019-01-23T10:44:31.124+0000,2021-11-16T11:38:14.087+0000,Fixed,Major
TEZ-1257,Error on empty partition when using OnFileUnorderedKVOutput and ShuffledMergedInput,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12390992'>],"Encountering exception

{code}
org.apache.tez.dag.api.TezUncheckedException: Path component must start with: attempt InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=0], attemptNumber=0, pathComponent=]
        at org.apache.tez.runtime.library.common.InputAttemptIdentifier.<init>(InputAttemptIdentifier.java:45)
        at org.apache.tez.runtime.library.common.InputAttemptIdentifier.<init>(InputAttemptIdentifier.java:51)
        at org.apache.tez.runtime.library.common.shuffle.impl.ShuffleInputEventHandler.processDataMovementEvent(ShuffleInputEventHandler.java:81)
        at org.apache.tez.runtime.library.common.shuffle.impl.ShuffleInputEventHandler.handleEvent(ShuffleInputEventHandler.java:66)
        at org.apache.tez.runtime.library.common.shuffle.impl.ShuffleInputEventHandler.handleEvents(ShuffleInputEventHandler.java:59)
{code}

This is because the pathComponent is not set by UnorderedPartitionedKVWriter for empty partition

{code}
if (emptyPartitions.cardinality() != numPartitions) {
      // Populate payload only if at least 1 partition has data
      payloadBuidler.setHost(host);
      payloadBuidler.setPort(shufflePort);
      payloadBuidler.setPathComponent(outputContext.getUniqueIdentifier());
    }
{code}

The combination of OnFileUnorderedKVOutput and ShuffledMergedInput works fine otherwise if there are no empty partitions.",2014-07-06T19:44:49.098+0000,2014-09-06T01:35:13.515+0000,Fixed,Blocker
ORC-367,Boolean columns are read incorrectly when using seek.,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12534750'>, <JIRA IssueLink: id='12534738'>]","HIVE-19465 is the issue to upgrade Hive to latest 1.5.0.

While upgrading, there are two tests that fail:
{code}
org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testSeek
org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testZeroCopySeek
{code}

Problem seems to be at reading time for boolean type (value always false). Reverting ORC-187 (which was pushed in 1.5.0) from release fixes those tests, hence I am wondering whether this is a bug introduced by that patch or maybe I need to change something in Hive side that I am missing.",2018-05-22T16:27:16.692+0000,2018-05-25T21:41:50.825+0000,Fixed,Major
SLIDER-116,Log handling for long-lived applications,SLIDER,Task,Resolved,[],5,"[<JIRA IssueLink: id='12395690'>, <JIRA IssueLink: id='12389403'>, <JIRA IssueLink: id='12389404'>, <JIRA IssueLink: id='12390319'>, <JIRA IssueLink: id='12390514'>]","Slider needs a solution for handling logs for long lived applications. The log files themselves could be huge and may or may not be ideal for pushing into HDFS. However, that should be an option. Otherwise, application owner should have an option of accessing logs from the local disk on hosts where container is/was active. Can YARN be configured to retain logs on a per application basis? ",2014-06-07T14:25:29.773+0000,2014-09-24T21:01:41.757+0000,Fixed,Major
YETUS-1106,Detect CMake test changes,YETUS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12612089'>],"test4tests uses a regex match to detect if there are any changes under *test* directory, by following the directory structure of a maven project. However, the tests in CMake are conventionally put under a directory named *tests*. This causes the regex match in test4tests to fail and thus results in a -1 overall score in the case of CMake.",2021-04-03T16:18:21.502+0000,2021-08-19T05:48:27.059+0000,Fixed,Critical
CALCITE-948,Indicator columns not preserved by RelFieldTrimmer,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12447814'>, <JIRA IssueLink: id='12447686'>]","CALCITE-828 introduced the usage of RelBuilder in RelFieldTrimmer. In Hive, we are hitting an issue in RelFieldTrimmer with the following query:
{noformat}
SELECT a FROM T1 GROUP BY a GROUPING SETS (a);
{noformat}

The problem is that the {{indicator}} boolean in the Aggregate operator is set to true, as grouping sets are present. In particular, the plan is the following:
{noformat}
HiveAggregate(group=[{0}] indicator=[true])
  HiveProject(a=[$0])
    HiveTableScan(table=[[default.t1]])
{noformat}

However, RelFieldTrimmer will remove the indicator columns, as the number of GroupingSets is <1. This creates a mismatch in the number of columns that results in assertion error. By the definition of the trim function over an Aggregate in RelFieldTrimmer, indicator columns should not be removed.

PS. I tried to reproduce the issue in Calcite using the following query:
{noformat}
select ename, grouping(ename) from emp group by ename, grouping sets (ename)
{noformat}
But I could not, as Calcite infers directly the constant value for the grouping column.",2015-11-03T15:46:10.251+0000,2015-11-10T08:02:37.798+0000,Fixed,Major
OOZIE-1643,Oozie doesn't parse Hadoop Job Id from the Hive action,OOZIE,Bug,Closed,[],1,[<JIRA IssueLink: id='12380262'>],"I'm not sure how long this has been going on (possibly for quite a while), but the Hive action isn't able to parse the Hadoop Job Ids launched by Hive.  

The way its supposed to work is that the {{HiveMain}} creates a {{hive-log4j.properties}} file which redirects the output from {{HiveCLI}} to the console (for easy viewing in the launcher, and creates a {{hive-exec-log4j.properties}} to redirect the output from one of the {{hive-exec}} classes to a log file; Oozie would then parse that log file for the Hadoop Job Ids.  

What's instead happening is that the {{HiveCLI}} is picking up a {{hive-log4j.properties}} file from {{hive-common.jar}} instead.  This is making it log everything to {{stderr}}.  Oozie then can't parse the Hadoop Job Id.

{noformat:title=stdout}
...
<<< Invocation of Hive command completed <<<

 Hadoop Job IDs executed by Hive: 


<<< Invocation of Main class completed <<<


Oozie Launcher, capturing output data:
=======================
#
#Mon Dec 16 16:01:34 PST 2013
hadoopJobs=


=======================
{noformat}

{noformat:title=stderr}
Picked up _JAVA_OPTIONS: -Djava.awt.headless=true
2013-12-16 16:01:20.884 java[59363:1703] Unable to load realm info from SCDynamicStore
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
Logging initialized using configuration in jar:file:/Users/rkanter/dev/hadoop-1.2.0/dirs/mapred/taskTracker/distcache/-4202506229388278450_-1489127056_2111515407/localhost/user/rkanter/share/lib/lib_20131216160106/hive/hive-common-0.10.0.jar!/hive-log4j.properties
Hive history file=/tmp/rkanter/hive_job_log_rkanter_201312161601_851054619.txt
OK
Time taken: 5.444 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201312161418_0008, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201312161418_0008
Kill Command = /Users/rkanter/dev/hadoop-1.2.0/libexec/../bin/hadoop job  -kill job_201312161418_0008
Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
2013-12-16 16:01:33,409 Stage-1 map = 0%,  reduce = 0%
2013-12-16 16:01:34,415 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201312161418_0008
Ended Job = 1084818925, job is filtered out (removed at runtime).
Ended Job = -956386500, job is filtered out (removed at runtime).
Moving data to: hdfs://localhost:8020/tmp/hive-rkanter/hive_2013-12-16_16-01-28_168_4802779111653057155/-ext-10000
Moving data to: /user/rkanter/examples/output-data/hive
MapReduce Jobs Launched: 
Job 0:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 6.284 seconds
Log file: /Users/rkanter/dev/hadoop-1.2.0/dirs/mapred/taskTracker/rkanter/jobcache/job_201312161418_0007/attempt_201312161418_0007_m_000000_0/work/hive-oozie-job_201312161418_0007.log  not present. Therefore no Hadoop jobids found
{noformat}",2013-12-17T00:11:19.920+0000,2014-12-09T00:44:58.892+0000,Fixed,Major
SPARK-21858,Make Spark grouping_id() compatible with Hive grouping__id,SPARK,Bug,Closed,[],2,"[<JIRA IssueLink: id='12513625'>, <JIRA IssueLink: id='12513626'>]","If you want to migrate some ETLs using `grouping__id` in Hive to Spark and use Spark `grouping_id()` instead of Hive `grouping__id`, you will find difference between their evaluations.

Here is an example.
{code:java}
select A, B, grouping__id/grouping_id() from t group by A, B grouping sets((), (A), (B), (A,B))
{code}

Running it on Hive and Spark separately, you'll find this: (the selected attribute in selected grouping set is represented by (/) and  otherwise by (x))
||A B||Binary Expression in Spark||Spark||Hive||Binary Expression in Hive||B A||
|(x) (x)|11|3|0|00|(x) (x)|
|(x) (/)|10|2|2|10|(/) (x)|
|(/) (x)|01|1|1|01|(x) (/)|
|(/) (/)|00|0|3|11|(/) (/)|

As shown above，In Hive, (/) set to 0, (x) set to 1, and in Spark it's opposite.
Moreover, attributes in `group by` will reverse firstly in Hive. In Spark it'll be evaluated directly.

In my opinion, I suggest that modifying the behavior of `grouping_id()` make it compatible with Hive `grouping__id`.
",2017-08-29T04:40:41.098+0000,2019-03-10T08:50:11.897+0000,Not A Problem,Major
CALCITE-1803,Push Project that follows Aggregate down to Druid,CALCITE,New Feature,Closed,[],3,"[<JIRA IssueLink: id='12506164'>, <JIRA IssueLink: id='12504482'>, <JIRA IssueLink: id='12516660'>]","Druid post aggregations are not supported when parsing SQL queries. By implementing post aggregations, we can offload some computation to the druid cluster rather than aggregate on the client side.

Example usage:
{{SELECT SUM(""column1"") - SUM(""column2"") FROM ""table"";}}
This query will be parsed into two separate Druid aggregations according to current rules. Then the results will be subtracted in Calcite. By using the {{postAggregations}} field in the druid query, the subtraction could be done in Druid cluster. Although the previous example is simple, the difference will be obvious when the number of result rows are large. (Multiple rows result will happen when group by is used).
Questions:
After I push Post aggregation into Druid query, what should I change on the project relational correlation? In the case of the example above, the {{BindableProject}} will have the expression to representation the subtraction. If I push the post aggregation into druid query, the expression of subtraction should be replaced by the representation of the post aggregations result. For now, the project expression seems can only point to the aggregations results. Since post aggregations have to point to aggregations results too, it could not be placed in the parallel level as aggregation. Where should I put post aggregations?",2017-05-23T00:07:46.573+0000,2017-10-05T22:38:01.168+0000,Fixed,Major
PHOENIX-4457,Account for the Table interface addition of checkAndMutate,PHOENIX,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12522201'>],"HBASE-19213 added a new method to Table:

{code}
+  CheckAndMutateBuilder checkAndMutate(byte[] row, byte[] family);
{code}

Need to account for this in our Table implementations.",2017-12-13T17:11:38.750+0000,2018-07-26T01:13:33.824+0000,Fixed,Blocker
PHOENIX-4053,Lock row exclusively when necessary for mutable secondary indexing,PHOENIX,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12510628'>, <JIRA IssueLink: id='12510631'>, <JIRA IssueLink: id='12510629'>]","From HBase 1.2 on, rows are not exclusively locked when the preBatchMutate call is made (see HBASE-18474). The mutable secondary index (global and local) depend on this to get a consistent snapshot of a row between the point when the current row value is looked up, and when the new row is written, until the mvcc is advanced. Otherwise, a subsequent update to a row may not see the current row state. Even with pre HBase 1.2 releases, the lock isn't held long enough for us. We need to hold the locks from the start of the preBatchMutate (when we read the data table to get the prior row values) until the mvcc is advanced (beginning of postBatchMutateIndispensably).

Given the above, it's best if Phoenix manages the row locking itself (mimicing the current HBase mechanism).",2017-07-28T20:41:37.418+0000,2017-10-11T22:36:04.444+0000,Fixed,Major
AVRO-499,reflection does not handle interfaces with overloaded method names,AVRO,Bug,Closed,[],2,"[<JIRA IssueLink: id='12331172'>, <JIRA IssueLink: id='12342686'>]","Avro protocols only permit a single message with a given name, while Java interfaces may overload a single method name with several signatures.  The reflect implementation currently does not address this.  Reflection of an interface with overloaded method names randomly selects one of these and ignores the others.",2010-04-02T21:40:10.455+0000,2011-08-30T19:22:18.575+0000,Fixed,Major
YETUS-139,plugins for project specific infra-error handling,YETUS,New Feature,Open,[],1,[<JIRA IssueLink: id='12447222'>],"Hadoop and HBase both have some project specific infra failures they check for. It would be nice ot have a generalized approach so others can leverage the same logic before realizing they need it.

* Hadoop Bad Perms - some hadoop tests change permissions in the target/ dir to simulate disk failures. These changes then wedge Jenkins because neither git nor rm can delete the files. Essentially, we'd need to walk the filesystem after test to ensure all directories are executable.
* HBase Zombies - HBase has some tests that historically crashed in a way that left a process around, eventually bringing down Jenkins worker hosts. So they have some code for finding them to report and clean up.",2015-10-27T21:19:56.566+0000,2015-10-27T21:23:22.066+0000,,Major
HCATALOG-425,Pig cannot read/write SMALLINT/TINYINT columns,HCATALOG,Bug,Closed,[],4,"[<JIRA IssueLink: id='12357372'>, <JIRA IssueLink: id='12353391'>, <JIRA IssueLink: id='12368953'>, <JIRA IssueLink: id='12356718'>]","Currently throw exception. We can always allow read and on write side, we can do out of boundary check at runtime.
This issue described in  HCATALOG-168, has not been fixed. It still throws an exception.
",2012-06-09T01:55:50.625+0000,2013-05-13T21:19:26.507+0000,Fixed,Major
SPARK-20187,Replace loadTable with moveFile to speed up load table for many output files,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12500049'>],"[HiveClientImpl.loadTable|https://github.com/apache/spark/blob/v2.1.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala#L667] load files one by one, so this step will take a long time if a job generates many files. There is a [Hive.moveFile api|https://github.com/apache/hive/blob/release-1.2.1/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2567] can speed up this step for {{create table tableName as select ...}} and {{insert overwrite table tableName select ...}}

Here are two APIs comparison:
{noformat:align=left|title=loadTable api: It took about 26 minutes(10:50:14 - 11:16:18) to load table}
17/04/01 10:50:04 INFO TaskSetManager: Finished task 207165.0 in stage 0.0 (TID 216796) in 5952 ms on jqhadoop-test28-8.int.yihaodian.com (executor 54) (216869/216869)
17/04/01 10:50:04 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/01 10:50:04 INFO DAGScheduler: ResultStage 0 (processCmd at CliDriver.java:376) finished in 541.797 s
17/04/01 10:50:04 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:376, took 551.208919 s
17/04/01 10:50:04 INFO FileFormatWriter: Job null committed.
17/04/01 10:50:14 INFO Hive: Replacing src:viewfs://cluster4/user/hive/warehouse/staging/.hive-staging_hive_2017-04-01_10-40-02_349_8047899863313770218-1/-ext-10000/part-00000-9335c5f3-60fa-418b-a466-2d76a5e84537-c000, dest: viewfs://cluster4/user/hive/warehouse/tmp.db/spark_load_slow/part-00000-9335c5f3-60fa-418b-a466-2d76a5e84537-c000, Status:true
17/04/01 10:50:14 INFO Hive: Replacing src:viewfs://cluster4/user/hive/warehouse/staging/.hive-staging_hive_2017-04-01_10-40-02_349_8047899863313770218-1/-ext-10000/part-00001-9335c5f3-60fa-418b-a466-2d76a5e84537-c000, dest: viewfs://cluster4/user/hive/warehouse/tmp.db/spark_load_slow/part-00001-9335c5f3-60fa-418b-a466-2d76a5e84537-c000, Status:true

...

17/04/01 11:16:11 INFO Hive: Replacing src:viewfs://cluster4/user/hive/warehouse/staging/.hive-staging_hive_2017-04-01_10-40-02_349_8047899863313770218-1/-ext-10000/part-99999-9335c5f3-60fa-418b-a466-2d76a5e84537-c000, dest: viewfs://cluster4/user/hive/warehouse/tmp.db/spark_load_slow/part-99999-9335c5f3-60fa-418b-a466-2d76a5e84537-c000, Status:true
17/04/01 11:16:18 INFO SparkSqlParser: Parsing command: `tmp`.`spark_load_slow`
17/04/01 11:16:18 INFO CatalystSqlParser: Parsing command: string
17/04/01 11:16:18 INFO CatalystSqlParser: Parsing command: string
17/04/01 11:16:18 INFO CatalystSqlParser: Parsing command: string
17/04/01 11:16:18 INFO CatalystSqlParser: Parsing command: string
17/04/01 11:16:18 INFO CatalystSqlParser: Parsing command: string
Time taken: 2178.736 seconds
17/04/01 11:16:18 INFO CliDriver: Time taken: 2178.736 seconds
{noformat}

{noformat:align=left|title=moveFile api: It took about 9 minutes(13:24:39 - 13:33:46) to load table|align=right}
17/04/01 13:24:38 INFO TaskSetManager: Finished task 210610.0 in stage 0.0 (TID 216829) in 5888 ms on jqhadoop-test28-28.int.yihaodian.com (executor 59) (216869/216869)
17/04/01 13:24:38 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/01 13:24:38 INFO DAGScheduler: ResultStage 0 (processCmd at CliDriver.java:376) finished in 532.409 s
17/04/01 13:24:38 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:376, took 539.337610 s
17/04/01 13:24:39 INFO FileFormatWriter: Job null committed.
17/04/01 13:24:39 INFO Hive: Replacing src:viewfs://cluster4/user/hive/warehouse/staging/.hive-staging_hive_2017-04-01_13-14-46_099_8962745596360417817-1/-ext-10000, dest: viewfs://cluster4/user/hive/warehouse/tmp.db/spark_load_slow_movefile, Status:true
17/04/01 13:33:46 INFO SparkSqlParser: Parsing command: `tmp`.`spark_load_slow_movefile`
17/04/01 13:33:46 INFO CatalystSqlParser: Parsing command: string
17/04/01 13:33:46 INFO CatalystSqlParser: Parsing command: string
17/04/01 13:33:46 INFO CatalystSqlParser: Parsing command: string
17/04/01 13:33:46 INFO CatalystSqlParser: Parsing command: string
17/04/01 13:33:46 INFO CatalystSqlParser: Parsing command: string
Time taken: 1142.671 seconds
17/04/01 13:33:46 INFO CliDriver: Time taken: 1142.671 seconds
{noformat}

More log can be find in attachments.",2017-04-01T10:07:25.037+0000,2017-04-06T06:35:48.890+0000,Duplicate,Major
HCATALOG-132,Add HCatalog to Maven Repository,HCATALOG,New Feature,Closed,[],8,"[<JIRA IssueLink: id='12355218'>, <JIRA IssueLink: id='12355805'>, <JIRA IssueLink: id='12353808'>, <JIRA IssueLink: id='12353807'>, <JIRA IssueLink: id='12358077'>, <JIRA IssueLink: id='12358186'>, <JIRA IssueLink: id='12345936'>, <JIRA IssueLink: id='12349816'>]",HCat should be in maven main repositories,2011-10-10T23:15:30.321+0000,2013-02-15T21:32:42.917+0000,Fixed,Major
CALCITE-3848,Rewriting for materialized view consisting of group by on join keys fails with Mappings$NoElementException,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12582749'>],"Test case
{code:java}
+  @Test public void testAggregateOnJoinKeys() {
+    checkMaterialize(
+        ""select \""deptno\"", \""empid\"", \""salary\""\n""
+            + ""from \""emps\""\n""
+            + ""group by \""deptno\"", \""empid\"", \""salary\"""",
+     ""select \""empid\"", \""depts\"".\""deptno\"" \n""
+        + ""from \""emps\""\n""
+        + ""join \""depts\"" on \""depts\"".\""deptno\"" = \""empid\"" group by \""empid\"", \""depts\"".\""deptno\"""",
+        HR_FKUK_MODEL,
+        CalciteAssert.checkResultContains(
+            ""EnumerableCalc(expr#0=[{inputs}], empid=[$t0], empid0=[$t0])\n""
+              + ""  EnumerableAggregate(group=[{1}])\n""
+                + ""    EnumerableHashJoin(condition=[=($1, $3)], joinType=[inner])\n""
+                + ""      EnumerableTableScan(table=[[hr, m0]])""));
+  }
+
{code}

Error:
{code}
Caused by: java.lang.RuntimeException: Error while applying rule MaterializedViewAggregateRule(Aggregate), args [rel#64476:EnumerableAggregate.ENUMERABLE.[](input=RelSubset#64475,group={0, 1})]
                at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:260)
                at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:634)
                at org.apache.calcite.tools.Programs.lambda$standard$3(Programs.java:286)
                at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:346)
                at org.apache.calcite.prepare.Prepare.optimize(Prepare.java:165)
                at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:290)
                at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:207)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:634)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:498)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:468)
                at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:231)
                at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:550)
                at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:675)
                at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)
                ... 16 more
            Next exception 1: [CIRCULAR REFERENCE SQLException]
            Next exception 2: [CIRCULAR REFERENCE RuntimeException]
            Next exception 3: org.apache.calcite.util.mapping.Mappings$NoElementException: source #0 has no target in mapping [size=1, sourceCount=2, targetCount=7, elements=[1:1]]
                at org.apache.calcite.util.mapping.Mappings$AbstractMapping.getTarget(Mappings.java:881)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewAggregateRule.rewriteView(MaterializedViewAggregateRule.java:677)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewRule.perform(MaterializedViewRule.java:485)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewOnlyAggregateRule.onMatch(MaterializedViewOnlyAggregateRule.java:63)
                at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:233)
                at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:634)
                at org.apache.calcite.tools.Programs.lambda$standard$3(Programs.java:286)
                at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:346)
                at org.apache.calcite.prepare.Prepare.optimize(Prepare.java:165)
                at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:290)
                at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:207)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:634)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:498)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:468)
                at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:231)
                at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:550)
                at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:675)
                at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)
                ... 16 more
                Caused by: [CIRCULAR REFERENCE PlaceholderException]
{code}",2020-03-09T22:28:07.217+0000,2020-05-24T12:00:30.813+0000,Fixed,Major
SPARK-5493,Support proxy users under kerberos,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12408028'>, <JIRA IssueLink: id='12490717'>]","When using kerberos, services may want to use spark-submit to submit jobs as a separate user. For example a service like hive might want to submit jobs as a client user.",2015-01-30T03:39:21.857+0000,2017-01-10T15:56:34.080+0000,Fixed,Major
AVRO-1402,Support for DECIMAL type,AVRO,New Feature,Closed,[],4,"[<JIRA IssueLink: id='12387641'>, <JIRA IssueLink: id='12378505'>, <JIRA IssueLink: id='12386675'>, <JIRA IssueLink: id='12478190'>]","Currently, Avro does not seem to support a DECIMAL type or equivalent.

http://avro.apache.org/docs/1.7.5/spec.html#schema_primitive

Adding DECIMAL support would be particularly interesting when converting types from Avro to Hive, since DECIMAL is already a supported data type in Hive (0.11.0).",2013-11-13T15:00:42.852+0000,2016-08-22T18:01:33.235+0000,Fixed,Minor
SLIDER-191,"if/when YARN publishes information about itself, GET it and use it for diagnostics and AM requests",SLIDER,Improvement,Open,[],1,[<JIRA IssueLink: id='12390672'>],"YARN-1565 proposes a REST API to query some of the YARN/cluster state -this can be used to build up the request for the AM, upload files, etc.

Once implemented, we should use it for any {{slider diagnostics}} command as well as for building up the AM launch",2014-07-01T09:52:03.932+0000,2014-10-02T18:38:36.320+0000,,Major
SPARK-30755,Update migration guide and add actionable exception for HIVE-15167,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12579945'>],"{noformat}
2020-01-27 05:11:20.446 - stderr> 20/01/27 05:11:20 INFO DAGScheduler: ResultStage 2 (main at NativeMethodAccessorImpl.java:0) failed in 1.000 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 13, 10.110.21.210, executor 1): java.lang.NoClassDefFoundError: org/apache/hadoop/hive/serde2/SerDe
  2020-01-27 05:11:20.446 - stderr>  at java.lang.ClassLoader.defineClass1(Native Method)
  2020-01-27 05:11:20.446 - stderr>  at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
  2020-01-27 05:11:20.446 - stderr>  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
  2020-01-27 05:11:20.446 - stderr>  at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
  2020-01-27 05:11:20.446 - stderr>  at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
  2020-01-27 05:11:20.446 - stderr>  at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
  2020-01-27 05:11:20.446 - stderr>  at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
  2020-01-27 05:11:20.446 - stderr>  at java.security.AccessController.doPrivileged(Native Method)
  2020-01-27 05:11:20.446 - stderr>  at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
  2020-01-27 05:11:20.446 - stderr>  at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  2020-01-27 05:11:20.446 - stderr>  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  2020-01-27 05:11:20.446 - stderr>  at java.lang.ClassLoader.loadClass(ClassLoader.java:405)
  2020-01-27 05:11:20.446 - stderr>  at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  2020-01-27 05:11:20.446 - stderr>  at java.lang.Class.forName0(Native Method)
  2020-01-27 05:11:20.446 - stderr>  at java.lang.Class.forName(Class.java:348)
  2020-01-27 05:11:20.446 - stderr>  at org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializerClass(TableDesc.java:76)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:119)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:104)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:267)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:208)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.scheduler.Task.run(Task.scala:117)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$6(Executor.scala:567)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1559)
  2020-01-27 05:11:20.447 - stderr>  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:570)
  2020-01-27 05:11:20.447 - stderr>  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  2020-01-27 05:11:20.447 - stderr>  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  2020-01-27 05:11:20.447 - stderr>  at java.lang.Thread.run(Thread.java:748)
  2020-01-27 05:11:20.447 - stderr> Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.serde2.SerDe
  2020-01-27 05:11:20.447 - stderr>  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  2020-01-27 05:11:20.447 - stderr>  at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  2020-01-27 05:11:20.447 - stderr>  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  2020-01-27 05:11:20.447 - stderr>  at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  2020-01-27 05:11:20.447 - stderr>  ... 31 more
{noformat}
",2020-02-07T11:00:45.552+0000,2020-02-17T17:28:01.532+0000,Fixed,Blocker
ZOOKEEPER-1364,Add orthogonal fault injection mechanism/framework,ZOOKEEPER,Test,Open,[],3,"[<JIRA IssueLink: id='12577079'>, <JIRA IssueLink: id='12573199'>, <JIRA IssueLink: id='12347137'>]",Hadoop has a mechanism for doing fault injection (HDFS-435). I think it would be useful if something similar would be available for ZooKeeper. ,2012-01-17T15:56:10.614+0000,2020-03-29T18:34:28.763+0000,,Major
TEZ-1128,OnFileUnorderedPartitionedKVOutput does not handle partitioning correctly with the MRPartitioner,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12388292'>],"Two issues:
   1) It does not set value of TezJobConfig.TEZ_RUNTIME_NUM_EXPECTED_PARTITIONS. So MRPartitioner assumes the number of partitions is 1 and goes with a default partitioner that writes only to last partition. 
  2) If one of the partitions has empty output (due to issue 1) it gets stuck without fetching input.",2014-05-18T23:39:51.975+0000,2014-09-06T01:35:40.133+0000,Fixed,Major
CALCITE-1579,Druid adapter: wrong semantics of groupBy query limit with granularity,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12491292'>],"Similar to CALCITE-1578, but for GroupBy queries. Limit is applied per granularity unit, not globally for the query.

Currently, the following SQL query infers granularity 'day' for Druid _groupBy_ and pushes the limit, which is incorrect.
{code:sql}
SELECT i_brand_id, floor_day(`__time`), max(ss_quantity), sum(ss_wholesale_cost) as s
FROM store_sales_sold_time_subset
GROUP BY i_brand_id, floor_day(`__time`)
ORDER BY s
LIMIT 10;
{code}",2017-01-16T15:39:29.348+0000,2017-03-24T03:20:12.102+0000,Fixed,Critical
BIGTOP-3314,Fix flink build failure with upgraded hadoop version,BIGTOP,Bug,Closed,[],1,[<JIRA IssueLink: id='12581403'>],"Flink 1.6.4 failed to build with new hadoop 2.10.0.

Error log as following:
{code:java}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /bigtop/build/flink/rpm/BUILD/flink-1.6.4/flink-yarn/src/test/java/org/apache/flink/yarn/AbstractYarnClusterTest.java:[89,41] no suitable method found for newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,<nulltype>,org.apache.hadoop.yarn.api.records.YarnApplicationState,<nulltype>,<nulltype>,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,<nulltype>,<nulltype>,float,<nulltype>,<nulltype>)
    method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token) is not applicable
      (actual and formal argument lists differ in length)
    method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token) is not applicable
      (actual and formal argument lists differ in length)
    method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token,java.util.Set<java.lang.String>,boolean,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token,java.util.Set<java.lang.String>,boolean,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token,java.util.Set<java.lang.String>,boolean,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
{code}",2020-02-20T13:56:22.388+0000,2020-02-28T05:05:52.523+0000,Fixed,Major
BIGTOP-1303,Pig 0.12.1 build is broken at the site target. Stack build can't proceed,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12387740'>, <JIRA IssueLink: id='12387757'>]",The site build doesn't go through. See the comment for the detailed message.,2014-05-08T18:08:13.379+0000,2016-04-08T14:10:58.932+0000,Cannot Reproduce,Blocker
TEZ-4230,"LocalContainerLauncher can kill task future too early, causing app hang",TEZ,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12598081'>, <JIRA IssueLink: id='12598250'>]","Reproduced issue in ptest run which I made to run against tez staging artifacts (https://repository.apache.org/content/repositories/orgapachetez-1068/)
http://ci.hive.apache.org/blue/organizations/jenkins/hive-precommit/detail/PR-1311/14/pipeline/417

I'm about to investigate this. I think Tez 0.10.0 cannot be released until we won't confirm if it's a hive or tez bug.

{code}
mvn test -Pitests,hadoop-2 -Dtest=TestMmCompactorOnTez -pl ./itests/hive-unit
{code}

tez setup:
https://github.com/apache/hive/commit/92516631ab39f39df5d0692f98ac32c2cd320997#diff-a22bcc9ba13b310c7abfee4a57c4b130R83-R97",2020-09-02T11:18:14.100+0000,2020-09-09T07:29:27.682+0000,Fixed,Major
GIRAPH-343,Use published hcatalog jars.,GIRAPH,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12358191'>, <JIRA IssueLink: id='12358187'>, <JIRA IssueLink: id='12358186'>, <JIRA IssueLink: id='12358236'>]",Shouldn't need special profile now that hcatalog has started pushing their jars to maven.,2012-09-25T15:58:15.709+0000,2012-09-27T08:32:38.826+0000,Fixed,Major
BEAM-10464,[HBaseIO] - Protocol message was too large.  May be malicious.,BEAM,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12607500'>],"Hi! I just got the following error perfoming a HBaseIO.read() from scan. 
{code:java}
Caused by: org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit. at org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110) at org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755) at org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701) at org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result.<init>(ClientProtos.java:4694) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result.<init>(ClientProtos.java:4658) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result$1.parsePartialFrom(ClientProtos.java:4767) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result$1.parsePartialFrom(ClientProtos.java:4762) at org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200) at org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser.parsePartialDelimitedFrom(AbstractParser.java:241) at org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser.parseDelimitedFrom(AbstractParser.java:253) at org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser.parseDelimitedFrom(AbstractParser.java:259) at org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser.parseDelimitedFrom(AbstractParser.java:49) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Result.parseDelimitedFrom(ClientProtos.java:5131) at org.apache.beam.sdk.io.hbase.HBaseResultCoder.decode(HBaseResultCoder.java:50) at org.apache.beam.sdk.io.hbase.HBaseResultCoder.decode(HBaseResultCoder.java:34) at org.apache.beam.sdk.coders.Coder.decode(Coder.java:159) at org.apache.beam.sdk.util.WindowedValue$FullWindowedValueCoder.decode(WindowedValue.java:602) at org.apache.beam.sdk.util.WindowedValue$FullWindowedValueCoder.decode(WindowedValue.java:593) at org.apache.beam.sdk.util.WindowedValue$FullWindowedValueCoder.decode(WindowedValue.java:539) at org.apache.beam.runners.spark.translation.ValueAndCoderLazySerializable.getOrDecode(ValueAndCoderLazySerializable.java:73) ... 61 more
{code}
It seems I'm scanning a family column bigger than 64MB, but HBaseIO doesn't provide any workaround to change the current sizeLimit of the protobuf decoder. How should we manage Big Data Datasets stored in HBase?",2020-07-13T09:09:38.074+0000,2021-01-28T12:40:01.794+0000,Workaround,P2
TEZ-4339,Expose real-time memory consumption of AM and task containers via DagClient,TEZ,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12627832'>, <JIRA IssueLink: id='12625683'>]","The idea is to simply expose how much memory do all the containers (that belong to the current DAG) consume. This info could be very useful for developers, or anyone who would want to monitor their applications.
I'm not yet sure how wasteful this addition would be in terms of client <-> AM communication, thinking about turning this feature off by default.",2021-10-17T15:31:48.939+0000,2021-12-03T13:48:16.458+0000,Fixed,Major
FLUME-1734,Create a Hive Sink based on the new Hive Streaming support,FLUME,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12392358'>, <JIRA IssueLink: id='12383099'>]","Hive 0.13 has introduced Streaming support which is itself transactional in nature and fits well with Flume's transaction model.
Short overview of  Hive's  Streaming support on which this sink is based can be found here:
http://hive.apache.org/javadocs/r0.13.1/api/hcatalog/streaming/index.html


This jira is for creating a sink that would continuously stream data into Hive tables using the above APIs. The primary goal being that the data streamed by the sink should be instantly queryable (using say Hive or Pig) without requiring additional post-processing steps on behalf of the users. Sink should manage the creation of new partitions periodically if needed.


",2012-11-20T20:07:33.238+0000,2015-04-17T00:19:45.123+0000,Fixed,Major
NIFI-2575,HiveQL Processors Fail due to invalid JDBC URI resolution when using Zookeeper URI,NIFI,Bug,Open,[],4,"[<JIRA IssueLink: id='12477641'>, <JIRA IssueLink: id='12530225'>, <JIRA IssueLink: id='12525036'>, <JIRA IssueLink: id='12565979'>]","When configuring a HiveQL processor using the Zookeeper URL (e.g. jdbc:hive2://ydavis-hdp-nifi-test-3.openstacklocal:2181,ydavis-hdp-nifi-test-1.openstacklocal:2181,ydavis-hdp-nifi-test-2.openstacklocal:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2), it appears that the JDBC driver does not properly build the the uri in the expected format.  This is because HS2 is storing JDBC parameters in ZK (https://issues.apache.org/jira/browse/HIVE-11581) and it is expecting the driver to be able to parse and use those values to configure the connection. However it appears the driver is expecting zookeeper to simply return the host:port and subsequently building an invalid URI.

This problem has result in two variation of errors. The following was experienced by [~mattyb149]

{noformat}
2016-08-15 12:45:12,918 INFO [Timer-Driven Process Thread-2] org.apache.hive.jdbc.Utils Resolved authority: hive.server2.authentication=KERBEROS;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=hdp-cluster-2-2.novalocal;hive.server2.thrift.port=10000;hive.server2.use.SSL=false;hive.server2.authentication.kerberos.principal=hive/_HOST@HDF.COM
2016-08-15 12:45:13,835 INFO [Timer-Driven Process Thread-2] org.apache.hive.jdbc.HiveConnection Will try to open client transport with JDBC Uri: jdbc:hive2://hive.server2.authentication=KERBEROS;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=hdp-cluster-2-2.novalocal;hive.server2.thrift.port=10000;hive.server2.use.SSL=false;hive.server2.authentication.kerberos.principal=hive/_HOST@HDF.COM/default;principal=hive/_HOST@HDF.COM;serviceDiscoveryMode=zookeeper;zooKeeperNamespace=hiveserver2
2016-08-15 12:45:13,835 INFO [Timer-Driven Process Thread-2] org.apache.hive.jdbc.HiveConnection Could not open client transport with JDBC Uri: jdbc:hive2://hive.server2.authentication=KERBEROS;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=hdp-cluster-2-2.novalocal;hive.server2.thrift.port=10000;hive.server2.use.SSL=false;hive.server2.authentication.kerberos.principal=hive/_HOST@HDF.COM/default;principal=hive/_HOST@HDF.COM;serviceDiscoveryMode=zookeeper;zooKeeperNamespace=hiveserver2
2016-08-15 12:45:13,836 INFO [Timer-Driven Process Thread-2] o.a.c.f.imps.CuratorFrameworkImpl Starting
2016-08-15 12:45:14,064 INFO [Timer-Driven Process Thread-2-EventThread] o.a.c.f.state.ConnectionStateManager State change: CONNECTED
2016-08-15 12:45:14,182 INFO [Curator-Framework-0] o.a.c.f.imps.CuratorFrameworkImpl backgroundOperationsLoop exiting
2016-08-15 12:45:14,337 ERROR [Timer-Driven Process Thread-2] o.a.nifi.processors.hive.SelectHiveQL SelectHiveQL[id=7aaffd71-0156-1000-d962-8102c06b23df] SelectHiveQL[id=7aaffd71-0156-1000-d962-8102c06b23df] failed to process due to java.lang.reflect.UndeclaredThrowableException; rolling back session: java.lang.reflect.UndeclaredThrowableException
2016-08-15 12:45:14,346 ERROR [Timer-Driven Process Thread-2] o.a.nifi.processors.hive.SelectHiveQL
java.lang.reflect.UndeclaredThrowableException: null
       	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) ~[na:na]
       	at org.apache.nifi.dbcp.hive.HiveConnectionPool.getConnection(HiveConnectionPool.java:255) ~[na:na]
       	at sun.reflect.GeneratedMethodAccessor331.invoke(Unknown Source) ~[na:na]
       	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_65]
       	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_65]
       	at org.apache.nifi.controller.service.StandardControllerServiceProvider$1.invoke(StandardControllerServiceProvider.java:174) ~[nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
       	at com.sun.proxy.$Proxy81.getConnection(Unknown Source) ~[na:na]
       	at org.apache.nifi.processors.hive.SelectHiveQL.onTrigger(SelectHiveQL.java:158) ~[na:na]
       	at org.apache.nifi.processor.AbstractProcessor.onTrigger(AbstractProcessor.java:27) ~[nifi-api-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
       	at org.apache.nifi.controller.StandardProcessorNode.onTrigger(StandardProcessorNode.java:1060) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
       	at org.apache.nifi.controller.tasks.ContinuallyRunProcessorTask.call(ContinuallyRunProcessorTask.java:136) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
       	at org.apache.nifi.controller.tasks.ContinuallyRunProcessorTask.call(ContinuallyRunProcessorTask.java:47) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
       	at org.apache.nifi.controller.scheduling.TimerDrivenSchedulingAgent$1.run(TimerDrivenSchedulingAgent.java:127) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
       	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_65]
       	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_65]
       	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_65]
       	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_65]
       	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_65]
       	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_65]
       	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_65]
Caused by: org.apache.commons.dbcp.SQLNestedException: Cannot create PoolableConnectionFactory (Could not open client transport for any of the Server URI's in ZooKeeper: Unable to read HiveServer2 uri from ZooKeeper)
       	at org.apache.commons.dbcp.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:1549) ~[na:na]

{noformat}

The following error was experienced by [~YolandaMDavis]

{noformat}
2016-08-15 19:22:27,338 INFO [Timer-Driven Process Thread-7] org.apache.hive.jdbc.Utils Supplied authorities: ydavis-hdp-nifi-test-3.openstacklocal:2181,ydavis-hdp-nifi-test-1.openstacklocal:2181,ydavis-hdp-nifi-test-2.openstacklocal:2181
2016-08-15 19:22:27,340 INFO [Timer-Driven Process Thread-7] o.a.c.f.imps.CuratorFrameworkImpl Starting
2016-08-15 19:22:27,360 INFO [Timer-Driven Process Thread-7-EventThread] o.a.c.f.state.ConnectionStateManager State change: CONNECTED
2016-08-15 19:22:27,365 INFO [Timer-Driven Process Thread-7] o.a.hive.jdbc.ZooKeeperHiveClientHelper Selected HiveServer2 instance with uri: hive.server2.authentication=NONE;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=ydavis-hdp-nifi-test-2.openstacklocal;hive.server2.thrift.port=10000;hive.server2.use.SSL=false
2016-08-15 19:22:27,365 INFO [Curator-Framework-0] o.a.c.f.imps.CuratorFrameworkImpl backgroundOperationsLoop exiting
2016-08-15 19:22:27,371 INFO [Timer-Driven Process Thread-7] org.apache.hive.jdbc.Utils Resolved authority: hive.server2.authentication=NONE;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=ydavis-hdp-nifi-test-2.openstacklocal;hive.server2.thrift.port=10000;hive.server2.use.SSL=false
2016-08-15 19:22:27,374 INFO [Timer-Driven Process Thread-7] org.apache.hive.jdbc.HiveConnection Will try to open client transport with JDBC Uri: jdbc:hive2://hive.server2.authentication=NONE;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=ydavis-hdp-nifi-test-2.openstacklocal;hive.server2.thrift.port=10000;hive.server2.use.SSL=false/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
2016-08-15 19:22:27,374 ERROR [Timer-Driven Process Thread-7] o.apache.nifi.processors.hive.PutHiveQL PutHiveQL[id=74c17a11-0156-1000-5d7e-40ed6aa18cbb] PutHiveQL[id=74c17a11-0156-1000-5d7e-40ed6aa18cbb] failed to process due to java.lang.NullPointerException; rolling back session: java.lang.NullPointerException
2016-08-15 19:22:27,383 ERROR [Timer-Driven Process Thread-7] o.apache.nifi.processors.hive.PutHiveQL 
java.lang.NullPointerException: null
	at org.apache.thrift.transport.TSocket.open(TSocket.java:170) ~[na:na]
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:266) ~[na:na]
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) ~[na:na]
	at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:204) ~[na:na]
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:176) ~[na:na]
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) ~[na:na]
	at org.apache.commons.dbcp.DriverConnectionFactory.createConnection(DriverConnectionFactory.java:38) ~[na:na]
	at org.apache.commons.dbcp.PoolableConnectionFactory.makeObject(PoolableConnectionFactory.java:582) ~[na:na]
	at org.apache.commons.dbcp.BasicDataSource.validateConnectionFactory(BasicDataSource.java:1556) ~[na:na]
	at org.apache.commons.dbcp.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:1545) ~[na:na]
	at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1388) ~[na:na]
	at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044) ~[na:na]
	at org.apache.nifi.dbcp.hive.HiveConnectionPool.getConnection(HiveConnectionPool.java:264) ~[na:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_101]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_101]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_101]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_101]
	at org.apache.nifi.controller.service.StandardControllerServiceProvider$1.invoke(StandardControllerServiceProvider.java:174) ~[nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at com.sun.proxy.$Proxy120.getConnection(Unknown Source) ~[na:na]
	at org.apache.nifi.processors.hive.PutHiveQL.onTrigger(PutHiveQL.java:152) ~[na:na]
	at org.apache.nifi.processor.AbstractProcessor.onTrigger(AbstractProcessor.java:27) ~[nifi-api-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.nifi.controller.StandardProcessorNode.onTrigger(StandardProcessorNode.java:1060) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.nifi.controller.tasks.ContinuallyRunProcessorTask.call(ContinuallyRunProcessorTask.java:136) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.nifi.controller.tasks.ContinuallyRunProcessorTask.call(ContinuallyRunProcessorTask.java:47) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.nifi.controller.scheduling.TimerDrivenSchedulingAgent$1.run(TimerDrivenSchedulingAgent.java:127) [nifi-framework-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_101]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_101]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
2016-08-15 19:22:27,384 ERROR [Timer-Driven Process Thread-7] o.apache.nifi.processors.hive.PutHiveQL PutHiveQL[id=74c17a11-0156-1000-5d7e-40ed6aa18cbb] PutHiveQL[id=74c17a11-0156-1000-5d7e-40ed6aa18cbb] failed to process session due to java.lang.NullPointerException: java.lang.NullPointerException

{noformat}

This problem appears to be resolved in the Apache 1.3 release which is not yet available.",2016-08-15T20:28:40.311+0000,2019-07-19T18:11:10.881+0000,,Major
SPARK-24374,SPIP: Support Barrier Execution Mode in Apache Spark,SPARK,Epic,Resolved,[],3,"[<JIRA IssueLink: id='12535735'>, <JIRA IssueLink: id='12535738'>, <JIRA IssueLink: id='12535736'>]","(See details in the linked/attached SPIP doc.)

{quote}

The proposal here is to add a new scheduling model to Apache Spark so users can properly embed distributed DL training as a Spark stage to simplify the distributed training workflow. For example, Horovod uses MPI to implement all-reduce to accelerate distributed TensorFlow training. The computation model is different from MapReduce used by Spark. In Spark, a task in a stage doesn’t depend on any other tasks in the same stage, and hence it can be scheduled independently. In MPI, all workers start at the same time and pass messages around. To embed this workload in Spark, we need to introduce a new scheduling model, tentatively named “barrier scheduling”, which launches tasks at the same time and provides users enough information and tooling to embed distributed DL training. Spark can also provide an extra layer of fault tolerance in case some tasks failed in the middle, where Spark would abort all tasks and restart the stage.

{quote}",2018-05-24T04:48:11.504+0000,2021-03-19T17:37:05.825+0000,Fixed,Major
BOOKKEEPER-355,"Ledger recovery will mark ledger as closed with -1, in case of slow bookie is added to ensemble during  recovery add",BOOKKEEPER,Bug,Closed,[],3,"[<JIRA IssueLink: id='12361840'>, <JIRA IssueLink: id='12362150'>, <JIRA IssueLink: id='12356830'>]","Scenario:
------------
1. Ledger is created with ensemble and quorum size as 2, written with one entry
2. Now first bookie is in the ensemble is made down.
3. Another client fence and trying to recover the same ledger
4. During this time ensemble change will happen and new bookie will be added. But this bookie is not able to connect.
5. This recovery will fail.
7. Now previously added bookie came up.
8. Another client trying to recover the same ledger.
9. Since new bookie is first in the ensemble, doRecoveryRead() is reading from that bookie and getting NoSuchLedgerException and closing the ledger with -1

i.e. Marking the ledger as empty, even though first client had successfully written one entry.",2012-08-08T10:02:04.467+0000,2013-02-13T15:46:59.819+0000,Fixed,Major
BIGTOP-1029,Add support for HBase 0.96+,BIGTOP,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12372237'>],"The next stable release series of Apache HBase, 0.96, has been refactored into multiple Maven modules. The steps required to build the sources and then to package up the build results are substantially different than for previous releases. 

The structure of the Maven modules also opens the door to separate client and server side packages.",2013-07-17T22:12:49.466+0000,2014-05-20T17:30:03.115+0000,Duplicate,Major
CALCITE-3982,Simplify FilterMergeRule to rely on RelBuilder instead of RexProgram,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12634302'>, <JIRA IssueLink: id='12587811'>]","This could potentially happen since Filter creation has a check on whether the expression is flat ([here|https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/rel/core/Filter.java#L74]) and Filter merge does not flatten an expression when it is created.

{noformat}
java.lang.AssertionError: AND(=($3, 100), OR(OR(null, IS NOT NULL(CAST(100):INTEGER)), =(CAST(100):INTEGER, CAST(200):INTEGER)))
	at org.apache.calcite.rel.core.Filter.<init>(Filter.java:74)
	at org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.<init>(HiveFilter.java:39)
	at org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories$HiveFilterFactoryImpl.createFilter(HiveRelFactories.java:126)
	at org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.filter(HiveRelBuilder.java:99)
	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:1055)
	at org.apache.calcite.rel.rules.FilterMergeRule.onMatch(FilterMergeRule.java:81)
{noformat}",2020-05-08T19:44:16.220+0000,2022-02-23T10:27:01.487+0000,Fixed,Major
HCATALOG-505,testSyntheticComplexSchema and testMapWithComplexData fail in JDK 1.7 64 bit,HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12377052'>],"Tests fail when doing the following:
{code}
private void verifyWriteRead 
{
   while (it.hasNext()) {
   Tuple input = data.get(i++);
   Tuple output = it.next();
   Assert.assertEquals(input.toString(), output.toString());
   LOG.info(""tuple : {} "",output);
}
{code}

and is reported in the following way:
{quote}
Testcase: testSyntheticComplexSchema took 17.693 sec
        FAILED
expected:<...1#test 1,ac test2#test 2...> but was:<...2#test 2,ac test1#test 1...>
junit.framework.ComparisonFailure: expected:<...1#test 1,ac test2#test 2...> but was:<...2#test 2,ac test1#test 1...>
        at org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.verifyWriteRead(TestHCatLoaderComplexSchema.java:203)
        at org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.testSyntheticComplexSchema(TestHCatLoaderComplexSchema.java:171)

Testcase: testTupleInBagInTupleInBag took 50.637 sec
Testcase: testMapWithComplexData took 12.558 sec
        FAILED
expected:<...2#(2,test 2),b test 1#(1,test 1...> but was:<...1#(1,test 1),b test 2#(2,test 2...>
junit.framework.ComparisonFailure: expected:<...2#(2,test 2),b test 1#(1,test 1...> but was:<...1#(1,test 1),b test 2#(2,test 2...>
        at org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.verifyWriteRead(TestHCatLoaderComplexSchema.java:203)
        at org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.testMapWithComplexData(TestHCatLoaderComplexSchema.java:284)
{quote}

We may need to change the way we are comparing complex tuples!!",2012-09-18T01:06:29.418+0000,2013-10-18T21:41:32.332+0000,Fixed,Minor
CALCITE-2170,Use Druid Expressions capabilities to improve the amount of work that can be pushed to Druid,CALCITE,New Feature,Closed,[],4,"[<JIRA IssueLink: id='12527392'>, <JIRA IssueLink: id='12527390'>, <JIRA IssueLink: id='12526489'>, <JIRA IssueLink: id='12527391'>]","Druid 0.11 has newly built in capabilities called Expressions that can be used to push expression like projects/aggregates/filters. 

In order to leverage this new feature, some changes need to be done to the Druid Calcite adapter. 

This is a link to the current supported functions and expressions in Druid
 [http://druid.io/docs/latest/misc/math-expr.html]
As you can see from the Docs an expression can be an actual tree of operators,
Expression can be used with Filters, Projects, Aggregates, PostAggregates and
Having filters. For Filters will have new Filter kind called Filter expression.
FYI, you might ask can we push everything as Expression Filter the short answer
is no because, other kinds of Druid filters perform better when used, Hence
Expression filter is a plan B sort of thing. In order to push expression as
Projects and Aggregates we will be using Expression based Virtual Columns.

The major change is the merging of the logic of pushdown verification code and
the Translation of RexCall/RexNode to Druid Json, native physical language. The
main drive behind this redesign is the fact that in order to check if we can
push down a tree of expressions to Druid we have to compute the Druid Expression
String anyway. Thus instead of having 2 different code paths, one for pushdown
validation and one for Json generation we can have one function that does both.
For instance instead of having one code path to test and check if a given filter
can be pushed or not and then having a translation layer code, will have
one function that either returns a valid Druid Filter or null if it is not
possible to pushdown. The same idea will be applied to how we push Projects and
Aggregates, Post Aggregates and Sort.

Here are the main elements/Classes of the new design. First will be merging the logic of
Translation of Literals/InputRex/RexCall to a Druid physical representation.
Translate leaf RexNode to Valid pair Druid Column + Extraction functions if possible
{code:java}
/**
 * @param rexNode leaf Input Ref to Druid Column
 * @param rowType row type
 * @param druidQuery druid query
 *
 * @return {@link Pair} of Column name and Extraction Function on the top of the input ref or
 * {@link Pair of(null, null)} when can not translate to valid Druid column
 */
 protected static Pair<String, ExtractionFunction> toDruidColumn(RexNode rexNode,
 RelDataType rowType, DruidQuery druidQuery
 )
{code}
In the other hand, in order to Convert Literals to Druid Literals will introduce
{code:java}
/**
 * @param rexNode rexNode to translate to Druid literal equivalante
 * @param rowType rowType associated to rexNode
 * @param druidQuery druid Query
 *
 * @return non null string or null if it can not translate to valid Druid equivalent
 */
@Nullable
private static String toDruidLiteral(RexNode rexNode, RelDataType rowType,
 DruidQuery druidQuery
)
{code}
Main new functions used to pushdown nodes and Druid Json generation.

Filter pushdown verification and generates is done via
{code:java}
org.apache.calcite.adapter.druid.DruidJsonFilter#toDruidFilters
{code}
For project pushdown added
{code:java}
org.apache.calcite.adapter.druid.DruidQuery#computeProjectAsScan.
{code}
For Grouping pushdown added
{code:java}
org.apache.calcite.adapter.druid.DruidQuery#computeProjectGroupSet.
{code}
For Aggregation pushdown added
{code:java}
org.apache.calcite.adapter.druid.DruidQuery#computeDruidJsonAgg
{code}
For sort pushdown added
{code:java}
org.apache.calcite.adapter.druid.DruidQuery#computeSort\{code}
Pushing of PostAggregates will be using Expression post Aggregates and use
{code}
org.apache.calcite.adapter.druid.DruidExpressions#toDruidExpression\{code}
to generate expression

For Expression computation most of the work is done here
{code:java}
org.apache.calcite.adapter.druid.DruidExpressions#toDruidExpression\{code}
This static function generates Druid String expression out of a given RexNode or
returns null if not possible.
{code}
@Nullable
public static String toDruidExpression(
final RexNode rexNode,
final RelDataType inputRowType,
final DruidQuery druidRel
)
{code:java}
In order to support various kind of expressions added the following interface
{code}
org.apache.calcite.adapter.druid.DruidSqlOperatorConverter\{code}
Thus user can implement custom expression converter based on the SqlOperator syntax and signature.
{code:java}
public interface DruidSqlOperatorConverter {
 /**
 * Returns the calcite SQL operator corresponding to Druid operator.
 *
 * @return operator
 */
 SqlOperator calciteOperator();
 /**
 * Translate rexNode to valid Druid expression.
 * @param rexNode rexNode to translate to Druid expression
 * @param rowType row type associated with rexNode
 * @param druidQuery druid query used to figure out configs/fields related like timeZone
 *
 * @return valid Druid expression or null if it can not convert the rexNode
 */
 @Nullable String toDruidExpression(RexNode rexNode, RelDataType rowType, DruidQuery druidQuery);
}
{code}
The Druid Query Class will provide
org.apache.calcite.adapter.druid.DruidQuery#getOperatorConversionMap which is a
map of SqlOperator to DruidSqlOperatorConverter.
Any feedback is welcome.

 ",2018-02-06T18:49:21.561+0000,2018-03-17T17:43:22.524+0000,Fixed,Major
DRILL-6916,"Fix extraneous ""${project.basedir}/src/site/resources/repo/"" directory appearance",DRILL,Bug,Resolved,[],1,[<JIRA IssueLink: id='12550498'>],"After upgrade HBase lib (in scope of DRILL-6349) in Drill project sources root directory the extraneous directory can be created in the process of {{mvn clean install -DskipTests -pl contrib/storage-hbase/}}
 It is related to absent {{maven-metadata.xml}} file in {{org.glassfish:javax.el}} transitive dependency from HBase lib.
 See info from {{mvn release:prepare -X}}:
{noformat}
[INFO] [DEBUG] Failure to find org.glassfish:javax.el/maven-metadata.xml in file:${project.basedir}/src/site/resources/repo was cached in the local repository, resolution will not be reattempted until the update interval of project.local has elapsed or updates are forced{noformat}
So it can be easily reproduced with removing {{org.glassfish:javax.el}} from local maven repo:
{noformat}
~/.m2/repository/org/glassfish/javax.el{noformat}
and making Drill build for HBase package:
{noformat}
mvn clean install -DskipTests -pl contrib/storage-hbase/{noformat}
Solution: update HBase lib to the latest 2.1.1 version, since HBASE-21005 resolved there.",2018-12-20T10:19:50.606+0000,2018-12-20T22:15:59.272+0000,Fixed,Blocker
PARQUET-54,Parquet Hive should resolve column names in case insensitive manner,PARQUET,Improvement,Open,[],1,[<JIRA IssueLink: id='12392931'>],Backport HIVE-7554,2014-07-30T15:34:08.500+0000,2014-07-30T15:34:23.199+0000,,Major
IMPALA-9470,Use Parquet bloom filters,IMPALA,New Feature,Resolved,"[<JIRA Issue: key='IMPALA-10640', id='13370047'>, <JIRA Issue: key='IMPALA-10641', id='13370050'>, <JIRA Issue: key='IMPALA-10642', id='13370051'>]",3,"[<JIRA IssueLink: id='12633928'>, <JIRA IssueLink: id='12609510'>, <JIRA IssueLink: id='12609650'>]","PARQUET-41 has been closed recently. This means Parquet-MR is capable of writing and reading bloom filters.

Currently bloom filters are per column chunk entries, i.e. with their help we can filter out entire row groups.

We already filter row groups in HdfsParquetScanner::NextRowGroup() based on column chunk statistics and dictionaries. Skipping row groups based on bloom filters could be also added to this funciton.

Impala could also write bloom filters.",2020-03-06T17:07:04.525+0000,2022-05-20T09:01:49.933+0000,Implemented,Major
AVRO-2328,Support distinguishing between LocalDateTime and Instant semantics in timestamps,AVRO,Task,Closed,[],2,"[<JIRA IssueLink: id='12555367'>, <JIRA IssueLink: id='12555376'>]","Different SQL engines of the Hadoop stack support different timestamp semantics. The range of supported semantics is about to be extended even further. While some of the new timestamp types can be added to SQL without explicit support from the file formats, others require new physical types. File format support would be beneficial even for timestamp semantics where it is not strictly required, because it would enable correct interpretation without an SQL schema or any other kind of manual configuration.

This JIRA is about supporting the LocalDateTime and Instant semantics. See [this document|https://docs.google.com/document/d/1E-7miCh4qK6Mg54b-Dh5VOyhGX8V4xdMXKIHJL36a9U/edit#] for details.",2019-02-28T16:34:42.991+0000,2020-11-02T14:32:45.065+0000,Fixed,Major
PHOENIX-2736,Fix possible data loss with local indexes when there are splits during bulkload,PHOENIX,Bug,Open,[],2,"[<JIRA IssueLink: id='12459274'>, <JIRA IssueLink: id='12459276'>]",Currently when there are splits during bulkload then LoadIncrementalHFiles move full HFile to first daughter region instead of properly spitting the HFile to two daughter region and also we may not properly replace the region start key if there are merges during bulkload. To fix this we can make HalfStoreFileReader configurable in LoadIncrementalHFiles and use IndexHalfStoreFileReader for local indexes.,2016-03-03T16:10:53.905+0000,2019-01-18T22:43:35.918+0000,,Major
CALCITE-2236,Druid adapter: Avoid duplication of fields names during Druid query planing,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12530897'>],"This issue occurs if two projects layers use the same fields name, it will lead to a Druid query with a duplicated field names.
I can not reproduce this in Calcite but it is reproducible in [Hive|https://issues.apache.org/jira/browse/HIVE-19044] (it has to deal on how different layers of project are getting names)
Here is an example of faulty query where ""$f4"" is used twice.
{code}
{""queryType"":""groupBy"",""dataSource"":""druid_tableau.calcs"",""granularity"":""all"",""dimensions"":[{""type"":""default"",""dimension"":""key"",""outputName"":""key"",""outputType"":""STRING""}],""limitSpec"":{""type"":""default""},""aggregations"":[{""type"":""doubleSum"",""name"":""$f1"",""fieldName"":""num0""},{""type"":""filtered"",""filter"":{""type"":""not"",""field"":{""type"":""selector"",""dimension"":""num0"",""value"":null}},""aggregator"":{""type"":""count"",""name"":""$f2"",""fieldName"":""num0""}},{""type"":""doubleSum"",""name"":""$f3"",""expression"":""(\""num0\"" * \""num0\"")""},{""type"":""doubleSum"",""name"":""$f4"",""expression"":""(\""num0\"" * \""num0\"")""}],""postAggregations"":[{""type"":""expression"",""name"":""$f4"",""expression"":""pow(((\""$f4\"" - ((\""$f1\"" * \""$f1\"") / \""$f2\"")) / \""$f2\""),0.5)""}],""intervals"":[""1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z""]}
{code}",2018-04-03T01:49:58.012+0000,2018-07-20T08:18:32.699+0000,Fixed,Major
TEZ-1141,DAGStatus.Progress should include number of failed and killed attempts,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12398571'>],Currently its impossible to know whether a job is seeing a lot of issues and failures because we only report running tasks. Eventually the job fails but before that we have no indication that a bunch of task failures have been happening.,2014-05-20T21:42:40.351+0000,2014-11-08T20:25:20.790+0000,Fixed,Major
BIGTOP-386,HBase execution script should exclude ZOOKEEPER_CONF,BIGTOP,Bug,Closed,[],3,"[<JIRA IssueLink: id='12347992'>, <JIRA IssueLink: id='12347885'>, <JIRA IssueLink: id='12347884'>]","HBase will always use ZK settings found in zoo.cfg if it included in the classpath.  This is a problem for any hbase tools that interact with multiple hbase clusters with distinct ZK quorums.  Until HBase changes it's behavior, any packaging should exclude by default ZOOKEEPER_CONF/zoo.cfg.

See https://issues.cloudera.org/browse/DISTRO-355, HBASE-4151, HBASE-4614, HBASE-4072 for more details.",2012-02-05T20:35:51.132+0000,2012-06-26T15:58:49.939+0000,Fixed,Major
YETUS-242,hadoop: add -Drequire.valgrind,YETUS,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12452050'>],Add -Drequire.valgrind to the Hadoop personality (HDFS-9448),2015-12-15T22:30:02.874+0000,2018-08-22T23:33:50.579+0000,Fixed,Trivial
AVRO-493,hadoop mapreduce support for avro data,AVRO,New Feature,Closed,[],2,"[<JIRA IssueLink: id='12333850'>, <JIRA IssueLink: id='12332079'>]",Avro should provide support for using Hadoop MapReduce over Avro data files.,2010-03-30T21:43:51.837+0000,2010-09-11T01:44:05.704+0000,Fixed,Major
SPARK-25797,Views created via 2.1 cannot be read via 2.2+,SPARK,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12546492'>, <JIRA IssueLink: id='12546491'>, <JIRA IssueLink: id='12546576'>, <JIRA IssueLink: id='12546490'>]","We ran into this issue when we update our Spark from 2.1 to 2.3. Below's a simple example to reproduce the issue.

Create views via Spark 2.1
{code:sql}
create view v1 as
select (cast(1 as decimal(18,0)) + cast(1 as decimal(18,0))) c1;
{code}

Query views via Spark 2.3
{code:sql}
select * from v1;
Error in query: Cannot up cast `c1` from decimal(20,0) to c1#3906: decimal(19,0) as it may truncate
{code}

After investigation, we found that this is because when a view is created via Spark 2.1, the expanded text is saved instead of the original text. Unfortunately, the blow expanded text is buggy.
{code:sql}
spark-sql> desc extended v1;
c1 decimal(19,0) NULL
Detailed Table Information
Database default
Table v1
Type VIEW
View Text SELECT `gen_attr_0` AS `c1` FROM (SELECT (CAST(CAST(1 AS DECIMAL(18,0)) AS DECIMAL(19,0)) + CAST(CAST(1 AS DECIMAL(18,0)) AS DECIMAL(19,0))) AS `gen_attr_0`) AS gen_subquery_0
{code}

We can see that c1 is decimal(19,0), however in the expanded text there is decimal(19,0) + decimal(19,0) which results in decimal(20,0). Since Spark 2.2, decimal(20,0) in query is not allowed to cast to view definition column decimal(19,0). ([https://github.com/apache/spark/pull/16561])

I further tested other decimal calculations. Only add/subtract has this issue.

Create views via 2.1:
{code:sql}
create view v1 as
select (cast(1 as decimal(18,0)) + cast(1 as decimal(18,0))) c1;
create view v2 as
select (cast(1 as decimal(18,0)) - cast(1 as decimal(18,0))) c1;
create view v3 as
select (cast(1 as decimal(18,0)) * cast(1 as decimal(18,0))) c1;
create view v4 as
select (cast(1 as decimal(18,0)) / cast(1 as decimal(18,0))) c1;
create view v5 as
select (cast(1 as decimal(18,0)) % cast(1 as decimal(18,0))) c1;
create view v6 as
select cast(1 as decimal(18,0)) c1
union
select cast(1 as decimal(19,0)) c1;
{code}

Query views via Spark 2.3
{code:sql}
select * from v1;
Error in query: Cannot up cast `c1` from decimal(20,0) to c1#3906: decimal(19,0) as it may truncate
select * from v2;
Error in query: Cannot up cast `c1` from decimal(20,0) to c1#3909: decimal(19,0) as it may truncate
select * from v3;
1
select * from v4;
1
select * from v5;
0
select * from v6;
1
{code}

Views created via Spark 2.2+ don't have this issue because Spark 2.2+ does not generate expanded text for view (https://issues.apache.org/jira/browse/SPARK-18209).",2018-10-22T07:15:04.710+0000,2018-10-29T04:28:52.758+0000,Fixed,Major
AMBARI-21577,Hive-Service check failing in post EU validation (BI-HDP),AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12510452'>],"Steps to reproduce:-
1. Installed a IOP cluster ambari-version:- 2.2.0/20160616_1658,BigInsights-4.2.0.0
2. Upgrade the ambari from 2.2.0 to 2.5.2.0-174(IOP Clusters)
3. Remove IOP Select.
4. Register HDP Stack to HDP-2.6.2.0-107.
5. EU
6. Post EU
Hive- Service check is failing :- 
{code}
HTTP/vs-iop420tofnsec-re-2.openstacklocal@EXAMPLE.COM is not allowed to impersonate ambari-qa
{code}
stderr:-
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/service_check.py"", line 194, in <module>
    HiveServiceCheck().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 329, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/service_check.py"", line 99, in service_check
    webhcat_service_check()
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/webhcat_service_check.py"", line 125, in webhcat_service_check
    logoutput=True)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of '/var/lib/ambari-agent/tmp/templetonSmoke.sh vs-iop420tofnsec-re-2.openstacklocal ambari-qa 20111 idtest.ambari-qa.1500877355.88.pig /etc/security/keytabs/smokeuser.headless.keytab true /usr/bin/kinit ambari-qa@EXAMPLE.COM /var/lib/ambari-agent/tmp' returned 1. Templeton Smoke Test (ddl cmd): Failed. : {""error"":""User: HTTP/vs-iop420tofnsec-re-2.openstacklocal@EXAMPLE.COM is not allowed to impersonate ambari-qa""}http_code <500>
{code} 
Screenshot:- !Screen Shot 2017-07-24 at 12.04.44 PM.png|thumbnail! 
Live-Server:- http://172.22.115.63:8080.
",2017-07-26T15:58:41.848+0000,2017-07-28T16:22:07.043+0000,Fixed,Major
PHOENIX-5468,[connectors] Enable running the test suite with JDK11,PHOENIX,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12569373'>, <JIRA IssueLink: id='12569375'>, <JIRA IssueLink: id='12569374'>]","As HBase runs on JDK11, Phoenix should also support JDK11.

A first step is making sure that we can run the tests on JDK11, while still building on JDK8.

Right now, the test suite cannot be run with JDK11, because it tries to run the failsafe tests with a VM setting that prevent JDK11 from starting.",2019-09-05T21:53:45.967+0000,2020-04-06T08:25:02.655+0000,Fixed,Major
THRIFT-4506,[CVE-2018-1320] Remove assertion in Java SASL code that would be ignored in release builds,THRIFT,Bug,Closed,[],1,[<JIRA IssueLink: id='12553266'>],"There is an assertion in the SASL transport for Java that will only be processed in debug builds, at https://github.com/apache/thrift/blob/master/lib/java/src/org/apache/thrift/transport/TSaslTransport.java#L298.  The preceeding while loop can be changed to guarantee this assertion in all builds.

https://cve.mitre.org/cgi-bin/cvename.cgi?name=2018-1320",2018-03-05T16:30:01.677+0000,2019-05-12T11:40:17.412+0000,Fixed,Minor
ORC-159,expose metadata buffer from metadata reader (for caching),ORC,Bug,Open,[],1,[<JIRA IssueLink: id='12498108'>],,2017-03-15T21:33:59.828+0000,2017-03-15T21:34:19.775+0000,,Major
PHOENIX-6740,Upgrade default supported Hadoop 3 version to 3.2.3 for HBase 2.5 profile,PHOENIX,Task,Resolved,[],2,"[<JIRA IssueLink: id='12643649'>, <JIRA IssueLink: id='12642458'>]","HBase is upgrading the minimum supported Hadoop to 3.2.3 for HBase 2.5, and we have a similar request from dependabot. ",2022-06-22T15:54:11.070+0000,2022-08-29T19:38:09.456+0000,Duplicate,Major
BIGTOP-894,Pig compilation fails on RPM systems on Bigtop trunk,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12366516'>, <JIRA IssueLink: id='12366525'>]","BIGTOP-870 bumped up version of Pig from 0.10 to 0.11 for Bigtop 0.6 release. However, while that seems to build Pig fine on Debian based systems, Pig compilation fails on RPM based systems with an error like:

{code}
04:27:41  compile:
04:27:41       [echo]  *** Compiling Pig UDFs ***
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/build.xml:93: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
04:27:41      [javac] Compiling 158 source files to /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/build/classes
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java:31: unmappable character for encoding ASCII
04:27:41      [javac]  * Note this function checks for Integer range ???2,147,483,648 to 2,147,483,647.
04:27:41      [javac]                                                ^
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java:31: unmappable character for encoding ASCII
04:27:41      [javac]  * Note this function checks for Integer range ???2,147,483,648 to 2,147,483,647.
04:27:41      [javac]                                                 ^
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java:31: unmappable character for encoding ASCII
04:27:41      [javac]  * Note this function checks for Integer range ???2,147,483,648 to 2,147,483,647.
04:27:41      [javac]                                                  ^
04:27:42      [javac] 3 errors
{code}

",2013-03-28T21:50:02.850+0000,2015-03-18T22:47:14.946+0000,Fixed,Blocker
SOLR-1431,CommComponent abstracted,SOLR,Improvement,Closed,[],6,"[<JIRA IssueLink: id='12327990'>, <JIRA IssueLink: id='12327991'>, <JIRA IssueLink: id='12339849'>, <JIRA IssueLink: id='12344046'>, <JIRA IssueLink: id='12339857'>, <JIRA IssueLink: id='12326891'>]",We'll abstract CommComponent in this issue.,2009-09-14T22:27:05.065+0000,2013-05-10T10:40:46.434+0000,Fixed,Major
SOLR-4526,Admin UI depends on optional system info,SOLR,Bug,Closed,[],2,"[<JIRA IssueLink: id='12365131'>, <JIRA IssueLink: id='12365132'>]","A user on IRC was having trouble getting file descriptor counts and JVM memory usage in the admin UI, but it worked perfectly fine on another system.  The problem system uses OpenJDK, the other one uses the Apple JDK.  The user had tracked it down to an exception while trying to get open file descriptor info.

Looking in the SystemInfoHandler.java file, I see a comment reference to com.sun.management.UnixOperatingSystemMXBean at the point where it is getting file descriptor info.  A little extra searching turned up ZOOKEEPER-1579 which refers to HBASE-6945, the same problem with OpenJDK.
",2013-03-04T17:45:45.342+0000,2017-02-16T15:55:32.401+0000,Fixed,Major
PHOENIX-4090,Remove HTrace based tracing,PHOENIX,Task,Open,[],1,[<JIRA IssueLink: id='12512090'>],"HTrace is lacking momentum and may be headed for the attic. 

See
https://lists.apache.org/thread.html/ff87eb11bbfc3ca27a83c0ab377459767a46175447f763b1bf052987@%3Cdev.htrace.apache.org%3E

​HBase is considering removal of all HTrace related code.

See https://lists.apache.org/thread.html/68791862c603984b804aa4032d47ea6001bc4684d140ad6b465b27cf@%3Cdev.hbase.apache.org%3E",2017-08-16T18:02:25.853+0000,2017-08-16T18:02:47.727+0000,,Major
PARQUET-363,Cannot construct empty MessageType for ReadContext.requestedSchema,PARQUET,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12435016'>, <JIRA IssueLink: id='12435015'>]","In parquet-mr 1.8.1, constructing empty {{GroupType}} (and thus {{MessageType}}) is not allowed anymore (see PARQUET-278). This change makes sense in most cases since Parquet doesn't support empty groups. However, there is one use case where an empty {{MessageType}} is valid, namely passing an empty {{MessageType}} as the {{requestedSchema}} constructor argument of {{ReadContext}} when counting rows in a Parquet file. The reason why it works is that, Parquet can retrieve row count from block metadata without materializing any columns. Take the following PySpark shell snippet ([1.5-SNAPSHOT|https://github.com/apache/spark/commit/010b03ed52f35fd4d426d522f8a9927ddc579209], which uses parquet-mr 1.7.0) as an example:
{noformat}
>>> path = 'file:///tmp/foo'
>>> # Writes 10 integers into a Parquet file
>>> sqlContext.range(10).coalesce(1).write.mode('overwrite').parquet(path)
>>> sqlContext.read.parquet(path).count()

10
{noformat}
Parquet related log lines:
{noformat}
15/08/21 12:32:04 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message root {
}


Catalyst form:
StructType()

15/08/21 12:32:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10 records.
15/08/21 12:32:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 12:32:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 10
{noformat}
We can see that Spark SQL passes no requested columns to the underlying Parquet reader. What happens here is that:

# Spark SQL creates a {{CatalystRowConverter}} with zero converters (and thus only generates empty rows).
# {{InternalParquetRecordReader}} first obtain the row count from block metadata ([here|https://github.com/apache/parquet-mr/blob/apache-parquet-1.8.1/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.java#L184-L186]).
# {{MessageColumnIO}} returns an {{EmptyRecordRecorder}} for reading the Parquet file ([here|https://github.com/apache/parquet-mr/blob/apache-parquet-1.8.1/parquet-column/src/main/java/org/apache/parquet/io/MessageColumnIO.java#L97-L99]).
# {{InternalParquetRecordReader.nextKeyValue()}} is invoked _n_ times, where _n_ equals to the row count. Each time, it invokes the converter created by Spark SQL and produces an empty Spark SQL row object.

This issue is also the cause of HIVE-11611.  Because when upgrading to Parquet 1.8.1, Hive worked around this issue by using {{tableSchema}} as {{requestedSchema}} when no columns are requested ([here|https://github.com/apache/hive/commit/3e68cdc9962cacab59ee891fcca6a736ad10d37d#diff-cc764a8828c4acc2a27ba717610c3f0bR233]). IMO this introduces a performance regression in cases like counting, because now we need to materialize all columns just for counting.",2015-08-21T06:46:52.543+0000,2018-04-21T12:39:05.994+0000,Fixed,Major
TEZ-1351,MROutput needs a flush method to ensure data is materialized for FileOutputCommitter,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12393013'>],"In MROutput.commit, we need to check isCommitRequired before invoking commitTask.

Currently we did this check inside Pig:
{code}
                if (fileOutput.isCommitRequired()) {
                    fileOutput.commit();
                }
{code}
However, in some loader, output file is generated only after fileOutput.close, which is part of fileOutput.commit. The isCommitRequired check is too early. A walk around is to invoke fileOutput.close before isCommitRequired:
{code}
                fileOutput.close();
                if (fileOutput.isCommitRequired()) {
                    fileOutput.commit();
                }
{code}
But we are told there is a plan to make MROutput.close private.",2014-07-31T18:51:03.339+0000,2014-09-06T01:35:31.006+0000,Fixed,Major
SQOOP-1198,Make Sqoop build aware of the protobuf library,SQOOP,Bug,Open,[],1,[<JIRA IssueLink: id='12377899'>],"Currently Sqoop build is using protobuf from local machine. As different Hadoop versions are using different protobuf versions, I would suggest to make the build aware of the version and pull in version that is required.",2013-09-12T07:33:12.954+0000,2017-10-30T14:04:56.930+0000,,Major
SPARK-3215,Add remote interface for SparkContext,SPARK,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12395159'>, <JIRA IssueLink: id='12399655'>, <JIRA IssueLink: id='12395148'>]","A quick description of the issue: as part of running Hive jobs on top of Spark, it's desirable to have a SparkContext that is running in the background and listening for job requests for a particular user session.

Running multiple contexts in the same JVM is not a very good solution. Not only SparkContext currently has issues sharing the same JVM among multiple instances, but that turns the JVM running the contexts into a huge bottleneck in the system.

So I'm proposing a solution where we have a SparkContext that is running in a separate process, and listening for requests from the client application via some RPC interface (most probably Akka).

I'll attach a document shortly with the current proposal. Let's use this bug to discuss the proposal and any other suggestions.",2014-08-25T22:27:35.316+0000,2015-11-23T19:43:01.342+0000,Won't Fix,Major
CALCITE-1306,Allow GROUP BY and HAVING to reference SELECT expressions by ordinal and alias,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12503083'>, <JIRA IssueLink: id='12473909'>, <JIRA IssueLink: id='12496203'>]","Allow GROUP BY and HAVING to reference SELECT expressions by ordinal and alias. It is not standard SQL, but MySQL and PostgreSQL allow it.

See [Stack Overflow: SQL - using alias in Group By|http://stackoverflow.com/questions/3841295/sql-using-alias-in-group-by].

It would be enabled only by new methods {{isGroupByOrdinal}} and {{isGroupByAlias}} in SqlConformance.

We might allow alias in the HAVING clause (as described in HIVE-10557) but ordinal does not make sense.

Expressions that are not available before grouping would be illegal; for instance:

{code}
select count(*) as c
from t
group by c
{code}

We'd also need rules to resolve ambiguous expressions. For instance, in

{code}
select e.empno as deptno
from emp as e join dept as d
where e.deptno = d.deptno
group by deptno
{code}

does {{deptno}} refer to {{e.deptno}}, {{d.deptno}}, or {{e.empno}}?",2016-07-06T01:19:54.091+0000,2017-06-26T08:29:37.816+0000,Fixed,Major
INFRA-23235,Use EIP in /etc/hosts cause binding error when running java problem,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12639481'>],"At least on hbase3, we have this in /etc/hosts

127.0.0.1 localhost
# 172.31.12.81 jenkins-hbase3
44.239.2.152 jenkins-hbase3.apache.org jenkins-hbase3§

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts

The jenkins-hbase3.apache.org and jenkins-hbase3 were both resolved to the EIP 44.239.2.152, but the actual IP of this node should be 172.31.12.81. This causes test failure on these nodes because of BindException.

org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [jenkins-hbase3.apache.org:0] java.net.BindException: Cannot assign requested address

I think on the local machine, we should just resolve them to the local IP, not the EIP.

PTAL. Thanks.",2022-05-06T15:25:03.328+0000,2022-05-29T08:55:29.618+0000,Fixed,Major
HDDS-4,Implement security for Hadoop Distributed Storage Layer ,HDDS,New Feature,Resolved,"[<JIRA Issue: key='HDDS-5', id='13154898'>, <JIRA Issue: key='HDDS-6', id='13154899'>, <JIRA Issue: key='HDDS-7', id='13154900'>, <JIRA Issue: key='HDDS-70', id='13159167'>, <JIRA Issue: key='HDDS-10', id='13155756'>, <JIRA Issue: key='HDDS-98', id='13161274'>, <JIRA Issue: key='HDDS-547', id='13187143'>, <JIRA Issue: key='HDDS-566', id='13188731'>, <JIRA Issue: key='HDDS-546', id='13187142'>, <JIRA Issue: key='HDDS-100', id='13161276'>, <JIRA Issue: key='HDDS-548', id='13187164'>, <JIRA Issue: key='HDDS-588', id='13190166'>, <JIRA Issue: key='HDDS-591', id='13190196'>, <JIRA Issue: key='HDDS-704', id='13192959'>, <JIRA Issue: key='HDDS-684', id='13192383'>, <JIRA Issue: key='HDDS-103', id='13161279'>, <JIRA Issue: key='HDDS-101', id='13161277'>, <JIRA Issue: key='HDDS-760', id='13195009'>, <JIRA Issue: key='HDDS-753', id='13194946'>, <JIRA Issue: key='HDDS-592', id='13190234'>, <JIRA Issue: key='HDDS-778', id='13195459'>, <JIRA Issue: key='HDDS-836', id='13198181'>, <JIRA Issue: key='HDDS-135', id='13163210'>, <JIRA Issue: key='HDDS-580', id='13189747'>, <JIRA Issue: key='HDDS-8', id='13155754'>, <JIRA Issue: key='HDDS-9', id='13155755'>, <JIRA Issue: key='HDDS-873', id='13200659'>, <JIRA Issue: key='HDDS-696', id='13192664'>, <JIRA Issue: key='HDDS-804', id='13196282'>, <JIRA Issue: key='HDDS-884', id='13201448'>, <JIRA Issue: key='HDDS-115', id='13161502'>, <JIRA Issue: key='HDDS-928', id='13204581'>, <JIRA Issue: key='HDDS-929', id='13204594'>, <JIRA Issue: key='HDDS-805', id='13196284'>, <JIRA Issue: key='HDDS-937', id='13205202'>, <JIRA Issue: key='HDDS-102', id='13161278'>, <JIRA Issue: key='HDDS-955', id='13207334'>, <JIRA Issue: key='HDDS-945', id='13205529'>, <JIRA Issue: key='HDDS-938', id='13205205'>, <JIRA Issue: key='HDDS-963', id='13207734'>, <JIRA Issue: key='HDDS-964', id='13207749'>, <JIRA Issue: key='HDDS-942', id='13205443'>, <JIRA Issue: key='HDDS-970', id='13208658'>, <JIRA Issue: key='HDDS-967', id='13208186'>, <JIRA Issue: key='HDDS-540', id='13186819'>, <JIRA Issue: key='HDDS-597', id='13190246'>, <JIRA Issue: key='HDDS-960', id='13207507'>, <JIRA Issue: key='HDDS-984', id='13210225'>, <JIRA Issue: key='HDDS-943', id='13205447'>, <JIRA Issue: key='HDDS-975', id='13208908'>, <JIRA Issue: key='HDDS-980', id='13209784'>, <JIRA Issue: key='HDDS-593', id='13190239'>, <JIRA Issue: key='HDDS-1012', id='13211906'>, <JIRA Issue: key='HDDS-1063', id='13214288'>, <JIRA Issue: key='HDDS-581', id='13189748'>, <JIRA Issue: key='HDDS-1039', id='13213172'>, <JIRA Issue: key='HDDS-1107', id='13215791'>, <JIRA Issue: key='HDDS-1110', id='13215849'>, <JIRA Issue: key='HDDS-1101', id='13215701'>, <JIRA Issue: key='HDDS-1060', id='13214247'>, <JIRA Issue: key='HDDS-1038', id='13213168'>, <JIRA Issue: key='HDDS-1176', id='13217901'>, <JIRA Issue: key='HDDS-1061', id='13214269'>, <JIRA Issue: key='HDDS-1190', id='13218520'>, <JIRA Issue: key='HDDS-1204', id='13218808'>, <JIRA Issue: key='HDDS-134', id='13163195'>, <JIRA Issue: key='HDDS-1183', id='13218270'>, <JIRA Issue: key='HDDS-1111', id='13215851'>, <JIRA Issue: key='HDDS-1216', id='13219377'>, <JIRA Issue: key='HDDS-594', id='13190243'>, <JIRA Issue: key='HDDS-1235', id='13220243'>, <JIRA Issue: key='HDDS-596', id='13190245'>, <JIRA Issue: key='HDDS-1236', id='13220245'>, <JIRA Issue: key='HDDS-1245', id='13220749'>, <JIRA Issue: key='HDDS-1253', id='13221108'>, <JIRA Issue: key='HDDS-1043', id='13213434'>, <JIRA Issue: key='HDDS-1087', id='13215141'>, <JIRA Issue: key='HDDS-1254', id='13221177'>, <JIRA Issue: key='HDDS-595', id='13190244'>, <JIRA Issue: key='HDDS-1246', id='13220753'>, <JIRA Issue: key='HDDS-1296', id='13222221'>, <JIRA Issue: key='HDDS-1119', id='13216051'>, <JIRA Issue: key='HDDS-1055', id='13214065'>, <JIRA Issue: key='HDDS-1062', id='13214286'>, <JIRA Issue: key='HDDS-1118', id='13216050'>, <JIRA Issue: key='HDDS-1215', id='13219376'>, <JIRA Issue: key='HDDS-747', id='13194746'>, <JIRA Issue: key='HDDS-1299', id='13222242'>, <JIRA Issue: key='HDDS-1291', id='13221966'>, <JIRA Issue: key='HDDS-1255', id='13221208'>, <JIRA Issue: key='HDDS-1132', id='13216570'>, <JIRA Issue: key='HDDS-1317', id='13222921'>, <JIRA Issue: key='HDDS-1318', id='13222923'>, <JIRA Issue: key='HDDS-1423', id='13227378'>, <JIRA Issue: key='HDDS-1430', id='13227580'>, <JIRA Issue: key='HDDS-1439', id='13228132'>, <JIRA Issue: key='HDDS-1455', id='13229660'>, <JIRA Issue: key='HDDS-1471', id='13230388'>, <JIRA Issue: key='HDDS-1472', id='13230390'>]",1,[<JIRA IssueLink: id='12527853'>],"In HDFS-7240, we have created a scalable block layer that facilitates separation of namespace and block layer.  Hadoop Distributed Storage Layer (HDSL) allows us to scale HDFS(HDFS-10419) and as well as create ozone (HDFS-13074).

This JIRA is an umbrella JIRA that tracks the security-related work items for Hadoop Distributed Storage Layer.",2018-02-22T23:54:39.997+0000,2019-05-22T16:00:16.137+0000,Fixed,Major
TEZ-872,Both tez.runtime.intermediate-output.should-compress and tez.runtime.intermediate-input.is-compressed seem unnecessary,TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12385032'>, <JIRA IssueLink: id='12383415'>, <JIRA IssueLink: id='12383417'>]",This is confusing. If the output is compressed then the input should also be compressed. Why should 2 configs need to be set for this. Same for the compression codec configs. How can they be different for input and output.,2014-02-21T18:41:17.811+0000,2014-09-06T01:35:42.948+0000,Duplicate,Major
ORC-234,Create a shims module,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12513973'>],"We currently build all of ORC with Hadoop 2.6. Our shims let us run with older versions back to Hadoop 2.2. However, it is easy to mess up and use Hadoop 2.6 features in core. Therefore, I propose:

* We create a shim module that compiles with Hadoop 2.6 and move HadoopShims and HadoopShimsCurrent into it.
* We build core and mapreduce with Hadoop 2.2.
* Tools & bench will continue to use Hadoop 2.6.",2017-09-01T21:27:19.234+0000,2018-06-18T00:44:28.327+0000,Fixed,Major
TEZ-3271,Provide mapreduce failures.maxpercent equivalent,TEZ,New Feature,Closed,[],1,[<JIRA IssueLink: id='12469040'>],"There is a certain category of work that need not have 100% of tasks succeed to cause the work to be considered a success. To meet that end, I propose we provide a tez equivalent of mapreduce.map.failures.maxpercent and mapreduce.reduce.failures.maxpercent. In this way a vertex will be considered a success if the number of failures is below a configured threshold.",2016-05-25T03:25:47.250+0000,2017-08-22T00:02:30.921+0000,Fixed,Major
INFRA-23557,Need SonarCloud permission to create quality gate profiles,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12645119'>],"In the context of HIVE-26196 I would need to create a new quality gate profile (the most permissive possible) for Apache Hive.

I am an admin on the Hive project in SonarCloud (https://sonarcloud.io/project/overview?id=apache_hive), but it does not seem to be enough to create a quality gate profile.

Could you grant me and [~zabetak] such permission?",2022-08-05T13:25:13.745+0000,2022-08-07T17:03:40.318+0000,Fixed,Major
CALCITE-448,FilterIntoJoinRule creates filters containing invalid RexInputRef,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12400163'>],"Filter should be allowed to be constructed even when types are {{INTEGER}} && {{INTEGER NOT NULL}} respectively. Currently, it fails with :
{code}
 java.lang.AssertionError: type mismatch:
type1:
INTEGER NOT NULL
type2:
INTEGER
	at org.eigenbase.relopt.RelOptUtil.eq(RelOptUtil.java:1566)
	at org.eigenbase.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:899)
	at org.eigenbase.rex.RexProgramBuilder$RegisterInputShuttle.visitInputRef(RexProgramBuilder.java:878)
	at org.eigenbase.rex.RexInputRef.accept(RexInputRef.java:102)
	at org.eigenbase.rex.RexShuttle.visitList(RexShuttle.java:129)
	at org.eigenbase.rex.RexShuttle.visitCall(RexShuttle.java:78)
	at org.eigenbase.rex.RexProgramBuilder$RegisterShuttle.visitCall(RexProgramBuilder.java:843)
	at org.eigenbase.rex.RexProgramBuilder$RegisterShuttle.visitCall(RexProgramBuilder.java:841)
	at org.eigenbase.rex.RexCall.accept(RexCall.java:105)
	at org.eigenbase.rex.RexProgramBuilder.registerInput(RexProgramBuilder.java:272)
	at org.eigenbase.rex.RexProgramBuilder.addCondition(RexProgramBuilder.java:247)
	at org.eigenbase.relopt.RelOptUtil.pushFilterPastProject(RelOptUtil.java:2367)
	at org.eigenbase.rel.rules.PushFilterPastProjectRule.onMatch(PushFilterPastProjectRule.java:80)
	at org.eigenbase.relopt.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:321)
	at org.eigenbase.relopt.hep.HepPlanner.applyRule(HepPlanner.java:488)
	at org.eigenbase.relopt.hep.HepPlanner.applyRules(HepPlanner.java:365)
	at org.eigenbase.relopt.hep.HepPlanner.executeInstruction(HepPlanner.java:258)
	at org.eigenbase.relopt.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:68)
	at org.eigenbase.relopt.hep.HepPlanner.executeProgram(HepPlanner.java:179)
	at org.eigenbase.relopt.hep.HepPlanner.findBestExp(HepPlanner.java:166)
{code}",2014-10-30T00:37:19.328+0000,2015-02-08T08:56:31.418+0000,Fixed,Major
TEPHRA-299,Executing a large batch delete is very slow,TEPHRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12558926'>],"I noticed that batch deletes are quire slow. In the profiler I found that almost all of the time is spent in org.apache.hadoop.hbase.regionserver.wal.FSHLog.blockOnSync().

Looking at TransactionProcessor.preDelete it is obvious why:

The batch delete is translated into *single* puts that are added to the region one by one, so each time the WAL is flushed.

 ",2019-04-13T19:13:57.825+0000,2020-12-04T04:46:01.283+0000,Fixed,Major
ZOOKEEPER-224,Deploy ZooKeeper jars/artifacts to a Maven Repository,ZOOKEEPER,Task,Closed,[],5,"[<JIRA IssueLink: id='12330719'>, <JIRA IssueLink: id='12327691'>, <JIRA IssueLink: id='12339379'>, <JIRA IssueLink: id='12328673'>, <JIRA IssueLink: id='12327004'>]","I've created the maven poms needed for the 3.0.0 release.  

The directory structure and artifacts located at:
http://people.apache.org/~chirino/zk-repo/
aka
people.apache.org:/x1/users/chirino/public_html/zk-repo


Just need sto get GPG signed by the project KEY and deployed to:
people.apache.org:/www/people.apache.org/repo/m2-ibiblio-rsync-repository

Who's the current ZooKeeper release manager?
",2008-11-18T01:02:01.621+0000,2011-06-01T07:22:30.671+0000,Fixed,Critical
ORC-27,C++ reader does not read dates correctly prior to 1583,ORC,Bug,Closed,[],3,"[<JIRA IssueLink: id='12602085'>, <JIRA IssueLink: id='12576907'>, <JIRA IssueLink: id='12606382'>]","Reproducer:
1) create a Hive table with a date column
2) insert date 01-01-01
3) get the corresponding orc file from HDFS
4) read with the C++ reader

The reader produces 0-12-30 (off by 2 days)",2015-09-11T15:40:23.290+0000,2021-01-12T05:07:38.809+0000,Fixed,Minor
INFRA-22542,Add Hive to sonarcloud.io,INFRA,Improvement,Closed,[],1,[<JIRA IssueLink: id='12639155'>],"following the steps from:
https://cwiki.apache.org/confluence/display/INFRA/SonarQube+Analysis

github account ids of proposed project admins:
* kgyrtkirk
* zabetak
* kasakrisz 
",2021-11-17T10:31:57.755+0000,2022-05-02T15:13:58.720+0000,Fixed,Major
OOZIE-1457,Create a Hive Server 2 action,OOZIE,New Feature,Resolved,"[<JIRA Issue: key='OOZIE-1963', id='12732118'>, <JIRA Issue: key='OOZIE-1964', id='12732123'>, <JIRA Issue: key='OOZIE-2006', id='12741942'>]",6,"[<JIRA IssueLink: id='12374381'>, <JIRA IssueLink: id='12380121'>, <JIRA IssueLink: id='12394904'>, <JIRA IssueLink: id='12388218'>, <JIRA IssueLink: id='12396403'>, <JIRA IssueLink: id='12396605'>]","We should update the Hive action to support Hive Server 2 (Beeline); we'll also need to add a new {{Credentials}} implementation to get the delegation token (unlike the Hive Metastore, the existing {{HCatCredentials}} cannot be reused for Hive Server 2 because of the JDBC connection).  We may need to bump up the Hive version to support this.  

We should create a ""Hive Server 2 action"" that uses Beeline to connect to Hive Server 2.  We'll also need to add a new {{Credentials}} implementation to get the delegation token (unlike the Hive Metastore, the existing {{HCatCredentials}} cannot be reused for Hive Server 2 because of the JDBC connection).  

This will definitely work with Hive 0.13 (which hasn't been released) but may also work with Hive 0.12, though probably not with Kerberos.  I believe the delegation token work required isn't in Hive 0.12.  ",2013-07-16T21:56:18.766+0000,2019-02-12T00:40:20.920+0000,Fixed,Minor
TEZ-2821,Build with hadoop-2.6.1 fails ,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12437287'>],"{code}
[ERROR] /mnt/nfs0/jzhang/tez-autobuild/tez/tez-plugins/tez-yarn-timeline-history/src/test/java/org/apache/tez/tests/MiniTezClusterWithTimeline.java:[92,5] no suitable constructor found for MiniYARNCluster(java.lang.String,int,int,int,int,boolean)
    constructor org.apache.hadoop.yarn.server.MiniYARNCluster.MiniYARNCluster(java.lang.String,int,int,int,int) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.yarn.server.MiniYARNCluster.MiniYARNCluster(java.lang.String,int,int,int) is not applicable
      (actual and formal argument lists differ in length)

{code}",2015-09-14T03:31:00.630+0000,2015-09-14T20:23:26.972+0000,Invalid,Major
BIGTOP-1874,HBase build failed due to Codehaus repository is out of service,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12425343'>],"It seems that the codehaus repo is out of service and causing the hbase build not being able to download its dependencies . I encounter this first on the [HBase nightly build|http://bigtop01.cloudera.org:8080/view/Bigtop-trunk/job/Docker-Bigtop-trunk-HBase/BUILD_ENVIRONMENTS=centos-6,label=docker-slave-03/34/console], then manually tested on 4 docker build slaves separately. The results are consistent.
",2015-05-17T03:48:53.602+0000,2015-05-21T15:54:50.360+0000,Fixed,Major
TEZ-4447,Collect VertexStatus properly when DAGClientServer is not used (local mode without network),TEZ,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12647214'>, <JIRA IssueLink: id='12647215'>]","While using tez.local.mode.without.network, the following exception happens from hive.
{code}
2022-09-08T06:16:42,173 ERROR [6129b4da-18c7-4d58-839d-c8d6255a5317 Listener at 0.0.0.0/56326] tez.TezTask: Failed to execute tez graph.
java.lang.NullPointerException: null
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.collectCommitInformation(TezTask.java:367) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:279) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:354) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:327) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:244) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:105) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:370) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:205) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:154) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:149) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:185) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:228) [hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255) [hive-cli-4.0.0-alpha-2-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:200) [hive-cli-4.0.0-alpha-2-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:126) [hive-cli-4.0.0-alpha-2-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:421) [hive-cli-4.0.0-alpha-2-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:352) [hive-cli-4.0.0-alpha-2-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:727) [hive-it-util-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:697) [hive-it-util-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:114) [hive-it-util-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157) [hive-it-util-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
	at org.apache.hadoop.hive.cli.TestIcebergLlapLocalCliDriver.testCliDriver(TestIcebergLlapLocalCliDriver.java:60) [test-classes/:?]
...
{code}

this is because vertex status call falls back to relying on a dag rpc client -> server connection as it's not implemented correctly in TEZ-4236

tested on hive's current master with tez 0.10.2:
{code}
mvn test -Dtest.output.overwrite=true -Pitests,iceberg -Denforcer.skip=true -pl itests/qtest-iceberg -Dtest=TestIcebergLlapLocalCliDriver -Dqfile=llap_iceberg_read_orc.q,vectorized_iceberg_read_parquet.q
{code}",2022-09-08T13:19:47.965+0000,2022-09-14T10:44:33.625+0000,Fixed,Major
TEZ-983,Support a helper function to extract required additional tokens from a file,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12385659'>],"Oozie, etc that use Hive/Pig indirectly provide tokens to downstream applications by writing them into a file and expect the downstream application to read and use them.

Support a helper function for the above.",2014-03-27T21:19:28.530+0000,2014-03-30T08:01:42.846+0000,Fixed,Major
TEZ-1012,Add helper methods for MR Distributed Cache settings,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12385989'>],,2014-04-02T21:46:54.592+0000,2014-04-10T22:56:34.517+0000,,Major
FLINK-13718,Disable HBase tests,FLINK,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12572339'>, <JIRA IssueLink: id='12567889'>]","The HBase tests are categorically failing on Java 11. Given that HBase itself does not support Java 11 at this point we should just disable these tests for the time being.

{code}
HBaseConnectorITCase.activateHBaseCluster:81->HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189->HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader
  HBaseLookupFunctionITCase.activateHBaseCluster:95->HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189->HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader
  HBaseSinkITCase.activateHBaseCluster:91->HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189->HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader
{code}",2019-08-14T13:32:03.195+0000,2019-10-20T10:27:19.178+0000,Fixed,Major
SLIDER-930,Incorporate Yarn feature of resetting AM failure count into Slider AM,SLIDER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12435874'>],"YARN-611 provides this feature. Currently Slider apps are bound by the number set for yarn.resourcemanager.am.max-retries in the cluster. By default this value is set to 2, which is very low for long running services.

Slider AM should use the feature provided in YARN-611 and set an interval after which the failure count will be reset to 0.

I believe the API to call on ApplicationSubmissionContext is attemptFailuresValidityInterval. To start with Slider can set it to 5 mins which should be a reasonable default.",2015-08-28T18:09:48.831+0000,2015-11-23T17:19:36.011+0000,Fixed,Major
RATIS-272,LogService: Design ideal API,RATIS,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12539819'>],"With influence from Apache DistributedLog, Kafka, and BookKeeper, design an API that balances the ideal notion of what a distribute log system should look like, but also considers the needs of HBase to replace a WAL.",2018-07-26T16:15:28.927+0000,2018-09-07T22:07:19.874+0000,Fixed,Major
PARQUET-278,enforce non empty group on MessageType level,PARQUET,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12435010'>, <JIRA IssueLink: id='12435015'>]","As columnar format, parquet currently does not support empty struct/group without leaves. We should throw when constructing an empty GroupType to give a clear message.",2015-05-11T18:04:41.971+0000,2018-02-05T19:41:06.695+0000,Fixed,Major
DATAFU-14,Add NGram Tokenizer to datafu.pig.text.lucene,DATAFU,Improvement,Closed,[],1,[<JIRA IssueLink: id='12423018'>],"See https://github.com/rjurney/datafu/blob/lucene/src/java/datafu/pig/text/lucene/NGramTokenize.java

Held up by http://stackoverflow.com/questions/21064520/how-to-use-lucene-shinglefilter-could-not-find-implementing-class-for-org-apach/21067142?noredirect=1#21067142",2014-01-17T23:34:44.282+0000,2020-01-14T18:44:31.975+0000,Won't Do,Major
TEZ-391,SharedEdge - Support for passing same output from a vertex as input to two different vertices,TEZ,Sub-task,Open,[],3,"[<JIRA IssueLink: id='12385846'>, <JIRA IssueLink: id='12385844'>, <JIRA IssueLink: id='12388793'>]",  We need this for lot of usecases. For cases where multi-query is turned off and for optimizing unions. Currently those are BROADCAST or ONE-ONE edges and we write the output multiple times.,2013-08-23T18:52:11.301+0000,2017-03-14T03:40:23.249+0000,,Major
IMPALA-5267,test_seq_writer_hive_compatibility hits error running statement on Hive,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12503198'>],"The ASF master core build sees an error from Hive during the map reduce when performing the ""select count(*) from table"" portion of the test_seq_writer_hive_compatibility test. This may be a Hive bug, but we should track down whether there is anything about this test that is triggering it.

F query_test/test_compressed_formats.py::TestTableWriters::()::test_seq_writer_hive_compatibility[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none]
 query_test/test_compressed_formats.py:177: in test_seq_writer_hive_compatibility
     output = self.run_stmt_in_hive('select count(*) from %s' % table_name)
 common/impala_test_suite.py:609: in run_stmt_in_hive
     raise RuntimeError(stderr)
 E   RuntimeError: SLF4J: Class path contains multiple SLF4J bindings.
 E   SLF4J: Found binding in [jar:file:/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/cdh_components/hbase-1.2.0-cdh5.12.0-SNAPSHOT/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
 E   SLF4J: Found binding in [jar:file:/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/cdh_components/hadoop-2.6.0-cdh5.12.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
 E   SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 E   SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
 E   scan complete in 5ms
 E   Connecting to jdbc:hive2://localhost:11050
 E   Connected to: Apache Hive (version 1.1.0-cdh5.12.0-SNAPSHOT)
 E   Driver: Hive JDBC (version 1.1.0-cdh5.12.0-SNAPSHOT)
 E   Transaction isolation: TRANSACTION_REPEATABLE_READ
 E   INFO  : Compiling command(queryId=jenkins_20170501011717_5640b961-12ca-4ac9-a823-31d19af5b369): select count(*) from test_seq_writer_hive_compatibility_e3728f35.seq_tbl_GZIP_RECORD
 E   INFO  : Semantic Analysis Completed
 E   INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
 E   INFO  : Completed compiling command(queryId=jenkins_20170501011717_5640b961-12ca-4ac9-a823-31d19af5b369); Time taken: 0.173 seconds
 E   INFO  : Executing command(queryId=jenkins_20170501011717_5640b961-12ca-4ac9-a823-31d19af5b369): select count(*) from test_seq_writer_hive_compatibility_e3728f35.seq_tbl_GZIP_RECORD
 E   INFO  : Query ID = jenkins_20170501011717_5640b961-12ca-4ac9-a823-31d19af5b369
 E   INFO  : Total jobs = 1
 E   INFO  : Launching Job 1 out of 1
 E   INFO  : Starting task [Stage-1:MAPRED] in serial mode
 E   INFO  : Number of reduce tasks determined at compile time: 1
 E   INFO  : In order to change the average load for a reducer (in bytes):
 E   INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
 E   INFO  : In order to limit the maximum number of reducers:
 E   INFO  :   set hive.exec.reducers.max=<number>
 E   INFO  : In order to set a constant number of reducers:
 E   INFO  :   set mapreduce.job.reduces=<number>
 E   INFO  : number of splits:1
 E   INFO  : Submitting tokens for job: job_local220383829_0007
 E   INFO  : The url to track the job: http://localhost:8080/
 E   INFO  : Job running in-process (local Hadoop)
 E   INFO  : 2017-05-01 01:17:03,363 Stage-1 map = 0%,  reduce = 0%
 E   ERROR : Ended Job = job_local220383829_0007 with errors
 E   ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
 E   INFO  : MapReduce Jobs Launched: 
 E   INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
 E   INFO  : Total MapReduce CPU Time Spent: 0 msec
 E   INFO  : Completed executing command(queryId=jenkins_20170501011717_5640b961-12ca-4ac9-a823-31d19af5b369); Time taken: 2.767 seconds
 E   Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask (state=08S01,code=2)
 E   Closing: 0: jdbc:hive2://localhost:11050

In the hive logs, there is this error:
2017-05-01 01:17:02,873 FATAL mr.ExecMapper (ExecMapper.java:map(178)) - java.lang.IllegalStateException: Invalid input path hdfs://localhost:20500/test-warehouse/decimal_tbl/d6=1/decimal_tbl.txt
	at org.apache.hadoop.hive.ql.exec.MapOperator.getNominalPath(MapOperator.java:410)
	at org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:446)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1051)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:490)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:270)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2017-05-01 01:17:02,873 INFO  exec.MapOperator (Operator.java:close(595)) - 137 finished. closing... ",2017-05-01T21:13:56.642+0000,2017-05-10T18:22:08.057+0000,Duplicate,Major
ACCUMULO-3396,HDFS reads are hanging,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12403670'>],"On large clusters we are seeing various forms of HDFS reads hanging:

Queries that never return.
Major compactions that hang.

Accumulo 1.6.1 incorporates detectors that report hanging major compactions and a monitor display that reports scans by age.

Stack traces show readers in sun.nio.ch.EPollArrayWrapper.epollWait and in org.apache.hadoop.ipc.Client.Call(Client.java:1362).

Netstat results for the tablet server shows many connections with a single byte waiting on the Recv-Q of the process, and no bytes waiting on the Send-Q.

strace of the jvm shows the typical jvm thread noise (futex calls)

jstack shows lots of read-requests to the NN.

long-running MajC's do complete, albeit slowly.


",2014-12-09T18:37:00.136+0000,2014-12-16T16:40:57.971+0000,Duplicate,Blocker
SLIDER-183,Long lived application support,SLIDER,Umbrella,Resolved,[],3,"[<JIRA IssueLink: id='12390513'>, <JIRA IssueLink: id='12390514'>, <JIRA IssueLink: id='12471181'>]",Tracks all issues related to long lived application support in Slider.,2014-06-27T17:24:32.960+0000,2016-06-22T00:28:24.757+0000,Fixed,Major
YETUS-335,switch Yetus Dockerfile to use ruby 2.0,YETUS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12460882'>],"
{code}
Step 16 : RUN gem install rubocop --no-ri --no-rdoc
 ---> Running in f27550136210
Invalid gemspec in [/var/lib/gems/1.9.1/specifications/unicode-display_width-1.0.2.gemspec]: Illformed requirement [""< 3.0.0, >= 1.9.3""]
Invalid gemspec in [/var/lib/gems/1.9.1/specifications/unicode-display_width-1.0.2.gemspec]: Illformed requirement [""< 3.0.0, >= 1.9.3""]
Invalid gemspec in [/var/lib/gems/1.9.1/specifications/unicode-display_width-1.0.2.gemspec]: Illformed requirement [""< 3.0.0, >= 1.9.3""]
ERROR:  Error installing rubocop:
	rubocop requires unicode-display_width (>= 1.0.1, ~> 1.0)
The command '/bin/sh -c gem install rubocop --no-ri --no-rdoc' returned a non-zero code: 1
{code}
",2016-03-15T16:03:17.202+0000,2016-03-16T19:24:44.786+0000,Fixed,Blocker
RANGER-448,"HBase fix for scan tables issue, HBASE-13482, should be applied in Ranger HBase plugin",RANGER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12423395'>],"HBase fix for scan tables issue, in HBASE-13482, needs to be applied in Ranger HBase plugin as well.",2015-05-02T01:18:55.779+0000,2015-05-02T05:10:37.103+0000,Fixed,Major
INFRA-12493,Current jenkins/junit plugin is a bit buggy,INFRA,Improvement,Closed,[],1,[<JIRA IssueLink: id='12478498'>],"The currently installed jenkins/junit plugin(https://wiki.jenkins-ci.org/display/JENKINS/JUnit+Plugin) causes hive/ptest executions to report invalid run-times.

I've fixed the problem in that plugin; and opened a pullrequest, but I'm not sure when they will release a newer version of the plugin.

current options:

* optionally wait for the newer version...but I can't tell when they will be merging and releasing a new version (with the fix included)
* downgrade to junit 1.10 - i've checked..it works ""better"" with it
hpi packages are here:
http://updates.jenkins-ci.org/download/plugins/junit/
* build a hpi from the patched source at
https://github.com/kgyrtkirk/junit-plugin/tree/JENKINS-37598
using {{mvn package}}...but they have encountered some 1.x related issues lately... prior to the 1.18 release so I would be a bit skeptical skipping their release process in this case.

more details:
HIVE-14572",2016-08-24T23:20:10.748+0000,2017-06-17T07:31:25.686+0000,Fixed,Minor
SPARK-32502,Please fix CVE related to Guava 14.0.1,SPARK,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12596908'>, <JIRA IssueLink: id='12594906'>, <JIRA IssueLink: id='12594908'>]","Please fix the following CVE related to Guava 14.0.1
|cve|severity|cvss|
|CVE-2018-10237|medium|5.9|

 

Our security team is trying to block us from using spark because of this issue

 

One thing that's very weird is I see from this [pom file|[https://github.com/apache/spark/blob/v3.0.0/common/network-common/pom.xml]] you reference guava but it's not clear what version.

 

But if I look on [maven|[https://mvnrepository.com/artifact/org.apache.spark/spark-network-common_2.12/3.0.0]] the guava reference is not showing up

 

Is this reference somehow being shaded into the network common jar?  It's not clear to me.

 

Also, I've noticed code like [this file|[https://github.com/apache/spark/blob/v3.0.0/common/network-common/src/main/java/org/apache/spark/network/util/LimitedInputStream.java]] which is a copy-paste of some guava source code.

 

The CVE scanner we use Twistlock/Palo Alto Networks - Prisma Cloud Compute Edition is very thorough and will find CVEs in copy-pasted code and shaded jars.

 

Please fix this CVE so we can use spark",2020-07-30T19:40:05.752+0000,2020-08-16T17:24:53.263+0000,Duplicate,Major
TEZ-1030,Address intermittent errors created due to race condition in YARN-1915,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12387036'>],,2014-04-08T23:42:09.545+0000,2014-10-29T04:07:31.404+0000,Won't Fix,Major
TEZ-2669,Propagation of errors from plugins to the AM for error reporting,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12452263'>, <JIRA IssueLink: id='12455299'>]",,2015-07-30T20:08:11.268+0000,2016-05-18T04:57:56.284+0000,Fixed,Blocker
HCATALOG-630,org.apache.hcatalog.data.JsonSerDe cannot handle maps with numeric values ,HCATALOG,Bug,Open,[],1,[<JIRA IssueLink: id='12444537'>],"Steps to replicate:

1) Create a Hive table

CREATE TABLE test_map (test map<STRING,INT> )
ROW FORMAT SERDE 'org.apache.hcatalog.data.JsonSerDe'
STORED AS TEXTFILE;

2) Load the data:

LOAD DATA LOCAL INPATH '/home/dvasilen/Downloads/QA_TEST/test_map.json' OVERWRITE INTO TABLE test_map;  

3) The test_map.json contains the single line:

{""test"":{""key"":1}}

4) Run ""select * from test_map;"" from Hive client

5) Note the following exception being thrown:

13/03/13 13:34:20 ERROR CliDriver: Failed with exception java.io.IOException:org.apache.hadoop.hive.serde2.SerDeException: org.codehaus.jackson.JsonParseException: Current token (FIELD_NAME) not numeric, can not use numeric value accessors
 at [Source: java.io.ByteArrayInputStream@2be32be3; line: 1, column: 17]
java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: org.codehaus.jackson.JsonParseException: Current token (FIELD_NAME) not numeric, can not use numeric value accessors
 at [Source: java.io.ByteArrayInputStream@2be32be3; line: 1, column: 17]
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:521)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:466)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1387)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:270)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:613)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:600)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: org.apache.hadoop.hive.serde2.SerDeException: org.codehaus.jackson.JsonParseException: Current token (FIELD_NAME) not numeric, can not use numeric value accessors
 at [Source: java.io.ByteArrayInputStream@2be32be3; line: 1, column: 17]
	at org.apache.hcatalog.data.JsonSerDe.deserialize(JsonSerDe.java:171)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:506)
	... 13 more
Caused by: org.codehaus.jackson.JsonParseException: Current token (FIELD_NAME) not numeric, can not use numeric value accessors
 at [Source: java.io.ByteArrayInputStream@2be32be3; line: 1, column: 17]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1291)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:385)
	at org.codehaus.jackson.impl.JsonNumericParserBase._parseNumericValue(JsonNumericParserBase.java:399)
	at org.codehaus.jackson.impl.JsonNumericParserBase.getIntValue(JsonNumericParserBase.java:254)
	at org.apache.hcatalog.data.JsonSerDe.extractCurrentField(JsonSerDe.java:218)
	at org.apache.hcatalog.data.JsonSerDe.extractCurrentField(JsonSerDe.java:279)
	at org.apache.hcatalog.data.JsonSerDe.populateRecord(JsonSerDe.java:188)
	at org.apache.hcatalog.data.JsonSerDe.deserialize(JsonSerDe.java:167)
	... 14 more


 
If you change the value type to string 

CREATE TABLE test_map (test map<STRING,STRING> )
ROW FORMAT SERDE 'org.apache.hcatalog.data.JsonSerDe'
STORED AS TEXTFILE;


and 

{""test"":{""key"":""string""}}

then it works correctly.

I see the same issue in 0.4.0 as well as 0.5.0.

",2013-03-13T18:36:55.603+0000,2015-10-01T23:21:33.922+0000,,Major
PHOENIX-6734,Revert default HBase version to 2.4.10,PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12644255'>],"Revert default HBase version to 2.4.10 to avoid the paging bug in 2.4.11 and 2.4.12.
We can go back to 2.4.13 when it's released.",2022-06-16T08:01:17.524+0000,2022-07-21T17:36:01.943+0000,Fixed,Major
TEZ-4137,Input/Output/Processor should merge payload to local conf,TEZ,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12586493'>],"This patch introduces config merging to various Input and Output processors. As described in https://issues.apache.org/jira/browse/TEZ-4073 , we need to reduce the size of the configuration objects transferred over the wire. There are two improvements we are planning to do regarding to that:
 # Skip sending default configs and configuration coming from xml files in payload
 # Send dag, vertex and session configurations in layers instead of sending dag + vertex + session configs all together three times.

In order to achieve these,
 * We need to expose local config on Task side through TaskContext.
 * Input/Output/Processors must merge the config from user payload to local config in their TaskContext

Since runtime components did not have access to local config before, tez clients sent all config required at runtime in user payload. After this change, tez clients can reduce their payload size.",2020-04-01T06:53:28.383+0000,2020-06-12T05:11:46.495+0000,Fixed,Major
PARQUET-1178,Parquet modular encryption,PARQUET,New Feature,Resolved,"[<JIRA Issue: key='PARQUET-1227', id='13139610'>, <JIRA Issue: key='PARQUET-1228', id='13139615'>, <JIRA Issue: key='PARQUET-1229', id='13139617'>, <JIRA Issue: key='PARQUET-1232', id='13139681'>, <JIRA Issue: key='PARQUET-1286', id='13156302'>, <JIRA Issue: key='PARQUET-1398', id='13180196'>, <JIRA Issue: key='PARQUET-1401', id='13180488'>, <JIRA Issue: key='PARQUET-1419', id='13186344'>, <JIRA Issue: key='PARQUET-1450', id='13194095'>, <JIRA Issue: key='PARQUET-1477', id='13204423'>, <JIRA Issue: key='PARQUET-1618', id='13244030'>, <JIRA Issue: key='PARQUET-1619', id='13244068'>, <JIRA Issue: key='PARQUET-1807', id='13287953'>, <JIRA Issue: key='PARQUET-1884', id='13316383'>]",9,"[<JIRA IssueLink: id='12546815'>, <JIRA IssueLink: id='12615613'>, <JIRA IssueLink: id='12580272'>, <JIRA IssueLink: id='12598285'>, <JIRA IssueLink: id='12540805'>, <JIRA IssueLink: id='12547935'>, <JIRA IssueLink: id='12541277'>, <JIRA IssueLink: id='12560140'>, <JIRA IssueLink: id='12541520'>]","A mechanism for modular encryption and decryption of Parquet files. Allows to keep data fully encrypted in the storage - while enabling efficient analytics on the data, via reader-side extraction / authentication / decryption of data subsets required by columnar projection and predicate push-down.

Enables fine-grained access control to column data by encrypting different columns with different keys.

Supports a number of encryption algorithms, to account for different security and performance requirements.",2017-12-20T15:29:44.470+0000,2021-05-17T03:02:33.844+0000,Done,Major
ORC-906,Upgrade branch-1.6 to storage-api 2.7.3,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12620945'>, <JIRA IssueLink: id='12620944'>]",Upgrade branch-1.6 to storage-api 2.7.3.,2021-08-03T22:50:54.033+0000,2021-08-12T16:52:41.326+0000,Fixed,Major
HCATALOG-237,Switch from using StorageDrivers to SerDes to do data (de)serialization,HCATALOG,Improvement,Closed,"[<JIRA Issue: key='HCATALOG-239', id='12540244'>, <JIRA Issue: key='HCATALOG-240', id='12540245'>, <JIRA Issue: key='HCATALOG-241', id='12540246'>, <JIRA Issue: key='HCATALOG-242', id='12540247'>, <JIRA Issue: key='HCATALOG-243', id='12540248'>, <JIRA Issue: key='HCATALOG-249', id='12540772'>, <JIRA Issue: key='HCATALOG-252', id='12541141'>, <JIRA Issue: key='HCATALOG-258', id='12542799'>, <JIRA Issue: key='HCATALOG-259', id='12542800'>, <JIRA Issue: key='HCATALOG-265', id='12543122'>, <JIRA Issue: key='HCATALOG-268', id='12543433'>, <JIRA Issue: key='HCATALOG-282', id='12544689'>, <JIRA Issue: key='HCATALOG-283', id='12544692'>, <JIRA Issue: key='HCATALOG-279', id='12544353'>, <JIRA Issue: key='HCATALOG-285', id='12544855'>, <JIRA Issue: key='HCATALOG-289', id='12545023'>, <JIRA Issue: key='HCATALOG-295', id='12545397'>, <JIRA Issue: key='HCATALOG-308', id='12546218'>]",3,"[<JIRA IssueLink: id='12347700'>, <JIRA IssueLink: id='12348627'>, <JIRA IssueLink: id='12348626'>]","HCatalog started by creating its own classes, InputStorageDriver and OutputStorageDriver, to do data conversion between the storage layer Input/OutputFormats and the HCatInput/OutputFormats.  These provide very similar functionality to Hive's SerDe class, though with a much simpler interface.

This usage of separate classes has led to a number of issues for HCatalog.  One, it cannot make use of existing Hive SerDes.  Two, it has led to a need to make HCat specific extensions of Hive interfaces (such as the StorageHandler) to provide the StorageDescriptors.  Three, it means that users who already have Hive installed cannot use HCatalog without first updating every partition in their metastore with storage driver information.

I propose we switch to using SerDes for this.  To address the issue of the more complicated SerDe interface we can provide adaptor classes that make writing new SerDes easy in simple cases.",2012-01-27T21:08:12.593+0000,2012-05-17T01:20:25.486+0000,Fixed,Major
SPARK-23831,Add org.apache.derby to IsolatedClientLoader,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12559450'>],"Add org.apache.derby to IsolatedClientLoader，otherwise it may throw an exception:
{noformat}
[info] Cause: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2439ab23, see the next exception for details.
[info] at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
[info] at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
[info] at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)
[info] at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)
[info] at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)
[info] at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)
{noformat}
How to reproduce:
{noformat}
build/sbt clean package -Phive -Phive-thriftserver
export SPARK_PREPEND_CLASSES=true
bin/spark-sql --conf spark.sql.hive.metastore.version=2.3.4 --conf spark.sql.hive.metastore.jars=maven -e ""create table t1 as select 1 as c""
{noformat}",2018-03-30T10:38:29.083+0000,2019-04-26T02:19:34.816+0000,Fixed,Major
SQOOP-1215,Add HBase bulk load tests,SQOOP,Bug,Open,[],2,"[<JIRA IssueLink: id='12376668'>, <JIRA IssueLink: id='12376667'>]","We've added support for HBase bulk load in SQOOP-1032, however due to HBASE-7744 we did not introduced automatic tests. We should do that as soon as HBASE-7744 will get resolved.",2013-10-10T23:27:26.030+0000,2013-10-10T23:28:14.982+0000,,Major
THRIFT-4889,Add SASL support for non-blocking server,THRIFT,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12563525'>, <JIRA IssueLink: id='12571575'>]","Currently, it seems the sasl is available only for blocking server. But in some circumstances the server cannot handle the workload/nb of connections with the blocking IO, we need SASL support to secure the non-blocking IO servers in such situations.",2019-06-18T09:55:17.638+0000,2021-02-11T22:26:02.102+0000,Fixed,Major
TEZ-719,Add a memory manager for Containers,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12380894'>],"Tasks running in containers typically have multiple Inputs / Outputs.

At least the current set of Inputs / Outputs assume they have the entire JVM memory available to them.

A container level memory controller could serve requests from each of these Inputs / Outputs - and potentially the Processor itself at some point.

The current set of Inputs / Outputs also need to change to work with this / ensure they have limited JVM resources.
This could be configured by the DAG submitter itself - but the framework (for it's own library pieces at least) could determine this.",2014-01-09T23:36:12.354+0000,2014-01-09T23:43:27.664+0000,,Major
YETUS-636,Test Patch should track consumed cli options and note unused ones,YETUS,Wish,Resolved,[],2,"[<JIRA IssueLink: id='12560845'>, <JIRA IssueLink: id='12534210'>]","It'd be nice if either test-patch or the core library could track which cli options get consumed (either by the specific tool, core, or plugins) and at debug level note which ones weren't recognized.

Doing so would have made it easier in HBASE-20591 to recognize the gap YETUS-635 is trying to fill. I imagine there will be other similar failures in the same category.",2018-05-16T17:57:55.363+0000,2019-05-15T20:28:11.098+0000,Implemented,Minor
PARQUET-164,Warn when parquet memory manager kicks in,PARQUET,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12409725'>, <JIRA IssueLink: id='12404979'>, <JIRA IssueLink: id='12404977'>, <JIRA IssueLink: id='12404976'>]",In PARQUET-108 we implemented a memory manager for parquet. I think we should warn in the close() method if we had to adjust the memory down so that users know their memory is being adjusted down and block size will be less than expected.,2015-01-09T02:51:22.965+0000,2015-05-19T18:27:52.875+0000,Fixed,Major
TWILL-87,Adding Container Placement control (Placement Policy API) ,TWILL,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12390560'>, <JIRA IssueLink: id='12486852'>, <JIRA IssueLink: id='12424465'>]","Yarn AMRMClient provides API to control container placement. We need to enhance Twill API so that user can specify container placement policies. Twill could use AMRMClient to try allocating containers according to specified placement policy.

Added Placement Policy API in TwillSpecification. For now, the Placement policy type includes: (a) DISTRIBUTED, which tries to spawn specified runnables on different hosts.  (b) DEFAULT, i.e. no special placement policy requirements.

Implementation Detail: The DISTRIBUTED runnable instances are provisioned one after another (as opposed to grouping provision requests in one allocate call based on ResourceSpecs). AM blacklists the hosts on which existing DISTRIBUTED runnables are running. If no container is provisioned for MAX_CONSTRAINED_PROVISION_ATTEMPTS number of attempts, AM relaxes blacklist constraints (or any other constraint). 

Also, it make sense to specify Hosts and Racks through Placement Policy API instead of using Resource Specification. So, moved that logic into Placement Policy too.
    

Tests
    1. Added unit tests to test Placement Policy (using MiniYarnCluster)
          (a) Specify DISTRIBUTED runnables, runnables with Hosts and Racks in a Twill App and verify all constraints and appropriately honored.
          (b) Specify DISTRIBUTED and DEFAULT runnables in a Twill App and verify all constraints and appropriately honored. Increase number of instances for all runnables and verify all constraints and appropriately honored.
          (c) Tested DISTRIBUTED placement policy under stress (i.e. not enough resources available to honor constraints). Verify that AM relaxes constraints and try it's best.  

    2. Tested on a real Cluster.
         

Please review the API and changes in PR - https://github.com/apache/incubator-twill/pull/7
    
  



",2014-06-18T22:19:45.846+0000,2016-11-17T21:36:30.337+0000,Fixed,Major
ZEPPELIN-2221,Spark jobs UI from the paragraph is broken in YARN,ZEPPELIN,Bug,Closed,[],3,"[<JIRA IssueLink: id='12533071'>, <JIRA IssueLink: id='12503835'>, <JIRA IssueLink: id='12503744'>]","Discussed on PR-1663 for ZEPPELIN-1692
https://github.com/apache/zeppelin/pull/1663#issuecomment-283477396

When we click on any of Spark jobs UI from the paragraph links, 
link is leading to
{noformat}
http://hostname.domain.com:8088/proxy/application_1488384993892_0001/jobs/job/

and that error shows HTTP ERROR 400

Page reads

Problem accessing /jobs/job/.

Reason: requirement failed: Missing id parameter
Powered by Jetty://
{noformat}

A little bit more information - the link on paragprah leads to
{noformat}
http://10.20.32.57:28009/jobs/job?id=123

 Spark Driver UI.
Spark Driver web server then redirects to a link like I sent in my previous post,
and it misses job id in the redirected URL, i.e.

http://host.domain.com:8088/proxy/application_1488384993892_0044/jobs/job/
To your question on consistency - yes it happens in 100% cases, I never saw this new feature works for us.
Is this Spark version dependent or something? We're running CDH Spark 2.

{noformat}

According to https://github.com/apache/spark/pull/5947 URL format is different in YARN and non-YARN modes? Was PR-1663 for ZEPPELIN-1692 tested on both of these modes? Not sure what else might break those links.",2017-03-06T19:48:54.961+0000,2020-12-24T03:20:16.205+0000,Fixed,Major
OOZIE-1178,Workflow Application Master in YARN,OOZIE,New Feature,Open,[],4,"[<JIRA IssueLink: id='12471959'>, <JIRA IssueLink: id='12355662'>, <JIRA IssueLink: id='12355649'>, <JIRA IssueLink: id='12355650'>]","It is useful to have a workflow application master, which will be capable of running a DAG of jobs. The workflow client submits a DAG request to the AM and then the AM will manage the life cycle of this application in terms of requesting the needed resources from the RM, and starting, monitoring and retrying the application's individual tasks.

Compared to running Oozie with the current MapReduce Application Master, these are some of the advantages:
 - Less number of consumed resources, since only one application master will be spawned for the whole workflow.
 - Reuse of resources, since the same resources can be used by multiple consecutive jobs in the workflow (no need to request/wait for resources for every individual job from the central RM).
 - More optimization opportunities in terms of collective resource requests.
 - Optimization opportunities in terms of rewriting and composing jobs in the workflow (e.g. pushing down Mappers).
 - This Application Master can be reused/extended by higher systems like Pig and hive to provide an optimized way of running their workflows.",2012-07-30T20:57:49.434+0000,2018-05-25T13:35:51.623+0000,,Major
CASSANDRA-3250,fsync the directory after new sstable or commit log segment are created,CASSANDRA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12379890'>],"The mannual of fsync said:
bq.   Calling  fsync()  does  not  necessarily  ensure  that  the entry in the directory containing the file has also reached disk.  For that an explicit fsync() on a file descriptor for the directory is also needed.

At least on ext4, syncing the directory is a must to have step, as described by [1]. Otherwise, the new sstables or commit logs could be missed after crash even if itself is synced. 

Unfortunately, JVM does not provide an approach to sync the directory...

[1] http://www.linuxfoundation.org/news-media/blogs/browse/2009/03/don%E2%80%99t-fear-fsync
",2011-09-23T14:39:16.657+0000,2019-04-16T09:32:50.389+0000,Fixed,Low
CALCITE-2077,"Druid adapter: Use ""scan"" query rather than ""select"" query",CALCITE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12521384'>, <JIRA IssueLink: id='12521385'>]","The biggest difference between select query and scan query is that, scan query doesn't retain all rows in memory before rows can be returned to client.
Select query on very large segments will cause memory pressure on druid historicals if too many rows required by select query whereas Scan query doesn't have this issue.
Scan query can also return all rows without issuing another pagination query, which is extremely useful when query against historical or realtime node directly.
",2017-12-03T07:43:43.879+0000,2018-03-17T17:43:19.781+0000,Fixed,Major
SPARK-24472,Orc RecordReaderFactory throws IndexOutOfBoundsException,SPARK,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12535919'>, <JIRA IssueLink: id='12582770'>, <JIRA IssueLink: id='12581542'>]","When the column number of the underlying file schema is greater than the column number of the table schema, Orc RecordReaderFactory will throw IndexOutOfBoundsException. ""spark.sql.hive.convertMetastoreOrc"" should be turned off to use HiveTableScanExec. Here is a reproducer:

{code}
scala> :paste
// Entering paste mode (ctrl-D to finish)

Seq((""abc"", 123, 123L)).toDF(""s"", ""i"", ""l"").write.partitionBy(""i"").format(""orc"").mode(""append"").save(""/tmp/orctest"")

spark.sql(""""""
CREATE EXTERNAL TABLE orctest(s string)
PARTITIONED BY (i int)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
WITH SERDEPROPERTIES (
  'serialization.format' = '1'
)
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
LOCATION '/tmp/orctest'
"""""")

spark.sql(""msck repair table orctest"")

spark.sql(""set spark.sql.hive.convertMetastoreOrc=false"")


// Exiting paste mode, now interpreting.

18/06/05 15:34:52 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> spark.read.format(""orc"").load(""/tmp/orctest"").show()
+---+---+---+
|  s|  l|  i|
+---+---+---+
|abc|123|123|
+---+---+---+


scala> spark.sql(""select * from orctest"").show()
18/06/05 15:34:59 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.IndexOutOfBoundsException: toIndex = 2
	at java.util.ArrayList.subListRangeCheck(ArrayList.java:1004)
	at java.util.ArrayList.subList(ArrayList.java:996)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.getSchemaOnRead(RecordReaderFactory.java:161)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.createTreeReader(RecordReaderFactory.java:66)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:202)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1215)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1113)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/06/05 15:34:59 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.IndexOutOfBoundsException: toIndex = 2
	at java.util.ArrayList.subListRangeCheck(ArrayList.java:1004)
	at java.util.ArrayList.subList(ArrayList.java:996)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.getSchemaOnRead(RecordReaderFactory.java:161)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.createTreeReader(RecordReaderFactory.java:66)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:202)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1215)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1113)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

18/06/05 15:34:59 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.IndexOutOfBoundsException: toIndex = 2
	at java.util.ArrayList.subListRangeCheck(ArrayList.java:1004)
	at java.util.ArrayList.subList(ArrayList.java:996)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.getSchemaOnRead(RecordReaderFactory.java:161)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.createTreeReader(RecordReaderFactory.java:66)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:202)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1215)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1113)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1600)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1588)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1587)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1821)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1770)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1759)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2051)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2070)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3285)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2496)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2496)
  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3266)
  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3265)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2496)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2710)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:733)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:692)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:701)
  ... 51 elided
Caused by: java.lang.IndexOutOfBoundsException: toIndex = 2
  at java.util.ArrayList.subListRangeCheck(ArrayList.java:1004)
  at java.util.ArrayList.subList(ArrayList.java:996)
  at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.getSchemaOnRead(RecordReaderFactory.java:161)
  at org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.createTreeReader(RecordReaderFactory.java:66)
  at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:202)
  at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
  at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)
  at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)
  at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1215)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1113)
  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)
  at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)
  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)
  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:109)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
{code}

The issue is [HiveTableScanExec|https://github.com/apache/spark/blob/93df3cd03503fca7745141fbd2676b8bf70fe92f/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScanExec.scala#L138] sets ""serdeConstants.LIST_COLUMNS"" and trigger [this line in RecordReaderFactory|https://github.com/apache/hive/blob/release-1.2.1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderFactory.java#L161]
",2018-06-05T22:45:27.456+0000,2020-03-10T08:55:55.430+0000,Duplicate,Major
FLUME-2199,Flume builds with new version require mvn install before site can be generated,FLUME,Bug,Patch Available,[],1,[<JIRA IssueLink: id='12377808'>],"At this point, if you change the version for Flume, you need to run a mvn install before you can run with -Psite (or, for that matter, javadoc:javadoc) enabled. This is because the top-level POM in flume.git/pom.xml is both the parent POM and the root of the reactor - since it's the parent, it's got to run before any of the children that inherit from it, but site generation should be running *after* all the children, so that it probably pulls in the reactor's build of each child module, rather than having to pull in one already installed/deployed before the build starts.

There are a bunch of other reasons to split parent POM and top-level POM, but that's the biggest one right there. 

Also, the javadoc jar generation is a bit messed up - every module's javadoc jar contains not only its own javadocs but the javadocs for every Flume module it depends on. That, again, may make sense in a site context for the top-level, but not for the individual modules. This results in unnecessary bloat in the javadoc jars, and unnecessary time spent downloading the ""*-javadoc-resources.jar"" for every dependency each module has, due to how the javadoc plugin works. Also the whole site generation per-module thing, which I am not a fan of in most cases. I don't think it's needed here. Tweaking the site plugin not to run anywhere but the top-level and the javadoc plugin to not do the dependency aggregation anywhere but the top-level should make a big difference on build speed.",2013-09-23T22:54:15.130+0000,2015-05-04T21:49:55.123+0000,,Major
SENTRY-432,Synchronization of HDFS permissions with Sentry permissions,SENTRY,New Feature,Resolved,"[<JIRA Issue: key='SENTRY-433', id='12741073'>, <JIRA Issue: key='SENTRY-434', id='12741075'>, <JIRA Issue: key='SENTRY-435', id='12741076'>, <JIRA Issue: key='SENTRY-436', id='12741077'>, <JIRA Issue: key='SENTRY-437', id='12741078'>, <JIRA Issue: key='SENTRY-438', id='12741079'>, <JIRA Issue: key='SENTRY-439', id='12741080'>]",1,[<JIRA IssueLink: id='12396635'>],"An HDFS file or directory that is associated with an Authorizable Object managed by Sentry (Such as a HiveMetaStore table partition, a Solr/Search collection/document or an HBase Table etc.) should have the permissions that reflect those that were granted/revoked via Sentry. 

This logic should be enforced by a Sentry Authorization Plugin which would be an implementation of the HDFS AuthorizationProvider as described in [HDFS-6826|https://issues.apache.org/jira/browse/HDFS-6826]

This is an umbrella JIRA",2014-09-12T04:59:32.257+0000,2015-05-29T01:49:53.170+0000,Fixed,Major
SPARK-4921,TaskSetManager mistakenly returns PROCESS_LOCAL for NO_PREF tasks,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12404045'>, <JIRA IssueLink: id='12404046'>]","During research for HIVE-9153, we found that TaskSetManager returns PROCESS_LOCAL for NO_PREF tasks, which may caused performance degradation. Changing the return value to NO_PREF, as demonstrated in the attached patch, seemingly improves the performance.",2014-12-22T18:35:10.714+0000,2017-05-23T16:28:12.235+0000,Won't Fix,Major
LENS-518,Move Hive dependency to Apache Hive,LENS,Wish,Closed,[],32,"[<JIRA IssueLink: id='12431038'>, <JIRA IssueLink: id='12433404'>, <JIRA IssueLink: id='12433405'>, <JIRA IssueLink: id='12433409'>, <JIRA IssueLink: id='12464173'>, <JIRA IssueLink: id='12464172'>, <JIRA IssueLink: id='12433410'>, <JIRA IssueLink: id='12433791'>, <JIRA IssueLink: id='12433792'>, <JIRA IssueLink: id='12434069'>, <JIRA IssueLink: id='12433407'>, <JIRA IssueLink: id='12433400'>, <JIRA IssueLink: id='12422069'>, <JIRA IssueLink: id='12433399'>, <JIRA IssueLink: id='12453362'>, <JIRA IssueLink: id='12453363'>, <JIRA IssueLink: id='12464174'>, <JIRA IssueLink: id='12464171'>, <JIRA IssueLink: id='12464175'>, <JIRA IssueLink: id='12422068'>, <JIRA IssueLink: id='12453353'>, <JIRA IssueLink: id='12457382'>, <JIRA IssueLink: id='12466103'>, <JIRA IssueLink: id='12453071'>, <JIRA IssueLink: id='12457374'>, <JIRA IssueLink: id='12457376'>, <JIRA IssueLink: id='12457377'>, <JIRA IssueLink: id='12433401'>, <JIRA IssueLink: id='12458735'>, <JIRA IssueLink: id='12457375'>, <JIRA IssueLink: id='12457869'>, <JIRA IssueLink: id='12467328'>]",Creating wish for moving to Apache hive dependency instead of forked hive dependency. We will create follow up issues in lens and hive make them link here.,2015-04-22T08:37:12.778+0000,2016-08-18T09:21:22.295+0000,Fixed,Major
PHOENIX-5219,Fix ConcurrentMutationsIT testLockUntilMVCCAdvanced and testRowLockDuringPreBatchMutateWhenIndexed failures on the master branch,PHOENIX,Test,Closed,[],1,[<JIRA IssueLink: id='12558023'>],"These tests are failing because when we set the timestamp in {{Indexer.preBatchMutateWithExceptions}} we call {{KeyValueUtil.ensureKeyValues}} on the list of cells and then call {{KeyValue.getTimestampOffset}}

Pre 2.0 the cells objects are NoTagsKeyValue (which extend KeyValue). In 2.0 they are NoTagsByteBufferKeyValue (which doesn't extend KeyValue) so {{KeyValueUtil.ensureKeyValues}} returns a new list of objects on which we set the timestamp. 
The fix us to use  {{CellUtil.setTimestamp}} to the set the timestamp directly on the cell. ",2019-03-28T23:36:05.795+0000,2021-02-10T10:00:56.756+0000,Fixed,Major
PHOENIX-2124,Hybrid Logical Clocks for Phoenix,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12430973'>],"Over at the HBase land, we have a proposal to use HLC for HBase updates. I think that we can also use them in Phoenix, so that we have a strictly monotonic clock and timestamps become reliable. I believe that HL clocks have  the potential to eliminate clock-skew problems that Phoenix is susceptible for. 

I have to spend some more time to fill up the Phoenix part of the proposal to see what exactly needs to be changed. But please feel free to suggest and comment on whether this makes sense for Phoenix. 

Below is the proposal for HBASE-14070: 
HBase and Phoenix uses systems physical clock (PT) to give timestamps to events (read and writes). This works mostly when the system clock is strictly monotonically increasing and there is no cross-dependency between servers clocks. However we know that leap seconds, general clock skew and clock drift are in fact real.
This jira proposes using Hybrid Logical Clocks (HLC) as an implementation of hybrid physical clock + a logical clock. HLC is best of both worlds where it keeps causality relationship similar to logical clocks, but still is compatible with NTP based physical system clock. HLC can be represented in 64bits.
A design document is attached and also can be found here: 
https://docs.google.com/document/d/1LL2GAodiYi0waBz5ODGL4LDT4e_bXy8P9h6kWC05Bhw/edit#
",2015-07-15T22:03:00.382+0000,2015-07-15T22:03:22.533+0000,,Major
AMBARI-2940,"After restarting YARN, the number of lost nodes is incorrect",AMBARI,Task,Resolved,[],1,[<JIRA IssueLink: id='12373989'>],YARNs lost node count is not accurate. Hence Ambari has to dynamically calculate it.,2013-08-16T22:12:54.702+0000,2013-08-16T22:21:26.097+0000,Fixed,Major
DRILL-4271,Drill query does not return all results from HBase when the Hbase contains more than 6 columns,DRILL,Bug,Resolved,[],1,[<JIRA IssueLink: id='12461421'>],"Hbase `test1` contains 1 column family & 5 columns, 10000000 rows
Hbase `test2` contains 1 column family & 6 columns, 10000000 rows

Query: select count ( * ) from `hbase`.`test1`
Result:  10000000 rows

Query: select count ( * ) from `hbase`.`test2`
Result:  6724 rows

Note:
Correct row count returned with the hbase table with 1 column family 5 or less columns

",2016-01-14T02:14:01.591+0000,2016-03-21T16:23:02.311+0000,Duplicate,Critical
PHOENIX-4693,Force random miniHBaseCluster ports,PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12532060'>],"Ankit found that lots of ITs are failing due to port-conflicts

This lead me to find HBASE-20224 which changed the default value of {{hbase.localcluster.assign.random.ports}} from true to false.

Unless HBase changes this back, we need to forcibly set this property in Phoenix, otherwise our parallel tests will fail.",2018-04-18T16:53:54.050+0000,2018-07-26T01:15:44.360+0000,Fixed,Blocker
TEZ-2851,Support a way for upstream applications to pass in a caller context to Tez,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12447155'>],,2015-09-22T21:35:37.946+0000,2015-10-27T04:59:11.223+0000,Fixed,Major
SLIDER-67,Slider not building against Hadoop trunk,SLIDER,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12388315'>, <JIRA IssueLink: id='12388314'>]","Slider doesn't build against hadoop trunk, HDFS-6181 renamed some constants -HDFS-6418 flags this.

We could just inline the field, but that would hide the regression, a regression others may encounter

",2014-05-19T10:45:00.069+0000,2014-07-10T19:52:42.409+0000,Fixed,Major
BIGTOP-2342,Set yarn.log.server.url to point to JH server,BIGTOP,Bug,Closed,[],1,[<JIRA IssueLink: id='12457754'>],"Bigtop sets up a MapReduce JobHistoryServer, but YARN Nodemanager is unaware of the existence of the MapReduce JobHistoryServer when a MapReduce job completes.  The NodeManager UI uses the undocumented yarn.log.server.url yarn-site.xml property to point back to the JobHistoryServer Web UI for completed jobs.



",2016-02-20T01:03:53.023+0000,2017-03-29T18:46:14.445+0000,Fixed,Major
BIGTOP-3347,Fix failures of Hive ProxyFileSystem against Hadoop 2.10.0,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12586850'>, <JIRA IssueLink: id='12586852'>]","Tests using {{pfile://...}} path fails with ""Wrong FS error"" like HIVE-6244 and HIVE-21683. The cause seems to be changes of FilterFileSystem in Hadoop side. While HIVE-21683 is proposing to use javaassist for covering all methods, We should apply fix for the hive-2.3.6 and hadoop-2.10.0 ahead for bigtop-1.5.0 release.",2020-04-27T06:30:00.834+0000,2020-05-05T15:55:40.092+0000,Fixed,Major
ORC-389,Add ability to not decode Acid metadata columns,ORC,Improvement,Closed,[],1,[<JIRA IssueLink: id='12539523'>],"for example, the split is from base_x and there are no relevant delete_delta/",2018-07-25T19:38:00.686+0000,2018-09-25T21:21:49.037+0000,Fixed,Major
AMBARI-11192,The Default hdfs-site.xml Should Have Client Retry Logic Enabled For Rolling Upgrade - reverted,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12461262'>],"Install HDP 2.2.0 (champlain using Ambari 2.1)
Register and Install new version of HDP Dal
Click on Perform Upgrade

Observed Error on UI:
Reason: The hdfs-site.xml property dfs.client.retry.policy.enabled should be set to true.
Failed on: HDFS",2015-05-16T22:38:20.678+0000,2016-03-18T22:22:10.939+0000,Not A Problem,Blocker
INFRA-22743,Upload snapshots from GitHub Actions,INFRA,Github Integration,Closed,[],1,[<JIRA IssueLink: id='12631342'>],,2022-01-16T01:12:06.993+0000,2022-03-10T17:26:35.549+0000,Fixed,Major
INFRA-8338,YARN issues lack release notes,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12396578'>],"Comparing the HADOOP project JIRAs, it looks like YARN is missing the release note field.  Looking through the admin section, I can't see why it isn't there though.  It'd be fab to get a release note field so that our release notes generator actually works properly.",2014-09-11T15:43:23.405+0000,2014-10-13T08:30:45.734+0000,Fixed,Major
PHOENIX-1250,guava dependency out-dated causing NoSuchMethod error,PHOENIX,Bug,Closed,[],1,[<JIRA IssueLink: id='12396667'>],"Phoenix has a dependency on Guava 12.0.1 (released in 2012). My project uses Guava 16.0.1. The latest version is 18. Apparently, the Closeables.closeQuitely(Closeable) no longer exists in 16.0.1 and later. Possibly earlier versions. 

I get the error below when attempting to get a connection. This is blocking any attempts to integrate Phoenix because I cannot downgrade our project dependency to a 2 year old version. 

Please review all project dependencies and consider uprading to more recent versions.

java.sql.SQLException: java.lang.NoSuchMethodError: com.google.common.io.Closeables.closeQuietly(Ljava/io/Closeable;)V
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.metaDataCoprocessorExec(ConnectionQueryServicesImpl.java:947)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1144)
	at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:114)
	at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:1315)
	at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:445)
	at org.apache.phoenix.compile.CreateTableCompiler$2.execute(CreateTableCompiler.java:183)
	at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:256)
	at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:248)
	at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
	at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:246)
	at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:960)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl$9.call(ConnectionQueryServicesImpl.java:1519)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl$9.call(ConnectionQueryServicesImpl.java:1489)
	at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:77)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:1489)
	at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:162)
	at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.connect(PhoenixEmbeddedDriver.java:129)
	at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:133)
	at java.sql.DriverManager.getConnection(DriverManager.java:571)
	at java.sql.DriverManager.getConnection(DriverManager.java:187)
",2014-09-12T10:59:27.841+0000,2015-11-21T02:15:42.126+0000,Fixed,Major
PARQUET-247,Add DATE mapping in ValidTypeMap of filter2,PARQUET,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12420666'>, <JIRA IssueLink: id='12423197'>, <JIRA IssueLink: id='12569317'>]","When Hive use Parquet filter predicate, the Date type is converted to Integer. In {{ValidTypeMap}}, it map the class and Parquet type. It throw exception when checking the data type Date.

We should add the map to support Date.",2015-04-08T08:19:32.061+0000,2019-09-05T07:55:14.575+0000,Duplicate,Major
CASSANDRA-1131,Towards A Formalization Of The Cassandra Daemon,CASSANDRA,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12332370'>, <JIRA IssueLink: id='12332514'>]","Right now, we've two implementations of the Cassandra daemon, one that's based on Thrift, and the other on Avro, both of which suffer from a lack of reuse. Specifically, even though they both share the same lifecycle and bootstrapping mechanism, they share no logic.

Here, we propose an abstraction that captures the concept of a Cassandra daemon, which will allow consumers to seamlessly switch the underlying implementation of their Cassandra daemons. In essence, it defines not only a way to activate and deactivate a Cassandra daemon, but also hooks into its lifecycle methods.",2010-05-25T22:36:14.460+0000,2019-04-16T09:33:23.070+0000,Fixed,Normal
TEZ-2232,Allow setParallelism to be called multiple times before tasks get scheduled,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12411878'>],"Currently, this is allowed only once currently. It is harder to support this after the vertex tasks have already started running. But allowing it before tasks start running is actually trivial. This just allows VertexManagers to change their minds multiple times before they start the vertex processing.",2015-03-25T20:56:51.444+0000,2015-06-30T04:52:31.181+0000,Fixed,Major
TEZ-1350,Create a framework specific config to enable timeline server,TEZ,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12393012'>],,2014-07-31T18:21:09.612+0000,2015-02-06T05:13:45.886+0000,Not A Problem,Major
BIGTOP-1789,Spark 1.3.0 incompatible with Hive 1.1.0,BIGTOP,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12420527'>, <JIRA IssueLink: id='12412114'>, <JIRA IssueLink: id='12412331'>]",See BIGTOP-1778. We should fix it.,2015-03-27T21:32:20.178+0000,2015-11-07T07:19:55.817+0000,Won't Fix,Critical
TEZ-3860,JDK9: ReflectionUtils may not use URLClassLoader,TEZ,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12518555'>, <JIRA IssueLink: id='12580001'>, <JIRA IssueLink: id='12597970'>]","The following code
https://github.com/apache/tez/blob/master/tez-api/src/main/java/org/apache/tez/common/ReflectionUtils.java#L125

is not compatible with JDK9 since the classloader is an AppClassLoader

causes exceptions like this:
{code}
java.lang.ClassCastException: java.base/jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to java.base/java.net.URLClassLoader
	at org.apache.tez.common.ReflectionUtils.addResourcesToSystemClassLoader(ReflectionUtils.java:125) ~[tez-api-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.utils.RelocalizationUtils.addUrlsToClassPath(RelocalizationUtils.java:57) ~[tez-common-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.app.dag.impl.DAGImpl$StartTransition.transition(DAGImpl.java:1793) ~[tez-dag-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.app.dag.impl.DAGImpl$StartTransition.transition(DAGImpl.java:1776) ~[tez-dag-0.9.0.jar:0.9.0]
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362) ~[hadoop-yarn-common-2.8.1.jar:?]
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) ~[hadoop-yarn-common-2.8.1.jar:?]
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) ~[hadoop-yarn-common-2.8.1.jar:?]
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) ~[hadoop-yarn-common-2.8.1.jar:?]
	at org.apache.tez.state.StateMachineTez.doTransition(StateMachineTez.java:59) ~[tez-dag-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.app.dag.impl.DAGImpl.handle(DAGImpl.java:1156) [tez-dag-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.app.dag.impl.DAGImpl.handle(DAGImpl.java:147) [tez-dag-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.app.DAGAppMaster$DagEventDispatcher.handle(DAGAppMaster.java:2251) [tez-dag-0.9.0.jar:0.9.0]
	at org.apache.tez.dag.app.DAGAppMaster$DagEventDispatcher.handle(DAGAppMaster.java:2242) [tez-dag-0.9.0.jar:0.9.0]
	at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:180) [tez-common-0.9.0.jar:0.9.0]
	at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:115) [tez-common-0.9.0.jar:0.9.0]
	at java.base/java.lang.Thread.run(Thread.java:844) [?:?]
{code}",2017-10-26T06:40:22.954+0000,2020-12-15T18:37:30.625+0000,Fixed,Major
SPARK-23209,HiveDelegationTokenProvider throws an exception if Hive jars are not the classpath,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12525176'>],"While doing some Hive-on-Spark testing against the Spark 2.3.0 release candidates we came across a bug (see HIVE-18436).

Stack-trace:

{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConf
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.getDelegationTokenProviders(HadoopDelegationTokenManager.scala:68)
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.<init>(HadoopDelegationTokenManager.scala:54)
        at org.apache.spark.deploy.yarn.security.YARNHadoopDelegationTokenManager.<init>(YARNHadoopDelegationTokenManager.scala:44)
        at org.apache.spark.deploy.yarn.Client.<init>(Client.scala:123)
        at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1502)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 10 more
{code}

Looks like the bug was introduced by SPARK-20434. SPARK-20434 changed {{HiveDelegationTokenProvider}} so that it constructs {{o.a.h.hive.conf.HiveConf}} inside {{HiveCredentialProvider#hiveConf}} rather than trying to manually load the class via the class loader. Looks like with the new code the JVM tries to load {{HiveConf}} as soon as {{HiveDelegationTokenProvider}} is referenced. Since there is no try-catch around the construction of {{HiveDelegationTokenProvider}} a {{ClassNotFoundException}} is thrown, which causes spark-submit to crash. Spark's {{docs/running-on-yarn.md}} says ""a Hive token will be obtained if Hive is on the classpath"". This behavior would seem to contradict that.",2018-01-25T01:27:37.500+0000,2018-01-29T22:10:15.226+0000,Fixed,Blocker
TEZ-2628,History logging plugin to write ATS events to HDFS,TEZ,Improvement,Open,[],2,"[<JIRA IssueLink: id='12480498'>, <JIRA IssueLink: id='12431389'>]",This provides another history logging alternative that conceptually the same as the timeline logging service but logs the entities to a file rather than posting the events to the timeline server directly.  When coupled with the timeline store plugin from YARN-3942 it allows the Tez job to be decoupled from the timeline server yet the Tez UI can still function properly.,2015-07-20T20:40:55.008+0000,2017-03-14T03:40:28.461+0000,,Major
ZOOKEEPER-1095,Simple leader election recipe,ZOOKEEPER,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12388260'>, <JIRA IssueLink: id='12339835'>]",Leader election recipe originally contributed to ZOOKEEPER-1080.,2011-06-15T17:04:15.612+0000,2014-05-17T12:03:02.364+0000,Fixed,Major
BIGTOP-648,hbase-thrift cannot be started properly,BIGTOP,Bug,Closed,[],2,"[<JIRA IssueLink: id='12368436'>, <JIRA IssueLink: id='12353907'>]","although hbase-master/regionserver/rest all started fine, fail to start hbase-thrift (SLES)
{noformat}
# service hbase-thrift start
Starting  (hbase-thrift):                                                                                      done
starting thrift, logging to /var/log/hbase/hbase-hbase-thrift-pkgtest-sles64-11.out

# service hbase-thrift status
thrift is not running                                                                                          failed
# cat /var/log/hbase/hbase-hbase-thrift-pkgtest-sles64-11.out
Exception in thread ""main"" java.lang.AssertionError: Exactly one option out of [-hsha, -nonblocking, -threadpool, -threadedselector] has to be specified
	at org.apache.hadoop.hbase.thrift.ThriftServerRunner$ImplType.setServerImpl(ThriftServerRunner.java:201)
	at org.apache.hadoop.hbase.thrift.ThriftServer.processOptions(ThriftServer.java:169)
	at org.apache.hadoop.hbase.thrift.ThriftServer.doMain(ThriftServer.java:85)
	at org.apache.hadoop.hbase.thrift.ThriftServer.main(ThriftServer.java:192)

{noformat}

tried to follow the message to add the option, doesn't work (Centos5)
{noformat}
# service hbase-thrift start -nonblocking
starting thrift, logging to /var/log/hbase/hbase-hbase-thrift-pkgtest-centos64-5.mtv.cloudera.com.out
Exception in thread ""main"" java.lang.AssertionError: Exactly one option out of [-hsha, -nonblocking, -threadpool, -threadedselector] has to be specified
	at org.apache.hadoop.hbase.thrift.ThriftServerRunner$ImplType.setServerImpl(ThriftServerRunner.java:201)
	at org.apache.hadoop.hbase.thrift.ThriftServer.processOptions(ThriftServer.java:169)
	at org.apache.hadoop.hbase.thrift.ThriftServer.doMain(ThriftServer.java:85)
	at org.apache.hadoop.hbase.thrift.ThriftServer.main(ThriftServer.java:192)
{noformat}

tried a full restart also (SLES)
{noformat}
# service hbase-thrift --full-restart
Stopping  (hbase-thrift):                                                                                      done
no thrift to stop because kill -0 of pid 9010 failed with status 1

Starting  (hbase-thrift):                                                                                      done
starting thrift, logging to /var/log/hbase/hbase-hbase-thrift-pkgtest-sles64-11.out
Exception in thread ""main"" 
# service hbase-thrift status
thrift is not running                                                                                          failed
{noformat}",2012-06-22T21:39:23.491+0000,2013-06-21T23:55:07.686+0000,Fixed,Blocker
AMBARI-12420,"Alert ""DataNode Process"" creates ERROR messages in datanode log",AMBARI,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12434225'>, <JIRA IssueLink: id='12452293'>]","Each time DataNode Process runs it creates error in datanode log;
{code}
ERROR datanode.DataNode (DataXceiver.java:run(278)) - golem6.labs.teradata.com:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:60681 dst: /127.0.0.1:50010
java.io.EOFException
	at java.io.DataInputStream.readShort(DataInputStream.java:315)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:58)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
{code}",2015-07-15T14:46:11.322+0000,2019-10-04T16:12:16.958+0000,Duplicate,Major
IMPALA-283,select count(*) produces inconsistent results for hbase tables because of NULL projection,IMPALA,Bug,Open,[],7,"[<JIRA IssueLink: id='12547041'>, <JIRA IssueLink: id='12553364'>, <JIRA IssueLink: id='12547039'>, <JIRA IssueLink: id='12553061'>, <JIRA IssueLink: id='12521594'>, <JIRA IssueLink: id='12553059'>, <JIRA IssueLink: id='12506400'>]","When count(*) is clubbed with another aggregation columns, it produces different results.

{code}
[localhost:21000] > select count(*) from functional.alltypesagg;
Query: select count(*) from functional.alltypesagg
Query finished, fetching results ...
+----------+
| count(*) |
+----------+
| 10000    |
+----------+
Returned 1 row(s) in 0.26s
[localhost:21000] > select count(*) from functional_hbase.alltypesagg;
Query: select count(*) from functional_hbase.alltypesagg
Query finished, fetching results ...
+----------+
| count(*) |
+----------+
| 10000    |
+----------+
Returned 1 row(s) in 0.65s
[localhost:21000] > select count(*), count(int_col) from functional_hbase.alltypesagg;
Query: select count(*), count(int_col) from functional_hbase.alltypesagg
Query finished, fetching results ...
+----------+----------------+
| count(*) | count(int_col) |
+----------+----------------+
| 9990     | 9990           |
+----------+----------------+
Returned 1 row(s) in 0.91s
[localhost:21000] > select count(*), count(int_col) from functional.alltypesagg;
Query: select count(*), count(int_col) from functional.alltypesagg
Query finished, fetching results ...
+----------+----------------+
| count(*) | count(int_col) |
+----------+----------------+
| 10000    | 9990           |
+----------+----------------+
Returned 1 row(s) in 0.26s
{code}",2013-04-17T23:02:37.000+0000,2019-02-05T13:08:17.393+0000,,Minor
CALCITE-949,Assertion error in FixNullabilityShuttle in RexUtil,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12447813'>],"In RexUtil.java, we can find the following class that is used to fix an expression to match changes in nullability of input fields, which is specially useful for inner/outer joins:

{code}
  public static class FixNullabilityShuttle extends RexShuttle {
    private final List<RelDataType> typeList;
    private final RexBuilder rexBuilder;

    public FixNullabilityShuttle(RexBuilder rexBuilder,
        List<RelDataType> typeList) {
      this.typeList = typeList;
      this.rexBuilder = rexBuilder;
    }

    @Override public RexNode visitInputRef(RexInputRef ref) {
      final RelDataType rightType = typeList.get(ref.getIndex());
      final RelDataType refType = ref.getType();
      if (refType == rightType) {
        return ref;
      }
      final RelDataType refType2 =
          rexBuilder.getTypeFactory().createTypeWithNullability(refType,
              rightType.isNullable());
      if (refType2 == rightType) {
        return new RexInputRef(ref.getIndex(), refType2);
      }
      throw new AssertionError(""mismatched type "" + ref + "" "" + rightType);
    }
  }
{code}

We are hitting the last assertion error in Hive. The reason seems to be that after adjusting the nullability, {{refType2 != rightType}} (which I think makes sense). In particular, it seems that the last {{if}} clause should be removed.",2015-11-04T13:18:24.141+0000,2016-01-21T22:17:39.145+0000,Invalid,Major
IMPALA-9485,Enable file handle cache for EC files,IMPALA,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12582831'>, <JIRA IssueLink: id='12600649'>, <JIRA IssueLink: id='12582832'>]","Now that HDFS-14308 has been fixed, we can re-enable the file handle cache for EC files.",2020-03-11T01:36:41.712+0000,2020-10-09T17:46:42.500+0000,Fixed,Major
ACCUMULO-715,use curator,ACCUMULO,Improvement,Resolved,"[<JIRA Issue: key='ACCUMULO-1310', id='12643082'>, <JIRA Issue: key='ACCUMULO-1297', id='12643048'>]",3,"[<JIRA IssueLink: id='12372129'>, <JIRA IssueLink: id='12386864'>, <JIRA IssueLink: id='12385510'>]","Curator seems to have distilled a lot of zookeeper lesson's learned. Consider using it as our interface to zookeeper.

Curator advantages:
 * handles zookeeper retries, via configurable policies
 * works around some very strange failure scenarios
 * simplified API
 * unlike ZooCache and ZooLock, Curator does not need to be maintained by Accumulo developers
",2012-08-03T18:33:24.824+0000,2020-03-11T21:46:28.313+0000,Won't Fix,Major
SQOOP-927,Sqoop2: Integration: Mapreduce specific tests should be running on MiniCluster,SQOOP,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12368887'>],Mapreduce specific tests should be running on MiniCluster rather than on LocalRunner as they behave differently.,2013-03-03T05:17:21.481+0000,2013-10-23T01:31:10.784+0000,Fixed,Major
AVRO-997,Generic API should require GenericEnumSymbol when writing Avro Enums,AVRO,Bug,Closed,[],3,"[<JIRA IssueLink: id='12573564'>, <JIRA IssueLink: id='12358672'>, <JIRA IssueLink: id='12486251'>]","I have a schema like:

{code}
[
{
  ""type"": ""enum"",
  ""name"": ""Gender"",
  ""symbols"": [""M"", ""F""]
},

{
  ""type"" : ""record"",
  ""name"" : ""Foo"",
  ""fields"" : [
    { ""type"" : [""Gender"", ""null""], ""name"" : ""gender"" },
    ...
  ]
}
]
{code}

I build a record like {{Foo foo = new Foo(); foo.gender = Gender.M;}}

When I go to serialize this, I get:
{code}Not in union [{""type"":""enum"",""name"":""Gender"",""symbols"":[""M"",""F""]},""null""]: M
	at org.apache.avro.generic.GenericData.resolveUnion(GenericData.java:482)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:70)
	at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:104)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:65)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:57)
{code}",2012-01-18T16:40:54.647+0000,2019-11-07T09:37:47.322+0000,Fixed,Major
AMBARI-12995,"Ambari alerts reports ""UNKNOWN"" error for secondary YARN RM and NM in a kerberoized YARN HA deployment",AMBARI,Bug,Open,[],2,"[<JIRA IssueLink: id='12440117'>, <JIRA IssueLink: id='12440115'>]","What is observed:

On my currently active YARN NodeManager and ResourceManager, Ambari
alerts are fine.

On the secondary YARN NodeManager and ResourceManager, Ambari reports
""Status: Unknown"" / ""HTTP 200 response (metrics unavailable)"".  This
is for the alerts:
 - NodeManager Health Summary
 - ResourceManager CPU Utilization
 - ResourceManager RPC Latency

The Ambari web interface does not make this error obvious, as it says
""0 alerts"" in the top bar. But you can see the alerts with ""unknown""
status when you go to the ambari alerts page, or if you query the
alerts API.

What is expected:
Ambari alerts does not generate any alarms on a secondary YARN HA node as long as the node is responsive.


---
A network dump of the ambari poll against the secondary RM looks like:

Request:
""""""
GET /jmx?qry=Hadoop:service=ResourceManager,name=RMNMInfo HTTP/1.1
...
""""""

Response:
""""""
HTTP/1.1 200 OK
...
Refresh: 3; url=http://{my-primary-rm}:8088/jmx
Content-Length: 106
Server: Jetty(6.1.26.hwx)

This is standby RM. Redirecting to the current active RM:
http://{my-primary-rm}:8088/jmx
""""""

--
I'm also filing a JIRA against YARN (per request from jhurley) and will post that info here.
--

Comment from Jonathan Hurley jhurley@hortonworks.com:

This is caused by how YARN does HA mode. With two YARN RMs, the standby RM returns a 200 response with a JavaScript redirect instead of an 3xx redirection. When not using Kerberos, Ambari should be able to parse the headers and follow the JS-based redirect. However, on a Kerberized cluster, we use curl which cannot do this. Therefore, requests against the secondary RM will return an UNKNOWN response since it did get a 200. I think a few things can be improved here:

1) There should be a ticket filed for YARN to have their HA mode use a proper redirect
2) Ambari might not want to produce an UNKNOWN response here since it gives a false feeling that something went wrong.
",2015-09-03T13:45:05.974+0000,2019-10-04T16:12:16.891+0000,,Major
BIGTOP-2349,Hive-2.0 spec files need to handle jdbc standalone jars in new location,BIGTOP,Bug,Patch Available,[],1,[<JIRA IssueLink: id='12458360'>],See HIVE-13134,2016-02-26T06:21:38.115+0000,2016-03-28T18:34:42.026+0000,,Major
OOZIE-1174,Upgrade to antlr 3.4 when we switch to Hive 0.11.0,OOZIE,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12363202'>],"In Hive 0.11.0, they're upgrading to antlr 3.4 so we shouldn't need to use antlr 3.0.1 anymore.  

(This may not be for a while, but just so we don't forget)",2013-01-17T00:50:51.129+0000,2016-08-16T12:50:21.523+0000,Done,Major
TEZ-3087,Tez UI 2: Add log links in task & attempt details page,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12459432'>],"Requirement:
- Task attempt details page must have a link to view the attempt's inProgressLog file, and a button to download the completedLog on attempt completion.
- Task details page have a view link & download button as above to the successful attempt if available or the final failed attempt.

View link is currently blocked by YARN-3698.

- Ensure that the functionality is inline with TEZ-3101, and the view link protocol (http / https) is taken from yarn.http.policy configuration in yarn-site.

PS: The log links were visible in the task and attempts tables of Tez UI 1, but wasn't added in Tez UI 2 based on comments.",2016-02-01T22:25:56.679+0000,2016-07-09T19:16:08.448+0000,Fixed,Major
ACCUMULO-1522,investigate HBASE-8755 for improved WAL performance,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12377248'>],,2013-06-18T17:49:30.560+0000,2020-10-28T23:09:20.921+0000,Abandoned,Major
SAMZA-228,Report progress in Samza's YARN AM,SAMZA,Bug,Open,[],2,"[<JIRA IssueLink: id='12386164'>, <JIRA IssueLink: id='12386385'>]","Samza's YARN AM does not report any progress right now. We should figure out what the proper approach to this is, and implement it. Some discussion about this is on SAMZA-58.",2014-04-07T15:37:25.029+0000,2014-04-11T17:55:16.155+0000,,Major
IMPALA-8347,Support read/write Hive ACID tables,IMPALA,Epic,Open,"[<JIRA Issue: key='IMPALA-8813', id='13247934'>]",1,[<JIRA IssueLink: id='12557412'>],Hive added ACID transactional support in HIVE-5317. Impala should be able to read Hive ACID tables.,2019-03-25T23:29:28.782+0000,2019-08-06T07:21:28.590+0000,,Critical
AVRO-1412,Python Avro library can't read Avros made with Pig builtin AvroStorage,AVRO,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12380186'>, <JIRA IssueLink: id='12379972'>, <JIRA IssueLink: id='12379973'>]","Using this script:
from avro import schema, datafile, io
import pprint
import sys
import json
field_id = None
Optional key to print
if (len(sys.argv) > 2):
field_id = sys.argv[2]
Test reading avros
rec_reader = io.DatumReader()
Create a 'data file' (avro file) reader
df_reader = datafile.DataFileReader(
open(sys.argv[1]),
rec_reader
)
the last line fails with:
Traceback (most recent call last):
File ""/Users/rjurney/bin/cat_avro"", line 22, in <module>
rec_reader
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/datafile.py"", line 247, in _init_
self.datum_reader.writers_schema = schema.parse(self.get_meta(SCHEMA_KEY))
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 784, in parse
return make_avsc_object(json_data, names)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 740, in make_avsc_object
return RecordSchema(name, namespace, fields, names, type, doc, other_props)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 653, in _init_
other_props)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 294, in _init_
new_name = names.add_name(name, namespace, self)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 268, in add_name
raise SchemaParseException(fail_msg)
avro.schema.SchemaParseException: record is a reserved type name.",2013-12-13T23:51:09.918+0000,2013-12-20T04:39:32.733+0000,Fixed,Major
INFRA-10719,Enable gitpubsub for HBase,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12447847'>],I have pushed a branch asf-site to the HBase Git repo. Can you please enable it on gitpubsub? Do we need to disable the svnpubsub afterward?,2015-11-04T21:44:09.255+0000,2015-12-07T07:08:39.404+0000,Done,Major
IMPALA-4894,Impala changes table schema when you do ALTER TABLE .. ADD COLUMNS but not partition schema resulting in false NULL values in Hive.,IMPALA,Bug,Open,[],2,"[<JIRA IssueLink: id='12512735'>, <JIRA IssueLink: id='12512736'>]","Adding a column in IMPALA for a partitioned table causes Hive to display ""NULL"" values for the newly added column. 

== Reproduction Details ==

<<< 1. Setup using impala. 

-- Impala Setup --


{code:java}
create table parttest (a int) partitioned by (c int);
insert into table parttest partition (c=1) values (1);
insert into table parttest partition (c=2) values (2);
alter table parttest add columns (b int);
insert into table parttest partition (c=1) values (3,3);
{code}


<<< 2. Query using impala and hive to see the different results. 

-- Query Results --

*Note that Hive reports value of ""b"" as NULL for ""C=1"" whereas Impala correctly reports it as 3.
*
- Impala -

{code:java}
[jrepo7-2:21000] > select * from parttest;
Query: select * from parttest
+---+------+---+
| a | b    | c |
+---+------+---+
| 3 | 3    | 1 |
| 2 | NULL | 2 |
| 1 | NULL | 1 |
+---+------+---+
{code}

- Hive -


{code:java}
hive> select * from parttest;
OK
1  NULL  1
3  NULL  1
2  NULL  2

{code}


<<< 3. Do the following describes in hive to see the metadata difference. 

{code}
hive> desc parttest;
OK
a                     int
b                     int
c                     int

# Partition Information
# col_name              data_type             comment

c                     int
{code}

- Describe partition

{code}
hive> desc parttest partition (c=1);
OK
a                     int
c                     int

# Partition Information
# col_name              data_type             comment

c                     int
{code}

<<< 3.a. Alternatively you can do the following in the metastore database to see the difference. 


{code:java}
select tbl.TBL_NAME, tbl.TBL_ID, tbl.SD_ID table_sd_id, col.CD_ID, col.COLUMN_NAME from TBLS tbl join SDS sd on tbl.SD_ID = sd.SD_ID join COLUMNS_V2 col on sd.CD_ID = col.CD_ID where tbl.TBL_NAME = ""parttest"";

select part.SD_ID partition_sd_id, part.PART_NAME, col.COLUMN_NAME from PARTITIONS part join TBLS tbl on part.TBL_ID = tbl.TBL_ID join SDS sd on part.SD_ID = sd.SD_ID join COLUMNS_V2 col on col.CD_ID = sd.CD_ID where tbl.TBL_NAME = ""parttest"";
{code}


Example: 



{code:java}
mysql> select tbl.TBL_NAME, tbl.TBL_ID, tbl.SD_ID table_sd_id, col.CD_ID, col.COLUMN_NAME from TBLS tbl join SDS sd on tbl.SD_ID = sd.SD_ID join COLUMNS_V2 col on sd.CD_ID = col.CD_ID where tbl.TBL_NAME = ""parttest"";
+----------+--------+-------------+-------+-------------+
| TBL_NAME | TBL_ID | table_sd_id | CD_ID | COLUMN_NAME |
+----------+--------+-------------+-------+-------------+
| parttest |   3775 |       11273 |  3781 | a           |
| parttest |   3775 |       11273 |  3781 | b           |
+----------+--------+-------------+-------+-------------+
2 rows in set (0.01 sec)

mysql>
mysql> select part.SD_ID partition_sd_id, part.PART_NAME, col.COLUMN_NAME from PARTITIONS part join TBLS tbl on part.TBL_ID = tbl.TBL_ID join SDS sd on part.SD_ID = sd.SD_ID join COLUMNS_V2 col on col.CD_ID = sd.CD_ID where tbl.TBL_NAME = ""parttest"";
+-----------------+-----------+-------------+
| partition_sd_id | PART_NAME | COLUMN_NAME |
+-----------------+-----------+-------------+
|           11274 | c=1       | a           |
|           11275 | c=2       | a           |
+-----------------+-----------+-------------+
2 rows in set (0.00 sec)
{code}


Workarounds: 

- ALTER TABLE ... CASCADE in hive instead of impala which will update metadata for table and push to partitions. 

- COMPUTE STATS INCREMENTAL in Impala for affected partitions to update metadata.",2017-02-07T02:25:33.000+0000,2017-08-23T19:43:16.120+0000,,Major
SPARK-15343,NoClassDefFoundError when initializing Spark with YARN,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12466589'>, <JIRA IssueLink: id='12470636'>]","I'm trying to connect Spark 2.0 (compiled from branch-2.0) with Hadoop.

Spark compiled with:
{code}
./dev/make-distribution.sh -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0 -DskipTests
{code}

I'm getting following error
{code}
mbrynski@jupyter:~/spark$ bin/pyspark
Python 3.4.0 (default, Apr 11 2014, 13:05:11)
[GCC 4.8.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Warning: Master yarn-client is deprecated since 2.0. Please use master ""yarn"" with specified deploy mode instead.
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel).
16/05/16 11:54:41 WARN SparkConf: The configuration key 'spark.yarn.jar' has been deprecated as of Spark 2.0 and may be removed in the future. Please use the new key 'spark.yarn.jars' instead.
16/05/16 11:54:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/05/16 11:54:42 WARN AbstractHandler: No Server set for org.spark_project.jetty.server.handler.ErrorHandler@f7989f6
16/05/16 11:54:43 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
Traceback (most recent call last):
  File ""/home/mbrynski/spark/python/pyspark/shell.py"", line 38, in <module>
    sc = SparkContext()
  File ""/home/mbrynski/spark/python/pyspark/context.py"", line 115, in __init__
    conf, jsc, profiler_cls)
  File ""/home/mbrynski/spark/python/pyspark/context.py"", line 172, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
  File ""/home/mbrynski/spark/python/pyspark/context.py"", line 235, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
  File ""/home/mbrynski/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py"", line 1183, in __call__
  File ""/home/mbrynski/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py"", line 312, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig
        at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:45)
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:163)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:150)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:148)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:502)
        at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:236)
        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
        at py4j.GatewayConnection.run(GatewayConnection.java:211)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 19 more
{code}

On 1.6 everything works fine. I'm using HDP2.2 (Hadoop 2.6.0)
I have HADOOP_CONF_DIR and SPARK_HOME env variables.",2016-05-16T11:59:46.478+0000,2020-05-17T18:15:06.405+0000,Not A Problem,Critical
ACCUMULO-3908,Instrument client API for metrics,ACCUMULO,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12433493'>],"In talking about ACCUMULO-2388, we noted that the client implementation doesn't have instrumentation for collecting important metrics. This would have benefit for users trying to debug performance of their applications as well as use for the developers to validate some of our decisions.

We have hadoop metrics2 on the servers right now to fill this gap. I'm not sure what we would want to use for this. Metrics2 might not be the best tool in its current state. I've heard decent things about dropwizard metrics (formerly codahale metrics) in the past. I'm sure there are many we could use.",2015-06-18T20:35:32.481+0000,2021-09-03T22:15:43.385+0000,Incomplete,Major
TEZ-1171,Vertex remains in INITED state if all source vertices start while the vertex was in INITIALIZING state,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12389174'>],,2014-06-02T20:21:13.524+0000,2014-09-06T01:35:53.602+0000,Fixed,Major
BIGTOP-3708,Fix build failure of Hive against Hadoop 3.3.3,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12644182'>],"Building Hive with Hadoop 3.3.3 failed
{code:java}
[ERROR] /home/ubuntu/bigtop-dev-2/bigtop/output/hive/hive-3.1.3/shims/scheduler/src/main/java/org/apache/hadoop/hive/schshim/FairSchedulerShim.java:[31,68] org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy is not public in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair; cannot be accessed from outside package
[ERROR] /home/ubuntu/bigtop-dev-2/bigtop/output/hive/hive-3.1.3/shims/scheduler/src/main/java/org/apache/hadoop/hive/schshim/FairSchedulerShim.java:[43,48] no suitable constructor found for AllocationFileLoaderService(no arguments)
    constructor org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.AllocationFileLoaderService(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.AllocationFileLoaderService(org.apache.hadoop.yarn.util.Clock,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /home/ubuntu/bigtop-dev-2/bigtop/output/hive/hive-3.1.3/shims/scheduler/src/main/java/org/apache/hadoop/hive/schshim/FairSchedulerShim.java:[57,49] incompatible types: org.apache.hadoop.conf.Configuration cannot be converted to org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler
[ERROR] /home/ubuntu/bigtop-dev-2/bigtop/output/hive/hive-3.1.3/shims/scheduler/src/main/java/org/apache/hadoop/hive/schshim/FairSchedulerShim.java:[59,5] cannot find symbol
  symbol:   class QueuePlacementPolicy
  location: class org.apache.hadoop.hive.schshim.FairSchedulerShim
[ERROR] /home/ubuntu/bigtop-dev-2/bigtop/output/hive/hive-3.1.3/shims/scheduler/src/main/java/org/apache/hadoop/hive/schshim/FairSchedulerShim.java:[59,55] cannot find symbol
  symbol:   method getPlacementPolicy()
  location: class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration{code}
 

 

 ",2022-06-23T07:35:27.491+0000,2022-07-21T09:10:34.461+0000,Fixed,Major
MGPG-79,plugin hangs on first run on Windows Git Bash with Kleopatra,MGPG,Bug,Closed,[],1,[<JIRA IssueLink: id='12646239'>],"found during release 3.0.0 vote: [https://lists.apache.org/thread.html/r855a3f8d4b161da297191e5a7ce0e7e5be81895e60fe5b7be16cc604%40%3Cdev.maven.apache.org%3E]

{quote}the plugin simply hangs and does not continue if I use 3.0.0 after
starting Kleopatra (https://docs.kde.org/stable5/en/pim/kleopatra/).

When I first use 1.6 of the plugin after starting Kleopatra, the
password dialog pops up (as ususal), the files are signed *and after
this I am able to use 3.0.0 just fine!*

So with 3.0.0 the initial communication with the agent seems to be broken...

$ mvn -version
Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
Maven home: C:\_dev\Maven\latest
Java version: 11.0.6, vendor: AdoptOpenJDK, runtime: C:\_dev\Java\latest
Default locale: de_DE, platform encoding: Cp1252
OS name: ""windows 10"", version: ""10.0"", arch: ""amd64"", family: ""windows""

$ gpg --version
gpg (GnuPG) 2.2.19-unknown
libgcrypt 1.8.5

Git Bash.{quote}",2020-04-16T06:20:41.007+0000,2022-08-22T15:41:08.508+0000,Fixed,Major
TEZ-3892,getClient API for TezClient,TEZ,New Feature,Closed,[],2,"[<JIRA IssueLink: id='12525698'>, <JIRA IssueLink: id='12530430'>]","This is a proposed opt-in feature.

Tez AM already supports long-lived sessions, if desired a AM session can live indefinitely.

However, new clients cannot connect to a long-lived AM session through the standard TezClient API. 

TezClient API only provides a ""start"" method to initiate a connection, which always allocates a new AM from YARN.
 # For interactive BI use-cases, this startup time can be significant.
 # Hive is implementing a HiveServer2 High Availability feature.
 ** When the singleton HS2 master server fails, the HS2 client is quickly redirected to a pre-warmed HS2 backup. 
 # For the failover to complete quickly end-to-end, a Tez AM must also be pre-warmed and ready to accept connections.

For more information, see design for: https://issues.apache.org/jira/browse/HIVE-18281.
----
Anticipated changes:
 # A getClient{{(ApplicationId)}} method is added to TezClient. The functionality is similar to {{start}}
 ** Code related to launching a new AM from the RM is factored out.
 ** Since {{start}} and getClient will share some code, this code is refactored into reusable helper methods.
 ** A usage example is added to {{org/apache/tez/examples}}
 # It is not a goal of this JIRA to ensure that running Tez DAGs can be recovered by a client using the getClient API. The goal is only for maintaining a pool of warm Tez AMs to skip RM/container/JVM startup.",2018-01-30T21:10:54.594+0000,2019-04-19T19:21:01.045+0000,Fixed,Major
SPARK-32558,ORC target files that Spark_3.0 produces does not work with Hive_2.1.1 (work-around of using spark.sql.orc.impl=hive is also not working),SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12595147'>],"Steps to reproduce the issue:

------------------------------- 

Download Spark_3.0 from [https://spark.apache.org/downloads.html]

 

Step 1) Create ORC File by using the default Spark_3.0 Native API from spark shell .
{code:java}
[linuxuser1@irlrhellinux1 bin]$ ./spark-shell

Welcome to Spark version 3.0.0

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_191)

Type in expressions to have them evaluated. Type :help for more information.
 scala> spark.sql(""set spark.sql.orc.impl"").show()

+-------------------------+
|               key| value| 
+-------------------------+
|spark.sql.orc.impl|native|
+-------------------------+
 
scala> spark.sql(""CREATE table df_table(col1 string,col2 string)"") res1: org.apache.spark.sql.DataFrame = []
scala> spark.sql(""insert into df_table values('col1val1','col2val1')"")
org.apache.spark.sql.DataFrame = []

scala> val dFrame = spark.sql(""select * from df_table"") dFrame: org.apache.spark.sql.DataFrame = [col1: string, col2: string]
scala> dFrame.show()

+-----------------+
|    col1|    col2|
+-----------------+
|col1val1|col2val1|
+-----------------+

scala> dFrame.toDF().write.format(""orc"").save(""/export/home/linuxuser1/spark-3.0.0-bin-hadoop2.7/tgt/ORC_File_Tgt1"")
{code}
 

Step 2) Copy the ORC files created in Step(1) to HDFS /tmp on a Hadoop cluster (which has Hive_2.1.1, for example CDH_6.x) and run the following command to analyze or read metadata from the ORC files. As you see below, it fails to fetch the metadata from the ORC file.
{code:java}
adpqa@irlhadoop1 bug]$ hive --orcfiledump /tmp/ORC_File_Tgt1/part-00000-6ce5f13f-a33a-4bc0-b82b-3a89c27a5ddd-c000.snappy.orc Processing data file /tmp/df_table/part-00000-6ce5f13f-a33a-4bc0-b82b-3a89c27a5ddd-c000.snappy.orc [length: 414]
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 7
at org.apache.orc.OrcFile$WriterVersion.from(OrcFile.java:145)
at org.apache.orc.impl.OrcTail.getWriterVersion(OrcTail.java:74)
at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:385)
at org.apache.orc.OrcFile.createReader(OrcFile.java:222)
at org.apache.orc.tools.FileDump.getReader(FileDump.java:255)
at org.apache.orc.tools.FileDump.printMetaDataImpl(FileDump.java:328)
at org.apache.orc.tools.FileDump.printMetaData(FileDump.java:307)
at org.apache.orc.tools.FileDump.main(FileDump.java:154)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:313)
at org.apache.hadoop.util.RunJar.main(RunJar.java:227)
{code}
Step 3) Now Create ORC File using the Hive API (as suggested by Spark in [https://spark.apache.org/docs/latest/sql-migration-guide.html] by setting spark.sql.orc.impl as hive)
{code:java}
scala> spark.sql(""set spark.sql.orc.impl=hive"")
res6: org.apache.spark.sql.DataFrame = [key: string, value: string]
scala> spark.sql(""set spark.sql.orc.impl"").show()

+------------------------+
|               key|value| 
+------------------------+
|spark.sql.orc.impl| hive|
+------------------------+

scala> spark.sql(""CREATE table df_table2(col1 string,col2 string)"")
scala> spark.sql(""insert into df_table2 values('col1val1','col2val1')"") res8: org.apache.spark.sql.DataFrame = []
scala> val dFrame2 = spark.sql(""select * from df_table2"") dFrame2: org.apache.spark.sql.DataFrame = [col1: string, col2: string]
scala> dFrame2.toDF().write.format(""orc"").save(""/export/home/linuxuser1/spark-3.0.0-bin-hadoop2.7/tgt/ORC_File_Tgt2"")
{code}
 

Step 4) Copy the ORC files created in Step(3) to HDFS /tmp on a Hadoop cluster (which has Hive_2.1.1, for example CDH_6.x) and run the following command to analyze or read metadata from the ORC files. As you see below, it fails with the same exception to fetch the metadata even after following the workaround suggested by spark to set spark.sql.orc.impl to hive
{code:java}
[adpqa@irlhadoop1 bug]$ hive --orcfiledump /tmp/ORC_File_Tgt2/part-00000-6d81ea27-ea5b-4f31-b1f7-47d805f98d3e-c000.snappy.orc
Processing data file /tmp/df_table2/part-00000-6d81ea27-ea5b-4f31-b1f7-47d805f98d3e-c000.snappy.orc [length: 414]
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 7
at org.apache.orc.OrcFile$WriterVersion.from(OrcFile.java:145)
at org.apache.orc.impl.OrcTail.getWriterVersion(OrcTail.java:74)
at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:385)
at org.apache.orc.OrcFile.createReader(OrcFile.java:222)
at org.apache.orc.tools.FileDump.getReader(FileDump.java:255)
at org.apache.orc.tools.FileDump.printMetaDataImpl(FileDump.java:328)
at org.apache.orc.tools.FileDump.printMetaData(FileDump.java:307)
at org.apache.orc.tools.FileDump.main(FileDump.java:154)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:313)
at org.apache.hadoop.util.RunJar.main(RunJar.java:227)
{code}
*Note: The same case works fine if you try metadata fetch from Hive_2.3 or above versions.*

*So the main concern here is that setting {{spark.sql.orc.impl}} to hive is not producing ORC files that will work with Hive_2.1.1 or below.
 Can someone help here. Is there any other workaround available? Can this be looked into on priority? Thank you.
  
 References:
 [https://spark.apache.org/docs/latest/sql-migration-guide.html]  (workaround of setting spark.sql.orc.impl=hive is mentioned here which is not working):
{quote}Since Spark 2.4, Spark maximizes the usage of a vectorized ORC reader for ORC files by default. To do that, spark.sql.orc.impl and spark.sql.orc.filterPushdown change their default values to native and true respectively. ORC files created by native ORC writer cannot be read by some old Apache Hive releases. Use spark.sql.orc.impl=hive to create the files shared with Hive 2.1.1 and older.""""
{quote}
https://issues.apache.org/jira/browse/SPARK-26932
 https://issues.apache.org/jira/browse/HIVE-16683",2020-08-06T08:23:27.550+0000,2020-08-09T19:00:08.918+0000,Invalid,Major
SPARK-21635,ACOS(2) and ASIN(2) should be null,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12511180'>],"ACOS(2) and ASIN(2) should be null, I have create a patch for Hive.",2017-08-04T05:55:19.525+0000,2017-10-28T08:58:23.336+0000,Won't Fix,Major
PHOENIX-4512,Account for change in Cell.DataType->Cell.Type (HBASE-19626),PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12523354'>],"Some more compilation issues on the tail of Cell changes

{noformat}
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /Users/jelser/projects/phoenix.git/phoenix-core/src/main/java/org/apache/phoenix/util/PhoenixKeyValueUtil.java:[28,36] cannot find symbol
  symbol:   class DataType
  location: interface org.apache.hadoop.hbase.Cell
[ERROR] /Users/jelser/projects/phoenix.git/phoenix-core/src/main/java/org/apache/phoenix/util/PhoenixKeyValueUtil.java:[79,42] cannot find symbol
  symbol:   class DataType
  location: class org.apache.phoenix.util.PhoenixKeyValueUtil
[ERROR] /Users/jelser/projects/phoenix.git/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java:[664,24] cannot find symbol
  symbol: class DataType
[ERROR] /Users/jelser/projects/phoenix.git/phoenix-core/src/main/java/org/apache/phoenix/util/PhoenixKeyValueUtil.java:[60,60] cannot find symbol
  symbol:   variable DataType
  location: class org.apache.phoenix.util.PhoenixKeyValueUtil
[ERROR] /Users/jelser/projects/phoenix.git/phoenix-core/src/main/java/org/apache/phoenix/util/PhoenixKeyValueUtil.java:[67,43] cannot find symbol
  symbol:   variable DataType
  location: class org.apache.phoenix.util.PhoenixKeyValueUtil
[ERROR] /Users/jelser/projects/phoenix.git/phoenix-core/src/main/java/org/apache/phoenix/util/PhoenixKeyValueUtil.java:[74,26] cannot find symbol
  symbol:   variable DataType
  location: class org.apache.phoenix.util.PhoenixKeyValueUtil
[INFO] 6 errors
{noformat}

FYI [~sergey.soldatov]",2018-01-02T22:48:55.514+0000,2018-07-26T01:13:53.047+0000,Fixed,Blocker
TEZ-955,Tez should close inputs after calling processor's close,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12385014'>, <JIRA IssueLink: id='12540103'>]","Hive flushes the processor pipeline in the close method. That might require reading additional input. Apparently the inputs are already closed in that case - which leads to a race condition where sometimes the reducer just hangs (there should also be an exception when read is called on a closed input).

This is the stack trace you'll see when that happens:

Thread 30938: (state = BLOCKED)
sun.misc.Unsafe.park(boolean, long) @bci=0 (Interpreted frame)
java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=186 (Interpreted frame)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=2043 (Interpreted frame)
java.util.concurrent.LinkedBlockingQueue.take() @bci=29, line=442 (Interpreted frame)
org.apache.tez.runtime.library.shuffle.common.impl.ShuffleManager.getNextInput() @bci=67, line=608 (Interpreted frame)
org.apache.tez.runtime.library.common.readers.ShuffledUnorderedKVReader.moveToNextInput() @bci=26, line=172 (Interpreted frame)
org.apache.tez.runtime.library.common.readers.ShuffledUnorderedKVReader.next() @bci=30, line=113 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer[], org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe[]) @bci=158, line=99 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable() @bci=78, line=150 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(java.lang.Object, int) @bci=12, line=197 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=3, line=638 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject() @bci=109, line=670 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject() @bci=455, line=754 (Compiled frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(java.lang.Object, int) @bci=251, line=229 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=3, line=638 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject() @bci=109, line=670 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject() @bci=455, line=754 (Compiled frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(java.lang.Object, int) @bci=251, line=229 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=3, line=638 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject() @bci=109, line=670 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject() @bci=455, line=754 (Compiled frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(java.lang.Object, int) @bci=251, line=229 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=3, line=638 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject() @bci=109, line=670 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject() @bci=455, line=754 (Compiled frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(java.lang.Object, int) @bci=251, line=229 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=3, line=638 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject() @bci=109, line=670 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject() @bci=455, line=754 (Compiled frame)
org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(java.lang.Object, int) @bci=251, line=229 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(java.lang.Object, int) @bci=121, line=87 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.forward(java.lang.Object, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector) @bci=63, line=791 (Compiled frame)
org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(java.lang.Object[], org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$AggregationBuffer[]) @bci=97, line=1064 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.GroupByOperator.flush() @bci=143, line=1089 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(boolean) @bci=125, line=1138 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.Operator.close(boolean) @bci=60, line=575 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close() @bci=65, line=348 (Interpreted frame)
org.apache.hadoop.hive.ql.exec.tez.TezProcessor.close() @bci=11, line=74 (Interpreted frame)
org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.close() @bci=130, line=325 (Interpreted frame)
org.apache.hadoop.mapred.YarnTezDagChild$4.run() @bci=112, line=529 (Interpreted frame)
java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Interpreted frame)
javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=415 (Interpreted frame)
org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1548 (Interpreted frame)
org.apache.hadoop.mapred.YarnTezDagChild.main(java.lang.String[]) @bci=1139, line=515 (Interpreted frame)",2014-03-19T18:27:08.820+0000,2018-08-01T20:08:51.955+0000,Fixed,Major
STORM-3358,Upgrade Storm to Hadoop 3,STORM,Dependency upgrade,In Progress,[],1,[<JIRA IssueLink: id='12560149'>],"We should upgrade to Hadoop 3 at some point.

We are currently blocked by https://issues.apache.org/jira/browse/HBASE-22027, but I believe that is the only bit that prevents us from upgrading.",2019-03-14T11:47:40.933+0000,2019-05-03T12:18:24.851+0000,,Major
HCATALOG-263,Make HBaseHCatStorageHandler work with Hive,HCATALOG,Improvement,Open,[],1,[<JIRA IssueLink: id='12348295'>],,2012-02-17T19:30:23.255+0000,2012-02-23T21:12:14.823+0000,,Major
SPARK-29260,Enable supported Hive metastore versions once it support altering database location,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12570706'>, <JIRA IssueLink: id='12570707'>]",Hive 3.x is supported currently. Hive 2.2.1 and Hive 2.4.0 have not released.,2019-09-26T11:18:15.607+0000,2022-06-03T02:57:54.975+0000,Fixed,Major
TEZ-3196,java.lang.InternalError from decompression codec is fatal to a task during shuffle,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12462690'>, <JIRA IssueLink: id='12531346'>]","Many codecs throw java.lang.InternalError when their native implementations encounter an error in the codec.  This is not treated like a fetch failure and instead is fatal to the task.  The task should treat codec errors during fetch like other fetch failures and retry, hopefully triggering a re-run of the upstream task if necessary.",2016-04-04T19:30:11.502+0000,2018-04-09T14:19:38.404+0000,Fixed,Major
SQOOP-834,duplicate of data exporting to hbase,SQOOP,Bug,Open,[],1,[<JIRA IssueLink: id='12363410'>],"calling the HBASE Put.add() statement on an unchanged (previously inserted) row/value 
will cause a data duplication (only the timestamp associated will be incremented)

{code}
hbase(main):030:0> get ""dump_HKFAS.sales_order"", ""1"", {COLUMN => ""mysql:created_at"", VERSIONS => 4}
COLUMN                             CELL                                                                                             
mysql:created_at                  timestamp=1358853505756, value=2011-12-21 18:07:38.0                                             
mysql:created_at                  timestamp=1358790515451, value=2011-12-21 18:07:38.0                                             
2 row(s) in 0.0040 seconds
{code}

today's sqoop run
{code}
hbase(main):031:0> Date.new(1358853505756).toString()
=> ""Tue Jan 22 11:18:25 UTC 2013""
{code}
yesterday's sqoop run
{code}
hbase(main):032:0> Date.new(1358790515451).toString()
=> ""Mon Jan 21 17:48:35 UTC 2013""
{code}

I did verified that this is a desired behavior on server side, according to HBASE-7645

I'd expect instead that a rerun of SQOOP would not cause a reversioning of all rows in the tables in HBase, but just an update of the changed fields
",2013-01-22T17:35:21.452+0000,2013-01-22T17:36:58.247+0000,,Major
CALCITE-1290,"When converting to CNF, fail if the expression size exceeds a threshold",CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12478463'>, <JIRA IssueLink: id='12504278'>, <JIRA IssueLink: id='12470517'>]","When converting to conjunctive normal form (CNF), fail if the expression exceeds a threshold. CNF can explode exponentially in the size of the input expression, but rarely does so in practice. Add a {{maxNodeCount}} parameter to {{RexUtil.toCnf}} and throw or return null if it is exceeded.

I don't believe it is possible to predict the size of the CNF from the input expression (especially if there are duplicate terms) but I might be wrong.",2016-06-13T18:56:08.307+0000,2017-05-22T19:29:39.133+0000,Fixed,Major
PHOENIX-1731,Add getNextIndexedKey() to IndexHalfStoreFileReader and FilteredKeyValueScanner,PHOENIX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12410888'>, <JIRA IssueLink: id='12410536'>]","See HBASE-13109, which changes two private interfaces, which breaks Phoenix compilation, which uses these interfaces.

On the jira we decided not to remove those changes (that's why they're marked private).

But, we can easily fix this in Phoenix by adding these methods to the classes that cause the problem, being care not to add the override annotation. The method can safely return null, in which case the optimization will not be used when these classes are used.",2015-03-13T00:11:20.484+0000,2015-11-21T02:16:20.018+0000,Fixed,Major
OOZIE-611,distcp action does not have documentation,OOZIE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12346792'>],The discp action does not have a twiki page.,2011-11-22T09:57:43.115+0000,2013-08-31T00:02:10.446+0000,Fixed,Major
SLIDER-479,Provide a slider command to kill all stranded containers continuing to run post stop command,SLIDER,Bug,Open,[],1,[<JIRA IssueLink: id='12424639'>],"A container can continue to run even after a slider stop command has been issued. One such scenarios is when NM of a non Slider-AM node is lost (for some intermittent reason) and then slider stop command is issued. YARN will not be able to clean up the stranded agent (and the application processes). In such a scenario even if the NM is brought back up later YARN does not kill these containers.

In a large cluster with several applications deployed/managed by slider there could easily be numerous such stranded containers.

Slider client could expose a ""stop-all"" command or maybe an option ""stop --clean"" (or anything appropriate for this task) to do the cleanup. It can bring up the Slider-AM in clean mode (say) which will not start any application but will simply register to ZK and wait for these stranded agents to heart-beat into it. Subsequently each one of these agents should receive a terminate command from the AM and do necessary cleanup and shutdown.

This new command can be issued only after an application has been stopped. When invoked while the application is running this command should ignore/fail providing relevant information. This command can also provide a summary of how many stranded containers it cleaned up.
",2014-10-03T07:46:05.373+0000,2015-05-13T16:57:19.257+0000,,Major
AMBARI-24256,Ambari Grafana HBase metrics wrong - readRequestCount > totalRequestCount!,AMBARI,Bug,Open,[],2,"[<JIRA IssueLink: id='12538741'>, <JIRA IssueLink: id='12538740'>]","Ambari Grafana metrics appear to have another bug in HBase - Home -> Num Requests - Total.

writeRequestCount and totalRequestCount are about the same but readRequestCount is consistently triple that of totalRequestCount which cannot possibly be true...

totalRequestCount should == writeRequestCount + readRequestCount, but this is not the case.

It looks like this may be due to multi requests being counted only once, see HBASE-18469.

This is not at all intuitive, it would have been better to have made totalRequestCount == writeRequestCount + readRequestCount and made a new metric to count multi requests as a single request.",2018-07-06T14:36:18.211+0000,2018-07-16T15:41:04.390+0000,,Major
SENTRY-1977,Switch to using new Hive notifications format,SENTRY,Bug,Open,[],2,"[<JIRA IssueLink: id='12517076'>, <JIRA IssueLink: id='12517077'>]","HIVE-15180 changed format of the notification messages, so essentially now we do not need custom Sentry JSON factory. Instead we should get location info from the new messages.",2017-10-10T18:58:44.819+0000,2018-08-07T18:31:04.217+0000,,Major
ORC-160,expose index read planning in RecordReaderUtils,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12498109'>],"Since EncodedReader was not made part of ORC project at split time, it doesn't have parity in utility methods access. In particular, it needs to plan index reads to be able to read them thru the cache.",2017-03-15T21:36:06.210+0000,2018-05-14T22:25:07.336+0000,Fixed,Major
CALCITE-1659,Simplifying CAST(characterLiteral as TIMESTAMP) should round the sub-second fraction,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12495584'>, <JIRA IssueLink: id='12495588'>]","Timestamps are losing sub-second parts after Calcite literal constant reduction.

{noformat}
select cast(""1970-12-31 15:59:58.174"" as TIMESTAMP) from src limit 1;
{noformat}

yields {{1970-12-31 15:59:58}} if CBO is enabled in Hive. {{ZonelessTimestamp.toString()}} contains a truncation 0-19 (ie. removes sub-second part from string representation).",2017-02-26T10:25:50.631+0000,2017-03-24T03:20:01.040+0000,Fixed,Major
CALCITE-1758,Push to Druid OrderBy/Limit operation over time dimension and additional columns,CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12501123'>],"Push as much as we can order by limit operators.
This use to be not possible because of the way druid bucket the data. 
With Extraction dimension specs we can now set granularity to all and craft the extraction dimension spec in order to make time as new group by column.",2017-04-18T17:49:50.319+0000,2017-06-26T08:29:33.896+0000,Fixed,Major
PHOENIX-1674,Snapshot isolation transaction support through Tephra,PHOENIX,Improvement,Resolved,"[<JIRA Issue: key='PHOENIX-1820', id='12818776'>, <JIRA Issue: key='PHOENIX-1812', id='12788355'>, <JIRA Issue: key='PHOENIX-1813', id='12788357'>, <JIRA Issue: key='PHOENIX-1821', id='12818803'>, <JIRA Issue: key='PHOENIX-1830', id='12819541'>, <JIRA Issue: key='PHOENIX-1831', id='12819567'>, <JIRA Issue: key='PHOENIX-1832', id='12819575'>, <JIRA Issue: key='PHOENIX-1835', id='12820172'>, <JIRA Issue: key='PHOENIX-1881', id='12821714'>, <JIRA Issue: key='PHOENIX-1882', id='12821715'>, <JIRA Issue: key='PHOENIX-1898', id='12822764'>, <JIRA Issue: key='PHOENIX-1900', id='12822798'>, <JIRA Issue: key='PHOENIX-1901', id='12822801'>, <JIRA Issue: key='PHOENIX-1902', id='12822806'>, <JIRA Issue: key='PHOENIX-1946', id='12827121'>, <JIRA Issue: key='PHOENIX-1947', id='12827171'>, <JIRA Issue: key='PHOENIX-1991', id='12831306'>, <JIRA Issue: key='PHOENIX-2185', id='12856688'>, <JIRA Issue: key='PHOENIX-2286', id='12895835'>, <JIRA Issue: key='PHOENIX-2294', id='12896550'>, <JIRA Issue: key='PHOENIX-2312', id='12902878'>, <JIRA Issue: key='PHOENIX-2356', id='12908776'>, <JIRA Issue: key='PHOENIX-2358', id='12908778'>, <JIRA Issue: key='PHOENIX-2361', id='12909094'>, <JIRA Issue: key='PHOENIX-2362', id='12909096'>, <JIRA Issue: key='PHOENIX-2375', id='12910567'>, <JIRA Issue: key='PHOENIX-2376', id='12910576'>, <JIRA Issue: key='PHOENIX-2441', id='12915045'>, <JIRA Issue: key='PHOENIX-2450', id='12915880'>, <JIRA Issue: key='PHOENIX-2458', id='12916274'>, <JIRA Issue: key='PHOENIX-2459', id='12916777'>, <JIRA Issue: key='PHOENIX-2462', id='12916841'>, <JIRA Issue: key='PHOENIX-2472', id='12917341'>, <JIRA Issue: key='PHOENIX-2497', id='12920142'>, <JIRA Issue: key='PHOENIX-2478', id='12917442'>]",4,"[<JIRA IssueLink: id='12512806'>, <JIRA IssueLink: id='12409923'>, <JIRA IssueLink: id='12409922'>, <JIRA IssueLink: id='12409921'>]",Tephra (http://tephra.io/ and https://github.com/caskdata/tephra) is one option for getting transaction support in Phoenix. Let's use this JIRA to discuss the way in which this could be integrated along with the pros and cons.,2015-02-18T22:15:41.883+0000,2017-08-24T07:47:03.855+0000,Fixed,Major
SUBMARINE-2,Hadoop {Submarine} Project: Simple and scalable deployment of deep learning training / serving jobs on Hadoop,SUBMARINE,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12547574'>, <JIRA IssueLink: id='12547573'>, <JIRA IssueLink: id='12547572'>]","Description:

*Goals:*
 - Allow infra engineer / data scientist to run *unmodified* Tensorflow jobs on YARN.
 - Allow jobs easy access data/models in HDFS and other storages.
 - Can launch services to serve Tensorflow/MXNet models.
 - Support run distributed Tensorflow jobs with simple configs.
 - Support run user-specified Docker images.
 - Support specify GPU and other resources.
 - Support launch tensorboard if user specified.
 - Support customized DNS name for roles (like tensorboard.$user.$domain:6006)

*Why this name?*
 - Because Submarine is the only vehicle can let human to explore deep places. B-)

h3. {color:#ff0000}Please refer to on-going design doc, and add your thoughts: [https://docs.google.com/document/d/199J4pB3blqgV9SCNvBbTqkEoQdjoyGMjESV4MktCo0k/edit#|https://docs.google.com/document/d/199J4pB3blqgV9SCNvBbTqkEoQdjoyGMjESV4MktCo0k/edit?usp=sharing]{color}

 

Submarine all document list in here: https://issues.apache.org/jira/browse/SUBMARINE-113 

 

*{color:#333333}See Also:{color}*
 * {color:#333333}Zeppelin integration with Submarine design: [https://docs.google.com/document/d/16YN8Kjmxt1Ym3clx5pDnGNXGajUT36hzQxjaik1cP4A/edit#heading=h.4jov859x47qe]{color}",2018-04-09T21:37:09.136+0000,2019-12-17T04:04:14.828+0000,Fixed,Major
CALCITE-3898,RelOptPredicateList may generate incorrect map of constant values,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12585233'>],"The method relies on {{RexUtil.predicateConstants}} which in turn calls {{RexUtil.canAssignFrom}}. {{RexUtil.canAssignFrom}} is skipping any check on precision and scale. I observed the error in Hive when two VARCHAR types with different precision were given to the method, which was resulting on considering the result of the narrowing cast as the value of the reference. This lead to incorrect results.",2020-04-06T17:35:14.994+0000,2020-05-24T11:59:25.225+0000,Fixed,Critical
CALCITE-2883,HepPlanner subprogram may loop till getting out of memory,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12555429'>],"Consider the following two hep programs.

Program 1:
{code}
final HepProgramBuilder programBuilder = new HepProgramBuilder();
programBuilder.addMatchOrder(HepMatchOrder.BOTTOM_UP);
programBuilder.addRuleInstance(JoinToMultiJoinRule.INSTANCE);
programBuilder.addRuleInstance(LoptOptimizeJoinRule.INSTANCE);
final HepProgram program = programBuilder.build();
{code}

Program 2:
{code}
final HepProgramBuilder programBuilder = new HepProgramBuilder();
final HepProgramBuilder subprogramBuilder = new HepProgramBuilder();
subprogramBuilder.addMatchOrder(HepMatchOrder.BOTTOM_UP);
subprogramBuilder.addRuleInstance(JoinToMultiJoinRule.INSTANCE);
subprogramBuilder.addRuleInstance(LoptOptimizeJoinRule.INSTANCE);
programBuilder.addSubprogram(subprogramBuilder.build());
final HepProgram program = programBuilder.build();
{code}

I would expect both programs to behave similarly. However, program 2 will loop indefinitely. The reason is that {{HepPlanner}} subprogram execution loops if subprogram generates any new expression.
https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/plan/hep/HepPlanner.java#L339
This does not seem right since planner can control exiting the program (and thus, subprogram) depending on its own internal state and configuration properties, e.g., match limit.",2019-03-01T00:44:19.811+0000,2019-03-01T21:23:13.794+0000,Invalid,Major
CALCITE-2128,"In JDBC adapter, add SQL dialect for Jethro Data",CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12523935'>],Calcite Jdbc operators code needs to be public so it could be used in hive code to support usage of external jdbc tables in hive,2018-01-10T12:26:16.977+0000,2018-03-17T17:43:15.204+0000,Fixed,Major
ACCUMULO-143,Accumulo Hive,ACCUMULO,Task,Resolved,[],2,"[<JIRA IssueLink: id='12388105'>, <JIRA IssueLink: id='12368601'>]",Need to look into adding support for Accumulo to Hive,2011-11-14T13:52:18.779+0000,2014-08-22T20:42:21.821+0000,Won't Fix,Major
CALCITE-688,splitCondition does not behave correctly when one side of the condition references columns from different inputs,CALCITE,Bug,Closed,[],4,"[<JIRA IssueLink: id='12421823'>, <JIRA IssueLink: id='12433831'>, <JIRA IssueLink: id='12425151'>, <JIRA IssueLink: id='12421822'>]",,2015-04-20T15:09:14.695+0000,2015-08-10T19:05:48.600+0000,Fixed,Major
TEZ-3293,Fetch failures can cause a shuffle hang waiting for memory merge that never starts,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12469047'>],Tez jobs can hang in shuffle waiting for a memory merge that never starts.  When a MapOutput is reserved it increments usedMemory but when it is unreserved it decrements usedMemory _and_ commitMemory.  If enough shuffle failures occur of sufficient size then commitMemory may never reach the merge threshold even after all outstanding transfers have committed and thus hang the shuffle.,2016-06-08T20:19:32.836+0000,2016-07-09T19:16:01.879+0000,Fixed,Critical
IMPALA-10247,Data loading of functional-query ORC fails with EOFException,IMPALA,Bug,Open,[],2,"[<JIRA IssueLink: id='12603254'>, <JIRA IssueLink: id='12602114'>]","Data loading of functional-query on ORC tables occasionally fails with
{code:java}
16:41:21 Loading custom schemas (logging to /data/jenkins/workspace/impala-asf-master-core-erasure-coding/repos/Impala/logs/data_loading/load-custom-schemas.log)... 
16:41:24   Loading custom schemas OK (Took: 0 min 4 sec)
16:41:24 Started Loading functional-query data in background; pid 23644.
16:41:24 Started Loading TPC-H data in background; pid 23645.
16:41:24 Loading functional-query data (logging to /data/jenkins/workspace/impala-asf-master-core-erasure-coding/repos/Impala/logs/data_loading/load-functional-query.log)... 
16:41:24 Started Loading TPC-DS data in background; pid 23646.
16:41:24 Loading TPC-H data (logging to /data/jenkins/workspace/impala-asf-master-core-erasure-coding/repos/Impala/logs/data_loading/load-tpch.log)... 
16:41:24 Loading TPC-DS data (logging to /data/jenkins/workspace/impala-asf-master-core-erasure-coding/repos/Impala/logs/data_loading/load-tpcds.log)... 
16:48:51   Loading workload 'tpch' using exploration strategy 'core' OK (Took: 7 min 27 sec)
16:50:53     FAILED (Took: 9 min 29 sec)
16:50:53     'load-data functional-query exhaustive' failed. Tail of log: {code}
This looks similar to IMPALA-9923 but have a different error stacktrace:
{code:java}
2020-10-13T16:50:50,369  INFO [HiveServer2-Background-Pool: Thread-23853] ql.Driver: Executing command(queryId=jenkins_20201013165050_5dc3d632-a5c3-4f85-b2d3-8c1dc6682322): INSERT OVERWRITE TABLE tpcds_orc_def.web_sales
SELECT * FROM tpcds.web_sales
......
2020-10-13T16:50:53,423  INFO [HiveServer2-Background-Pool: Thread-23832] FileOperations: Reading manifest hdfs://localhost:20500/test-warehouse/managed/jointbl_orc_def/_tmp.base_0000001_0/000000_0.manifest
2020-10-13T16:50:53,423  INFO [HiveServer2-Background-Pool: Thread-23832] FileOperations: Reading manifest hdfs://localhost:20500/test-warehouse/managed/jointbl_orc_def/_tmp.base_0000001_0/000000_1.manifest
2020-10-13T16:50:53,423  INFO [HiveServer2-Background-Pool: Thread-23832] FileOperations: Looking at manifest file: hdfs://localhost:20500/test-warehouse/managed/jointbl_orc_def/_tmp.base_0000001_0/000000_0.manifest
2020-10-13T16:50:53,424 ERROR [HiveServer2-Background-Pool: Thread-23832] exec.Task: Job Commit failed with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.io.EOFException)'
org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1468)
        at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:798)
        at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803)
        at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803)
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.close(TezTask.java:627)
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:342)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:721)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:488)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:482)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:225)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:322)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:340)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.hadoop.hive.ql.exec.Utilities.handleDirectInsertTableFinalPath(Utilities.java:4587)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1462)
        ... 29 more
 {code}
The failed query is
{code:sql}
INSERT OVERWRITE TABLE tpcds_orc_def.web_sales
SELECT * FROM tpcds.web_sales
{code}
 ",2020-10-16T06:21:26.268+0000,2020-11-17T20:02:06.815+0000,,Critical
ATLAS-247,Hive Column level lineage,ATLAS,New Feature,Resolved,[],4,"[<JIRA IssueLink: id='12481429'>, <JIRA IssueLink: id='12481283'>, <JIRA IssueLink: id='12480835'>, <JIRA IssueLink: id='12458097'>]","hive_column is not inherited from DataSet, thus can't be using hive_process to track column level lineages

Is there specific reason that hive_column is not inheriting from Data Set? ",2015-10-23T14:24:30.261+0000,2016-09-30T07:21:57.950+0000,Fixed,Major
SPARK-2420,Dependency changes for compatibility with Hive,SPARK,Wish,Resolved,"[<JIRA Issue: key='SPARK-2848', id='12731833'>]",3,"[<JIRA IssueLink: id='12391719'>, <JIRA IssueLink: id='12391622'>, <JIRA IssueLink: id='12391725'>]","During the prototyping of HIVE-7292, many library conflicts showed up because Spark build contains versions of libraries that's vastly different from current major Hadoop version. It would be nice if we can choose versions that's in line with Hadoop or shading them in the assembly. Here are the wish list:

1. Upgrade protobuf version to 2.5.0 from current 2.4.1
2. Shading Spark's jetty and servlet dependency in the assembly.
3. guava version difference. Spark is using a higher version. I'm not sure what's the best solution for this.

The list may grow as HIVE-7292 proceeds.

For information only, the attached is a patch that we applied on Spark in order to make Spark work with Hive. It gives an idea of the scope of changes.",2014-07-09T19:44:42.144+0000,2014-11-01T13:54:18.087+0000,Fixed,Major
BIGTOP-2114,hive is broken after BIGTOP-2104,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12448092'>, <JIRA IssueLink: id='12447997'>]","Since BIGTOP-2104 was committed, the hive failed to build with the following error: 
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project spark-client: Compilation failure
[ERROR] /ws/build/hive/rpm/BUILD/apache-hive-1.2.1-src/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:[441,11] org.apache.hive.spark.client.RemoteDriver.ClientListener is not abstract and does not override abstract method onBlockUpdated(org.apache.spark.scheduler.SparkListenerBlockUpdated) in org.apache.spark.scheduler.SparkListener
",2015-11-06T06:43:49.890+0000,2015-11-06T19:36:09.023+0000,Fixed,Critical
FLUME-2433,Add kerberos support for Hive sink,FLUME,Bug,Patch Available,[],3,"[<JIRA IssueLink: id='12520629'>, <JIRA IssueLink: id='12392376'>, <JIRA IssueLink: id='12392358'>]","Add kerberos authentication support for Hive sink
FYI: The HCatalog API support for Kerberos is not available in hive 0.13.1 
this should be available in the next hive release.",2014-07-24T20:51:03.695+0000,2017-12-19T23:27:38.720+0000,,Major
TAJO-374,Investigate more efficient intermediate shuffle methods,TAJO,Improvement,Resolved,"[<JIRA Issue: key='TAJO-991', id='12731697'>, <JIRA Issue: key='TAJO-992', id='12731699'>]",2,"[<JIRA IssueLink: id='12379362'>, <JIRA IssueLink: id='12379363'>]","h3. Motivation

Currently, Tajo materializes intermediate data on local disks. Tajo stores one file for each partition. It becomes inefficient and not scalable as data volume and increase. In MR, this challenge was resolved by sorting intermediate key-values, grouping the same key data, and indexing on keys. But, It requires unnecessary sort and disk I/O. This is not feasible in Tajo.

h3. References
 * TAJO-292 is an ad-hoc resolution to reduce the number of intermediate files. But, it still is not scalable.
 * Optimizing MapReduce Job Performance (http://www.slideshare.net/cloudera/mr-perf)
 * Multilevel aggregation for Hadoop/MapReduce (http://www.slideshare.net/ozax86/prestrata-hadoop-word-meetup)
 * SAILFISH: A FRAMEWORK FOR LARGE SCALE DATA PROCESSING (http://research.yahoo.com/files/yl-2012-002.pdf)
 * MAPREDUCE-4502 - Node-level aggregation with combining the result of maps
 * MAPREDUCE-2841 - Task level native optimization",2013-12-03T15:31:57.921+0000,2014-09-14T13:33:49.860+0000,Fixed,Major
ACCUMULO-2353,Test improvments to java.io.InputStream.seek() for possible Hadoop patch,ACCUMULO,Task,Resolved,[],1,[<JIRA IssueLink: id='12459963'>],"At some point (early Java 7 I think, then backported to around Java 6 Update 45), the java.io.InputStream.seek() method was changed from reading byte[512] to byte[2048]. The difference can be seen in DeflaterInputStream, which has not been updated:

{noformat}
    public long skip(long n) throws IOException {
        if (n < 0) {
            throw new IllegalArgumentException(""negative skip length"");
        }
        ensureOpen();

        // Skip bytes by repeatedly decompressing small blocks
        if (rbuf.length < 512)
            rbuf = new byte[512];

        int total = (int)Math.min(n, Integer.MAX_VALUE);
        long cnt = 0;
        while (total > 0) {
            // Read a small block of uncompressed bytes
            int len = read(rbuf, 0, (total <= rbuf.length ? total : rbuf.length));

            if (len < 0) {
                break;
            }
            cnt += len;
            total -= len;
        }
        return cnt;
    }
{noformat}

and java.io.InputStream in Java 6 Update 45:

{noformat}
    // MAX_SKIP_BUFFER_SIZE is used to determine the maximum buffer skip to
    // use when skipping.
    private static final int MAX_SKIP_BUFFER_SIZE = 2048;

    public long skip(long n) throws IOException {

	long remaining = n;
	int nr;

	if (n <= 0) {
	    return 0;
	}
	
	int size = (int)Math.min(MAX_SKIP_BUFFER_SIZE, remaining);
	byte[] skipBuffer = new byte[size];

	while (remaining > 0) {
	    nr = read(skipBuffer, 0, (int)Math.min(size, remaining));
	    
	    if (nr < 0) {
		break;
	    }
	    remaining -= nr;
	}
	
	return n - remaining;
    }
{noformat}

In sample tests I saw about a 20% improvement in skip() when seeking towards the end of a locally cached compressed file. Looking at the DecompressorStream in HDFS, the seek method is a near copy of the old InputStream method:

{noformat}
  private byte[] skipBytes = new byte[512];
  @Override
  public long skip(long n) throws IOException {
    // Sanity checks
    if (n < 0) {
      throw new IllegalArgumentException(""negative skip length"");
    }
    checkStream();
    
    // Read 'n' bytes
    int skipped = 0;
    while (skipped < n) {
      int len = Math.min(((int)n - skipped), skipBytes.length);
      len = read(skipBytes, 0, len);
      if (len == -1) {
        eof = true;
        break;
      }
      skipped += len;
    }
    return skipped;
  }
{noformat}

This task is to evaluate the changes to DecompressorStream with a possible patch to HDFS and possible bug request to Oracle to port the InputStream.seek changes to DeflaterInputStream.seek",2014-02-11T23:54:37.343+0000,2016-10-20T16:21:27.448+0000,Won't Fix,Minor
BIGTOP-1278,bump pig version to 0.12.1,BIGTOP,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12386700'>],,2014-04-17T00:44:22.983+0000,2014-04-24T23:01:24.044+0000,Fixed,Major
CHUKWA-444,Redefine Chukwa time series storage,CHUKWA,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12333130'>],"The current Chukwa Record format is not suitable for data visualization.  It is more like an archive format which combines data from multiple sources (hosts), and group them into a sorted time partitioned sequence file.  Most of people collected data for two reasons, archive and data analysis.  The current chukwa record format is fine for archive, but it is not so great for data analysis.  Data analysis could be further break down into two different types.  1) Data can be aggregated and summarized, such as metrics.  2) Data that can not be summarized, like job history.  Type 1 data is useful for visualization by graph, and type 2 data is useful by plain text viewing or search for a particular event.

By the above rational, it probably makes sense to restructure Chukwa Records for data analysis.  Outside of Hadoop world, rrdtools is great for time series data storage, and optimized for metrics from a single source, i.e. a host.  RRD data file fragments badly when there are hundred of thousands of sources.  Chukwa time series data storage should be able to combine multiple data sources into one Chukwa file to combat file fragmentation problem.",2009-12-30T08:17:01.421+0000,2013-05-02T02:29:30.185+0000,Fixed,Major
AVRO-993,Add methods back to GenericDatumReader that were removed in AVRO-839,AVRO,Bug,Closed,[],2,"[<JIRA IssueLink: id='12347299'>, <JIRA IssueLink: id='12347088'>]","As part of AVRO-839 three methods were moved from GenericDatumReader to GenericData:
* newRecord(Object, Schema)
* createFixed(Object old, Schema schema)
* createFixed(Object, byte[], Schema)

Although their visibility is protected, they are part of the public API because GenericDatumReader is public.  Therefore these methods should have been deprecated before being removed.  The removal of these methods caused PIG-2463.",2012-01-16T07:48:26.474+0000,2013-05-02T02:29:48.578+0000,Fixed,Major
THRIFT-2678,Make max message size configurable,THRIFT,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12395195'>, <JIRA IssueLink: id='12395194'>]",THRIFT-2660 set a max message size of 100MB. This is likely large enough for most use cases but this hard limit should be configurable.,2014-08-26T16:10:12.988+0000,2014-10-31T01:55:50.213+0000,Fixed,Major
GORA-89,Avoid HBase MiniCluster restarts to shorten gora-hbase tests,GORA,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12357019'>, <JIRA IssueLink: id='12348517'>]","Currently our hbase tests are taking forever and a day. We should shorten the time by avoiding MiniCluster restarts.
Just implement the cluster as a singleton and clean up the tables in
between test by doing a scan and deletes for all rows. It's much
faster than restarting the cluster.

For code referenece please see the implementation here[1]. The class is
HBaseClusterSingleton. It needs some refactoring but I think it's
enough to speed your tests.

Thanks Ioan for the heads up.

[1] http://svn.apache.org/repos/asf/james/mailbox/trunk/hbase/src/test/java/org/apache/james/mailbox/hbase/",2012-01-30T15:23:37.522+0000,2022-03-03T14:27:46.799+0000,Fixed,Critical
IMPALA-10316,load_nested.py failed due to out of memory during Jenkins GVO,IMPALA,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12641860'>, <JIRA IssueLink: id='12642289'>, <JIRA IssueLink: id='12640447'>, <JIRA IssueLink: id='12623029'>]","The following job failed due to out of memory:

[https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/12588] (please click on ""Don't keep this build forever"" once this issue is resolved)

Relevant log lines:
{noformat}
02:33:42 Loading nested orc data (logging to /home/ubuntu/Impala/logs/data_loading/load-nested.log)... 
02:35:39     FAILED (Took: 1 min 57 sec)
02:35:39     '/home/ubuntu/Impala/testdata/bin/load_nested.py -t tpch_nested_orc_def -f orc/def' failed. Tail of log:
02:35:39 2020-11-11 02:35:06,225 INFO:load_nested[348]:Executing: 
02:35:39 
02:35:39       CREATE EXTERNAL TABLE supplier
02:35:39       STORED AS orc
02:35:39       TBLPROPERTIES('orc.compress' = 'ZLIB','external.table.purge'='TRUE')
02:35:39       AS SELECT * FROM tmp_supplier
02:35:39 Traceback (most recent call last):
02:35:39   File ""/home/ubuntu/Impala/testdata/bin/load_nested.py"", line 415, in <module>
02:35:39     load()
02:35:39   File ""/home/ubuntu/Impala/testdata/bin/load_nested.py"", line 349, in load
02:35:39     hive.execute(stmt)
02:35:39   File ""/home/ubuntu/Impala/tests/comparison/db_connection.py"", line 206, in execute
02:35:39     return self._cursor.execute(sql, *args, **kwargs)
02:35:39   File ""/home/ubuntu/Impala/infra/python/env-gcc7.5.0/lib/python2.7/site-packages/impala/hiveserver2.py"", line 331, in execute
02:35:39     self._wait_to_finish()  # make execute synchronous
02:35:39   File ""/home/ubuntu/Impala/infra/python/env-gcc7.5.0/lib/python2.7/site-packages/impala/hiveserver2.py"", line 413, in _wait_to_finish
02:35:39     raise OperationalError(resp.errorMessage)
02:35:39 impala.error.OperationalError: Error while compiling statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1605060173780_0039_2_00, diagnostics=[Task failed, taskId=task_1605060173780_0039_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Container container_1605060173780_0039_01_000002 finished with diagnostics set to [Container failed, exitCode=-104. [2020-11-11 02:35:11.768]Container [pid=16810,containerID=container_1605060173780_0039_01_000002] is running 7729152B beyond the 'PHYSICAL' memory limit. Current usage: 1.0 GB of 1 GB physical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing container.{noformat}",2020-11-11T09:55:16.829+0000,2022-06-21T17:35:26.467+0000,Fixed,Critical
SPARK-2895,Support mapPartitionsWithContext in Spark Java API,SPARK,New Feature,Resolved,[],6,"[<JIRA IssueLink: id='12393856'>, <JIRA IssueLink: id='12397835'>, <JIRA IssueLink: id='12393608'>, <JIRA IssueLink: id='12395488'>, <JIRA IssueLink: id='12394587'>, <JIRA IssueLink: id='12393607'>]","This is a requirement from Hive on Spark, mapPartitionsWithContext only exists in Spark Scala API, we expect to access from Spark Java API. 
For HIVE-7627, HIVE-7843, Hive operators which are invoked in mapPartitions closure need to get taskId.",2014-08-07T02:51:04.861+0000,2014-09-27T04:30:49.863+0000,Fixed,Major
SLIDER-500,registration failing if username isn't valid DNS entry,SLIDER,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12398739'>],"registration failing if the user has underscore ""user_name"" as check for DNS-validity fails. Registry was expected to punycode to valid entries.

this may be a yarn registry bug, but slider could work around it",2014-10-10T17:32:05.138+0000,2014-10-13T18:36:50.225+0000,Fixed,Blocker
IMPALA-10934,Enable table definition over a single file,IMPALA,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12623752'>],"Some improvements are needed to enable table definition over a single file (HIVE-25569) to work from Impala as well.

note: that these changes will not work by themselfs - they will need the FS drivers which are in HIVE-25569 to be available",2021-09-28T14:45:50.343+0000,2022-01-12T23:39:13.854+0000,Fixed,Major
TEZ-2168,Fix application dependencies on mutually exclusive artifacts: tez-yarn-timeline-history and tez-yarn-timeline-history-with-acls,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12420531'>],"Feedback from Chris Wensel on dev list related to simplifying the build file for an app dependent on Tez. 

Currently, based on hadoop versions, one needs to choose between mutually exclusive artifacts: tez-yarn-timeline-history and tez-yarn-timeline-history-with-acls.

Suggested options: 

create a single artifact tez-yarn-timeline-history compiled with a default dep of hadoop 2.6, that includes the Manager. update the TezClient code to gracefully fail if the Manager is not applicable (the runtime env is Hadoop 2.4).

or

offer tez-yarn-timeline-history-with-acls as an optional artifact for Hadoop 2.6 deployments, with the single Manager class in it, which in turn requires the tez-yarn-timeline-history artifact -- which is sufficient for a 2.4 runtime. if the user provides the additional -with-acls artifact, they are knowingly going to have problems on Hadoop 2.4.


",2015-03-02T22:44:47.159+0000,2015-06-30T04:53:21.787+0000,Fixed,Major
OOZIE-2355,Hive2 Action doesn't pass along oozie configs to jobconf,OOZIE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12471953'>, <JIRA IssueLink: id='12436475'>, <JIRA IssueLink: id='12444326'>]","We're currently only passing along the MR yarn tags property to the child job in the Hive2 action.  We do this via the {{\--hiveconf}} CLI argument, so it has to be explicitly done, unlike most other action types.  We should also do this for any {{oozie.\*}} properties other than {{oozie.launcher.*}} ones.  This way, properties such as {{oozie.job.id}} and {{oozie.job.info}} will be correctly set in the child jobs launched by Hive Server 2.",2015-09-03T23:28:13.671+0000,2016-12-02T21:03:09.601+0000,Fixed,Major
INFRA-20074,"Please up the max user processes on hadoop build boxes; currently 10,000",INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12584829'>],"Please up the maximum user processes count on the hadoop build boxes. It is currently 10000 (see ulimit -a below). Can we set it to 16k? Or 14k? (presuming 32k is upper bound as it can be on some linux). Rationale is that each hadoop box has two executors. Let the executors have half the allow threads (w/ some space for the os).

The context is that the hbase is project trying to run tests faster by upping how many tests we can run in parallel. After study (HBASE-24072) we have found that we are bound by thread count; at extremes, we can run into 'Cannot create native thread'.

I tried upping process count in test context but didn't work.

Thanks",2020-04-04T00:16:10.590+0000,2020-04-06T17:26:08.157+0000,Fixed,Major
TEZ-3571,Tez UI: Display a Total Timeline View for Hive Queries,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12517726'>],"This view would visualize perfLogs, and gives a graphical representation of whats happened while in the query.",2017-01-07T11:51:11.977+0000,2017-10-17T06:50:35.904+0000,Fixed,Major
IMPALA-8152,Aggregate Commands on HBase Table Omit Null Values,IMPALA,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12553364'>, <JIRA IssueLink: id='12553058'>, <JIRA IssueLink: id='12553059'>]","We have an HBase-backed impala table, which has a string column (for the purpose of this jira, {{sCol}})

There are records where that column is null, which we can observe with queries like {{select * from table where sCol is null limit 1}}

However, when we run these commands, we get bad results:
{code:sql}
-- Returns 0
select count(*) from table where sCol is null;
-- Returns only rows for string values (we only have a few options in this case), no row for null
select sCol, count(*) from table group by sCol
{code}

These commands work as expected on parquet-backed tables. They also do not work in Hive, where I will file a jira shortly.
 ",2019-02-01T13:58:26.549+0000,2019-02-05T13:08:17.375+0000,Duplicate,Major
ORC-491,PPD: Column name lookups need to look a struct deeper for ACID,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12584973'>, <JIRA IssueLink: id='12558733'>]","{code}
  public static int[] mapSargColumnsToOrcInternalColIdx(
                            List<PredicateLeaf> sargLeaves,
                            SchemaEvolution evolution) {
    int[] result = new int[sargLeaves.size()];
    Arrays.fill(result, -1);
    for(int i=0; i < result.length; ++i) {
      String colName = sargLeaves.get(i).getColumnName();
      result[i] = findColumns(evolution, colName);
    }
    return result;
  }
{code}

returns -1 for all data column names, because it was one level deeper.

{code}
sarg = [(IS_NULL date_time_date)]
colName = date_time_date
struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<date_time_date:date,term:string>>
{code}",2019-04-10T21:40:03.771+0000,2020-04-06T13:47:02.450+0000,Fixed,Major
ORC-141,inconsistent memorymanager usage after PhysicalWriter patch,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12493578'>],,2017-02-03T02:34:37.046+0000,2017-02-13T19:10:10.185+0000,Fixed,Major
TEZ-2169,Add NDC context to various threads and pools,TEZ,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12409617'>, <JIRA IssueLink: id='12409740'>]",,2015-03-03T23:24:47.536+0000,2015-06-30T04:53:21.413+0000,Fixed,Major
HDDS-1,Remove SCM Block DB,HDDS,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12533528'>, <JIRA IssueLink: id='12533527'>]",The block/key information is maintained by Ozone Master (a.k.a. KSM). This ticket is opened to remove the redundant block db at SCM. ,2018-04-25T16:00:26.490+0000,2018-07-03T21:09:34.413+0000,Fixed,Major
INFRA-9360,Github is out of sync with Hive,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12412304'>],For a few days now. Please fix at your earliest convenience.,2015-03-30T17:49:49.918+0000,2015-12-07T05:51:16.328+0000,Done,Blocker
TEZ-2234,Add API for statistics information - allow vertex managers to get output size per source vertex,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12411880'>],Vertex managers may need per source vertex output stats to make reconfiguration decisions.,2015-03-25T21:10:04.058+0000,2015-06-30T04:53:18.108+0000,Fixed,Major
SPARK-4762,Add support for tuples in 'where in' clause query,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12402794'>],"Currently, in the where in clause the filter is applied only on a single column. We can enhance it to accept filter on multiple columns.

So current support is for queries like :
Select * from table where c1 in (value1,value2,...value n);

Need to add support for queries like :
Select * from table where (c1,c2,... cn) in ((value1,value2...value n), (value1' , value2' ... ,value n').... )

Also, we can add optimized version of where in clause of tuples , where we create a hashset of the filter tuples for matching rows.

This also requires a change in the hive parser since currently there is no support for multiple columns in IN clause.",2014-12-05T09:01:08.694+0000,2014-12-19T20:31:42.978+0000,Won't Fix,Major
SQOOP-1032,Add the --bulk-load-dir option to support the HBase doBulkLoad function,SQOOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12376667'>, <JIRA IssueLink: id='12376663'>]","HBase supply the LoadIncrementalHFiles.LoadIncrementalHFiles method for bulk load, so this feature added with --bulk-load-dir option to support the HBase doBulkLoad function",2013-05-07T11:12:26.280+0000,2013-10-11T01:29:00.675+0000,Fixed,Major
FLINK-19447,"HBaseConnectorITCase.HBaseTestingClusterAutoStarter failed with ""Master not initialized after 200000ms""",FLINK,Bug,Closed,[],1,[<JIRA IssueLink: id='12599735'>],"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407

{code}
2020-09-28T21:52:21.2146147Z org.apache.flink.connector.hbase2.HBaseConnectorITCase  Time elapsed: 208.382 sec  <<< ERROR!
2020-09-28T21:52:21.2146638Z java.io.IOException: Shutting down
2020-09-28T21:52:21.2147004Z 	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:266)
2020-09-28T21:52:21.2147637Z 	at org.apache.hadoop.hbase.MiniHBaseCluster.<init>(MiniHBaseCluster.java:116)
2020-09-28T21:52:21.2148120Z 	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1142)
2020-09-28T21:52:21.2148831Z 	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1107)
2020-09-28T21:52:21.2149347Z 	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1061)
2020-09-28T21:52:21.2149896Z 	at org.apache.flink.connector.hbase2.util.HBaseTestingClusterAutoStarter.setUp(HBaseTestingClusterAutoStarter.java:122)
2020-09-28T21:52:21.2150721Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-28T21:52:21.2151136Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-28T21:52:21.2151609Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-28T21:52:21.2152039Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-28T21:52:21.2152462Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-28T21:52:21.2152941Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-28T21:52:21.2153489Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-28T21:52:21.2153962Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-09-28T21:52:21.2154406Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-28T21:52:21.2154828Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-28T21:52:21.2155381Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
2020-09-28T21:52:21.2155864Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
2020-09-28T21:52:21.2156378Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-28T21:52:21.2156865Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
2020-09-28T21:52:21.2157458Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
2020-09-28T21:52:21.2157993Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
2020-09-28T21:52:21.2158470Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2020-09-28T21:52:21.2158890Z Caused by: java.lang.RuntimeException: Master not initialized after 200000ms
2020-09-28T21:52:21.2159350Z 	at org.apache.hadoop.hbase.util.JVMClusterUtil.waitForEvent(JVMClusterUtil.java:229)
2020-09-28T21:52:21.2159823Z 	at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:197)
2020-09-28T21:52:21.2160270Z 	at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:413)
2020-09-28T21:52:21.2160800Z 	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:259)
2020-09-28T21:52:21.2161096Z 	... 22 more
{code}",2020-09-29T06:43:41.335+0000,2021-02-19T07:32:38.442+0000,Fixed,Blocker
BIGTOP-1913,Update hive to 1.2.1,BIGTOP,Bug,Resolved,[],5,"[<JIRA IssueLink: id='12433128'>, <JIRA IssueLink: id='12440705'>, <JIRA IssueLink: id='12437408'>, <JIRA IssueLink: id='12429214'>, <JIRA IssueLink: id='12429215'>]","I had issues with complex data structures in 1.0.0. 

Building hive-1.2.1 was surprisingly easy and the issues I had before (jline jar incompatibilites) seems to be fixed by a recent zookeeper update.",2015-06-29T20:16:30.514+0000,2015-11-06T23:42:45.577+0000,Fixed,Major
OOZIE-3492,[spark-action] Missing HADOOP_CONF_DIR property,OOZIE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12586461'>, <JIRA IssueLink: id='12561426'>]","Spark action fails with the following error:
{noformat}
Failing Oozie Launcher, When running with master 'yarn-cluster' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
org.apache.spark.SparkException: When running with master 'yarn-cluster' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
        at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:657)
        at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:290)
        at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:251)
        at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:120)
        at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$1.<init>(SparkSubmit.scala:926)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:926)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:81)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:939)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:948)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
        at org.apache.oozie.action.hadoop.SparkMain.runSpark(SparkMain.java:186)
        at org.apache.oozie.action.hadoop.SparkMain.run(SparkMain.java:93)
        at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:104)
        at org.apache.oozie.action.hadoop.SparkMain.main(SparkMain.java:60)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.oozie.action.hadoop.LauncherAM.runActionMain(LauncherAM.java:410)
        at org.apache.oozie.action.hadoop.LauncherAM.access$300(LauncherAM.java:55)
        at org.apache.oozie.action.hadoop.LauncherAM$2.run(LauncherAM.java:223)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.oozie.action.hadoop.LauncherAM.run(LauncherAM.java:217)
        at org.apache.oozie.action.hadoop.LauncherAM$1.run(LauncherAM.java:153)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.oozie.action.hadoop.LauncherAM.main(LauncherAM.java:141){noformat}
if the followings are true:
 * yarn.nodemanager.env-whitelist does not contain {{HADOOP_CONF_DIR}}
 * Hadoop version >= 3.1 (or YARN-7677 is backported)",2019-05-23T11:36:01.888+0000,2020-04-21T17:15:03.380+0000,Fixed,Major
CALCITE-2050,Exception when pushing postaggregates into Druid,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12520103'>, <JIRA IssueLink: id='12520104'>, <JIRA IssueLink: id='12520105'>]","After Calcite is upgraded to 1.14 and the rule to push post-aggregations to Druid is enabled, the following query will fail:
{code}
EXPLAIN
SELECT language, robot, sum(added) - sum(delta) AS a
FROM druid_table_1
WHERE extract (week from `__time`) IN (10,11)
  AND robot='Bird Call'
GROUP BY language, robot;
{code}

The error we get is the following:
{code}
Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" language, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" robot, DOUBLE a) NOT NULL
expression type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" language, DOUBLE postagg#0) NOT NULL
set is rel#1507:HiveProject.HIVE.[](input=HepRelVertex#1514,language=$0,robot=CAST(_UTF-16LE'Bird Call'):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"",a=-($1, $2))
expression is DruidQuery#1516
{code}",2017-11-14T21:33:01.225+0000,2017-12-09T18:21:21.591+0000,Fixed,Major
ZOOKEEPER-1568,multi should have a non-transaction version,ZOOKEEPER,Improvement,Open,[],1,[<JIRA IssueLink: id='12359343'>],"Currently multi is transactional, i.e. all or none.  However, sometimes, we don't want that.  We want all operations to be executed.  Even some operation(s) fails, it is ok. We just need to know the result of each operation.",2012-10-23T17:43:55.795+0000,2013-12-20T20:04:06.129+0000,,Major
SLIDER-938,Add ability to resize containers (Hadoop 2.8+),SLIDER,New Feature,Open,[],1,[<JIRA IssueLink: id='12440087'>],Hadoop 2.8 will add container resize in YARN-1197: support that for dynamic container resize,2015-09-16T15:52:27.526+0000,2017-03-09T06:16:16.989+0000,,Major
MRUNIT-67,o.a.h.mrunit.mapreduce.MapReduceDriver should support a combiner,MRUNIT,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12349304'>],"o.a.h.mrunit.MapReduceDriver supports a combiner, so should o.a.h.mrunit.mapreduce.MapReduceDriver",2012-02-26T16:11:38.368+0000,2012-03-15T08:26:07.159+0000,Fixed,Major
CURATOR-111,CuratorFramework.Builder should allow adding multiple auths,CURATOR,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12404588'>],"Currently, one can add a single authentication scheme/bytes when building CuratorFramework. It would be handy to add multiple.",2014-06-04T03:58:38.195+0000,2015-01-13T12:49:42.153+0000,Fixed,Major
TEZ-3286,Allow clients to set processor reserved memory per vertex (instead of per container),TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12471415'>, <JIRA IssueLink: id='12474061'>, <JIRA IssueLink: id='12474063'>]","tez.task.scale.memory.reserve-fraction can be set by clients to control how much memory is available to the processor. Ths values applies at a container level though, instead of at a vertex level.
In case of a hash-join - the processor typically needs more memory. In case of  a Shuffle join - the processor may not need as much. In DAGs with a mix of map joins and shuffle joins - setting this at a container level is sub-optimal.

To a large extent this comes down to propagating vertex configs to the container / task.",2016-06-02T23:43:54.663+0000,2016-07-24T03:30:33.148+0000,Fixed,Major
SPARK-4290,Provide an equivalent functionality of distributed cache as MR does,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12400790'>, <JIRA IssueLink: id='12400791'>]","MapReduce allows client to specify files to be put in distributed cache for a job and the framework guarentees that the file will be available in local file system of a node where a task of the job runs and before the tasks actually starts. While this might be achieved with Yarn via hacks, it's not available in other clusters. It would be nice to have such an equivalent functionality like this in Spark.

It would also complement Spark's broadcast variable, which may not be suitable in certain scenarios.",2014-11-07T01:53:03.884+0000,2015-02-08T23:30:28.063+0000,Not A Problem,Major
STORM-1014,Use Hive Streaming API bucket info to bucket correctly,STORM,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12435772'>],"The Storm bolt get a random bucket and writes data to it. Hive has expectation that rows (tuples for storm) are distributed across buckets using Hive's hash distribution. Writing to a random bucket by Storm leads to Hive optimizations that rely on bucketing to return incorrect results.

The solution is for Storm Hive Bolt to use Hive bucket distribution information and put the rows/tuples in the correct buckets. This relies on Hive-11672. 

This might require a shuffle within Storm. ",2015-08-28T00:28:17.761+0000,2015-11-10T00:17:21.387+0000,Implemented,Critical
PHOENIX-1681,Use the new Region interfaces,PHOENIX,Sub-task,Closed,[],5,"[<JIRA IssueLink: id='12408958'>, <JIRA IssueLink: id='12420572'>, <JIRA IssueLink: id='12408956'>, <JIRA IssueLink: id='12408957'>, <JIRA IssueLink: id='12412420'>]","HBase is introducing a new interface, Region, a supportable public/evolving subset of HRegion. Use this instead of HRegion in all places where we are using HRegion today",2015-02-24T19:01:25.999+0000,2015-11-21T02:16:44.132+0000,Fixed,Major
INFRA-17632,ReviewBoard integration for hbase-git repo,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12551739'>],"the HBase project recently migrated to gitbox and reviewboard doesn't appear to work any more.

I still see two repos on reviewboard:

* hbase which I think is maybe from way back when we were svn?
* hbase-git which just shows blanks; I'm guessing it's pointing at git-wip still

Could we remove the svn one and update the other to point at gitbox?",2019-01-12T14:46:33.713+0000,2019-01-12T16:32:45.833+0000,Fixed,Major
PHOENIX-4825,Replace usage of HBase Base64 implementation with java.util.Base64,PHOENIX,Task,Closed,[],2,"[<JIRA IssueLink: id='12539904'>, <JIRA IssueLink: id='12579369'>]",,2018-07-30T18:18:43.671+0000,2021-02-10T10:01:36.478+0000,Fixed,Blocker
INFRA-23236,Please add 2 large spec nodes to ci-hbase,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12639482'>],"the HBase project has a plan for building out some more of our ci in a way that needs larger single nodes than the ones used primarily on ci-hbase.

please add 2 ""large spec"" nodes to the ci-hbase jenkins controller.

thread: https://lists.apache.org/thread/qwcqyb9h2t8949nojss75w078g13lcy9",2022-05-06T15:33:06.365+0000,2022-05-17T14:36:47.058+0000,Fixed,Major
ZOOKEEPER-2175,Checksum validation for malformed packets needs to handle.,ZOOKEEPER,Bug,Open,[],1,[<JIRA IssueLink: id='12422472'>]," *Session Id from ZK :* 

2015-04-15 21:24:54,257 | INFO  | CommitProcessor:22 | Established session 0x164cb2b3e4b36ae4 with negotiated timeout 45000 for client /160.149.0.117:44586 | org.apache.zookeeper.server.ZooKeeperServer.finishSessionInit(ZooKeeperServer.java:623)
2015-04-15 21:24:54,261 | INFO  | NIOServerCxn.Factory:160-149-0-114/160.149.0.114:24002 | Successfully authenticated client: authenticationID=hdfs/hadoop@HADOOP.COM;  authorizationID=hdfs/hadoop@HADOOP.COM. | org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handleAuthorizeCallback(SaslServerCallbackHandler.java:118)
2015-04-15 21:24:54,261 | INFO  | NIOServerCxn.Factory:160-149-0-114/160.149.0.114:24002 | Setting authorizedID: hdfs/hadoop@HADOOP.COM | org.apache.zookeeper.server.auth.SaslServerCallbackHandler.handleAuthorizeCallback(SaslServerCallbackHandler.java:134)
2015-04-15 21:24:54,261 | INFO  | NIOServerCxn.Factory:160-149-0-114/160.149.0.114:24002 | adding SASL authorization for authorizationID: hdfs/hadoop@HADOOP.COM | org.apache.zookeeper.server.ZooKeeperServer.processSasl(ZooKeeperServer.java:1009)
2015-04-15 21:24:54,262 | INFO  | ProcessThread(sid:22 cport:-1): | Got user-level KeeperException when processing  *{color:red}sessionid:0x164cb2b3e4b36ae4{color}*  type:create cxid:0x3 zxid:0x20009fafc txntype:-1 reqpath:n/a Error Path:/hadoop-ha/hacluster/ActiveStandbyElectorLock Error:KeeperErrorCode = NodeExists for /hadoop-ha/hacluster/ActiveStandbyElectorLock | org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:648)

 *ZKFC Received :*  ZK client

2015-04-15 21:24:54,237 | INFO  | main-SendThread(160-149-0-114:24002) | Socket connection established to 160-149-0-114/160.149.0.114:24002, initiating session | org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:854)
2015-04-15 21:24:54,257 | INFO  | main-SendThread(160-149-0-114:24002) | Session establishment complete on server 160-149-0-114/160.149.0.114:24002,  *{color:blue}sessionid = 0x144cb2b3e4b36ae4 {color}* , negotiated timeout = 45000 | org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1259)
2015-04-15 21:24:54,260 | INFO  | main-EventThread | EventThread shut down | org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
2015-04-15 21:24:54,262 | INFO  | main-EventThread | Session connected. | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:547)
2015-04-15 21:24:54,264 | INFO  | main-EventThread | Successfully authenticated to ZooKeeper using SASL. | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:573)

one bit corrupted..please check the following for same..

144cb2b3e4b36ae4=1010001001100101100101011001111100100101100110110101011100100
164cb2b3e4b36ae4=1011001001100101100101011001111100100101100110110101011100100",2015-04-24T02:17:57.695+0000,2016-04-07T07:45:22.738+0000,,Major
YETUS-634,maven plugin dropping '--batch-mode' maven argument,YETUS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12534209'>],"while tracking down an unrelated issue, I noticed that HBase tests that rely on maven weren't including the '--batch-mode' flag, even though it's the first thing the plugin defines.

The same thing shows up in current Hadoop precommit runs.",2018-05-16T16:50:19.673+0000,2018-05-20T14:46:44.910+0000,Fixed,Minor
BIGTOP-960,Remove workaround for HBASE-6263 in hbase-thrift start,BIGTOP,Bug,Closed,[],2,"[<JIRA IssueLink: id='12368436'>, <JIRA IssueLink: id='12368435'>]",,2013-05-02T01:30:40.132+0000,2013-06-21T23:49:44.975+0000,Fixed,Major
TEZ-711,Fix memory leak when not reading from inputs due to caching,TEZ,Bug,Open,[],4,"[<JIRA IssueLink: id='12380822'>, <JIRA IssueLink: id='12384902'>, <JIRA IssueLink: id='12380806'>, <JIRA IssueLink: id='12380807'>]","  When you are reading from inputs and caching objects with vertex scope, you don't have to read the input again when container is reused. But it allocates memory and that leaks causing OOM. KeyValueReader does not have a API to close the reader to clear allotted memory without reading from it. Also if there was a option to pre-close inputs in Processor and not fetch input at all over the wire and do shuffle/sort it would be a good optimization.",2014-01-08T18:40:10.465+0000,2017-03-14T03:40:35.842+0000,,Critical
TEZ-3532,Backport MAPREDUCE-6808. Log map attempts as part of shuffle handler audit log,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12489410'>],,2016-11-07T21:01:20.375+0000,2017-08-22T00:02:55.997+0000,Fixed,Major
OOZIE-3227,Eliminate duplicate dependencies when using Hadoop 3 DistributedCache,OOZIE,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12533449'>, <JIRA IssueLink: id='12536838'>, <JIRA IssueLink: id='12532409'>]","Using Hadoop 3 it is not allowed to have multiple dependencies with same file names on the list of *mapreduce.job.cache.files*.

The issue occurs when I have the same file name on multiple sharelib folders and/or my application's lib folder. This can be avoided but not easy all the time.

I suggest to remove the duplicates from this list.
A quick workaround for the source code in JavaActionExecutor is like:
{code}
            removeDuplicatedDependencies(launcherJobConf, ""mapreduce.job.cache.files"");
            removeDuplicatedDependencies(launcherJobConf, ""mapreduce.job.cache.archives"");
......
private void removeDuplicatedDependencies(JobConf conf, String key) {
        final Map<String, String> nameToPath = new HashMap<>();
        StringBuilder uniqList = new StringBuilder();
        for(String dependency: conf.get(key).split("","")) {
            final String[] arr = dependency.split(""/"");
            final String dependencyName = arr[arr.length - 1];
            if(nameToPath.containsKey(dependencyName)) {
                LOG.warn(dependencyName + "" ["" + dependency + ""] is already defined in "" + key + "". Skipping..."");
            } else {
                nameToPath.put(dependencyName, dependency);
                uniqList.append(dependency).append("","");
            }
        }
        uniqList.setLength(uniqList.length() - 1);
        conf.set(key, uniqList.toString());
    }
{code}

Other way is to eliminate the deprecated *org.apache.hadoop.filecache.DistributedCache*.

I am going to have a deeper understanding how we should use distributed cache and all the comments are welcome.",2018-04-23T15:07:45.078+0000,2019-01-18T09:03:05.686+0000,Fixed,Major
HCATALOG-647,Webhcat resource /templeton/v1/status doesn't track Hive job progress,HCATALOG,Bug,Open,[],1,[<JIRA IssueLink: id='12380579'>],"Problem: Unable to track Hive job progress through webhcat.
Cause:
TempletonUtils has code to parse PIG and JAR child jobid and percentage progress but none for Hive jobs. extractPercentComplete() and extractChildJobId() are supposed to do this parsing. 
Effect: 

/templeton/v1/queue/$job_id?user.name=$user returns no job progress info. The jobid param in the json result that's supposed to contain the child hive jobid contains the templetoncontrollerjob id instead leaving the parent jobid null.",2013-05-31T16:56:31.359+0000,2014-01-03T01:15:02.917+0000,,Major
CALCITE-1652,"Allow GROUPING to have multiple arguments, like GROUPING_ID",CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12495012'>, <JIRA IssueLink: id='12495004'>]","When initially added to the SQL standard {{GROUPING}} took one argument, a column reference. Now it can have multiple arguments, like the {{GROUPING_ID}} function.

In SQL standard 2014, see section 6.9 &lt;set function specification&gt;, and feature T433, ""Multiargument GROUPING function"".

This change would extend {{GROUPING}} to allow one or more arguments (i.e. what {{GROUPING_ID}} currently does), and make {{GROUPING_ID}} a synonym for {{GROUPING}}. {{GROUP_ID}} is not affected, and continues to take zero arguments.",2017-02-21T19:18:52.767+0000,2017-03-24T03:20:09.145+0000,Fixed,Major
PHOENIX-4440,Local index split/merge IT tests are failing,PHOENIX,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12523437'>],IndexHalfStoreFileReaderGenerator#preStoreFileReaderOpen is not getting called and going by default behaviour so split/merge not working.  ,2017-12-07T04:20:38.815+0000,2018-07-26T01:15:18.425+0000,Fixed,Major
SPARK-25855,Don't use Erasure Coding for event log files,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12546796'>, <JIRA IssueLink: id='12546925'>]","While testing spark with hdfs erasure coding (new in hadoop 3), we ran into a bug with the event logs.  The main issue was a bug in hdfs (HDFS-14027), but it did make us wonder whether Spark should be using EC for event log files in general.  Its a poor choice because EC currently implements {{hflush()}} or {{hsync()}} as no-ops, which mean you won't see anything in your event logs until the app is complete.  That isn't necessarily a bug, but isn't really great.  So I think we should ensure EC is always off for event logs.

IIUC there is *not* a problem with applications which die without properly closing the outputstream.  It'll take a while for the NN to realize the client is gone and finish the block, but the data should get there eventually.

Also related are SPARK-24787 & SPARK-19531.

The space savings from EC would be nice as the event logs can get somewhat large, but I think other factors outweigh this.",2018-10-26T18:07:14.624+0000,2019-12-26T05:08:54.862+0000,Fixed,Major
PARQUET-41,Add bloom filters to parquet statistics,PARQUET,New Feature,Resolved,"[<JIRA Issue: key='PARQUET-319', id='12840412'>, <JIRA Issue: key='PARQUET-1326', id='13166381'>, <JIRA Issue: key='PARQUET-1327', id='13166385'>, <JIRA Issue: key='PARQUET-1328', id='13166387'>, <JIRA Issue: key='PARQUET-1329', id='13166388'>, <JIRA Issue: key='PARQUET-1332', id='13167539'>, <JIRA Issue: key='PARQUET-1342', id='13169103'>, <JIRA Issue: key='PARQUET-1377', id='13178399'>, <JIRA Issue: key='PARQUET-1380', id='13179104'>, <JIRA Issue: key='PARQUET-1391', id='13179794'>, <JIRA Issue: key='PARQUET-1453', id='13195167'>, <JIRA Issue: key='PARQUET-1495', id='13210716'>, <JIRA Issue: key='PARQUET-1516', id='13212138'>, <JIRA Issue: key='PARQUET-1592', id='13238753'>, <JIRA Issue: key='PARQUET-1625', id='13244957'>, <JIRA Issue: key='PARQUET-1630', id='13248767'>, <JIRA Issue: key='PARQUET-1795', id='13284808'>]",7,"[<JIRA IssueLink: id='12537385'>, <JIRA IssueLink: id='12404638'>, <JIRA IssueLink: id='12608386'>, <JIRA IssueLink: id='12613501'>, <JIRA IssueLink: id='12581573'>, <JIRA IssueLink: id='12609513'>, <JIRA IssueLink: id='12609648'>]","For row groups with no dictionary, we could still produce a bloom filter. This could be very useful in filtering entire row groups.
Pull request:
https://github.com/apache/parquet-mr/pull/215",2014-07-24T06:28:17.838+0000,2021-04-18T04:11:59.886+0000,Fixed,Major
SENTRY-2430,Avoid getting the complete partition objects and fetch the location information only.,SENTRY,Sub-task,Open,[],1,[<JIRA IssueLink: id='12545981'>],"With the changes added as part of HIVE-20306, HiveMetaStoreClient now exposes new API ""

""getPartitionsWithSpecs"" using which sentry can request specific information for partition.

In simple words sentry could say fetch me the location information for the partitions in given table. Here is simple example on how it can be used.

 

 

 

 
{code:java}
    GetPartitionsRequest request = new GetPartitionsRequest();

    GetPartitionsProjectSpec projectSpec = new GetPartitionsProjectSpec();

    projectSpec.setFieldList(Arrays.asList(""sd.location""));

    GetPartitionsFilterSpec filter = new GetPartitionsFilterSpec();

    filter.setDbName(""compdb"");

    filter.setTblName(""comptbl"");

    request.setFilterSpec(filter);

    request.setProjectionSpec(projectSpec);

    GetPartitionsResponse response;

    try {

      response = client.getPartitionsWithSpecs(request);

    } catch (Exception ex) {

      ex.printStackTrace();

      LOG.error(""Exception while retrieving partitions"", ex);

      throw ex;

    }
{code}
 

 ",2018-10-18T14:22:40.176+0000,2018-11-07T14:35:44.366+0000,,Major
TEZ-3967,DAGImpl: dag lock is unfair and can starve the writers,TEZ,Bug,Patch Available,[],1,[<JIRA IssueLink: id='12537887'>],"Found when debugging HIVE-20103, that a reader arriving when another reader is active can postpone a writer from obtaining a write-lock.

This is fundamentally bad for the DAGImpl as useful progress can only happen when the writeLock is held.

{code}
  public void handle(DAGEvent event) {
...
    try {
      writeLock.lock();
{code}

{code}
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007efb02246f40> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
        at org.apache.tez.dag.app.dag.impl.DAGImpl.handle(DAGImpl.java:1162)
        at org.apache.tez.dag.app.dag.impl.DAGImpl.handle(DAGImpl.java:149)
        at org.apache.tez.dag.app.DAGAppMaster$DagEventDispatcher.handle(DAGAppMaster.java:2251)
        at org.apache.tez.dag.app.DAGAppMaster$DagEventDispatcher.handle(DAGAppMaster.java:2242)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:180)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:115)
        at java.lang.Thread.run(Thread.java:745)
{code}

while read-lock is passed around between 

{code}
       at org.apache.tez.dag.app.dag.impl.DAGImpl.getDAGStatus(DAGImpl.java:901)
        at org.apache.tez.dag.app.dag.impl.DAGImpl.getDAGStatus(DAGImpl.java:940)
        at org.apache.tez.dag.api.client.DAGClientHandler.getDAGStatus(DAGClientHandler.java:73)
{code}

calls.",2018-07-06T08:11:12.193+0000,2020-05-08T08:04:54.829+0000,,Major
IMPALA-3531,Implement deferrable and optionally enforced PK/FK constraints,IMPALA,New Feature,In Progress,"[<JIRA Issue: key='IMPALA-2112', id='13052730'>, <JIRA Issue: key='IMPALA-8208', id='13216070'>, <JIRA Issue: key='IMPALA-8290', id='13220051'>, <JIRA Issue: key='IMPALA-8291', id='13220052'>, <JIRA Issue: key='IMPALA-9104', id='13265164'>, <JIRA Issue: key='IMPALA-9158', id='13268693'>, <JIRA Issue: key='IMPALA-9256', id='13274856'>, <JIRA Issue: key='IMPALA-9336', id='13282079'>, <JIRA Issue: key='IMPALA-9419', id='13287274'>]",5,"[<JIRA IssueLink: id='12497508'>, <JIRA IssueLink: id='12504248'>, <JIRA IssueLink: id='12556302'>, <JIRA IssueLink: id='12497353'>, <JIRA IssueLink: id='12593689'>]","Oracle has ""RELY NOVALIDATE"" option for constraints.. Could be easier for Hive to start with something like that for PK/FK constraints. So CBO has more information for optimizations. It does not have to actually check if that constraint is relationship is true; it can just ""rely"" on that constraint.

https://docs.oracle.com/database/121/SQLRF/clauses002.htm#sthref2289

So it would be helpful with join cardinality estimates, and with cases like IMPALA-2929.

https://docs.oracle.com/database/121/DWHSG/schemas.htm#DWHSG9053
""Overview of Constraint States"":
- Enforcement
- Validation
- Belief

So FK/PK with ""rely novalidate"" will have Enforcement&Validate disabled but Belief = RELY as it is possible to do in Oracle and now in Hive (HIVE-13076).

It opens a lot of ways to do additional ways to optimize execution plans.
As exxplined in Tom Kyte's ""Metadata matters""
http://www.peoug.org/wp-content/uploads/2009/12/MetadataMatters_PEOUG_Day2009_TKyte.pdf
pp.30 - ""Tell us how the tables relate and we can remove them from the plan..."".
pp.35 - ""Tell us how the tables relate and we have more access paths available..."".

Also it might be helpful when Impala is being integrated with Kudu as the latter have to have a PK.",2016-05-12T18:30:26.000+0000,2020-07-23T23:42:04.670+0000,,Major
KNOX-1586,YARN v1 and v2 UI - Handle http vs https for node links,KNOX,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12547835'>, <JIRA IssueLink: id='12547854'>, <JIRA IssueLink: id='12566758'>]",Currently the links to YARN nodes include the scheme. This needs to be handled by Knox as well.,2018-11-09T14:04:03.802+0000,2021-10-09T18:47:06.104+0000,Fixed,Minor
PHOENIX-6438,Compilation failures after changes to Private annotated HBase class RpcControllerFactory,PHOENIX,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12612502'>, <JIRA IssueLink: id='12612503'>, <JIRA IssueLink: id='12612394'>]","After HBASE-25735 (not released yet, but coming up in 2.4.3)
there are compilation failures in phoenix-core/src/main/java/org/apache/hadoop/hbase/ipc/controller/ClientRpcControllerFactory.java, phoenix-core/src/main/java/org/apache/hadoop/hbase/ipc/controller/InterRegionServerMetadataRpcControllerFactory.java. 

Phoenix can't be taking this dependency on a Private annotated class and/or the annotation must be changed to LimitedPrivate.",2021-04-08T02:15:59.492+0000,2021-04-09T00:53:29.791+0000,Fixed,Major
BIGTOP-1281,bump hbase version to 0.98.2,BIGTOP,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12386685'>, <JIRA IssueLink: id='12388741'>, <JIRA IssueLink: id='12386655'>]",Hbase needs to be bring up to 0.98. I'd suggest to go to 0.98.1 instead.,2014-04-18T00:29:33.103+0000,2014-05-25T00:21:59.451+0000,Fixed,Major
SLIDER-1174,Support Tensorflow on Slider,SLIDER,New Feature,Resolved,[],4,"[<JIRA IssueLink: id='12484281'>, <JIRA IssueLink: id='12484282'>, <JIRA IssueLink: id='12484280'>, <JIRA IssueLink: id='12484283'>]","1. Enable YARN and Slider to have the capability to run tensorflow cluster, which contains a set of ""tasks"" that participate in the distributed execution of a TensorFlow graph
2. Take the advantage of YARN to manage cluster resources and dynamically generate clusterSpec for tensorflow",2016-10-26T03:08:31.474+0000,2017-08-14T08:53:24.773+0000,Fixed,Major
SPARK-17495,Hive hash implementation,SPARK,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12583288'>, <JIRA IssueLink: id='12484011'>, <JIRA IssueLink: id='12536320'>]","Spark internally uses Murmur3Hash for partitioning. This is different from the one used by Hive. For queries which use bucketing this leads to different results if one tries the same query on both engines. For us, we want users to have backward compatibility to that one can switch parts of applications across the engines without observing regressions.",2016-09-10T19:33:20.463+0000,2020-03-17T00:11:21.337+0000,Fixed,Minor
ORC-16,Support LZ4 Compression,ORC,Improvement,Open,[],1,[<JIRA IssueLink: id='12531580'>],"Though similar to snappy, it is a different algorithm with some different characteristics and has wide use and availability.",2015-06-24T22:15:16.438+0000,2018-04-12T03:38:44.816+0000,,Minor
PARQUET-334,"UT TestSummary failed with ""java.lang.RuntimeException: Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from null"" when Pig >=0.15",PARQUET,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12435515'>, <JIRA IssueLink: id='12432028'>]","org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias B
at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1694)
at org.apache.pig.PigServer.registerQuery(PigServer.java:623)
at org.apache.pig.PigServer.registerQuery(PigServer.java:636)
at parquet.pig.summary.TestSummary.testMaxIsZero(TestSummary.java:154)
...
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.lang.RuntimeException: Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from null
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:307)
at org.apache.pig.PigServer.launchPlan(PigServer.java:1390)
at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1375)
at org.apache.pig.PigServer.execute(PigServer.java:1364)
at org.apache.pig.PigServer.access$500(PigServer.java:113)
at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1689)
... 32 more
Caused by: java.lang.RuntimeException: Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from null
at parquet.pig.summary.Summary.setInputSchema(Summary.java:266)
at org.apache.pig.newplan.logical.expression.ExpToPhyTranslationVisitor.visit(ExpToPhyTranslationVisitor.java:530)
at org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:132)
at org.apache.pig.newplan.ReverseDependencyOrderWalkerWOSeenChk.walk(ReverseDependencyOrderWalkerWOSeenChk.java:69)
at org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:808)
at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:87)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:258)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:293)
... 37 more
Caused by: java.lang.NullPointerException
at parquet.pig.summary.Summary.setInputSchema(Summary.java:261)
... 46 more

It relates to a change on pig side: pig/src/org/apache/pig/newplan/logical/expression/ExpToPhyTranslationVisitor.java introduced by PIG-3294",2015-07-15T13:30:57.822+0000,2015-12-14T23:30:35.970+0000,Fixed,Critical
AMBARI-12135,"After HDP installation, hdfs fsck reporting CORRUPT status (Missing blocks)",AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12431911'>],"This issue is 100% reproducible and has been happening on Ambari/HDP builds for at least a couple weeks. After completing an installation (Using blueprint), the hdfs fsck / reports as CORRUPT. Even after letting the cluster sit idle for a couple hours, it is still in the corrupt state

{code}
hades1:/var/opt/teradata/packages # su hdfs
hdfs@hades1:/var/opt/teradata/packages> hdfs fsck /
Connecting to namenode via http://hades1.labs.teradata.com:50070/fsck?ugi=hdfs&path=%2F
FSCK started by hdfs (auth:SIMPLE) from /39.0.8.2 for path / at Wed Jun 24 20:40:17 GMT 2015
...
/apps/hbase/data/WALs/hades4.labs.teradata.com,16020,1435168292684/hades4.labs.teradata.com%2C16020%2C1435168292684.default.1435175500556: MISSING 1 blocks of total size 83 B.
/apps/hbase/data/WALs/hades5.labs.teradata.com,16020,1435168290466/hades5.labs.teradata.com%2C16020%2C1435168290466..meta.1435175562144.meta: MISSING 1 blocks of total size 83 B.
/apps/hbase/data/WALs/hades5.labs.teradata.com,16020,1435168290466/hades5.labs.teradata.com%2C16020%2C1435168290466.default.1435175498500: MISSING 1 blocks of total size 83 B.
/apps/hbase/data/WALs/hades6.labs.teradata.com,16020,1435168292373/hades6.labs.teradata.com%2C16020%2C1435168292373.default.1435175500301: MISSING 1 blocks of total size 83 B..................................................................................................
....................................................................................................
....................................................................................................
........................................................................................Status: CORRUPT
 Total size:    723977553 B (Total open files size: 332 B)
 Total dirs:    79
 Total files:   388
 Total symlinks:                0 (Files currently being written: 5)
 Total blocks (validated):      387 (avg. block size 1870743 B) (Total open file blocks (not validated): 4)
  ********************************
  UNDER MIN REPL'D BLOCKS:      4 (1.0335917 %)
  dfs.namenode.replication.min: 1
  CORRUPT FILES:        4
  MISSING BLOCKS:       4
  MISSING SIZE:         332 B
  ********************************
 Minimally replicated blocks:   387 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     3.0
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          3
 Number of racks:               1
FSCK ended at Wed Jun 24 20:40:17 GMT 2015 in 7 milliseconds


The filesystem under path '/' is CORRUPT
hdfs@hades1:/var/opt/teradata/packages>
{code}",2015-06-24T20:40:33.146+0000,2015-07-23T17:08:23.212+0000,Duplicate,Critical
PHOENIX-5852,The zkConnectionString in LoadBalance is incorrect,PHOENIX,Bug,Closed,[],1,[<JIRA IssueLink: id='12588361'>],"The connectionString given to CuratorFrameworkFactory#newClient should look like “host1:port1,host2:port2,host3:port3”. But the format of zkConnectionString returned by org.apache.phoenix.loadbalancer.service.LoadBalanceZookeeperConfImpl#

getZkConnectString is “host1,host2,host3:port”. This is equal to ""host1:2181,host2:2181,host3:port"", it's not correct if your zookeeper's port  is not the default 2181.",2020-04-16T11:17:07.339+0000,2021-08-03T08:35:31.351+0000,Fixed,Major
TEZ-3151,expose DAG credentials to plugins,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12458962'>],"Tez plugins need to pass credentials (e.g. HBase tokens, etc.) to tasks. Right now they only have access to AM credentials.",2016-02-29T19:50:34.265+0000,2016-05-18T04:57:51.931+0000,Fixed,Major
HCATALOG-391,Tracking Jira for hacks to cover dependency bugs,HCATALOG,Bug,Open,[],1,[<JIRA IssueLink: id='12351121'>],,2012-04-25T21:35:20.650+0000,2012-04-25T21:36:32.535+0000,,Major
PHOENIX-1408,Don't disable table before modifying HTable metadata,PHOENIX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12400754'>, <JIRA IssueLink: id='12400751'>]","In 0.98, HBase supports modifying the HTable metadata without disabling the table first. We should remove our calls to htable.disableTable() and htable.enableTable() in ConnectionQueryServicesImpl when we modify the table metadata. The only time we still need to disable the table is before we drop it.",2014-11-05T01:54:53.569+0000,2015-11-21T02:16:30.205+0000,Fixed,Major
ZOOKEEPER-1162,"consistent handling of jute.maxbuffer when attempting to read large zk ""directories""",ZOOKEEPER,Improvement,Open,[],6,"[<JIRA IssueLink: id='12453232'>, <JIRA IssueLink: id='12618434'>, <JIRA IssueLink: id='12618435'>, <JIRA IssueLink: id='12453266'>, <JIRA IssueLink: id='12342798'>, <JIRA IssueLink: id='12342505'>]","Recently we encountered a sitaution where a zk directory got sucessfully populated with 250k elements.  When our system attempted to read the znode dir, it failed because the contents of the dir exceeded the default 1mb jute.maxbuffer limit.  There were a few odd things

1) It seems odd that we could populate to be very large but could not read the listing 
2) The workaround was bumping up jute.maxbuffer on the client side
Would it make more sense to have it reject adding new znodes if it exceeds jute.maxbuffer? 
Alternately, would it make sense to have zk dir listing ignore the jute.maxbuffer setting?",2011-08-25T05:36:23.620+0000,2022-02-03T08:50:18.708+0000,,Major
LEGAL-167,Move CC-BY to Category B,LEGAL,Question,Closed,[],2,"[<JIRA IssueLink: id='12432290'>, <JIRA IssueLink: id='12425271'>]","Summary:

Careful reading recent versions of the CC-By licenses identify restrictions that are not highlighted on the creativecommons summary page for this license.  These restrictions go beyond what is required by the Apache License, Version 2.0, and furthermore has been deemed incompatible with GPL.  We also have input that this is problematic for proprietary (non-open source) usages.

In the ensuing discussion, we have had questions concerning the necessity for certain terms in our license, questions concerning the viability of the existing categorization of licenses (A, B, X), questions concerning the validity of legal interpretations of our licensees.

All these are valid questions, and can be pursued separately.  Meanwhile we need something until at least one of those efforts are resolved.  My proposal remains that CC-BY licenses be moved to Category B as a stop-gap solution.  Such a change would not prevent existing projects from continuing to include such unmodified artifacts in their releases, it merely would require them to conspicuously state that they did so to allow downsteam consumers to make an informed decision.  Nor would it completely prevent modifications of such content, instead it would require separate approval to do so.

Partial list of relevant prior discussion (feel free to point to more in additional comments):

http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3CCAFG6u8FJmnE21QcS_eP5SnvbLMgp4eKYvT13qEXALJafdoFO_A%40mail.gmail.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3C01f301ce5829%24aed32660%240c797320%24%40rosenlaw.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3CCAFG6u8HB%2Bj2H1J_WMH1D%3DfQf%2BuG5QE0brv7Fi1%2B1XozKEf%3Dn8Q%40mail.gmail.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3COFCAE2B77A.4DC34960-ON85257B75.006149FE-85257B75.0061C5FF%40us.ibm.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3CCAFG6u8FDjKbuzr3Mcns1Xwneydc-fGp6C7ZJ_nY9vvJ4p8LANA%40mail.gmail.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3C063201ce5bdc%2401a730b0%2404f59210%24%40rosenlaw.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201306.mbox/%3C152301ce6267%24ff57f0f0%24fe07d2d0%24%40rosenlaw.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3CCAFG6u8Gvzy%3DN-ezZY%3DOW5XxEp%2B4W%3D%2B7E%2B9fpZWcsxQU%2BjQjXog%40mail.gmail.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3C06ca01ce5c08%24f8627960%24e9276c20%24%40rosenlaw.com%3E
http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201305.mbox/%3COF7C4060BB.133DE47E-ON85257B7A.0009A3AC-85257B7A.000C0686%40us.ibm.com%3E",2013-06-07T14:35:19.434+0000,2016-02-26T04:15:43.229+0000,Fixed,Major
SPARK-29245,CCE during creating HiveMetaStoreClient ,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12570641'>],"From `master` branch build, when I try to connect to an external HMS, I hit the following.
{code}
19/09/25 10:58:46 ERROR hive.log: Got exception: java.lang.ClassCastException class [Ljava.lang.Object; cannot be cast to class [Ljava.net.URI; ([Ljava.lang.Object; and [Ljava.net.URI; are in module java.base of loader 'bootstrap')
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.net.URI; ([Ljava.lang.Object; and [Ljava.net.URI; are in module java.base of loader 'bootstrap')
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:200)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
{code}

With HIVE-21508, I can get the following.
{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.4)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sql(""show databases"").show
+--------------------+
|        databaseName|
+--------------------+
|      .....  |
...
{code}

With 2.3.7-SNAPSHOT, the following basic tests are tested.
- SHOW DATABASES / TABLES
- DESC DATABASE / TABLE
- CREATE / DROP / USE DATABASE
- CREATE / DROP / INSERT / LOAD / SELECT TABLE",2019-09-25T11:20:16.974+0000,2020-04-20T20:39:25.332+0000,Fixed,Major
SENTRY-716,Hive pluing does not correctly enforce privileges for new in case of nested queries,SENTRY,Bug,Open,[],1,[<JIRA IssueLink: id='12433882'>],"A nested query on view incorrectly enforces base table privileges instead of view privileges. For example,
{noformat}
create view v1 as select * from t1;
grant select on table v1 to role test;
select * from ( select * from v1) v2;
{noformat}
doesn't work if you have read privilege on view v1. It only works when you have read privilege on underlying table t1.
",2015-05-01T18:54:47.513+0000,2015-10-09T23:48:51.218+0000,,Major
YETUS-587,Report if junit drops a significant amount of tests,YETUS,New Feature,Open,[],2,"[<JIRA IssueLink: id='12526405'>, <JIRA IssueLink: id='12525398'>]","
I believe precommit should be able to guesstimate how many tests results are expected from junit by taking the number @test lines subtracted by the number of @ignore lines. Precommit can then compare that to the actual numbers received.  If that number is below some configurable percentage, precommit should throw a very significant warning probably in lieu of reporting the actual failed tests since things almost certainly went very very wrong.",2017-11-17T22:22:05.876+0000,2019-06-08T13:18:25.123+0000,,Major
PHOENIX-4360,Prevent System.Catalog from splitting,PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12528300'>],"Just talked to [~jamestaylor].

It turns out that currently System.Catalog is not prevented from splitting generally, but does not allow splitting within a schema.

In the multi-tenant case that is not good enough. When System.Catalog splits and a base table and view end up in different regions the following can happen:
* DROP CASCADE no longer works for those views
* Adding/removing columns to/from the base table no longer works

Until PHOENIX-3534 is done we should simply prevent System.Catalog from splitting. (just like HBase:meta)
[~apurtell]",2017-11-09T01:04:58.991+0000,2018-07-26T01:14:57.418+0000,Fixed,Blocker
KNOX-1873,Add HiveServer2 UI proxy support,KNOX,New Feature,Open,[],1,[<JIRA IssueLink: id='12562523'>],Currently Knox supports the HiveServer2 JDBC API. HiveServer2 has a UI added in Hive 2.0.0 as part of HIVE-12338. This will probably require a separate service definition since we can't reuse the Hive JDBC dispatch for the standard UI.,2019-06-06T15:04:33.450+0000,2021-10-09T18:40:18.440+0000,,Major
INFRA-6740,Apache CMS build not kicking off,INFRA,Task,Closed,[],2,"[<JIRA IssueLink: id='12375008'>, <JIRA IssueLink: id='12375009'>]","I am working on moving the Apache Hive project to CMS. We have source tree here:
http://svn.apache.org/repos/asf/hive/cms/

However when I commit a change, a build never kicks off according to:
http://ci.apache.org/builders/hive-site-staging

Also troubling is that we get a 404 when hitting our staging site:
http://hive.staging.apache.org/

Even though the index.mdtext exists. Any idea what I am screwing up?",2013-09-10T17:31:36.850+0000,2014-02-02T19:08:44.813+0000,Fixed,Major
SPARK-2633,enhance spark listener API to gather more spark job information,SPARK,New Feature,Resolved,[],6,"[<JIRA IssueLink: id='12399770'>, <JIRA IssueLink: id='12395093'>, <JIRA IssueLink: id='12393525'>, <JIRA IssueLink: id='12394571'>, <JIRA IssueLink: id='12394422'>, <JIRA IssueLink: id='12392229'>]","Based on Hive on Spark job status monitoring and statistic collection requirement, try to enhance spark listener API to gather more spark job information.",2014-07-23T02:04:57.164+0000,2014-10-26T18:10:08.757+0000,Duplicate,Critical
PHOENIX-1282,Remove usage of KeyValueUtil.ensureKeyValue,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12397468'>],"We're doing a push in HBase to get rid of these.
It was just added a temporary relieve while we covert everything to Cells.

In most cases the Cell API is all that is needed and we should remove all usage of this method.",2014-09-23T05:58:14.514+0000,2015-02-10T19:45:18.190+0000,,Major
SQOOP-2145,Default Hive home is not being set properly under certain circumstances,SQOOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12409323'>, <JIRA IssueLink: id='12425845'>]","Currently, we have a bunch of _HOME defaults that are detected in bin/configure-sqoop. However, HIVE_HOME is not there. The only default for HIVE_HOME is hardcoded in java at https://github.com/apache/sqoop/blob/trunk/src/java/org/apache/sqoop/SqoopOptions.java#L64

and that's not enough. The reason is often times people have tarballs like 
~/sqoop, ~/hive, etc. and it would be good to have their hive import jobs work out of the book. It makes sense to add some smartness (we already have that for other components like hcatalog) related to HIVE_HOME in configure-sqoop.

This doesn't regress anything because it only sets the HIVE_HOME if it's not already set and the directory it's setting it to exists and is valid.",2015-02-27T23:05:09.268+0000,2015-05-27T05:47:37.017+0000,Fixed,Major
CALCITE-1502,AssertionError when case statement is used with optional value and literal using ReduceExpressionsRule,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12519044'>, <JIRA IssueLink: id='12519042'>]","AssertionError when we use case with optional value and literal using ReduceExpressionsRule:
{noformat}
SELECT CASE WHEN 1=2 THEN cast((values(1)) as integer) ELSE 2 end from (values(1))
{noformat}
Stack trace:
{noformat}
java.lang.AssertionError: Internal error: Cannot add expression of different type to set:
set type is RecordType(INTEGER EXPR$0) NOT NULL
expression type is RecordType(INTEGER NOT NULL EXPR$0) NOT NULL
set is rel#14:LogicalProject(input=HepRelVertex#13,EXPR$0=CASE(=(1, 2), CAST($1):INTEGER, 2))
expression is LogicalProject#16
{noformat}
",2016-11-22T11:06:17.127+0000,2017-11-01T16:37:17.759+0000,Fixed,Major
TEZ-1693,ARCHIVE local resources are not supported in Tez DAGs,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12399469'>],"{code}
2014-10-21 16:42:17,919 ERROR [main]: exec.Task (TezTask.java:execute(180)) - Failed to execute tez graph.
java.lang.IllegalArgumentException: LocalResourceType: ARCHIVE is not supported, only FILE is supported
    at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)
    at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:365)
    at org.apache.tez.client.TezClient.submitDAG(TezClient.java:344)
    at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:368)
    at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:159)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1607)
{code}",2014-10-21T23:53:39.350+0000,2014-11-24T04:39:47.576+0000,Not A Problem,Major
BIGTOP-3333,Fix Hive build after upgrading Spark to 2.4.5,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12585211'>, <JIRA IssueLink: id='12586852'>]","After BIGTOP-3165, building Hive came to fail as follows, due to the API change of Spark:

{code}
$ ./gradlew hive-pkg

(snip)

[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/sekikn/repos/bigtop/output/hive/hive-2.3.6/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java:[87,26] cannot find symbol
  symbol:   method addListener(org.apache.hadoop.hive.ql.exec.spark.status.impl.JobMetricsListener)
  location: class org.apache.spark.scheduler.LiveListenerBus
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Hive 2.3.6:
[INFO] 
[INFO] Hive ............................................... SUCCESS [  2.115 s]
[INFO] Hive Shims Common .................................. SUCCESS [  3.889 s]
[INFO] Hive Shims 0.23 .................................... SUCCESS [  2.686 s]
[INFO] Hive Shims Scheduler ............................... SUCCESS [  0.976 s]
[INFO] Hive Shims ......................................... SUCCESS [  0.793 s]
[INFO] Hive Common ........................................ SUCCESS [  5.762 s]
[INFO] Hive Service RPC ................................... SUCCESS [  2.480 s]
[INFO] Hive Serde ......................................... SUCCESS [  4.136 s]
[INFO] Hive Metastore ..................................... SUCCESS [ 16.040 s]
[INFO] Hive Vector-Code-Gen Utilities ..................... SUCCESS [  0.306 s]
[INFO] Hive Llap Common ................................... SUCCESS [  3.953 s]
[INFO] Hive Llap Client ................................... SUCCESS [  2.334 s]
[INFO] Hive Llap Tez ...................................... SUCCESS [  2.591 s]
[INFO] Spark Remote Client ................................ SUCCESS [  3.028 s]
[INFO] Hive Query Language ................................ FAILURE [ 17.783 s]
[INFO] Hive Llap Server ................................... SKIPPED
[INFO] Hive Service ....................................... SKIPPED
[INFO] Hive Accumulo Handler .............................. SKIPPED
[INFO] Hive JDBC .......................................... SKIPPED
[INFO] Hive Beeline ....................................... SKIPPED
[INFO] Hive CLI ........................................... SKIPPED
[INFO] Hive Contrib ....................................... SKIPPED
[INFO] Hive Druid Handler ................................. SKIPPED
[INFO] Hive HBase Handler ................................. SKIPPED
[INFO] Hive JDBC Handler .................................. SKIPPED
[INFO] Hive HCatalog ...................................... SKIPPED
[INFO] Hive HCatalog Core ................................. SKIPPED
[INFO] Hive HCatalog Pig Adapter .......................... SKIPPED
[INFO] Hive HCatalog Server Extensions .................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client .................. SKIPPED
[INFO] Hive HCatalog Webhcat .............................. SKIPPED
[INFO] Hive HCatalog Streaming ............................ SKIPPED
[INFO] Hive HPL/SQL ....................................... SKIPPED
[INFO] Hive Llap External Client .......................... SKIPPED
[INFO] Hive Shims Aggregator .............................. SKIPPED
[INFO] Hive TestUtils ..................................... SKIPPED
[INFO] Hive Packaging ..................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  01:09 min
[INFO] Finished at: 2020-04-03T10:55:51+09:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure
[ERROR] /home/sekikn/repos/bigtop/output/hive/hive-2.3.6/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java:[87,26] cannot find symbol
[ERROR]   symbol:   method addListener(org.apache.hadoop.hive.ql.exec.spark.status.impl.JobMetricsListener)
[ERROR]   location: class org.apache.spark.scheduler.LiveListenerBus
[ERROR] 
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hive-exec
debian/rules:32: recipe for target 'override_dh_auto_build' failed
make[1]: *** [override_dh_auto_build] Error 1
make[1]: Leaving directory '/home/sekikn/repos/bigtop/output/hive/hive-2.3.6'
debian/rules:29: recipe for target 'build' failed
make: *** [build] Error 2
dpkg-buildpackage: error: debian/rules build subprocess returned exit status 2
debuild: fatal error at line 1152:
dpkg-buildpackage -rfakeroot -us -uc -ui -b
 failed

> Task :hive-deb FAILED

FAILURE: Build failed with an exception.
{code}",2020-04-03T02:03:32.819+0000,2020-04-27T23:43:26.244+0000,Fixed,Major
BEAM-7435,Add HBaseIO tests with provided HBase 2 client/server,BEAM,Improvement,Open,[],1,[<JIRA IssueLink: id='12580036'>],"Users of HBaseIO can now provided their client dependency (BEAM-9278). We need to validate now that if they provide a client for HBase 2 that the IO behaves correctly.

At the moment the use of the HBase client API that Beam does is 100% compatible so we need to enforce this via tests.",2019-05-27T12:45:58.059+0000,2022-06-04T01:54:41.891+0000,,P3
SOLR-2563,Allow generic pluggable file system implementations,SOLR,New Feature,Open,[],2,"[<JIRA IssueLink: id='12339371'>, <JIRA IssueLink: id='12339370'>]","For things like configuration files, they can be loaded from places other than the local filesystem, such as Zookeeper or HDFS.  In this issue I will abstract that functionality out.",2011-05-31T17:30:35.588+0000,2011-06-02T16:48:37.879+0000,,Major
TEZ-244,Min resource capability is removed from YARN AMRMProtocol after YARN-787,TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12370630'>, <JIRA IssueLink: id='12370610'>, <JIRA IssueLink: id='12370609'>]","For now, MR was 'hacked' to look in configuration for min-resource-capability. I'm doing the same for TEZ. MR should change after MAPREDUCE-5311 at which point TEZ can change too.",2013-06-16T21:06:58.134+0000,2013-06-17T04:41:32.518+0000,Duplicate,Major
HADOOP-18417,Upgrade Maven Surefire plugin to 3.0.0-M7,HADOOP,Improvement,Reopened,[],3,"[<JIRA IssueLink: id='12646356'>, <JIRA IssueLink: id='12646402'>, <JIRA IssueLink: id='12646415'>]","The Maven Surefire plugin 3.0.0-M1 doesn't always include the launcher as part of it's setup, which can cause problems with Yarn tests. Some of the Yarn modules use Jupiter, which may be a complicating factor.  Switching to 3.0.0-M7 fixes the issue.

This is currently blocking MAPREDUCE-7386",2022-08-23T19:06:19.777+0000,2022-09-11T01:35:11.853+0000,,Blocker
CRUNCH-340,Create HCatSource and HCatTarget,CRUNCH,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12386963'>, <JIRA IssueLink: id='12519526'>]","This patch adds HCatSource, which enables crunch pipeline to read from Hive tables. This is the very first version, leaving a few TODOs in code.

It adds new dependency from crunch-core to hcatalog (as well as several hive components). I guess maybe we should create a new subproject (e.g. crunch-hcatalog) rather than add it into crunch-core.",2014-02-09T16:26:34.543+0000,2017-12-10T16:57:47.182+0000,Fixed,Major
ORC-611,Incorrect min-max stats for sub-millisecond timestamps,ORC,Bug,Closed,"[<JIRA Issue: key='ORC-663', id='13326502'>]",1,[<JIRA IssueLink: id='12587732'>],"The issue is related to the precision of storing timestamps:
- nanoseconds for the data itself
- only milliseconds for min-max statistics

Both min and max are rounded to the same value, while min should be rounded down and max should be rounded up to ensure that the values are actually within that range.

Repro in Hive:
{code}
create table tsstat (ts timestamp) stored as orc;
insert into tsstat values (""1970-01-01 00:00:00.0005"")
select * from tsstat where ts = ""1970-01-01 00:00:00.0005"";
-- returned 0 rows
{code}

Both the Java and the C++ writer has this issue (thanks [~stigahuang] for looking them up):
https://github.com/apache/orc/blob/fea154436c37c81a16b13d879b510096cfaa2946/java/core/src/java/org/apache/orc/impl/writer/TimestampTreeWriter.java#L108
https://github.com/apache/orc/blob/fea154436c37c81a16b13d879b510096cfaa2946/c%2B%2B/src/ColumnWriter.cc#L1800

I guess that there are already files with this issue in production, so I think that the only way to fix this is to hack the reader:
- decrease/increase min/max stats with 1 ms after reading them from file
- also be careful about the values pushed down, as the same precision loss can occur there to, eg. ""WHERE ts <'1970-01-01 00:00:00.0005' AND ts > '1970-01-01 00:00:00.0004'"" shouldn't be converted to ts < ""1970-01-01"" AND ts > ""1970-01-01""

The issue was discovered during an Impala review: https://gerrit.cloudera.org/#/c/15403/1/be/src/exec/hdfs-orc-scanner.cc@875",2020-03-16T10:59:31.847+0000,2020-09-15T00:01:43.388+0000,Fixed,Major
THRIFT-484,Ability to use a slice of a buffer instead of a direct byte[] for binary fields,THRIFT,New Feature,Closed,[],1,[<JIRA IssueLink: id='12324630'>],"The HBase folks who are considering using Thrift for their internal and external RPC have suggested that it would be a useful performance enhancement to be able to use a subslice of a byte array as the value of a binary field, without first doing a new byte array copy. Done correctly, this would mean that the data could be copied once from when it is read in from the socket to when it is written out as part of HBase's internal datastructures. 

This enhancement seems like it could boost the performance of things in Hadoop, as well, so it could be a really nice option to investigate.",2009-05-05T17:04:19.187+0000,2010-07-28T20:25:35.103+0000,Duplicate,Major
ATLAS-2891,Incorrect column lineage: each output column has input from *all columns* of the input table,ATLAS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12544041'>],"Column lineage generated by Atlas Hive hook is incorrect for certain queries - like the following INSERT:

{noformat}
CREATE TABLE source_tbl(col_001 INT, col_002 INT, col_003 INT);

CREATE TABLE target_tbl(col_001 INT, col_002 INT, col_003 INT);

INSERT INTO target_tbl SELECT v1.col_001, v1.col_002, v1.col_003 FROM (SELECT col_001, col_002, col_003, ROW_NUMBER() OVER() AS r_num FROM source_tbl) v1;

{noformat}

In this case, lineage for each column in target_tbl shows input from all columns in source_tbl. In this case, the lineage information provided to post hooks (like Atlas hook) contains 3 entries, one for each column in target_tbl. Note the dependency for each column has all columns of the source_tbl.

{noformat}
DependencyKey=default.target_tbl:FieldSchema(name:col_001, type:int, comment:null)
Dependency=[SCRIPT]
           [default.source_tbl(src):FieldSchema(name:col_001, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:col_002, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:col_003, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:BLOCK__OFFSET__INSIDE__FILE, type:bigint, comment:),
            default.source_tbl(src):FieldSchema(name:INPUT__FILE__NAME, type:string, comment:),
            default.source_tbl(src):FieldSchema(name:ROW__ID, type:struct<transactionId:bigint,bucketId:int,rowId:bigint>, comment:)
           ];
 
DependencyKey=default.target_tbl:FieldSchema(name:col_002, type:int, comment:null)
Dependency=[SCRIPT]
           [default.source_tbl(src):FieldSchema(name:col_001, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:col_002, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:col_003, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:BLOCK__OFFSET__INSIDE__FILE, type:bigint, comment:),
            default.source_tbl(src):FieldSchema(name:INPUT__FILE__NAME, type:string, comment:),
            default.source_tbl(src):FieldSchema(name:ROW__ID, type:struct<transactionId:bigint,bucketId:int,rowId:bigint>, comment:)
           ];
 
DependencyKey=default.target_tbl:FieldSchema(name:col_003, type:int, comment:null)
Dependency=[SCRIPT]
           [default.source_tbl(src):FieldSchema(name:col_001, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:col_002, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:col_003, type:int, comment:null),
            default.source_tbl(src):FieldSchema(name:BLOCK__OFFSET__INSIDE__FILE, type:bigint, comment:),
            default.source_tbl(src):FieldSchema(name:INPUT__FILE__NAME, type:string, comment:),
            default.source_tbl(src):FieldSchema(name:ROW__ID, type:struct<transactionId:bigint,bucketId:int,rowId:bigint>, comment:)
           ];
{noformat}


When INSERT statement doesn't include ""ROW_NUMBER() OVER() AS r_num"", the lineage details look correct.

This issue is seen in Hive version 1; but not in Hive2 or Hive3.",2018-09-25T21:47:18.926+0000,2019-09-26T00:38:46.365+0000,Fixed,Critical
CALCITE-2041,"When ReduceExpressionRule simplifies a nullable expression, allow the result to change type to NOT NULL",CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12602964'>, <JIRA IssueLink: id='12519536'>, <JIRA IssueLink: id='12520099'>]","In some cases, the user needs to select whether or not to add casts that match nullability.
One of the motivations behind this is to avoid unnecessary casts like the following example.
original filter 
{code}
OR(AND(>=($0, CAST(_UTF-16LE'2010-01-01 00:00:00 UTC'):TIMESTAMP_WITH_LOCAL_TIME_ZONE(15)), <=($0, CAST(_UTF-16LE'2012-03-01 00:00:00 UTC'):TIMESTAMP_WITH_LOCAL_TIME_ZONE(15))))
{code}
the optimized expression with matching nullability
{code}
OR(AND(CAST(>=($0, 2010-01-01 00:00:00)):BOOLEAN, CAST(<=($0, 2012-03-01 00:00:00)):BOOLEAN))
{code}
As you can see this extra cast gets into the way of following plan optimization steps.
The desired expression can be obtained by turning off the nullability matching.
{code}
OR(AND(>=($0, 2010-01-01 00:00:00), <=($0, 2012-03-01 00:00:00)))
{code}
",2017-11-08T00:27:44.261+0000,2020-11-12T18:25:59.163+0000,Fixed,Major
IMPALA-5442,Allow HDFS replication factor to be set,IMPALA,New Feature,Open,[],1,[<JIRA IssueLink: id='12508120'>],"Currently for small tables with just 1 HDFS file, the file block are potentially available on default 3 DN's. If the file content was static, we would run a hdfs dfs -setrep and then issue a refresh table/partition command to cache the newly replicated block locations. However since hdfs dfs -setrep is not synchronous, we don't really know when to issue a setrep command.

HDFS caching helps this a bit, but has the same issue that we need to do a setrep and issue the caching directive and then refresh the table to get the cached and on disk block locations into the catalog.

A good feature for small tables would be to allow HDFS replication factor be specified as part of the Impala INSERT INTO clause, (Cannot have this as part of create table options as Hive would not support this) ",2017-06-06T03:29:01.483+0000,2017-07-01T00:33:32.694+0000,,Major
IMPALA-8809,Refresh a subset of partitions for ACID tables,IMPALA,Improvement,In Progress,[],1,[<JIRA IssueLink: id='12575233'>],"Enhancing REFRESH logic to handle ACID tables was covered by this change: https://issues.apache.org/jira/browse/IMPALA-8600
Basically each user initiated REFRESH PARTITION is rejected meanwhile the REFRESH_PARTITION event in event processor are actually doing a full table load for ACID tables.

There is room for improvement: When a full table refresh is being executed on an ACID table we can have 2 scenarios:
- If there was some schema changes then reload the full table. Identify such a scenario should be possible by checking the table-level writeId. However, there is a bug in Hive that it doesn't update that field for partitioned tables (https://issues.apache.org/jira/browse/HIVE-22062). This would be the desired way but could also be workarounded by checking other fields lik lastDdlChanged or such.
- If a full table refresh is not needed then we should fetch the partition-level writeIds and reload only the ones that are out-of-date locally.",2019-07-30T15:30:00.323+0000,2020-12-21T20:48:46.228+0000,,Critical
ACCUMULO-4628,Provide Lexicoder for decimal data,ACCUMULO,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12501675'>],"Some fallout from HIVE-15795:

[~faganm] noticed that we don't have a lexicoder in Accumulo for {{BigDecimal}}. It would be nice to implement one that can do that.",2017-04-24T18:26:48.004+0000,2021-08-05T21:12:53.885+0000,Duplicate,Minor
BIGTOP-1297,Update puppet deployment recipes to reflect combination of HMaster and HRegionServer,BIGTOP,Bug,Open,[],3,"[<JIRA IssueLink: id='12448380'>, <JIRA IssueLink: id='12387683'>, <JIRA IssueLink: id='12387684'>]","Starting with HBase 0.99 Master and RegionServer are combined into the same process. Hence, an attempt to separately start master service and region service leads to port binding conflicts. On the Hbase master node we only need to start master service.

Perhaps, we can go further and remove {{hbase-regionserver}} startup script from master node.",2014-05-07T04:23:50.116+0000,2016-01-22T21:12:16.030+0000,,Major
SPARK-36964,Reuse CachedDNSToSwitchMapping for yarn  container requests,SPARK,Improvement,In Progress,[],1,[<JIRA IssueLink: id='12647456'>],"Similar to SPARK-13704​, In some cases, YarnAllocator add container requests with locality preference can be expensive, it may call the topology script for rack awareness.

When submit a very large job in a very large Yarn cluster, the topology script may take signifiant time to run. And this blocks receiving YarnSchedulerBackend's RequestExecutors rpc calls, This request comes from spark dynamic executor allocation thread, which may blocks the ExecutorAllocationListener, and then result in executorManagement queue backlog.

 

Some logs:
{code:java}
21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:839) at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:411) at org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:361) at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:316) at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:227) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ... 12 more21/09/29 12:04:35 WARN spark-dynamic-executor-allocation ExecutorAllocationManager: Unable to reach the cluster manager to request 1922 total executors!

21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:839) at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:411) at org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:361) at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:316) at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:227) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ... 12 more21/09/29 12:04:35 WARN spark-dynamic-executor-allocation ExecutorAllocationManager: Unable to reach the cluster manager to request 1922 total executors!


21/09/29 12:02:49 ERROR dag-scheduler-event-loop AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
21/09/29 12:02:49 WARN dag-scheduler-event-loop AsyncEventQueue: Dropped 1 events from executorManagement since the application started.
21/09/29 12:02:55 INFO spark-listener-group-eventLog AsyncEventQueue: Process of event SparkListenerExecutorAdded(1632888172920,543,org.apache.spark.scheduler.cluster.ExecutorData@8cfab8f5,None) by listener EventLoggingListener took 3.037686034s.
21/09/29 12:03:03 INFO spark-listener-group-eventLog AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1632888181779,BlockManagerId(1359, --, 57233, None),2704696934,Some(2704696934),Some(0)) by listener EventLoggingListener took 1.462598355s.
21/09/29 12:03:49 WARN dispatcher-BlockManagerMaster AsyncEventQueue: Dropped 74388 events from executorManagement since Wed Sep 29 12:02:49 CST 2021.
21/09/29 12:04:35 INFO spark-listener-group-executorManagement AsyncEventQueue: Process of event SparkListenerStageSubmitted(org.apache.spark.scheduler.StageInfo@52f810ad,{...}) by listener ExecutorAllocationListener took 116.526408932s.
21/09/29 12:04:49 WARN heartbeat-receiver-event-loop-thread AsyncEventQueue: Dropped 18892 events from executorManagement since Wed Sep 29 12:03:49 CST 2021.
21/09/29 12:05:49 WARN dispatcher-BlockManagerMaster AsyncEventQueue: Dropped 19397 events from executorManagement since Wed Sep 29 12:04:49 CST 2021.
{code}",2021-10-09T07:03:00.919+0000,2022-09-13T04:45:34.836+0000,,Major
HCATALOG-556,HCatalog-trunk doesn't build against Hive-0.10.0,HCATALOG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12361056'>, <JIRA IssueLink: id='12361055'>]","When one builds hcatalog-trunk/ against latest in hive-branch-0.10/, one sees compile errors, along the following lines:

<excerpt>

compile:
     [echo] hcatalog-core
    [javac] Compiling 1 source file to /Users/mithunr/workspace/dev/hcatalog/trunk.move.to.hive10/core/build/classes
    [javac] /Users/mithunr/workspace/dev/hcatalog/trunk.move.to.hive10/core/src/main/java/org/apache/hcatalog/har/HarOutputCommitterPostProcessor.java:50: cannot find symbol
    [javac] symbol  : variable Constants
    [javac] location: class org.apache.hcatalog.har.HarOutputCommitterPostProcessor
    [javac]         partition.getParameters().put(Constants.IS_ARCHIVED, ""true"");
    [javac]                                       ^
    [javac] 1 error

</excerpt>

The reason for these code breaks is HIVE-2715. Some of the thrift-generated class names have been changed (in conjunction with the move to thrift-9.)

The reason why these are not evident in trunk-builds at the moment is that the latest artifacts posted on repository.apache.org are outdated (i.e. pre-HIVE-2715.)
https://repository.apache.org/content/groups/snapshots/org/apache/hive/hive-metastore/0.10.0-SNAPSHOT/

Once the artifacts are updated, these problems should come to the fore. I'm working through the failures right now. I'll post a patch soon-ish.
",2012-11-28T21:51:35.903+0000,2013-02-15T21:32:48.244+0000,Fixed,Blocker
MASSEMBLY-941,file permissions removed during assembly:single since 3.2.0,MASSEMBLY,Bug,Open,[],5,"[<JIRA IssueLink: id='12604083'>, <JIRA IssueLink: id='12598539'>, <JIRA IssueLink: id='12604082'>, <JIRA IssueLink: id='12598625'>, <JIRA IssueLink: id='12597660'>]","Since 3.2.0, existing file permissions seem to be ignored when creating a tarball assembly, and files stored in the assembly do not have their original file permissions preserved.

Using version 3.1.1 of this plugin and earlier, when creating a tar.gz, existing file permissions are normally preserved. This is now broken in 3.2.0, unless the component descriptor explicitly sets the fileModes.

This was discovered trying to prepare a release candidate for Apache Accumulo using the apache-23.pom parent POM's predefined `source-release-tar` descriptor using the `single` goal. We noticed that the resulting source-release tarball had stripped all the executable permissions from our scripts, instead of preserving them. This makes the resulting source release more difficult to build from source.

A source-release assembly, and any other assembly that does not specify the file permissions explicitly, should preserve the existing file permissions, just as it used to with 3.1.1 and earlier.",2020-08-26T21:32:15.397+0000,2022-09-16T07:08:16.510+0000,,Critical
TEZ-2629,LimitExceededException in Tez client when DAG has exceeds the default max counters,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12431440'>, <JIRA IssueLink: id='12431439'>]","Original issue was HIVE-11303, seeing LimitExceededException when the client tries to get the counters for a completed job:

{noformat}
2015-07-17 18:18:11,830 INFO  [main]: counters.Limits (Limits.java:ensureInitialized(59)) - Counter limits initialized with parameters:  GROUP_NAME_MAX=256, MAX_GROUPS=500, COUNTER_NAME_MAX=64, MAX_COUNTERS=1200
2015-07-17 18:18:11,841 ERROR [main]: exec.Task (TezTask.java:execute(189)) - Failed to execute tez graph.
org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200
        at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)
        at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)
        at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)
        at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)
        at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)
        at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)
        at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}

It looks like Limits.ensureInitialized() is defaulting to an empty configuration, resulting in COUNTERS_MAX being set to the default of 1200 (even though Hive's configuration specified tez.counters.max=16000).

Per [~sseth]:
{quote}
I think the Tez client does need to make this call to setup the Configuration correctly. We do this for the AM and the executing task - which is why it works. Could you please open a Tez jira for this ?
Also, Limits is making use of Configuration instead of TezConfiguration for default initialization, which implies changes to tez-site on the local node won't be picked up.
{quote}",2015-07-21T00:51:03.404+0000,2015-09-01T21:31:07.652+0000,Fixed,Major
PHOENIX-3133,Investigate why offset queries with reverse scan take a long time,PHOENIX,Bug,Open,[],2,"[<JIRA IssueLink: id='12476563'>, <JIRA IssueLink: id='12476564'>]","We need to workaround HBASE-16296 because users of Phoenix won't see the fix until at least the fix makes it into a release version of HBase. Unfortunately, often times users are forced to stick to earlier version of HBase, even after a release. PHOENIX-3121 works around the issue when there's only a LIMIT clause. However, if there's a LIMIT and an OFFSET, the issue still occurs. 

Repro code courtesy, [~mujtabachohan] 

{code}
DDL:
CREATE TABLE IF NOT EXISTS XYZ.T (
		TENANT_ID CHAR(15) NOT NULL, 
		KEY_PREFIX CHAR(3) NOT NULL,
		CREATED_DATE DATE,
		CREATED_BY CHAR(15),
		LAST_UPDATE DATE,
		LAST_UPDATE_BY CHAR(15),
		SYSTEM_MODSTAMP DATE
		CONSTRAINT PK PRIMARY KEY (
		TENANT_ID, 
		KEY_PREFIX
		)
		) VERSIONS=1, IMMUTABLE_ROWS=true, MULTI_TENANT=true, REPLICATION_SCOPE=1
		
		CREATE VIEW IF NOT EXISTS XYZ.ABC_VIEW (
		ACTIVITY_DATE DATE NOT NULL,
		WHO_ID CHAR(15) NOT NULL,
		WHAT_ID CHAR(15) NOT NULL,
		CHANNEL_TYPE VARCHAR NOT NULL,
		CHANNEL_ACTION_TYPE VARCHAR NOT NULL,
		ENGAGEMENT_HISTORY_POC_ID CHAR(15) ,
		CHANNEL_CONTEXT VARCHAR
		CONSTRAINT PKVIEW PRIMARY KEY
		(
		ACTIVITY_DATE, WHO_ID, WHAT_ID, CHANNEL_TYPE, CHANNEL_ACTION_TYPE
		)
		)
		AS SELECT * FROM XYZ.T WHERE KEY_PREFIX = '08m' 


UPSERT records using this:

Connection con = DriverManager.getConnection(""jdbc:phoenix:samarthjai-ltm3.internal.salesforce.com"", new Properties());
		PreparedStatement pStatement;
		pStatement = con.prepareStatement(""upsert into XYZ.ABC_VIEW (ACTIVITY_DATE,CHANNEL_ACTION_TYPE,CHANNEL_TYPE,TENANT_ID,WHAT_ID,WHO_ID) values (TO_DATE('2010-11-11 00:00:00.000'),?,'ABC','00Dx0000000GyYS','701x00000000dzp','00Qx0000001S2qa')"");

		for (int i=0; i<10000000;i++) {
			pStatement.setString(1, UUID.randomUUID().toString());
			pStatement.execute();
			
			if (i % 10000 == 0) {
				con.commit();
				System.out.println(i);
			}
		}

Sample query:

@Test
    public void testLimitCacheQuery() throws Exception {
        String url = ""jdbc:phoenix:localhost:2181"";
        try (Connection conn = DriverManager.getConnection(url)) {
            PreparedStatement stmt = conn.prepareStatement(""select * from XYZ.ABC_VIEW where who_id = '00Qx0000001S2qa' and TENANT_ID='00Dx0000000GyYS' order by activity_date desc LIMIT 18 OFFSET 2"");
            stmt.setFetchSize(10);
            try (ResultSet rs = stmt.executeQuery()) {
                long startTime = System.currentTimeMillis();
                int record = 0;
                while (rs.next()) {
                    System.out.println(""Record ""+ (++record) + "" Time: "" + (System.currentTimeMillis() - startTime));
                    startTime = System.currentTimeMillis();
                }
            }
        }
    }
{code}
",2016-08-01T19:20:28.863+0000,2019-01-08T23:02:28.330+0000,,Major
HCATALOG-430,pig/MR jobs get incorrect binary type data from hcat table,HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12354115'>],"Pig is able to write binary type data into hcat table. But when pig reads from the table, there is garbage at the end of binary type column values. Values that are supposed to be null in a record are given the previous non null value.

This happens even for MR jobs reading from the hcat table.
",2012-06-12T02:40:10.742+0000,2012-07-02T10:45:19.214+0000,Duplicate,Major
YETUS-635,Maven plugin should fail when partially enabled,YETUS,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12534208'>],"The maven build plugin defines two additional test types: mvnsite and mvninstall. Currently, users can say they want to enable these tests without also stating they want the maven plugin enabled. This can lead to nonintuitive failures because those tests rely on command line arguments that are only handled by the maven plugin.

This came up in HBASE-20591, because the HBase project was attempting to have parallel test runs to cover unit tests with different dependency options and had 'mvninstall' without 'maven' result in failures due to stomping on the default maven repo. ",2018-05-16T17:18:03.746+0000,2018-07-19T21:23:50.552+0000,Fixed,Critical
SLIDER-46,AM fails to restart: NPE in addAllNMTokensFromPreviousAttempt,SLIDER,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12388034'>, <JIRA IssueLink: id='12388310'>]","AM restart fails with the following:

{noformat}
14/05/10 17:02:17 INFO appmaster.SliderAppMaster: Connecting to RM at 48058,address tracking URL=http://c6403.ambari.apache.org:48705
14/05/10 17:02:17 ERROR main.ServiceLauncher: java.lang.NullPointerException
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.convertToProtoFormat(RegisterApplicationMasterResponsePBImpl.java:384)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.access$100(RegisterApplicationMasterResponsePBImpl.java:53)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl$2$1.next(RegisterApplicationMasterResponsePBImpl.java:355)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl$2$1.next(RegisterApplicationMasterResponsePBImpl.java:344)
        at com.google.protobuf.AbstractMessageLite$Builder.checkForNullValues(AbstractMessageLite.java:336)
        at com.google.protobuf.AbstractMessageLite$Builder.addAll(AbstractMessageLite.java:323)
        at org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder.addAllNmTokensFromPreviousAttempts(YarnServiceProtos.java:2700)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.mergeLocalToBuilder(RegisterApplicationMasterResponsePBImpl.java:123)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.mergeLocalToProto(RegisterApplicationMasterResponsePBImpl.java:104)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.getProto(RegisterApplicationMasterResponsePBImpl.java:75)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:91)
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

Exception: java.lang.NullPointerException
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.convertToProtoFormat(RegisterApplicationMasterResponsePBImpl.java:384)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.access$100(RegisterApplicationMasterResponsePBImpl.java:53)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl$2$1.next(RegisterApplicationMasterResponsePBImpl.java:355)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl$2$1.next(RegisterApplicationMasterResponsePBImpl.java:344)
        at com.google.protobuf.AbstractMessageLite$Builder.checkForNullValues(AbstractMessageLite.java:336)
        at com.google.protobuf.AbstractMessageLite$Builder.addAll(AbstractMessageLite.java:323)
        at org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder.addAllNmTokensFromPreviousAttempts(YarnServiceProtos.java:2700)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.mergeLocalToBuilder(RegisterApplicationMasterResponsePBImpl.java:123)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.mergeLocalToProto(RegisterApplicationMasterResponsePBImpl.java:104)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.getProto(RegisterApplicationMasterResponsePBImpl.java:75)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:91)
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
{noformat}",2014-05-10T17:03:31.429+0000,2014-05-19T10:24:15.546+0000,Fixed,Major
PHOENIX-2916,Need a client-side level property to control write timeout,PHOENIX,New Feature,Reopened,[],1,[<JIRA IssueLink: id='12466965'>],"Currently, there's a client side property phoenix.query.timeoutMs which allows query to timeout after the specified time. However, there's no equivalent property to allow upserts (writes) to timeout if the client requires a shorted wait time than the default in Phoenix/HBase. This should apply to the underlying operations related to writes - upsert, commit. Also it should control timeout for writes in the HBase client, which might include Zookeeper client timeout as well.

Proposed property name: phoenix.update.timeoutMs",2016-05-19T18:26:19.810+0000,2017-06-28T22:57:00.817+0000,,Minor
SLIDER-34,Restarted AM cannot create containers,SLIDER,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12388127'>, <JIRA IssueLink: id='12388312'>]","{{TestKilledAM}} test  in {{slider-providers/hbase/slider-hbase-provider}} is timing out, waiting for the right number of worker nodes to come up. 

They aren't apparently because the restarted AM can't talk to the RM any more",2014-05-08T09:32:52.947+0000,2014-08-26T10:35:36.196+0000,Fixed,Major
LENS-1150,HiveDriver should create sessions independent of the transport,LENS,Task,Closed,[],2,"[<JIRA IssueLink: id='12467327'>, <JIRA IssueLink: id='12467328'>]",,2016-05-24T06:53:43.750+0000,2016-08-25T11:52:34.924+0000,Fixed,Major
ORC-406,ORC: Char(n) and Varchar(n) writers truncate to n bytes & corrupts multi-byte data,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12545197'>, <JIRA IssueLink: id='12543478'>]","https://github.com/apache/orc/blob/master/java/core/src/java/org/apache/orc/impl/writer/CharTreeWriter.java#L41

{code}
    itemLength = schema.getMaxLength();
    padding = new byte[itemLength];
  }
{code}

https://github.com/apache/orc/blob/master/java/core/src/java/org/apache/orc/impl/writer/VarcharTreeWriter.java#L48

{code}
      if (vector.noNulls || !vector.isNull[0]) {
        int itemLength = Math.min(vec.length[0], maxLength);
{code}

",2018-09-18T19:04:54.560+0000,2018-10-09T07:04:51.796+0000,Fixed,Major
APEXCORE-815,Whitelist CVE-2016-6811,APEXCORE,Task,Resolved,[],1,[<JIRA IssueLink: id='12534120'>],"There is an old vulnerability in Yarn version 2.7.3 and below (please see [CVE-2016-6811|https://www.cvedetails.com/cve/CVE-2016-6811]) that was recently marked as severity 9 and now it breaks Apex build.  Based on my analysis, the vulnerability affects Yarn cluster itself (see [YARN-5121|https://issues.apache.org/jira/browse/YARN-5121]).",2018-05-15T17:37:04.563+0000,2018-05-16T13:44:18.444+0000,Fixed,Major
ACCUMULO-3303,funky performance with large WAL,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12400877'>],"The tserver seems to get into a funky state when writing to a large write-ahead log. I ran some continuous ingest tests varying tserver.walog.max.size in {512M, 1G, 2G, 4G, 8G} and got some results that I have yet to understand. I was expecting to see the effects of walog metadata management as described in ACCUMULO-2889, but I also found an additional behavior of ingest slowing down for long periods when using a large walog size.

The cluster configuration was as follows:
{code}
Accumulo version: 1.6.2-SNAPSHOT (current head of origin/1.6)
Nodes: 4
Masters: 1
Slaves: 3
Cores per node: 24
Drives per node: 8x1TB data + 2 raided system
Memory per node: 64GB
tserver.memory.maps.max=2G
table.file.compress.type=snappy (for ci table only)
tserver.mutation.queue.max=16M
tserver.wal.sync.method=hflush
Native maps enabled
{code}",2014-11-05T23:20:33.994+0000,2014-12-08T15:53:56.036+0000,Won't Fix,Major
SENTRY-2109,Fix the logic of identifying HMS out of Sync and handle gaps and out-of-sequence notifications.,SENTRY,Bug,Patch Available,[],6,"[<JIRA IssueLink: id='12525272'>, <JIRA IssueLink: id='12525243'>, <JIRA IssueLink: id='12523751'>, <JIRA IssueLink: id='12525144'>, <JIRA IssueLink: id='12524613'>, <JIRA IssueLink: id='12523332'>]","Currently HMSFollower proactively checks if sentry is out of sync with HMS and initiates full snapshot, if needed.

There will be false positives with the current logic if there are gaps in the event-id in the notification log sequence.

This jira is aimed at making that logic robust.",2017-12-22T16:53:38.808+0000,2018-09-13T16:50:35.233+0000,,Major
CALCITE-1344,Incorrect inferred precision when BigDecimal value is less than 1,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12476797'>],"Method {{makeExactLiteral(BigDecimal)}} in RexBuilder infers the incorrect precision when decimal < 1 e.g. for 0.06 it infers the type to be Decimal(1,2) instead of Decimal(3,2).

{code:java}
  /**
   * Creates a numeric literal.
   */
  public RexLiteral makeExactLiteral(BigDecimal bd) {
    RelDataType relType;
    int scale = bd.scale();
    long l = bd.unscaledValue().longValue();
    assert scale >= 0;
    assert scale <= typeFactory.getTypeSystem().getMaxNumericScale() : scale;
    assert BigDecimal.valueOf(l, scale).equals(bd);
    if (scale == 0) {
      if ((l >= Integer.MIN_VALUE) && (l <= Integer.MAX_VALUE)) {
        relType = typeFactory.createSqlType(SqlTypeName.INTEGER);
      } else {
        relType = typeFactory.createSqlType(SqlTypeName.BIGINT);
      }
    } else {
      int precision = bd.unscaledValue().abs().toString().length();
      relType =
          typeFactory.createSqlType(SqlTypeName.DECIMAL, precision, scale);
    }
    return makeExactLiteral(bd, relType);
  }
{code}",2016-08-04T08:43:23.944+0000,2016-09-21T14:00:41.841+0000,Fixed,Major
INFRA-22846,"ci-hbase controller needs the ""Publish over SSH"" plugin",INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12632890'>],"members of the HBase community are attempting to migrate our build jobs from ci-hadoop to ci-hbase. We have several jobs that publish to the nightlies service via the {{sshTransfer}} step.

On January 22nd we hit an issue where jobs with this step fail with:

{code}
java.lang.NoSuchMethodError: No such DSL method 'sshTransfer' found among steps
{code}

We would like to have the needed plugin installed into the controller. If it is not available then we would like a pointer to instructions on how else to transfer artifacts to the nightlies service.",2022-02-07T02:44:07.000+0000,2022-02-08T14:23:02.099+0000,Fixed,Critical
INFRA-20269,Preparation to move hive precommit stuff to a new place,INFRA,Improvement,Closed,[],1,[<JIRA IssueLink: id='12588425'>],"My goal would be to enable the final evaluation of a new way to run hive tests (HIVE-22942).
but to do that I would like to ask a few things:

* configure the hive repo on github to push events to our jenkins instance (must have)
* is it possible to get some hostname for our jenkins instance? (it would be nice to have it)

jenkins instance is running at: http://130.211.9.232/
",2020-05-15T09:05:22.241+0000,2020-06-03T07:07:09.529+0000,Fixed,Major
PHOENIX-6766,Fix failure of sqlline due to conflicting jline dependency pulled from Hadoop 3.3,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12645346'>, <JIRA IssueLink: id='12645347'>]","{noformat}
$ mvn clean install -DskipTests -Dhadoop.version=3.3.4 -Dhbase.version=2.4.13 -Dhbase.profile=2.4
$ bin/sqlline.py
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.phoenix.shaded.org.jline.reader.impl.completer.StringsCompleter.<init>([Lorg/apache/phoenix/shaded/org/jline/reader/Candidate;)V
        at sqlline.SqlLineOpts.setOptionCompleters(SqlLineOpts.java:160)
        at sqlline.Application.getCommandHandlers(Application.java:294)
        at sqlline.SqlLine$Config.<init>(SqlLine.java:1946)
        at sqlline.SqlLine.setAppConfig(SqlLine.java:1875)
        at sqlline.SqlLine.<init>(SqlLine.java:229)
        at sqlline.SqlLine.start(SqlLine.java:266)
        at sqlline.SqlLine.main(SqlLine.java:206)
{noformat}
",2022-08-09T12:38:49.493+0000,2022-08-10T08:11:30.710+0000,Fixed,Major
FLUME-1206,Hbase sink should pull in dependencies based on profiles,FLUME,Bug,Open,[],2,"[<JIRA IssueLink: id='12352340'>, <JIRA IssueLink: id='12352344'>]",,2012-05-16T08:46:40.738+0000,2013-06-25T00:12:23.181+0000,,Minor
FLUME-1942,Flume should check if a file is closed if DFSClient#close() throws,FLUME,Bug,Open,[],1,[<JIRA IssueLink: id='12365486'>],,2013-03-11T20:15:01.656+0000,2013-03-11T20:17:02.385+0000,,Major
AVRO-739,Add Date/Time data types,AVRO,New Feature,Closed,[],6,"[<JIRA IssueLink: id='12396929'>, <JIRA IssueLink: id='12396932'>, <JIRA IssueLink: id='12369029'>, <JIRA IssueLink: id='12374405'>, <JIRA IssueLink: id='12353152'>, <JIRA IssueLink: id='12393499'>]",,2011-01-20T01:30:50.233+0000,2016-02-01T14:55:53.778+0000,Fixed,Major
WHIRR-391,Write a YARN (Hadoop MR2) service for Whirr,WHIRR,New Feature,Resolved,[],4,"[<JIRA IssueLink: id='12343940'>, <JIRA IssueLink: id='12343973'>, <JIRA IssueLink: id='12345069'>, <JIRA IssueLink: id='12349058'>]",,2011-10-03T23:57:16.630+0000,2012-05-28T19:53:01.999+0000,Fixed,Major
ZOOKEEPER-2139,"Support multiple ZooKeeper client, with different configurations, in a single JVM",ZOOKEEPER,Improvement,Closed,[],12,"[<JIRA IssueLink: id='12461661'>, <JIRA IssueLink: id='12477067'>, <JIRA IssueLink: id='12461744'>, <JIRA IssueLink: id='12471403'>, <JIRA IssueLink: id='12477444'>, <JIRA IssueLink: id='12464925'>, <JIRA IssueLink: id='12572908'>, <JIRA IssueLink: id='12411238'>, <JIRA IssueLink: id='12447967'>, <JIRA IssueLink: id='12491383'>, <JIRA IssueLink: id='12479805'>, <JIRA IssueLink: id='12532923'>]","I have two ZK client in one JVM, one is secure client and second is normal client (For non secure cluster).

""zookeeper.sasl.client"" system property is ""true"" by default, because of this my second client connection is failing.

We should pass all client configurations in client constructor like HDFS client.

For example :
{code}
public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, Configuration conf) throws IOException
	{
		......
		......
	}
{code}",2015-03-13T04:28:53.274+0000,2019-10-29T03:59:58.726+0000,Fixed,Blocker
SUREFIRE-1821,Broken junit report when parallel and rerunFailingTestsCount configureation is used,SUREFIRE,Bug,Closed,[],4,"[<JIRA IssueLink: id='12633978'>, <JIRA IssueLink: id='12620626'>, <JIRA IssueLink: id='12633954'>, <JIRA IssueLink: id='12620625'>]","*Description:*

When using *parallel* configuration with *rerunFailingTestsCount* bad xml report is generated for a failing test.

*How to reproduce:*

1. Configure forkCount, parallel and rerun count:
{code:java}
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-surefire-plugin</artifactId>
  <version>3.0.0-M5</version>
  <executions>
    <execution>
      <goals>
        <goal>test</goal>
      </goals>
    </execution>
  </executions>
  <configuration>
    <rerunFailingTestsCount>1</rerunFailingTestsCount>
    <forkCount>1</forkCount>
    <threadCount>1</threadCount>
    <parallel>all</parallel>
    <redirectTestOutputToFile>true</redirectTestOutputToFile>
  </configuration>
</plugin>
{code}
2. Create a failing test that outputs more than 1m characters:
{code:java}
public class AppTest {
    @Test
    public void testBug() {
        for(int i = 0;  i < 100000; i++){
            System.out.println(""Some output longer than 10 character"");
        }
        throw new NullPointerException();
    }
}
{code}
3. Run the test and check the report - you will see unexpected end of xml file:
{code:java}
    ...
    <property name=""sun.io.unicode.encoding"" value=""UnicodeBig""/>
    <property name=""java.class.version"" value=""52.0""/>
  </properties>
  <testcase name=""testBug"" classname=""com.mycompany.app.AppTest"" time=""0.015"">
    <error type=""java.lang.NullPointerException""><![CDATA[java.lang.NullPointerException
        at com.mycompany.app.AppTest.testBug(AppTest.java:12)
]]></error>
    <system-out><![CDATA[  <-- EOF
{code}
*Possible cause:*

It seems that Junit 4.7 Runner considers each rerun as a separate test set, so sends _testSetCompleted_ event each time, but after the first event _DeferredFileOutputStream_ and associated with it temp file gets freed(deleted), however _testSetCompleted_ event for rerun still tries to write something there. If temp file isn't yet created (still in memory) then there will be no bug and it gets created only after 1m characters were written to the stream. I wasn't able to understand wether multiple _testSetCompleted_ events is a bug here or a ""rerun"" trying to write to _DeferredFileOutputStream_ of initial test run.

 

*Project to reproduce:*

*[https://github.com/himos/surefire-report-bug]*",2020-07-15T12:39:28.276+0000,2022-02-19T18:11:41.795+0000,Fixed,Major
IMPALA-6232,Short circuit reads disabled when using Impala HDFS file handle cache,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12520694'>],"In Impala 2.10, the HDFS file handle cache was enabled by default. However, testing has revealed that in cases where files are overwritten or appended, the file handle can encounter an error that causes HDFS to disable short circuit reads for 10 minutes. See [HDFS-12528|https://issues.apache.org/jira/browse/HDFS-12528].

Due to this performance impact and the associated unpredictability, Impala should disable the file handle cache by default until this issue is resolved.",2017-11-22T00:46:15.830+0000,2017-12-05T21:38:59.249+0000,Fixed,Blocker
TEZ-1265,Custom input to fetch source task inputs in order,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12391012'>],"Consider the case of having to LIMIT m records after an Order by. A distributed orderby vertex produces data in sorted order from task0,task1...taskn. Each task limits its output to m records (the output count can be <m also). The limit vertex (parallelism 1) following the order by vertex has to fetch output of all n tasks, shuffle merge its inputs (to maintain the order) and then limit m records again.  So need a input that fetches from source tasks in order and reads them in order. Since data produced is ordered from task0,task1...taskn it can be consumed without shuffle and sort. If the limit is hit early it can skip fetching more task inputs. 

More details in https://issues.apache.org/jira/browse/PIG-4049?focusedCommentId=14053217&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14053217",2014-07-07T23:29:09.425+0000,2014-07-08T21:39:52.134+0000,,Major
PARQUET-116,Pass a filter object to user defined predicate in filter2 api,PARQUET,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12400392'>, <JIRA IssueLink: id='12400390'>]","Currently for creating a user defined predicate using the new filter api,  no value can be passed to create a dynamic filter at runtime. This reduces the usefulness of the user defined predicate, and  meaningful predicates cannot be created. We can add a generic Object value that is passed through the api, which can internally be used in the keep function of the user defined predicate for creating many different types of filters.
For example, in spark sql, we can pass in a list of filter values for a where IN clause query and filter the row values based on that list.",2014-10-18T05:41:49.572+0000,2015-04-07T20:46:10.078+0000,Fixed,Minor
SLIDER-582,Registry test may fail if ~ expands to a path with upper case,SLIDER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12400128'>],"{noformat}
2014-10-29 22:51:10,667 [JUnit] INFO  framework.SliderShell (?:call(?)) - /Users/smohanty/enlistments/incubator-slider/slider-assembly/target/slider-0.51.0-incubating-SNAPSHOT-all/slider-0.51.0-incubating-SNAPSHOT/bin/slider resolve --list --path \~
2014-10-29 22:51:10,667 [JUnit] DEBUG framework.SliderShell (?:call(?)) - export SLIDER_CONF_DIR=/Users/smohanty/enlistments/incubator-slider/src/test/clusters/remote/slider;
/Users/smohanty/enlistments/incubator-slider/slider-assembly/target/slider-0.51.0-incubating-SNAPSHOT-all/slider-0.51.0-incubating-SNAPSHOT/bin/slider resolve --list --path \~
2014-10-29 22:51:12,765 [JUnit] ERROR framework.SliderShell (?:call(?)) - 56 =>/Users/smohanty/enlistments/incubator-slider/slider-assembly/target/slider-0.51.0-incubating-SNAPSHOT-all/slider-0.51.0-incubating-SNAPSHOT/bin/slider resolve --list --path \~
2014-10-29 22:51:12,765 [JUnit] ERROR framework.SliderShell (NativeMethodAccessorImpl.java:invoke0(?)) - return code = 56
2014-10-29 22:51:12,765 [JUnit] INFO  framework.SliderShell (NativeMethodAccessorImpl.java:invoke0(?)) -
{noformat}

{noformat}
2014-10-29 22:51:12,652 [main] ERROR main.ServiceLauncher (ServiceLauncher.java:error(344)) - Exception: `/Users/smohanty': Invalid Path element ""Users""
org.apache.hadoop.registry.client.exceptions.InvalidPathnameException: `/Users/smohanty': Invalid Path element ""Users""
	at org.apache.hadoop.registry.client.binding.RegistryPathUtils.validateElementsAsDNS(RegistryPathUtils.java:78)
	at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.validatePath(RegistryOperationsService.java:91)
	at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.list(RegistryOperationsService.java:145)
	at org.apache.hadoop.registry.client.binding.RegistryUtils.statChildren(RegistryUtils.java:178)
	at org.apache.slider.client.SliderClient.actionResolve(SliderClient.java:2475)
	at org.apache.slider.client.SliderClient.exec(SliderClient.java:398)
	at org.apache.slider.client.SliderClient.runService(SliderClient.java:341)
	at org.apache.slider.core.main.ServiceLauncher.launchService(ServiceLauncher.java:188)
	at org.apache.slider.core.main.ServiceLauncher.launchServiceRobustly(ServiceLauncher.java:473)
	at org.apache.slider.core.main.ServiceLauncher.launchServiceAndExit(ServiceLauncher.java:403)
	at org.apache.slider.core.main.ServiceLauncher.serviceMain(ServiceLauncher.java:628)
	at org.apache.slider.Slider.main(Slider.java:49)
2014-10-29 22:51:12,655 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 56
{noformat}",2014-10-30T05:56:44.924+0000,2014-10-31T19:02:41.261+0000,Fixed,Major
THRIFT-1869,TThreadPoolServer (java) dies when threadpool is consumed,THRIFT,Bug,Closed,[],7,"[<JIRA IssueLink: id='12370965'>, <JIRA IssueLink: id='12370904'>, <JIRA IssueLink: id='12398629'>, <JIRA IssueLink: id='12366243'>, <JIRA IssueLink: id='12366244'>, <JIRA IssueLink: id='12365043'>, <JIRA IssueLink: id='12366832'>]","When TThreadPoolServer hit's max threads, it throws a RejectedExceptionException and dies. This is not expected by users of this class.

{noformat}
java.util.concurrent.RejectedExecutionException 
at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source) 
at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source) 
at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source) 
at org.apache.thrift.server.TThreadPoolServer.serve(TThreadPoolServer.java:108) 
{noformat}",2013-03-01T17:06:17.956+0000,2014-10-09T10:42:07.330+0000,Fixed,Major
ACCUMULO-2815,Kerberos authentication for clients,ACCUMULO,Improvement,Resolved,"[<JIRA Issue: key='ACCUMULO-3452', id='12764056'>]",10,"[<JIRA IssueLink: id='12405491'>, <JIRA IssueLink: id='12411758'>, <JIRA IssueLink: id='12407860'>, <JIRA IssueLink: id='12426082'>, <JIRA IssueLink: id='12409726'>, <JIRA IssueLink: id='12405492'>, <JIRA IssueLink: id='12409575'>, <JIRA IssueLink: id='12409700'>, <JIRA IssueLink: id='12426280'>, <JIRA IssueLink: id='12405490'>]","We have server authentication via Kerberos, but we don't have a way for clients to connect to Accumulo using Kerberos.

HBase context: http://hbase.apache.org/book/security.html#d248e5472

We'll have to look into how Authorizations and Permissions are assigned to these users and make sure the ZK-backed security mechanisms can still support this. It would be nice to not have to make a completely separate auth/permission mechanism when kerberos is being used.

As far as configuration, I imagine this would be a great fit for the often-proposed client-side configuration idea.",2014-05-15T18:31:52.438+0000,2015-05-31T16:55:21.499+0000,Fixed,Major
OOZIE-2209,"Oozie launchers to set ""java.io.tmpdir"" to ""./tmp""",OOZIE,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12452175'>, <JIRA IssueLink: id='12452174'>, <JIRA IssueLink: id='12426517'>]","We had couple of cases when pig launcher tried to create a relatively large job jar on /tmp/ and fail with not enough space.This can be avoided if the launcher used the current working directory which should have a lot more space.  Also, it'll always get cleaned up by the NodeManager even if the task crashes in the middle.",2015-04-16T23:33:19.068+0000,2015-12-16T20:56:11.262+0000,Fixed,Major
SPARK-34295,Allow option similar to mapreduce.job.hdfs-servers.token-renewal.exclude,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12607734'>],"MapReduce jobs can instruct YARN to skip renewal of tokens obtained from certain hosts by specifying the hosts with configuration mapreduce.job.hdfs-servers.token-renewal.exclude=<host1>,<host2>,..,<hostN>.

But seems Spark lacks of similar option. So the job submission fails if YARN fails to renew DelegationToken for any of the remote HDFS cluster.  The failure in DT renewal can happen due to many reason like Remote HDFS does not trust Kerberos identity of YARN etc.

",2021-01-29T23:31:50.419+0000,2021-03-24T08:12:17.210+0000,Fixed,Major
RANGER-895,Ranger Hive plugin to support column-masking,RANGER,Task,Resolved,[],2,"[<JIRA IssueLink: id='12461636'>, <JIRA IssueLink: id='12461638'>]",Apache Hive 2.1.0 has updated HiveAuthorizer interface to support column-masking and row-level filtering. Ranger Hive plugin should be updated to implement the new methods to provide column-masking. Updates to support row-level filtering will be tracked via a separate JIRA.,2016-03-23T10:55:44.860+0000,2016-04-28T03:44:08.667+0000,Fixed,Major
TEZ-4179,[Kubernetes] Extend NodeId in tez to support unique worker identity,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12588698'>],"In kubernetes environment where pods can have same host name and port, there can be situations where node trackers could be retaining old instance of the pod in its cache. In case of Hive LLAP, where the llap tez task scheduler maintains the membership of nodes based on zookeeper registry events there can be cases where NODE_ADDED followed by NODE_REMOVED event could end up removing the node/host from node trackers because of stable hostname and service port. The NODE_REMOVED event in this case is old stale event of the already dead pod but ZK will send only after session timeout (in case of non-graceful shutdown). If this sequence of events happen, a node/host is completely lost form the schedulers perspective. 

To support this scenario, tez can extend yarn's NodeId to include uniqueIdentifier. Llap task scheduler can construct the container object with this new NodeId that includes uniqueIdentifier as well so that stale events like above will only remove the host/node that matches the old uniqueIdentifier. ",2020-05-13T23:03:43.672+0000,2020-12-15T18:47:02.310+0000,Fixed,Major
MASSEMBLY-832,Consider dependency optional attribute when gather dependencies,MASSEMBLY,New Feature,Open,[],1,[<JIRA IssueLink: id='12629175'>],"In some scenario we would like to have the opportunity to skip optional dependencies
{code:xml}
<assembly ...>
  <id>dist</id>
  <formats>
    <format>zip</format>
  </formats>
    <includeBaseDirectory>false</includeBaseDirectory>
    <moduleSets>
      <moduleSet>
        <!-- Enable access to all projects in the current multimodule build! -->
        <useAllReactorProjects>true</useAllReactorProjects>
        <binaries>
          <outputDirectory>3rdparty</outputDirectory>
          <unpack>false</unpack>

          <dependencySets>
            <dependencySet>
              <!-- option 1 -->
              <considerOptional>true</considerOptional>
              <!-- or option 2 -->
              <includes>
                <!-- groupId:artifactId:type[:classifier][:optional]:version -->
                <include>*:*:*:false:*</include>
              </includes>
           </dependencySet>
         </dependencySets>
       </binaries>
    </moduleSet>
  </moduleSets>
</assembly>
{code}",2016-10-06T16:06:16.176+0000,2021-12-22T12:16:57.104+0000,,Minor
TEZ-2627,Support for Tez Job Priorities,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12431392'>],"When a Tez Job is submitted via TezClient, an ApplicationSubmissionContext is created before submitting the job. ApplicationSubmissionContext has a priority field which can be used to provide a priority for the job.

There is an ongoing effort in the Yarn Community to enable application priorities(https://issues.apache.org/jira/browse/YARN-1963).

https://issues.apache.org/jira/browse/YARN-2003 implements the necessary changes in RM and Capacity Scheduler.

",2015-07-20T20:25:01.116+0000,2019-10-14T15:37:39.873+0000,Fixed,Major
CALCITE-932,Fix muddled columns when RelFieldTrimmer is applied to Aggregate,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12446807'>, <JIRA IssueLink: id='12446915'>, <JIRA IssueLink: id='12446803'>]",,2015-10-22T18:02:35.420+0000,2015-11-10T08:02:45.073+0000,Fixed,Major
TWILL-256,Support fault tolerant AM restart,TWILL,New Feature,Open,[],2,"[<JIRA IssueLink: id='12528963'>, <JIRA IssueLink: id='12528952'>]",Details https://hortonworks.com/blog/apache-hadoop-yarn-hdp-2-2-fault-tolerance-features-long-running-services/,2018-03-08T23:22:23.400+0000,2018-03-09T01:22:21.282+0000,,Major
SPARK-24243,Expose exceptions from InProcessAppHandle,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12533717'>],"{{InProcessAppHandle}} runs {{SparkSubmit}} in a dedicated thread, any exceptions thrown are logged and then the state is set to {{FAILED}}. It would be nice to expose the {{Throwable}} object  to the application rather than logging it and dropping it. Applications may want to manipulate the underlying {{Throwable}} / control its logging at a finer granularity. For example, the app might want to call {{Throwables.getRootCause(throwable).getMessage()}} and expose the message to the app users.",2018-05-10T12:42:36.840+0000,2018-12-07T18:35:16.837+0000,Fixed,Major
CALCITE-1808,JaninoRelMetadataProvider loading cache might cause OOM error,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12621568'>],"JaninoRelMetadataProvider has a static handler cache with size increasing over time for long running HS2, ending up causing OOM errors.

{code:java}
  /** Cache of pre-generated handlers by provider and kind of metadata.
   * For the cache to be effective, providers should implement identity
   * correctly. */
  private static final LoadingCache<Key, MetadataHandler> HANDLERS =
      CacheBuilder.newBuilder().build(
          new CacheLoader<Key, MetadataHandler>() {
            public MetadataHandler load(@Nonnull Key key) {
              //noinspection unchecked
              return load3(key.def, key.provider.handlers(key.def),
                  key.relClasses);
            }
          });
...
  /** Key for the cache. */
  private static class Key {
    public final MetadataDef def;
    public final RelMetadataProvider provider;
    public final ImmutableList<Class<? extends RelNode>> relClasses;
...
{code}

The lifecycle for providers is per query and we have multiple providers instantiated on the lifecycle of a query. The entries are retained in the cache even when query planning has finished.",2017-05-26T10:46:36.866+0000,2021-08-23T08:27:23.126+0000,Fixed,Major
CALCITE-3189,Multiple fixes for Oracle SQL dialect,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12565829'>],"Among others, it includes i) SQL translation support for specific types (e.g. {{SMALLINT}} --> {{NUMBER(5)}}), ii) limiting max length of {{VARCHAR}} type (by default 4000), iii) creating datetime literals correctly, and iv) method to infer whether a given data type is supported or not.",2019-07-10T00:22:10.771+0000,2019-09-10T21:37:15.058+0000,Fixed,Major
ORC-158,do not assume an array-backed buffer in static ReaderImpl methods,ORC,Bug,In Progress,[],1,[<JIRA IssueLink: id='12498107'>],"For example, arrayOffset() usage causes errors with direct buffers.

Another useful thing to support would be an array of ByteBuffer-s, in case the read from e.g. ZCR, or cache, does not fit into one ByteBuffer, however unlikely that is",2017-03-15T21:32:16.027+0000,2017-06-30T21:15:11.550+0000,,Major
TEZ-4313,Apache Tez Release 0.10.1,TEZ,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12617532'>, <JIRA IssueLink: id='12643067'>]",,2021-06-16T08:46:53.965+0000,2022-07-01T05:56:51.259+0000,Fixed,Major
OOZIE-1087,Remove requirement of hive-default.xml from Hive action,OOZIE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12360771'>],"Since Hive 0.8.0, the hive-default.xml is no longer bundled in the Hive JARs, which means that if the user doesn't specify {{oozie.hive.defaults}}, then the hive action will throw a RuntimeException.  In fact, Hive actually ignores hive-default.xml, so we should remove the related behavior/properties/requriements from the Hive action altogether.  ",2012-11-26T21:32:18.274+0000,2013-07-20T00:02:58.282+0000,Fixed,Major
CALCITE-1620,"CAST('<string>' AS BINARY) is supported by Hive, but not by Calcite",CALCITE,Bug,Closed,[],4,"[<JIRA IssueLink: id='12493628'>, <JIRA IssueLink: id='12493571'>, <JIRA IssueLink: id='12495157'>, <JIRA IssueLink: id='12493574'>]","Hive supports {{CAST('1' as BINARY)}}. Result is 0x31 (can be tested with {noformat}SELECT HEX(CAST('1' as BINARY));{noformat}. If I'm not mistaken, the CAST result is the UTF-8 encoding of the input string.
CALCITE 1.11 throws AssertionException during compilation for such an expression:

{noformat}
Thread [db11b6e6-8fa7-48b3-82ef-c5b4550327ec main] (Suspended (exception AssertionError))             
                ConstantExpression.<init>(Type, Object) line: 50      
                Expressions.constant(Object, Type) line: 586             
                OptimizeShuttle.visit(UnaryExpression, Expression) line: 279
                UnaryExpression.accept(Shuttle) line: 37   
                Expressions.acceptExpressions(List<Expression>, Shuttle) line: 3184  
                MethodCallExpression.accept(Shuttle) line: 60         
                TernaryExpression.accept(Shuttle) line: 45
{noformat}",2017-02-06T21:35:18.083+0000,2017-03-03T11:01:19.000+0000,Not A Problem,Major
TEZ-1805,Tez client DAG cycle detection should detect self loops,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12402355'>],"There's a test in the hive suite that gets stuck and I'm not sure what's causing it.

Repro:

(In hive tree: https://github.com/apache/hive)

mvn clean install -DskipTests -Phadoop-2 && cd itests && mvn clean install -DskipTests -Phadoop-2

then:

mvn test -Dtest=TestMiniTezCliDriver -Phadoop-2 -Dqfile=lvj_mapjoin.q

I'll attach logs and stack traces. It seems application: pplication_1417137410462_0002 got stuck in that run. Only exception I saw is:

{noformat}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /Users/ghagleitner/Projects/hive-trunk2/itests/qtest/target/tmp/scratchdir/ghagleitner/_tez_session_dir/dc4fca20-4a39-4452-9\
75a-467bda4947ca/.tez/application_1417137410462_0001/recovery/1/summary (inode 16430): File does not exist. Holder DFSClient_NONMAPREDUCE_1900574341_1 does not have any open files.                                                          
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3083)                                                                                                                                                   
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3170)                                                                                                                                         
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3140)                                                                                                                                                 
  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:665)                                                                                                                                            
  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:499)                                                                                           
  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)                                                                                        
  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)                                                                                                                                       
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)                                                                                                                                                                                      
  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)                                                                                                                                                                             
  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)                                                                                                                                                                             
  at java.security.AccessController.doPrivileged(Native Method)                                                                                                                                                                               
  at javax.security.auth.Subject.doAs(Subject.java:394)                                                                                                                                                                                       
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)                                                                                                                                                     
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)                                                                                                                                                                               
                                                                                                                                                                                                                                              
  at org.apache.hadoop.ipc.Client.call(Client.java:1411)                                                                                                                                                                                      
  at org.apache.hadoop.ipc.Client.call(Client.java:1364)                                                                                                                                                                                      
  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)                                                                                                                                                       
  at com.sun.proxy.$Proxy14.complete(Unknown Source)                                                                                                                                                                                          
  at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)                                                                                                                                                                             
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                                                                                                                    
  at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                                                                                                         
  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)                                                                                                                                          
  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)                                                                                                                                                
  at com.sun.proxy.$Proxy14.complete(Unknown Source)                                                                                                                                                                                          
  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)                                                                                                               
  at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)                                                                                                                                                           
  at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)                                                                                                                                                                  
  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)                                                                                                                                                  
  at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)                                                                                                                                                               
  at org.apache.tez.dag.history.recovery.RecoveryService.serviceStop(RecoveryService.java:201)                                                                                                                                                
  at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)                                                                                                                                                                 
  at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)                                                                                                                                                              
  at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)                                                                                                                                                       
  at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)                                                                                                                                                               
  at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)                                                                                                                                                        
  at org.apache.tez.dag.history.HistoryEventHandler.serviceStop(HistoryEventHandler.java:80)                                                                                                                                                  
  at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)                                                                                                                                                                 
  at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)                                                                                                                                                              
  at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)                                                                                                                                                       
  at org.apache.tez.dag.app.DAGAppMaster.stopServices(DAGAppMaster.java:1504)                                                                                                                                                                 
  at org.apache.tez.dag.app.DAGAppMaster.serviceStop(DAGAppMaster.java:1643)                                                                                                                                                                  
  at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)                                                                                                                                                                 
  at org.apache.tez.dag.app.DAGAppMaster$DAGAppMasterShutdownHandler$AMShutdownRunnable.run(DAGAppMaster.java:698)                                                                                                                            
  at java.lang.Thread.run(Thread.java:695)                                                       
{noformat}

It's quite likely that we're screwing something up in Hive, but I can't find from the logs what's happening. Any insights?",2014-11-28T01:41:46.134+0000,2015-01-29T20:25:32.292+0000,Fixed,Major
RANGER-2745,execute hdfs dfs -ls / NPE,RANGER,Bug,Closed,[],1,[<JIRA IssueLink: id='12583415'>],"when I execute hdfs dfs -ls / ，Viewing the Hadoop logs found the following error
{quote}xxxx 15:20:23,058 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo from xxx:11136 Call#0 Retry#0
{quote}
{quote}java.lang.NullPointerException
{quote}
{quote}at org.apache.hadoop.hdfs.DFSUtil.bytes2String(DFSUtil.java:314)
{quote}
{quote}at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.getINodeAttrs(FSPermissionChecker.java:238)
{quote}
{quote}at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:183)
{quote}
{quote}at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1752)
{quote}
{quote}at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:100)
{quote}
{quote}at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3831)
{quote}
{quote}at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1012)
{quote}
{quote}at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:855)
{quote}
{quote}at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
{quote}
{quote}at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
{quote}
{quote}at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
{quote}
{quote}at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)
{quote}
{quote}at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)
{quote}
{quote}at java.security.AccessController.doPrivileged(Native Method)
{quote}
{quote}at javax.security.auth.Subject.doAs(Subject.java:422)
{quote}
{quote}at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1758)
{quote}
{quote}at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2213)
{quote}
But when I execute hdfs dfs -ls /tmp ，Did not report an error，And control permissions functions are normal.",2020-02-28T03:21:32.965+0000,2020-03-25T02:49:21.512+0000,Won't Fix,Major
SPARK-14044,Allow configuration of DynamicPartitionWriterContainer#writeRows to bypass sort step,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12530777'>, <JIRA IssueLink: id='12461450'>]","It would be very useful to allow the disabling of this block of code within {{DynamicPartitionWriterContainer#writeRows}} at runtime:

https://github.com/apache/spark/blob/8ef3399aff04bf8b7ab294c0f55bcf195995842b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriterContainer.scala#L379-L418

The use case is that an upstream {{groupBy}} has already sorted a great many fine grained groups which are the target of the {{partitionBy}}. This {{partitionBy}} shares the same keys as the {{groupBy}}. Currently, we can't even get Spark to succeed due to the sort step and data skew in the partitions. In general, this would make more efficient use of cluster resources.

For this to work, there needs to be a way to communicate between the {{groupBy}} and the {{partitionBy}} by way of some runtime configuration. This is very similar in function to Hive's {{hive.optimize.sort.dynamic.partition}} parameter.",2016-03-21T21:31:37.245+0000,2018-03-30T18:58:33.919+0000,Duplicate,Major
SPARK-2881,Snappy is now default codec - could lead to conflicts since uses /tmp,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12395592'>],"I was using spark master branch and I ran into an issue with Snappy since its now the default codec for shuffle. 

The issue was that someone else had run with snappy and it created /tmp/snappy-*.so but it had restrictive permissions so I was not able to use it or remove it.   This caused my spark job to not start.  

I was running in yarn client mode at the time.  Yarn cluster mode shouldn't have this issue since we change the java.io.tmpdir. 
I assume this would also affect standalone mode.

I'm not sure if this is a true blocker but wanted to file it as one at first and let us decide.",2014-08-06T17:08:26.696+0000,2014-09-01T05:49:31.431+0000,Fixed,Blocker
ORC-621,Need reader fix for ORC-569,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12586388'>, <JIRA IssueLink: id='12586389'>]","Hive implemented a reader fix for ORC-569 in HIVE-22480, but ORC needs the equivalent fix. As part of testing the fix, we need another fix and a couple of test cases.",2020-04-21T00:05:42.652+0000,2020-04-27T04:25:09.132+0000,Fixed,Major
BIGTOP-3677,Building HBase for CentOS 7 on arm64 and ppc64le fails due to missing libraries,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12639088'>],"[https://ci.bigtop.apache.org/view/Releases/job/Bigtop-3.1.0/1/DISTRO=centos-7,PLATFORM=aarch64-slave/console]

{code}
[ERROR] /ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/src/main/protobuf/Cell.proto [0:0]: /ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe)
/ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe)
/ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe)
/ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /ws/build/hbase/rpm/BUILD/hbase-2.4.11/hbase-protocol/target/protoc-plugins/protoc-2.5.0.2-linux-aarch_64.exe)
{code}",2022-04-27T06:15:54.373+0000,2022-05-06T10:41:54.099+0000,Fixed,Major
TEZ-3024,Move TaskCommunicator to correct package,TEZ,Task,Closed,[],1,[<JIRA IssueLink: id='12455300'>],Currently under org.apache.tez.dag.api. Should be under org.apache.tez.serviceplugins.api,2016-01-04T23:16:05.857+0000,2016-05-18T04:57:47.106+0000,Fixed,Major
RATIS-197,"In gRPC, reads are failing for size greater than 4MB",RATIS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12526520'>],"Read failing found in Ozone; see HDFS-13078 for the details.
",2018-01-23T12:10:58.128+0000,2018-02-07T20:22:30.040+0000,Fixed,Major
KYLIN-224,Leverage HBase Namespace to isolate Kylin HTables ,KYLIN,New Feature,Closed,[],6,"[<JIRA IssueLink: id='12513709'>, <JIRA IssueLink: id='12406487'>, <JIRA IssueLink: id='12475416'>, <JIRA IssueLink: id='12480584'>, <JIRA IssueLink: id='12518242'>, <JIRA IssueLink: id='12489199'>]","To well manage and isolate Kylin relative htable in HBase, using namespace is best way to isolate with other application's tables. 

Due to [HBASE-9206|https://issues.apache.org/jira/browse/HBASE-9206], Kylin do not support HBase namespace yet.



---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/281
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: Backlog
Created at: Fri Dec 26 14:05:42 CST 2014
State: open
",2015-01-09T15:30:42.648+0000,2017-10-23T09:20:32.089+0000,Duplicate,Major
ACCUMULO-2931,Ensure correct ordering of updates to a tablet across different WALs,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12390160'>],"I was talking to [~enis] today about common replication problems across HBase and Accumulo and he was telling me about the following:

A tablet is hosted by tserver1 using WAL1. That tablet moves to a different tserver for whatever reason (tserver1 failed, the balancer, etc) and starts getting used by tserver2 with WAL2.

In the simple case of replicating to another Accumulo instance with servers running NTP, this shouldn't be a big concern because the timestamp assigned to the updates will ensure a final consistent view. However, the intermediate view is incorrect. We can do a better job to ensure that we replicate the data in the correct order.

We already know the WALs that are used by a tablet and the time in which that tablet began using it (done by the TabletServer before any updates hit that Tablet) in the metadata table. We can use these records, in addition to the timestamp on the {{log}} column entries to determine the correct ordering for this Tablet WRT to all WALs. All the information is present so that the Master can assign the replication work in the correct order.

Some extra bookkeeping would also be required to either keep that {{log}} column around longer than the minc or recovery, or to record some additional piece of replication metadata that the master can read from the replication table.",2014-06-20T01:18:23.136+0000,2019-04-16T19:23:43.898+0000,Cannot Reproduce,Minor
PARQUET-131,Vectorized Reader In Parquet,PARQUET,Improvement,Open,"[<JIRA Issue: key='PARQUET-298', id='12834759'>, <JIRA Issue: key='PARQUET-299', id='12834760'>, <JIRA Issue: key='PARQUET-300', id='12834762'>, <JIRA Issue: key='PARQUET-301', id='12834763'>, <JIRA Issue: key='PARQUET-302', id='12834764'>, <JIRA Issue: key='PARQUET-303', id='12834765'>, <JIRA Issue: key='PARQUET-333', id='12844886'>]",1,[<JIRA IssueLink: id='12401294'>],"Vectorized Query Execution could have big performance improvement for SQL engines like Hive, Drill, and Presto. Instead of processing one row at a time, Vectorized Query Execution could streamline operations by processing a batch of rows at a time. Within one batch, each column is represented as a vector of a primitive data type. SQL engines could apply predicates very efficiently on these vectors, avoiding a single row going through all the operators before the next row can be processed.
As an efficient columnar data representation, it would be nice if Parquet could support Vectorized APIs, so that all SQL engines could read vectors from Parquet files, and do vectorized execution for Parquet File Format.
 
Detail proposal:
https://gist.github.com/zhenxiao/2728ce4fe0a7be2d3b30",2014-11-11T00:25:14.115+0000,2018-10-25T15:39:41.966+0000,,Major
ORC-577,Allow row-level filtering,ORC,New Feature,Closed,[],6,"[<JIRA IssueLink: id='12586370'>, <JIRA IssueLink: id='12586369'>, <JIRA IssueLink: id='12580108'>, <JIRA IssueLink: id='12607308'>, <JIRA IssueLink: id='12586356'>, <JIRA IssueLink: id='12585405'>]","Currently, ORC filters at three levels:
 * File level
 * Stripe (64 to 256mb) level
 * Row group (10k row) level

The filters are specified as Sargs (Search Arguments), which have a relatively small vocabulary. Furthermore, they only filter sets of rows if they can guarantee that none of the rows can pass the filter.

There are some use cases where the user needs to read a subset of the columns and apply more detailed row level filters. I'd suggest that we add a new method in Reader.Options

{{setRowFilter(String[] filterColumnNames, Consumer<VectorizedRowBatch> filterCallback))}}

Where the columns named in columnNames are read expanded first, then the filter is run and the rest of the data is read only if the predicate returns true.

",2019-12-11T21:08:17.788+0000,2021-09-21T20:44:43.465+0000,Fixed,Major
TEZ-1303,Change I/P/O/etc construction to make use of contexts,TEZ,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12392608'>, <JIRA IssueLink: id='12392970'>]","Instead of having an empty constructor and setting the context via an initialize method - thus allowing the context to be accessed via a final field.
Using initialize causes potential issues with visibility since object creation, initialize and access can be on completely different threads.",2014-07-22T01:55:59.247+0000,2014-07-31T00:29:28.824+0000,Fixed,Blocker
PHOENIX-5296,Ensure store file reader refcount is zero at end of relevant unit tests,PHOENIX,Test,Resolved,[],3,"[<JIRA IssueLink: id='12561446'>, <JIRA IssueLink: id='12607081'>, <JIRA IssueLink: id='12561445'>]","Unit and integration tests for functional areas where the implementation wraps scanners and uses a delegate pattern should check at the end of the test that no scanners were leaked (check that all scanners including the inner scanners were properly closed) by testing that the store reader reference count is zero on all stores. 

HBASE-22459 will offer a new metric and API which exposes this information. ",2019-05-23T18:07:18.478+0000,2021-01-28T05:59:05.656+0000,Fixed,Major
ORC-1059,Align findColumns behaviour between 1.6 and 1.7 release,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12628747'>, <JIRA IssueLink: id='12628722'>]","ORC-741 changed the behaviour of column resolution throwing an IllegalArgumentException when a column is not found in the read schema.

Column resolution is also using by mapSargColumnsToOrcInternalColId when pushing down filters and throws an IllegalArgumentException exception when a filter column is not part of the schema instead of ignoring it (as it was the case in 1.6 and back).

1.6 and back
{code:java}
try {
  TypeDescription readerColumn = evolution.getReaderBaseSchema().findSubtype(
      columnName, evolution.isSchemaEvolutionCaseAware);
  TypeDescription fileColumn = evolution.getFileType(readerColumn);
  return fileColumn == null ? -1 : fileColumn.getId();
} catch (IllegalArgumentException e) {
  if (LOG.isDebugEnabled()){
    LOG.debug(""{}"", e.getMessage());
  }
  return -1;
} {code}
1.7 and main
{code:java}
try {
  TypeDescription readerColumn = evolution.getReaderBaseSchema().findSubtype(
    columnName, evolution.isSchemaEvolutionCaseAware);
  return evolution.getFileType(readerColumn);
} catch (IllegalArgumentException e) {
  throw new IllegalArgumentException(""Filter could not find column with name: "" +
                                     columnName + "" on "" + evolution.getReaderBaseSchema(),
                                     e);
} {code}
This makes harder to upgrade to 1.7 on downstream consumers like Hive https://issues.apache.org/jira/browse/HIVE-25497",2021-12-13T12:35:05.376+0000,2022-02-02T23:55:18.510+0000,Fixed,Major
TEZ-606,Fix Tez to work with kerberos security,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12378134'>, <JIRA IssueLink: id='12379796'>]",,2013-11-08T00:33:03.440+0000,2014-03-01T04:19:31.002+0000,Fixed,Major
AMBARI-5976,User should be able choose in UI whether hive and pig is shipped by webhcat,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12389067'>],This requires allowing user to set {{templeton.hive.archive}} and {{templeton.pig.archive}} to empty value,2014-05-30T19:07:30.983+0000,2014-05-30T19:35:14.712+0000,Fixed,Major
IMPALA-11278,Cardinality of small HBase regions is overestimated since HBASE-26340,IMPALA,Bug,Open,[],2,"[<JIRA IssueLink: id='12639145'>, <JIRA IssueLink: id='12639146'>]","Impala uses the size of an HBase region to estimate the number of rows, and the API we use (https://hbase.apache.org/2.4/apidocs/org/apache/hadoop/hbase/RegionLoad.html#getStorefileSizeMB() ) returns a size at MB precision. Since HBASE-26340 it returns 1 instead of 0 for very small but not empty tables, which leads to massively overestimating its size (we handle 0 in a special way. so we didn't estimate  row count as 0: https://github.com/apache/impala/blob/78609dca32d8ce996247c9552ba676a853c74686/fe/src/main/java/org/apache/impala/catalog/FeHBaseTable.java#L585 )

In newer versions of HBase getStorefileSizeMB() is deprecated and there are functions to get the size at byte granulity. Using it could solve the massive overestimation, but it may make our planner tests more sensitive to small size changes in HBase regions.

HBASE-26340 is included in the Impala's HBase dependency since https://github.com/apache/impala/commit/ca48b940ec6281d492ad525418f234308a82eedf",2022-05-02T12:38:58.907+0000,2022-05-02T13:03:57.090+0000,,Major
SPARK-1825,Windows Spark fails to work with Linux YARN,SPARK,Bug,Closed,[],2,"[<JIRA IssueLink: id='12405245'>, <JIRA IssueLink: id='12402888'>]","Windows Spark fails to work with Linux YARN.
This is a cross-platform problem.

This error occurs when 'yarn-client' mode is used.
(yarn-cluster/yarn-standalone mode was not tested.)

On YARN side, Hadoop 2.4.0 resolved the issue as follows:
https://issues.apache.org/jira/browse/YARN-1824

But Spark YARN module does not incorporate the new YARN API yet, so problem persists for Spark.

First, the following source files should be changed:
- /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala
- /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnableUtil.scala

Change is as follows:
- Replace .$() to .$$()
- Replace File.pathSeparator for Environment.CLASSPATH.name to ApplicationConstants.CLASS_PATH_SEPARATOR (import org.apache.hadoop.yarn.api.ApplicationConstants is required for this)

Unless the above are applied, launch_container.sh will contain invalid shell script statements(since they will contain Windows-specific separators), and job will fail.
Also, the following symptom should also be fixed (I could not find the relevant source code):
- SPARK_HOME environment variable is copied straight to launch_container.sh. It should be changed to the path format for the server OS, or, the better, a separate environment variable or a configuration variable should be created.
- '%HADOOP_MAPRED_HOME%' string still exists in launch_container.sh, after the above change is applied. maybe I missed a few lines.

I'm not sure whether this is all, since I'm new to both Spark and YARN.",2014-05-14T02:38:06.969+0000,2015-02-02T02:30:06.041+0000,Fixed,Major
LUCENE-3296,Enable passing a config into PKIndexSplitter,LUCENE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12340691'>],I need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter.,2011-07-09T02:33:00.541+0000,2022-08-28T12:52:55.943+0000,Fixed,Trivial
BIGTOP-1846,Upgrade Hive to 1.2.0,BIGTOP,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12422301'>, <JIRA IssueLink: id='12422302'>, <JIRA IssueLink: id='12422303'>, <JIRA IssueLink: id='12429214'>]",We ran into a compatibility issue with Hive 1.1+ and Spark 1.3.x. A fix from upstream is HIVE-9726 and it is part of the merge (HIVE-10347) which is also in the queue for Hive 1.2 release.,2015-04-23T02:41:00.864+0000,2015-06-29T20:25:23.065+0000,Won't Fix,Major
CALCITE-3890,Derive IS NOT NULL filter for the inputs of inner join ,CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12644425'>],"We can infer IS NOT NULL predicate from join which implies some columns may not be null. For instance, 

 
{code:java}
select * from a join b on a.id = b.id;
{code}
we can infer a.id is not null/b.id is not null and push down them into the child node of the join. Then it becomes
{code:java}
select * from (select* from a where id is null) t1 join (select * from b where id is not null) on t1.id = t2.id;
{code}
 

 ",2020-04-01T04:09:25.788+0000,2022-08-03T16:21:54.561+0000,Fixed,Major
FLINK-12022,Enable StreamWriter to update file length on sync flush,FLINK,Improvement,Closed,[],1,[<JIRA IssueLink: id='12557529'>],"Currently, users of file systems that do not support truncating have to struggle with BucketingSink and use its valid length file to indicate the checkpointed data position. The problem is that by default the file length will only be updated when a block is full or the file is closed, but when the job crashes and the file is not closed properly, the file length is still behind its actual value and the checkpointed file length. When the job restarts, it looks like data loss, because the valid length is bigger than the file. This situation lasts until namenode notices the change of block size of the file, and it could be half an hour or more.

So I propose to add an option to StreamWriterBase to update file lengths on each flush. This can be expensive because it involves namenode and should be used when strong consistency is needed.",2019-03-26T13:07:38.605+0000,2021-12-16T02:26:12.942+0000,Won't Fix,Minor
ACCUMULO-3738,Restore a trace class for Hive to work,ACCUMULO,Task,Resolved,[],2,"[<JIRA IssueLink: id='12421550'>, <JIRA IssueLink: id='12421551'>]","Shame on me that I let HIVE-9082 slip through a couple of releases.

Backstory: Hive (Pig does this too) has some nice code lifted from HBase which accepts some Java class and figures out the jar which contains that class (which is on the classpath) and then adds that jar as a necessary dependency for the query execution (e.g. libjars for mapreduce).

The problem is that when I did the Hive stuff, I chose a bad class: trace/src/main/java/org/apache/accumulo/trace/instrument/Tracer.java. This went away in the HTrace switchover. Because Hive has had a few releases since when this went in, if we don't restore this class, users will have to jump through hoops to make 1.7.0 work with those intermediate versions of Hive (0.14.0, 1.2.0)

At this point, it's much easier to make an empty class in Accumulo that will allow Hive to continue operating as-is. After enough time/releases, we can remove it again from Accumulo.",2015-04-16T19:27:20.105+0000,2015-04-16T20:55:06.613+0000,Fixed,Major
IMPALA-8738,Add a column representing the type（table or view） in the show tables output,IMPALA,Improvement,In Progress,"[<JIRA Issue: key='IMPALA-9103', id='13265096'>]",1,[<JIRA IssueLink: id='12567095'>],"Now the output of the +*show tables*+ command in the system is as follows：
{code:java}
default> show tables;
Query: show tables
+----------+
| name |
+----------+
| table1 |
| view1 |
+----------+
{code}
I think we should add a column for the representation type，The output should be like this
{code:java}
default> show tables;
Query: show tables
+---------------+
| name | type |
+----------------+
| table1 |  table   |
| view1 |   view   |
+----------------+
{code}
 ",2019-07-03T08:14:16.777+0000,2019-10-16T01:21:52.456+0000,,Minor
REEF-568,Work around the federated YARN node reports problem,REEF,Task,Resolved,[],4,"[<JIRA IssueLink: id='12434234'>, <JIRA IssueLink: id='12434239'>, <JIRA IssueLink: id='12434175'>, <JIRA IssueLink: id='12433498'>]","When trying to use REEF with Federation, there's a problem on the node reports YARN sends us.
Just after initializing our yarn client library (hadoop-yarn-client-2.4.0), we ask for the RUNNING nodes in the cluster to populate our own Resource Catalog.
YARN replies with the nodes that belong to a 'random' sub-cluster; sometimes with the nodes in the correct sub-cluster (where the containers will be placed), and sometimes with other ones. That causes the application to randomly fail.
For example, we populate our resource catalog with nodes in sub-cluster 1, but the allocations are actually made on sub-cluster 2, so we fail.

We need to do a work around for this issue, as YARN folks are not sure when they will have the right.",2015-08-06T21:42:23.332+0000,2015-08-13T16:38:04.309+0000,Duplicate,Minor
BIGTOP-3615,Upgrade log4j2 of Hive 3.1.2 to 2.16.0,BIGTOP,Improvement,Resolved,[],5,"[<JIRA IssueLink: id='12628725'>, <JIRA IssueLink: id='12633221'>, <JIRA IssueLink: id='12628583'>, <JIRA IssueLink: id='12628585'>, <JIRA IssueLink: id='12628584'>]",HIVE-25795 upgraded log4j2 to 2.15.0 for CVE-2021-44228. We need the patch of HIVE-22278 too for Hive 3.1.2.,2021-12-14T11:07:03.132+0000,2022-02-10T06:06:59.932+0000,Fixed,Major
NUTCH-368,Message queueing system,NUTCH,New Feature,Closed,[],1,[<JIRA IssueLink: id='12313417'>],"This is an implementation of a filesystem-based message queueing system. The motivation for this functionality is explained in HADOOP-490 - there is nothing Nutch-specific in this implementation, so if it's considered generally useful it could be moved there.

Below are excerpts from the included javadocs.

The model of the system is as follows:

    * applications (including map-reduce jobs) may create their own separate message queueing area. Alternatively, they can specifically ask for a named message queue, belonging to a different application or existing as a system-wide queue. Message queues are created under ""/mq"" and then the message queue id (for map-reduce jobs this is a job id, or it can be any other name passed as job id to the constructor).
      Please see the example for more information.

    * a single unit of information passing through queues is a Msg, which has a unique identifier (consisting of creation time and publisher name), string subject, and content (Writable).

    * single MsgQueue in fact consists of any number of topics. There are four predefined ones: in, out, err, and ctrl.

    * messages are published to topics, which present a sequential view of messages, sorted by msgId (which corresponds to their order of arrival).

    * each message queue may periodically poll for changes (MsgQueue.startPolling()), using a separate thread. Polling updates the list of topics and messages. Poll interval is configurable, and defaults to 5 sec.

    * each detected change in the queue (add/remove topic, add/remove message) may be communicated to registered listeners. Out-of-band messages are not supported in this version, but it's not too complicated to add them. Applications can create listeners watching queues for newly added messages, or deleted messages, added topics or deleted topics, etc.

    * each instance of MsgQueue using the same physical queue maintains its own view of the queue, keeping track of topics and messages that it considers ""processed and discarded"". In other words, multiple readers and creators may modify queues, and each knows which messages it already processed and which ones are new. In a similar fashion, instances may willfully ""remove"" certain topics from their view, even though these topics still physically exist and are available for other instances (and later on they can ""add"" them to their view again).
      This somewhat complicated feature was implemented in order to support multiple readers for the same message (e.g. many tasks per one mapred job). Each task needs to register for the same queue, and if they didn't have their own views of the queue, messages would be consumed by the first task that got to them. As it is implemented now, each task may consume messages at its own pace. At the end of the job applications may elect to keep the queue around or to destroy it (and thus remove all topics and messages in it).

    * messages, topics and queues may be destroyed by any user, at which point they are physically removed from the filesystem. All users will gradually update their views, during the next poll operation.

    * there is a command-line tool to examine and modify queues, and also to retrieve and send simple text messages. You can run it like this:

         bin/nutch org.apache.nutch.util.msg.MsgQueueTool ...many options...
",2006-09-15T20:36:40.000+0000,2008-01-22T14:48:36.762+0000,Won't Fix,Major
SPARK-7789,sql  on security hbase:Token generation only allowed for Kerberos authenticated clients,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12425407'>, <JIRA IssueLink: id='12425406'>]","After creating a hbase table in beeline, then execute select sql statement, Executor occurs the exception:
{quote}
java.lang.IllegalStateException: Error while configuring input job properties
        at org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureTableJobProperties(HBaseStorageHandler.java:343)
        at org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureInputJobProperties(HBaseStorageHandler.java:279)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler(PlanUtils.java:804)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.configureInputJobPropertiesForStorageHandler(PlanUtils.java:774)
        at org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc(TableReader.scala:300)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:276)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:276)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:176)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:220)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
        at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:124)
        at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$1.getAuthenticationToken(AuthenticationProtos.java:4267)
        at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService.callMethod(AuthenticationProtos.java:4387)
        at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:7696)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:1877)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:1859)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32209)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2131)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:102)
        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)
        at java.lang.Thread.run(Thread.java:745)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:326)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1636)
        at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:92)
        at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:89)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)
        at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:95)
        at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:73)
        at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:86)
        at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:111)
        at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:108)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1672)
        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:312)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:108)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:215)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:196)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)
        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:321)
        at org.apache.hadoop.hive.hbase.HBaseStorageHandler.addHBaseDelegationToken(HBaseStorageHandler.java:371)
        at org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureTableJobProperties(HBaseStorageHandler.java:340)
{quote}",2015-05-21T09:13:24.461+0000,2019-01-04T14:39:50.311+0000,Incomplete,Major
TEZ-2712,Tez UI: Display the vertex description in the tooltip of vertex in DAG view UI,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12491518'>],"  Currently it only displays Vertex Name and Vertex ID ( and status, duration, parallelism and Processor name). PIG-4429 sets additional description about the vertex which would be good to have in the DAG view UI. ",2015-08-11T22:01:45.222+0000,2017-08-22T00:03:16.809+0000,Fixed,Major
HCATALOG-23,"hcat client does not support partition keys other than string, when creating tables",HCATALOG,New Feature,Open,[],2,"[<JIRA IssueLink: id='12346964'>, <JIRA IssueLink: id='12389134'>]",,2011-05-06T20:15:33.087+0000,2014-06-02T09:41:29.703+0000,,Major
BIGTOP-1007,Introduce a modules system for HBase coprocessor applications,BIGTOP,Improvement,Open,"[<JIRA Issue: key='BIGTOP-1133', id='12676057'>]",2,"[<JIRA IssueLink: id='12370386'>, <JIRA IssueLink: id='12370385'>]","Consider a modules system convention (""/etc/hbase/modules.d""), a common pattern used for example by Apache httpd, for easily installation and removal of HBase coprocessor applications.

Within the modules.d/ directory, one additional level of subdirectories can be created, into which a package can drop site xml fragments and scripts to execute after regionserver and master (re)start. Future packages that ship an HBase coprocessor application could then add configuration bits without concern about collisions and trigger a regionserver reload in postinstall.

HBase already ships a tool for merging configuration files. Changes required for this will be proposed upstream if needed.",2013-06-12T16:22:56.406+0000,2013-09-27T03:14:51.130+0000,,Major
OOZIE-3348,[Hive action] Remove dependency hive-contrib,OOZIE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12543401'>, <JIRA IssueLink: id='12543400'>]","As per HIVE-20020 the dependency {{org.apache.hive:hive-contrib}} is packaged differently - moved out of under Hive's lib.

{{oozie-sharelib-hive}} exports some of the Hive dependencies, and that includes {{hive-contrib.jar}}. In order to be synced with Hive we should remove it too.

Oozie doesn't depend on this jar runtime so there's no use of it being in {{oozie-sharelib-hive}} anyway.",2018-09-18T11:33:20.944+0000,2019-01-18T08:34:10.075+0000,Fixed,Major
INFRA-17640,git.apache.org/hbase.git not available,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12551762'>],"HBase repository was recently moved to GitBox and https://git.apache.org/hbase.git is not available anymore. Is it expected that repository is removed git.apache.org?

We were under the impression that git.apache.org will remain available and only git-wip-us will be deleted.",2019-01-13T13:09:03.240+0000,2019-01-13T13:14:25.695+0000,Won't Fix,Major
TEZ-1590,Fetchers should not report failures after the Processor on the task completes,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12400407'>, <JIRA IssueLink: id='12397010'>]",Details in https://issues.apache.org/jira/browse/PIG-4069?focusedCommentId=14136710&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14136710,2014-09-17T03:33:11.818+0000,2014-11-08T20:25:17.477+0000,Fixed,Major
FLINK-29046,HiveTableSourceStatisticsReportTest fails with Hadoop 3,FLINK,Bug,Closed,[],2,"[<JIRA IssueLink: id='12646175'>, <JIRA IssueLink: id='12646297'>]","
{code:java}
2022-08-19T13:35:56.1882498Z Aug 19 13:35:56 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 9.442 s  <<< FAILURE!
2022-08-19T13:35:56.1883817Z Aug 19 13:35:56 org.opentest4j.AssertionFailedError: 
2022-08-19T13:35:56.1884543Z Aug 19 13:35:56 
2022-08-19T13:35:56.1890435Z Aug 19 13:35:56 expected: TableStats{rowCount=3, colStats={f_boolean=ColumnStats(nullCount=1), f_smallint=ColumnStats(nullCount=0, max=128, min=100), f_decimal5=ColumnStats(nullCount=0, max=223.45, min=123.45), f_array=null, f_binary=null, f_decimal38=ColumnStats(nullCount=1, max=123433343334333433343334333433343334.34, min=123433343334333433343334333433343334.33), f_map=null, f_float=ColumnStats(nullCount=1, max=33.33300018310547, min=33.31100082397461), f_row=null, f_tinyint=ColumnStats(nullCount=0, max=3, min=1), f_decimal14=ColumnStats(nullCount=0, max=123333333355.33, min=123333333333.33), f_date=ColumnStats(nullCount=0, max=1990-10-16, min=1990-10-14), f_bigint=ColumnStats(nullCount=0, max=1238123899121, min=1238123899000), f_timestamp3=ColumnStats(nullCount=0, max=1990-10-16 12:12:43.123, min=1990-10-14 12:12:43.123), f_double=ColumnStats(nullCount=0, max=10.1, min=1.1), f_string=ColumnStats(nullCount=0, max=def, min=abcd), f_int=ColumnStats(nullCount=1, max=45536, min=31000)}}
2022-08-19T13:35:56.1902811Z Aug 19 13:35:56  but was: TableStats{rowCount=3, colStats={f_boolean=ColumnStats(nullCount=1), f_smallint=ColumnStats(nullCount=0, max=128, min=100), f_decimal5=ColumnStats(nullCount=0, max=223.45, min=0), f_array=null, f_binary=null, f_decimal38=ColumnStats(nullCount=1, max=123433343334333433343334333433343334.34, min=123433343334333433343334333433343334.33), f_map=null, f_float=ColumnStats(nullCount=1, max=33.33300018310547, min=33.31100082397461), f_row=null, f_tinyint=ColumnStats(nullCount=0, max=3, min=1), f_decimal14=ColumnStats(nullCount=0, max=123333333355.33, min=0), f_date=ColumnStats(nullCount=0, max=1990-10-16, min=1990-10-14), f_bigint=ColumnStats(nullCount=0, max=1238123899121, min=1238123899000), f_timestamp3=ColumnStats(nullCount=0, max=1990-10-16 12:12:43.123, min=1990-10-14 12:12:43.123), f_double=ColumnStats(nullCount=0, max=10.1, min=1.1), f_string=ColumnStats(nullCount=0, max=def, min=abcd), f_int=ColumnStats(nullCount=1, max=45536, min=31000)}}
2022-08-19T13:35:56.1908634Z Aug 19 13:35:56 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-19T13:35:56.1910402Z Aug 19 13:35:56 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-19T13:35:56.1912266Z Aug 19 13:35:56 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-19T13:35:56.1913257Z Aug 19 13:35:56 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.assertHiveTableOrcFormatTableStatsEquals(HiveTableSourceStatisticsReportTest.java:339)
2022-08-19T13:35:56.1914512Z Aug 19 13:35:56 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport(HiveTableSourceStatisticsReportTest.java:118)
2022-08-19T13:35:56.1915444Z Aug 19 13:35:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-19T13:35:56.1916130Z Aug 19 13:35:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-19T13:35:56.1916856Z Aug 19 13:35:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-19T13:35:56.1917571Z Aug 19 13:35:56 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-19T13:35:56.1918278Z Aug 19 13:35:56 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-19T13:35:56.1919020Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-19T13:35:56.1919923Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-19T13:35:56.1920841Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-19T13:35:56.1921877Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-19T13:35:56.1922778Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-08-19T13:35:56.1923726Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-19T13:35:56.1924761Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-19T13:35:56.1925690Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-19T13:35:56.1926590Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-19T13:35:56.1927507Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-19T13:35:56.1928422Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-19T13:35:56.1929216Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-19T13:35:56.1930018Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-19T13:35:56.1930866Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-19T13:35:56.1931868Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1932794Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-19T13:35:56.1933757Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-19T13:35:56.1934645Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-19T13:35:56.1935581Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-19T13:35:56.1936483Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1937381Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1938153Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1938980Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1939899Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1940713Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1941642Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1942726Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1943944Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-19T13:35:56.1945074Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-19T13:35:56.1946207Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-19T13:35:56.1947104Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1947941Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1948776Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1949613Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1950509Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1951326Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1952371Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1953430Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1954545Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-19T13:35:56.1955575Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-19T13:35:56.1956466Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1957359Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1958137Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1959059Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1959962Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1960783Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1961688Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1962766Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1963674Z Aug 19 13:35:56 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-19T13:35:56.1964372Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-19T13:35:56.1965093Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-19T13:35:56.1965758Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-19T13:35:56.1966500Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40205&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461",2022-08-19T14:01:31.045+0000,2022-08-26T08:26:31.991+0000,Fixed,Critical
PHOENIX-4806,make PhoenixStorageHander using only unmanaged tables,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12538139'>],"For managed table Hive is heavily using internal statistic which is not updated properly when data is changing from outside. So all custom storage handlers are moving to the schema when only unmanaged (external) tables are using.
The suggested lifecycle:
1) All tables should be created with external keyword
2) if Phoenix table doesn't exist, we create a new one and add a flag that it should be deleted on drop table from hive
3) If Phoenix table exists, no changes in the existing behavior.

Some of the storage handlers that are part of Hive distribution are already using this schema. Others are in progress.

FYI [~elserj]",2018-07-09T19:10:31.384+0000,2018-07-09T19:59:31.358+0000,,Major
FLINK-4142,Recovery problem in HA on Hadoop Yarn 2.4.1,FLINK,Bug,Closed,[],1,[<JIRA IssueLink: id='12475029'>],"On Hadoop Yarn 2.4.1, recovery in HA fails in the following scenario:

1) Kill application master, let it recover normally.
2) After that, kill a task manager.

Now, Yarn tries to restart the killed task manager in an endless loop. ",2016-07-01T14:29:25.860+0000,2016-12-19T12:46:57.443+0000,Fixed,Major
AVRO-647,"Break avro.jar into avro.jar, avro-dev.jar and avro-hadoop.jar",AVRO,Improvement,Closed,"[<JIRA Issue: key='AVRO-545', id='12464841'>, <JIRA Issue: key='AVRO-159', id='12438616'>]",5,"[<JIRA IssueLink: id='12333851'>, <JIRA IssueLink: id='12336579'>, <JIRA IssueLink: id='12336627'>, <JIRA IssueLink: id='12334539'>, <JIRA IssueLink: id='12336431'>]","Our dependencies are starting to get a little complicated on the Java side.

I propose we build two (possibly more) jars related to our major dependencies and functions.

1. avro.jar  (or perhaps avro-core.jar)
This contains all of the core avro functionality for _using_ avro as a library.  This excludes the specific compiler, avro idl, and other build-time or development tools, as well as avro packages for third party integration such as hadoop.  This jar should then have a minimal set of dependencies (jackson, jetty, SLF4J ?).

2. avro-dev.jar
This would contain compilers, idl, development tools, etc.  Most applications will not need this, but build systems and developers will.

3. avro-hadoop.jar
This would contain the hadoop API and possibly pig/hive/whatever related to that.  This makes it easier for pig/hive/hadoop to consume avro-core without circular dependencies. 
",2010-09-01T01:25:04.062+0000,2011-03-12T00:32:51.952+0000,Fixed,Major
SPARK-4687,SparkContext#addFile doesn't keep file folder information,SPARK,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12409698'>, <JIRA IssueLink: id='12402604'>, <JIRA IssueLink: id='12402622'>]","Files added with SparkContext#addFile are loaded with Utils#fetchFile before a task starts. However, Utils#fetchFile puts all files under the Spart root on the worker node. We should have an option to keep the folder information. ",2014-12-02T01:11:13.051+0000,2015-03-04T20:43:16.326+0000,Fixed,Major
ORC-15,Add floating point compression to ORC file,ORC,New Feature,Open,[],2,"[<JIRA IssueLink: id='12518430'>, <JIRA IssueLink: id='12363028'>]","Karol Wegrzycki, a CS student at University of Warsaw, has implemented an FPC compressor for doubles. It would be great to hook this up to the ORC file format so that we can get better compression for doubles.",2013-01-11T16:21:06.677+0000,2017-10-25T02:45:05.925+0000,,Major
PHOENIX-6385,Not to use Scan#setSmall for HBase 2.x versions,PHOENIX,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12610540'>],"{code:java}
/**
   * Set whether this scan is a small scan
   * <p>
   * Small scan should use pread and big scan can use seek + read seek + read is fast but can cause
   * two problem (1) resource contention (2) cause too much network io [89-fb] Using pread for
   * non-compaction read request https://issues.apache.org/jira/browse/HBASE-7266 On the other hand,
   * if setting it true, we would do openScanner,next,closeScanner in one RPC call. It means the
   * better performance for small scan. [HBASE-9488]. Generally, if the scan range is within one
   * data block(64KB), it could be considered as a small scan.
   * @param small
   * @deprecated since 2.0.0 and will be removed in 3.0.0. Use {@link #setLimit(int)} and
   *   {@link #setReadType(ReadType)} instead. And for the one rpc optimization, now we will also
   *   fetch data when openScanner, and if the number of rows reaches the limit then we will close
   *   the scanner automatically which means we will fall back to one rpc.
   * @see #setLimit(int)
   * @see #setReadType(ReadType)
   * @see <a href=""https://issues.apache.org/jira/browse/HBASE-17045"">HBASE-17045</a>
   */
  @Deprecated
  public Scan setSmall(boolean small) 
{code}
In 1.x there is seperate ClientSmallScanner impl doing single RPC for scan.  In 2.x by default it handles single RPC if there are less number of rows.    We might have to setLimit.

Also in 2.x setSmall() API sets the readType to be PRead.   But this is good for small reads like only one hfile blocks get read.  But what I noticed in phoenix is that even count(*) query result in setting Scan as small scan and end up using PRead (This will result in full scan).
So we should not be setting the ReadType at all.  Instead rely on HBase to intelligently switch scan type (start with PRead and if it scans more data switch to stream read)",2021-02-16T10:46:18.279+0000,2021-03-12T14:36:51.160+0000,Fixed,Major
INFRA-22760,Jenkins nodes for HBase,INFRA,Planned Work,Closed,[],1,[<JIRA IssueLink: id='12631797'>],"A new Jenkins Client Controller is online at ci-hbase.apache.org. 
Add some new nodes based out of Hetzner.",2022-01-18T07:54:27.962+0000,2022-02-18T16:20:17.282+0000,Fixed,Major
CALCITE-4704,Log produced plan after rule application using explain formatting,CALCITE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12631712'>, <JIRA IssueLink: id='12628781'>]","In many cases, we want to identify which rule lead to a certain transformation in the plan or need to observe how the query plan evolves by applying some rules in order to fix some bug or find the right place to introduce another optimization step.

Currently there are some logs during the application of a rule triggered by the [HepPlanner|https://github.com/apache/calcite/blob/e04f3b08dcfb6910ff4df3810772c346b25ed424/core/src/main/java/org/apache/calcite/plan/AbstractRelOptPlanner.java#L367] and [VolcanoPlanner|https://github.com/apache/calcite/blob/e04f3b08dcfb6910ff4df3810772c346b25ed424/core/src/main/java/org/apache/calcite/plan/volcano/VolcanoRuleCall.java#L126] but they more or less display only the top operator of the transformation and not the whole subtree. Moreover, the {{RelNode#toString}} used in these logs, is not self-contained, so in order to check the transformation the log entry needs to be matched with other logs.

It would help if instead of displaying only the top operator we logged the equivalent of explain (i.e., {{RelOptUtil.toString}}) on the transformed sub-tree. 

You can find below some extracts from the current logs and how the proposed log could look like. 

*HepPlanner*
 +Current logging+
{noformat}
2021-07-27 14:37:58,252 [ForkJoinPool-1-worker-9] DEBUG - call#0: Rule FilterIntoJoinRule arguments [rel#9:LogicalFilter.NONE.[](input=HepRelVertex#8,condition==($7, $8)), rel#7:LogicalJoin.NONE.[](left=HepRelVertex#5,right=HepRelVertex#6,condition=true,joinType=left)] produced rel#14:LogicalProject.NONE.[](input=LogicalJoin#13,inputs=0..7,exprs=[CAST($8):TINYINT, $9, $10])
2021-07-27 14:37:58,266 [ForkJoinPool-1-worker-9] DEBUG - call#1: Rule ProjectJoinTransposeRule arguments [rel#16:LogicalProject.NONE.[](input=HepRelVertex#15,inputs=0..7,exprs=[CAST($8):TINYINT, $9, $10]), rel#13:LogicalJoin.NONE.[](left=HepRelVertex#5,right=HepRelVertex#6,condition==($7, $8),joinType=inner)] produced rel#21:LogicalProject.NONE.[](input=LogicalJoin#20,inputs=0..7,exprs=[$11, $9, $10])
2021-07-27 14:37:58,269 [ForkJoinPool-1-worker-9] DEBUG - call#3: Rule ProjectMergeRule arguments [rel#11:LogicalProject.NONE.[](input=HepRelVertex#27,exprs=[$1]), rel#26:LogicalProject.NONE.[](input=HepRelVertex#25,inputs=0..7,exprs=[$11, $9, $10])] produced rel#28:LogicalProject.NONE.[](input=HepRelVertex#25,exprs=[$1])
2021-07-27 14:37:58,272 [ForkJoinPool-1-worker-9] DEBUG - call#4: Rule ProjectJoinTransposeRule arguments [rel#28:LogicalProject.NONE.[](input=HepRelVertex#25,exprs=[$1]), rel#24:LogicalJoin.NONE.[](left=HepRelVertex#22,right=HepRelVertex#23,condition==($7, $8),joinType=inner)] produced rel#33:LogicalProject.NONE.[](input=LogicalJoin#32,inputs=0)
2021-07-27 14:37:58,274 [ForkJoinPool-1-worker-9] DEBUG - call#6: Rule ProjectMergeRule arguments [rel#30:LogicalProject.NONE.[](input=HepRelVertex#22,exprs=[$1, $7]), rel#18:LogicalProject.NONE.[0](input=HepRelVertex#5,inputs=0..7)] produced rel#40:LogicalProject.NONE.[](input=HepRelVertex#5,exprs=[$1, $7])
2021-07-27 14:37:58,275 [ForkJoinPool-1-worker-9] DEBUG - call#7: Rule ProjectMergeRule arguments [rel#31:LogicalProject.NONE.[0](input=HepRelVertex#23,inputs=0), rel#19:LogicalProject.NONE.[[0], [3]](input=HepRelVertex#6,inputs=0..2,exprs=[CAST($0):TINYINT])] produced rel#42:LogicalProject.NONE.[0](input=HepRelVertex#6,inputs=0)
{noformat}
+Proposed logging+
{noformat}
2021-07-27 14:37:58,260 [ForkJoinPool-1-worker-9] DEBUG - Rule FilterIntoJoinRule produced:
 LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], DEPTNO0=[CAST($8):TINYINT], DNAME=[$9], LOC=[$10])
  LogicalJoin(condition=[=($7, $8)], joinType=[inner])
    LogicalTableScan(table=[[scott, EMP]])
    LogicalTableScan(table=[[scott, DEPT]])
2021-07-27 14:37:58,267 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectJoinTransposeRule produced:
 LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], DEPTNO0=[$11], DNAME=[$9], LOC=[$10])
  LogicalJoin(condition=[=($7, $8)], joinType=[inner])
    LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7])
      LogicalTableScan(table=[[scott, EMP]])
    LogicalProject(DEPTNO=[$0], DNAME=[$1], LOC=[$2], EXPR$0=[CAST($0):TINYINT])
      LogicalTableScan(table=[[scott, DEPT]])
2021-07-27 14:37:58,270 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(ENAME=[$1])
  LogicalJoin(condition=[=($7, $8)], joinType=[inner])
    LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7])
      LogicalTableScan(table=[[scott, EMP]])
    LogicalProject(DEPTNO=[$0], DNAME=[$1], LOC=[$2], EXPR$0=[CAST($0):TINYINT])
      LogicalTableScan(table=[[scott, DEPT]])
2021-07-27 14:37:58,272 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectJoinTransposeRule produced:
 LogicalProject(ENAME=[$0])
  LogicalJoin(condition=[=($1, $2)], joinType=[inner])
    LogicalProject(ENAME=[$1], DEPTNO=[$7])
      LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7])
        LogicalTableScan(table=[[scott, EMP]])
    LogicalProject(DEPTNO=[$0])
      LogicalProject(DEPTNO=[$0], DNAME=[$1], LOC=[$2], EXPR$0=[CAST($0):TINYINT])
        LogicalTableScan(table=[[scott, DEPT]])
2021-07-27 14:37:58,274 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(ENAME=[$1], DEPTNO=[$7])
  LogicalTableScan(table=[[scott, EMP]])
2021-07-27 14:37:58,275 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(DEPTNO=[$0])
  LogicalTableScan(table=[[scott, DEPT]])
{noformat}
*VolcanoPlanner*
 +Current logging+
{noformat}
2021-07-27 14:48:15,846 [ForkJoinPool-1-worker-9] DEBUG - call#3: Apply rule [FilterIntoJoinRule] to [rel#9:LogicalFilter.NONE.[](input=RelSubset#8,condition==($7, $8)), rel#7:LogicalJoin.NONE.[](left=RelSubset#5,right=RelSubset#6,condition=true,joinType=left)]
2021-07-27 14:48:15,850 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#14 via FilterIntoJoinRule
2021-07-27 14:48:15,851 [ForkJoinPool-1-worker-9] TRACE - call#3: Rule FilterIntoJoinRule arguments [rel#9:LogicalFilter.NONE.[](input=RelSubset#8,condition==($7, $8)), rel#7:LogicalJoin.NONE.[](left=RelSubset#5,right=RelSubset#6,condition=true,joinType=left)] created rel#14:LogicalProject
2021-07-27 14:48:15,858 [ForkJoinPool-1-worker-9] DEBUG - call#10: Apply rule [ProjectJoinTransposeRule] to [rel#16:LogicalProject.NONE.[](input=RelSubset#15,inputs=0..7,exprs=[CAST($8):TINYINT, $9, $10]), rel#13:LogicalJoin.NONE.[](left=RelSubset#5,right=RelSubset#6,condition==($7, $8),joinType=inner)]
2021-07-27 14:48:15,863 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#20 via ProjectJoinTransposeRule
2021-07-27 14:48:15,863 [ForkJoinPool-1-worker-9] TRACE - call#10: Rule ProjectJoinTransposeRule arguments [rel#16:LogicalProject.NONE.[](input=RelSubset#15,inputs=0..7,exprs=[CAST($8):TINYINT, $9, $10]), rel#13:LogicalJoin.NONE.[](left=RelSubset#5,right=RelSubset#6,condition==($7, $8),joinType=inner)] created rel#20:LogicalProject
2021-07-27 14:48:15,873 [ForkJoinPool-1-worker-9] DEBUG - call#13: Apply rule [ProjectMergeRule] to [rel#11:LogicalProject.NONE.[](input=RelSubset#10,exprs=[$1]), rel#16:LogicalProject.NONE.[](input=RelSubset#15,inputs=0..7,exprs=[CAST($8):TINYINT, $9, $10])]
2021-07-27 14:48:15,874 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#26 via ProjectMergeRule
2021-07-27 14:48:15,874 [ForkJoinPool-1-worker-9] TRACE - call#13: Rule ProjectMergeRule arguments [rel#11:LogicalProject.NONE.[](input=RelSubset#10,exprs=[$1]), rel#16:LogicalProject.NONE.[](input=RelSubset#15,inputs=0..7,exprs=[CAST($8):TINYINT, $9, $10])] created rel#26:LogicalProject
2021-07-27 14:48:15,884 [ForkJoinPool-1-worker-9] DEBUG - call#26: Apply rule [ProjectMergeRule] to [rel#11:LogicalProject.NONE.[](input=RelSubset#10,exprs=[$1]), rel#25:LogicalProject.NONE.[](input=RelSubset#24,inputs=0..7,exprs=[$11, $9, $10])]
2021-07-27 14:48:15,885 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#27 via ProjectMergeRule
2021-07-27 14:48:15,885 [ForkJoinPool-1-worker-9] TRACE - call#26: Rule ProjectMergeRule arguments [rel#11:LogicalProject.NONE.[](input=RelSubset#10,exprs=[$1]), rel#25:LogicalProject.NONE.[](input=RelSubset#24,inputs=0..7,exprs=[$11, $9, $10])] created rel#27:LogicalProject
2021-07-27 14:48:15,889 [ForkJoinPool-1-worker-9] DEBUG - call#28: Apply rule [ProjectJoinTransposeRule] to [rel#26:LogicalProject.NONE.[](input=RelSubset#15,exprs=[$1]), rel#13:LogicalJoin.NONE.[](left=RelSubset#5,right=RelSubset#6,condition==($7, $8),joinType=inner)]
2021-07-27 14:48:15,890 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#31 via ProjectJoinTransposeRule
2021-07-27 14:48:15,890 [ForkJoinPool-1-worker-9] TRACE - call#28: Rule ProjectJoinTransposeRule arguments [rel#26:LogicalProject.NONE.[](input=RelSubset#15,exprs=[$1]), rel#13:LogicalJoin.NONE.[](left=RelSubset#5,right=RelSubset#6,condition==($7, $8),joinType=inner)] created rel#31:LogicalProject
2021-07-27 14:48:15,897 [ForkJoinPool-1-worker-9] DEBUG - call#32: Apply rule [ProjectJoinTransposeRule] to [rel#27:LogicalProject.NONE.[](input=RelSubset#24,exprs=[$1]), rel#23:LogicalJoin.NONE.[](left=RelSubset#21,right=RelSubset#22,condition==($7, $8),joinType=inner)]
2021-07-27 14:48:15,898 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#40 via ProjectJoinTransposeRule
2021-07-27 14:48:15,898 [ForkJoinPool-1-worker-9] TRACE - call#32: Rule ProjectJoinTransposeRule arguments [rel#27:LogicalProject.NONE.[](input=RelSubset#24,exprs=[$1]), rel#23:LogicalJoin.NONE.[](left=RelSubset#21,right=RelSubset#22,condition==($7, $8),joinType=inner)] created rel#40:LogicalProject
2021-07-27 14:48:15,909 [ForkJoinPool-1-worker-9] DEBUG - call#49: Apply rule [ProjectMergeRule] to [rel#37:LogicalProject.NONE.[](input=RelSubset#21,exprs=[$1, $7]), rel#17:LogicalProject.NONE.[0](input=RelSubset#5,inputs=0..7)]
2021-07-27 14:48:15,909 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#46 via ProjectMergeRule
2021-07-27 14:48:15,909 [ForkJoinPool-1-worker-9] TRACE - call#49: Rule ProjectMergeRule arguments [rel#37:LogicalProject.NONE.[](input=RelSubset#21,exprs=[$1, $7]), rel#17:LogicalProject.NONE.[0](input=RelSubset#5,inputs=0..7)] created rel#46:LogicalProject
2021-07-27 14:48:15,914 [ForkJoinPool-1-worker-9] DEBUG - call#53: Apply rule [ProjectMergeRule] to [rel#38:LogicalProject.NONE.[](input=RelSubset#22,inputs=0), rel#18:LogicalProject.NONE.[[0], [3]](input=RelSubset#6,inputs=0..2,exprs=[CAST($0):TINYINT])]
2021-07-27 14:48:15,915 [ForkJoinPool-1-worker-9] DEBUG - Transform to: rel#47 via ProjectMergeRule
2021-07-27 14:48:15,915 [ForkJoinPool-1-worker-9] TRACE - call#53: Rule ProjectMergeRule arguments [rel#38:LogicalProject.NONE.[](input=RelSubset#22,inputs=0), rel#18:LogicalProject.NONE.[[0], [3]](input=RelSubset#6,inputs=0..2,exprs=[CAST($0):TINYINT])] created rel#47:LogicalProject
{noformat}
+Proposed logging+
{noformat}
2021-07-27 14:48:15,854 [ForkJoinPool-1-worker-9] DEBUG - Rule FilterIntoJoinRule produced:
 LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], DEPTNO0=[CAST($8):TINYINT], DNAME=[$9], LOC=[$10])
  LogicalJoin(condition=[=($7, $8)], joinType=[inner])
    LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
    LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
2021-07-27 14:48:15,867 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectJoinTransposeRule produced:
 LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], DEPTNO0=[$11], DNAME=[$9], LOC=[$10])
  LogicalJoin(condition=[=($7, $8)], joinType=[inner])
    LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7])
      LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
    LogicalProject(DEPTNO=[$0], DNAME=[$1], LOC=[$2], EXPR$0=[CAST($0):TINYINT])
      LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
2021-07-27 14:48:15,875 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(ENAME=[$1])
  LogicalJoin(subset=[rel#15:RelSubset#5.NONE.[]], condition=[=($7, $8)], joinType=[inner])
    LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
    LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
2021-07-27 14:48:15,886 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(ENAME=[$1])
  LogicalJoin(subset=[rel#24:RelSubset#8.NONE.[]], condition=[=($7, $8)], joinType=[inner])
    LogicalProject(subset=[rel#21:RelSubset#6.NONE.[0]], EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7])
      LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
    LogicalProject(subset=[rel#22:RelSubset#7.NONE.[]], DEPTNO=[$0], DNAME=[$1], LOC=[$2], EXPR$0=[CAST($0):TINYINT])
      LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
2021-07-27 14:48:15,893 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectJoinTransposeRule produced:
 LogicalProject(ENAME=[$0])
  LogicalJoin(condition=[=($1, $2)], joinType=[inner])
    LogicalProject(ENAME=[$1], DEPTNO=[$7])
      LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
    LogicalProject(DEPTNO=[$0])
      LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
2021-07-27 14:48:15,901 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectJoinTransposeRule produced:
 LogicalProject(ENAME=[$0])
  LogicalJoin(condition=[=($1, $2)], joinType=[inner])
    LogicalProject(ENAME=[$1], DEPTNO=[$7])
      LogicalProject(subset=[rel#21:RelSubset#6.NONE.[0]], EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7])
        LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
    LogicalProject(DEPTNO=[$0])
      LogicalProject(subset=[rel#22:RelSubset#7.NONE.[]], DEPTNO=[$0], DNAME=[$1], LOC=[$2], EXPR$0=[CAST($0):TINYINT])
        LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
2021-07-27 14:48:15,911 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(ENAME=[$1], DEPTNO=[$7])
  LogicalTableScan(subset=[rel#5:RelSubset#0.NONE.[0]], table=[[scott, EMP]])
2021-07-27 14:48:15,916 [ForkJoinPool-1-worker-9] DEBUG - Rule ProjectMergeRule produced:
 LogicalProject(DEPTNO=[$0])
  LogicalTableScan(subset=[rel#6:RelSubset#1.NONE.[0]], table=[[scott, DEPT]])
{noformat}",2021-07-27T12:01:10.690+0000,2022-01-21T10:46:41.320+0000,Fixed,Major
AMBARI-24301,API for HDFS Upgrade Domains management for scripted node additions,AMBARI,New Feature,Open,[],2,"[<JIRA IssueLink: id='12538870'>, <JIRA IssueLink: id='12538871'>]","HDFS Upgrade Domains is configured in a static JSON file.

Feature Request to add an Ambari API to be able to manage placement policy dynamically as the JSON file does not lend itself to scripted automation adding of datanodes. If needed, generate the JSON file (or otherwise press the HDFS developers to improve the design and then use their API).

[http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html]

 ",2018-07-18T09:35:46.814+0000,2018-07-19T10:26:38.920+0000,,Major
CRUNCH-534,Protobuf Size Limit Exception ,CRUNCH,Bug,Open,[],1,[<JIRA IssueLink: id='12430794'>],"With HBase's switch to ProtocolBuffers there is now an imposed restriction on the size of data (64MB) which can be stored in protocol buffers and therefore Cells/Puts/etc.  

{quote}
Call to  failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
{quote}

While a single cell being that big seems extreme this limit also counts towards Result objects so an entire row.

It is definitely an HBase problem[1] but Crunch also might have changes needed since we use protobuf serialization in our HBaseTypes.[2]  So might need to make sure error isn't only on our side as they work through the issue or might need to look at MOB support[3].

[1] - https://issues.apache.org/jira/browse/HBASE-13825
[2] - https://github.com/apache/crunch/blob/d176778cf803374506cb7743069a05e28e07e2cf/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HBaseTypes.java#L33
[3] - https://issues.apache.org/jira/browse/HBASE-11339",2015-07-02T02:21:19.542+0000,2015-07-14T18:41:23.929+0000,,Major
TEZ-1173,Need per output counters,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12389159'>],"Tez currently collect per output counter by setting ""tez.task.generate.counters.per.io"" and expose through a special counter group. This is mostly for debugging purpose for now as [~sseth] told me. Pig need per output counters so we don't need to go to hdfs to collect those information. That will take from several hundred milliseconds to several seconds, which is completely unnecessary and add overhead especially for iterative queries. We want to make it a public API.",2014-06-02T21:00:38.929+0000,2014-06-02T21:04:10.883+0000,,Major
SPARK-32764,compare of -0.0 < 0.0 return true,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12598207'>, <JIRA IssueLink: id='12598136'>]","{code:scala}
 val spark: SparkSession = SparkSession
      .builder()
      .master(""local"")
      .appName(""SparkByExamples.com"")
      .getOrCreate()
    spark.sparkContext.setLogLevel(""ERROR"")

    import spark.sqlContext.implicits._

    val df = Seq((-0.0, 0.0)).toDF(""neg"", ""pos"")
      .withColumn(""comp"", col(""neg"") < col(""pos""))
      df.show(false)

======

+----+---+----+
|neg |pos|comp|
+----+---+----+
|-0.0|0.0|true|
+----+---+----+{code}

I think that result should be false.

**Apache Spark 2.4.6 RESULT**
{code}
scala> spark.version
res0: String = 2.4.6

scala> Seq((-0.0, 0.0)).toDF(""neg"", ""pos"").withColumn(""comp"", col(""neg"") < col(""pos"")).show
+----+---+-----+
| neg|pos| comp|
+----+---+-----+
|-0.0|0.0|false|
+----+---+-----+
{code}",2020-09-01T10:30:30.557+0000,2020-09-08T03:46:03.074+0000,Fixed,Major
SPARK-14958,Failed task hangs if error is encountered when getting task result,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12465049'>],"In {{TaskResultGetter}}, if we get an error when deserialize {{TaskEndReason}}, TaskScheduler won't have a chance to handle the failed task and the task just hangs.
{code}
  def enqueueFailedTask(taskSetManager: TaskSetManager, tid: Long, taskState: TaskState,
    serializedData: ByteBuffer) {
    var reason : TaskEndReason = UnknownReason
    try {
      getTaskResultExecutor.execute(new Runnable {
        override def run(): Unit = Utils.logUncaughtExceptions {
          val loader = Utils.getContextOrSparkClassLoader
          try {
            if (serializedData != null && serializedData.limit() > 0) {
              reason = serializer.get().deserialize[TaskEndReason](
                serializedData, loader)
            }
          } catch {
            case cnd: ClassNotFoundException =>
              // Log an error but keep going here -- the task failed, so not catastrophic
              // if we can't deserialize the reason.
              logError(
                ""Could not deserialize TaskEndReason: ClassNotFound with classloader "" + loader)
            case ex: Exception => {}
          }
          scheduler.handleFailedTask(taskSetManager, tid, taskState, reason)
        }
      })
    } catch {
      case e: RejectedExecutionException if sparkEnv.isStopped =>
        // ignore it
    }
  }
{code}
In my specific case, I got a NoClassDefFoundError and the failed task hangs forever.",2016-04-27T13:54:49.552+0000,2017-01-05T22:55:25.659+0000,Fixed,Major
IMPALA-10437,Support SAML 2 browser profile authentication,IMPALA,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12606597'>, <JIRA IssueLink: id='12608341'>]",,2021-01-14T15:37:54.108+0000,2021-07-05T07:51:28.911+0000,Implemented,Major
HDDS-7128,Add ozone-client-minicluster module,HDDS,Improvement,Open,[],2,"[<JIRA IssueLink: id='12645742'>, <JIRA IssueLink: id='12645751'>]","As of now when at downstream(ex. Hive), when we try to add Ozone dependencies for the sake of running MiniOzoneCluster.

There are conflicts due to different versions of third-party libs like Guava.

Explore adding a shaded module for MiniOzoneCluster, which shades all such dependencies

[Exploratory]",2022-08-16T08:28:46.134+0000,2022-08-16T08:41:06.388+0000,,Major
OOZIE-2973,Make sure Oozie works with Hadoop 3 ,OOZIE,Bug,Open,[],10,"[<JIRA IssueLink: id='12569084'>, <JIRA IssueLink: id='12526483'>, <JIRA IssueLink: id='12526484'>, <JIRA IssueLink: id='12526485'>, <JIRA IssueLink: id='12521308'>, <JIRA IssueLink: id='12521297'>, <JIRA IssueLink: id='12545304'>, <JIRA IssueLink: id='12578307'>, <JIRA IssueLink: id='12507771'>, <JIRA IssueLink: id='12615374'>]","This JIRA is to track that Oozie works with Hadoop 3. 
- Build Oozie with latest hadoop 3 (e.g. assuming Apache Hadoop 3.0.0-alpha3 pass {{-D-Dhadoop.version=3.0.0-alpha3}} to maven) and execute tests
- It would be nice to create an Apache jenkins job that runs Oozie tests with hadoop 3 (also with other component) 

",2017-06-28T08:19:09.278+0000,2022-08-29T07:04:47.863+0000,,Blocker
ORC-166,add codec pool to ORC; make sure end is called on underlying codecs,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12498322'>],Subj,2017-03-17T20:42:52.171+0000,2017-05-08T17:22:38.703+0000,Fixed,Major
ZOOKEEPER-1159,ClientCnxn does not propagate session expiration indication,ZOOKEEPER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12342326'>],"ClientCnxn does not always propagate session expiration indication up to clients. If a reconnection attempt fails because the session has since expired, the KeeperCode is still Disconnected, but shouldn't it be set to Expired? Perhaps like so:

{code}
--- a/src/java/main/org/apache/zookeeper/ClientCnxn.java
+++ b/src/java/main/org/apache/zookeeper/ClientCnxn.java
@@ -1160,6 +1160,7 @@ public class ClientCnxn {
                     clientCnxnSocket.doTransport(to, pendingQueue, outgoingQueue);
 
                 } catch (Exception e) {
+                    Event.KeeperState eventState = Event.KeeperState.Disconnected;
                     if (closing) {
                         if (LOG.isDebugEnabled()) {
                             // closing so this is expected
@@ -1172,6 +1173,7 @@ public class ClientCnxn {
                         // this is ugly, you have a better way speak up
                         if (e instanceof SessionExpiredException) {
                             LOG.info(e.getMessage() + "", closing socket connection"");
+                            eventState = Event.KeeperState.Expired;
                         } else if (e instanceof SessionTimeoutException) {
                             LOG.info(e.getMessage() + RETRY_CONN_MSG);
                         } else if (e instanceof EndOfStreamException) {
@@ -1191,7 +1193,7 @@ public class ClientCnxn {
                         if (state.isAlive()) {
                             eventThread.queueEvent(new WatchedEvent(
                                     Event.EventType.None,
-                                    Event.KeeperState.Disconnected,
+                                    eventState,
                                     null));
                         }
                         clientCnxnSocket.updateNow();
{code}

This affects HBase. HBase master and region server processes will shut down by design if their session has expired, but will attempt to reconnect if they think they have been disconnected. The above prevents proper termination.",2011-08-20T17:16:28.885+0000,2018-05-08T20:43:15.031+0000,Won't Fix,Major
IMPALA-2019,Proper UTF-8 support in string functions,IMPALA,New Feature,Resolved,[],12,"[<JIRA IssueLink: id='12542240'>, <JIRA IssueLink: id='12605752'>, <JIRA IssueLink: id='12575328'>, <JIRA IssueLink: id='12597924'>, <JIRA IssueLink: id='12587182'>, <JIRA IssueLink: id='12597922'>, <JIRA IssueLink: id='12522151'>, <JIRA IssueLink: id='12597921'>, <JIRA IssueLink: id='12587514'>, <JIRA IssueLink: id='12619411'>, <JIRA IssueLink: id='12597926'>, <JIRA IssueLink: id='12646267'>]","As documented here: https://impala.apache.org/docs/build/html/topics/impala_string.html
Impala does not properly handle non-ASCII UTF-8 characters, and will return results in string functions such as length that are inconsistent with Hive.",2015-05-19T18:34:52.000+0000,2022-08-22T23:00:52.448+0000,Fixed,Critical
SPARK-23179,Support option to throw exception if overflow occurs during Decimal arithmetic,SPARK,Sub-task,Resolved,[],5,"[<JIRA IssueLink: id='12524868'>, <JIRA IssueLink: id='12524867'>, <JIRA IssueLink: id='12563768'>, <JIRA IssueLink: id='12564327'>, <JIRA IssueLink: id='12564328'>]","SQL ANSI 2011 states that in case of overflow during arithmetic operations, an exception should be thrown. This is what most of the SQL DBs do (eg. SQLServer, DB2). Hive currently returns NULL (as Spark does) but HIVE-18291 is open to be SQL compliant.

I propose to have a config option which allows to decide whether Spark should behave according to SQL standards or in the current way (ie. returning NULL).",2018-01-22T14:31:40.784+0000,2019-12-29T09:23:29.832+0000,Fixed,Major
PHOENIX-5161,ChangePermissionsIT is failing in CDH6 branch,PHOENIX,Task,Resolved,[],2,"[<JIRA IssueLink: id='12563253'>, <JIRA IssueLink: id='12554907'>]","{{ChangePermissionsIT.testReadPermsOnTableIndexAndView}} is failing in cdh6 branch - it keeps looping trying to shutdown minicluster 
",2019-02-25T23:26:18.812+0000,2022-01-31T06:06:08.948+0000,Won't Do,Major
HCATALOG-347,Pig loads all partitions when it is specifically told not to load all,HCATALOG,Bug,Closed,[],3,"[<JIRA IssueLink: id='12350607'>, <JIRA IssueLink: id='12350840'>, <JIRA IssueLink: id='12350844'>]","I have Pig script of this nature. It accesses a partitioned table, partitioned on gridname and dt (datestamp)
{code}
A = LOAD 'mytable' USING org.apache.hcatalog.pig.HCatLoader();
B = FILTER A BY gridname=='XY' and dt != '2012_03_21';
C = foreach B generate job_id, user;
store C into '/user/viraj/test/XY' using PigStorage();
{code}

I use this as some partitions of the table have not been populated.

I get an error:
Backend error message during job submission
{quote}
-------------------------------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Input path does not exist: hdfs://namenode/warehouse/database_confs/gridname=XY/dt=2012_03_21
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:282)
        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:962)
        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:979)
        at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1082)
{quote}

I suspect that the filter clause is not pushed up.
Regards
Viraj",2012-03-26T06:11:52.594+0000,2013-05-02T02:29:52.130+0000,Fixed,Major
HCATALOG-413,HBaseHCatStorageHandler fails to abort a transaction when used via HCatLoader,HCATALOG,Bug,Open,[],1,[<JIRA IssueLink: id='12352501'>],,2012-05-21T04:53:55.559+0000,2012-05-21T04:56:12.820+0000,,Major
REEF-35,Include vCore-checking in the YARN Container matching algorithm,REEF,Bug,Open,[],1,[<JIRA IssueLink: id='12401047'>],"This issue has been discovered while working on REEF-6.

As a workaround for YARN-2380, the part that examines vCores was removed from the matching algorithm

But this can result in allocating less vCores than requested. This happens when not using the default CapacityScheduler and also when certain race conditions are met.

Once YARN-2380 is resolved, we should be able to do vCore matching again.",2014-11-11T07:13:33.555+0000,2014-11-12T01:01:23.471+0000,,Major
ZOOKEEPER-431,Expose methods to ease ZK integration,ZOOKEEPER,Improvement,Closed,[],1,[<JIRA IssueLink: id='12325110'>],HBase committer here. I opened this jira to see if it's possible to include some changes in the way quorum servers are started in order to ease the integration with HBase. Basically the major things we would change is how methods are exposed in QPM/QPC and such.,2009-06-03T20:05:17.760+0000,2009-07-08T20:24:05.269+0000,Fixed,Major
TEZ-4073,Configuration: Reduce Vertex and DAG Payload Size,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12562160'>],"As the total number of vertices go up, the Tez protobuf transport starts to show up as a potential scalability problem for the task submission and the AM

{code}
public TezTaskRunner2(Configuration tezConf, UserGroupInformation ugi, String[] localDirs,
 ...
    this.taskConf = new Configuration(tezConf);
    if (taskSpec.getTaskConf() != null) {
      Iterator<Entry<String, String>> iter = taskSpec.getTaskConf().iterator();
      while (iter.hasNext()) {
        Entry<String, String> entry = iter.next();
        taskConf.set(entry.getKey(), entry.getValue());
      }
    }
{code}

The TaskSpec getTaskConf() need not include any of the default configs, since the keys are placed into an existing task conf.

{code}
    // Security framework already loaded the tokens into current ugi
    DAGProtos.ConfigurationProto confProto =
        TezUtilsInternal.readUserSpecifiedTezConfiguration(System.getenv(Environment.PWD.name()));
    TezUtilsInternal.addUserSpecifiedTezConfiguration(defaultConf, confProto.getConfKeyValuesList());
    UserGroupInformation.setConfiguration(defaultConf);
    Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();
{code}

At the very least, the DAG and Vertex do not both need to have the same configs repeated in them.

 !tez-protobuf-writing.png! 
+
 !tez-am-protobuf-reading.png! ",2019-05-30T06:01:29.878+0000,2019-10-17T20:31:02.823+0000,,Major
ORC-867,Upgrade hive-storage-api to 2.8.1,ORC,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12620160'>, <JIRA IssueLink: id='12620226'>, <JIRA IssueLink: id='12621137'>]",`hive-storage-api` has a dependency regression which `guava` dependency becomes a compile dependency. This will be fixed at 2.8.1.,2021-07-26T05:17:11.524+0000,2021-09-21T20:45:44.972+0000,Fixed,Major
FLINK-12532,Upgrade Avro to version 1.10.0,FLINK,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12580902'>, <JIRA IssueLink: id='12580811'>, <JIRA IssueLink: id='12599039'>]","Avro >= 1.9.x was released with many nice features including reduced size (1MB less), and removed dependencies, no paranmer, no shaded guava, security updates, so probably a worth upgrade.

There is at the moment (2020/08) still a blocker because of Hive related transitive dependencies bringing older versions of Avro, so we could say that this is somehow still blocked until HIVE-21737 is solved.",2019-05-16T10:12:29.868+0000,2020-12-03T16:00:30.875+0000,Implemented,Minor
CURATOR-443,Sleep too long when connection to zookeeper has not been established yet after construction,CURATOR,Bug,Resolved,[],1,[<JIRA IssueLink: id='12520631'>],"Recently we purge the complicated RecoverableZooKeeper dependency in hbase-client and use curator to get data from zookeeper.

But when debugging HBASE-19266, we found that if we get data immediately after the creation of CuratorFramework, the request will always cost 100x ms to complete.

After digging, we found that there is a sleep in CuratorFrameworkImpl if the connection is not established yet

https://github.com/apache/curator/blob/6d36a4793b31cdacaf4bbf6554e05d68bc680001/curator-framework/src/main/java/org/apache/curator/framework/imps/CuratorFrameworkImpl.java#L943

This is really a bad news for us. We decide to make the async hbase client fully asynchronous. You can see this example, where we execute the request to hbase directly in the netty event loop thread.

https://github.com/apache/hbase/blob/master/hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/HttpProxyExample.java

So if there is a sleep in foreground then the event loop will be stuck for 1 second which will cause very bad performance impact...",2017-11-21T07:56:58.270+0000,2018-01-02T16:04:35.010+0000,Fixed,Major
PHOENIX-6748,Add support for Java 17,PHOENIX,Improvement,Open,[],3,"[<JIRA IssueLink: id='12644115'>, <JIRA IssueLink: id='12644114'>, <JIRA IssueLink: id='12644113'>]","We cannot even run the test suite on JDK 17.
Most of the problems are coming from HBase, so we can't do much until HBase solves this problem.",2022-07-19T06:51:13.979+0000,2022-07-19T07:09:06.122+0000,,Major
SLIDER-170,switch to YARN Rest API for App creation,SLIDER,Sub-task,Open,[],5,"[<JIRA IssueLink: id='12403689'>, <JIRA IssueLink: id='12403126'>, <JIRA IssueLink: id='12390386'>, <JIRA IssueLink: id='12390385'>, <JIRA IssueLink: id='12403121'>]","Once YARN implements a REST API for app submission, we should switch to it -so that we can deploy over long-haul links to Knox-managed clusters",2014-06-25T17:26:52.411+0000,2014-12-16T20:27:16.702+0000,,Major
SENTRY-474,"SHOW GRANT ROLE in Hive should return ""grant_time"" in human readable format",SENTRY,Bug,Open,[],2,"[<JIRA IssueLink: id='12403088'>, <JIRA IssueLink: id='12403546'>]","Currently, ""SHOW GRANT ROLE"" will return the 'grant_time' in microseconds since epoch. It would be nice if this were in human readable format.

Current output: 1411801585902000
Desired output: Sat, Sep 27 2014 00:06:25.902

",2014-09-28T06:16:16.933+0000,2014-12-15T01:26:42.536+0000,,Minor
KYLIN-750,Merge cube segments from HBase table,KYLIN,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12425983'>, <JIRA IssueLink: id='12425984'>]","With the new cubing algorithm, there is no intermediate cuboid files persisted, so when merge the cube segments, need read the data from HBase table directly.",2015-05-06T02:15:11.347+0000,2015-08-27T09:25:09.693+0000,Fixed,Major
PHOENIX-1501,Remove or replace all uses of InterfaceAudience.Private HBase APIs and classes,PHOENIX,Task,Open,"[<JIRA Issue: key='PHOENIX-1479', id='12757541'>, <JIRA Issue: key='PHOENIX-1502', id='12759502'>, <JIRA Issue: key='PHOENIX-1636', id='12772482'>, <JIRA Issue: key='PHOENIX-1637', id='12772483'>, <JIRA Issue: key='PHOENIX-1681', id='12777254'>, <JIRA Issue: key='PHOENIX-1638', id='12772489'>, <JIRA Issue: key='PHOENIX-1717', id='12780687'>, <JIRA Issue: key='PHOENIX-2747', id='12947381'>]",4,"[<JIRA IssueLink: id='12407284'>, <JIRA IssueLink: id='12407233'>, <JIRA IssueLink: id='12407238'>, <JIRA IssueLink: id='12402753'>]","Umbrella issue for removing or replacing all uses of InterfaceAudience.Private HBase APIs and classes.

We won't be in a sustainable position if we don't establish a support contract with HBase for use of its private internals - each major HBase release will trigger potentially significant Phoenix refactoring.

Let's approach it on a case by case basis. We can ask HBase to make private APIs and classes supported by promoting their audience to LimitedPrivate(PHOENIX). If that fails, we can negotiate a supportable interface and contribute or assist in the necessary refactoring. If that fails, we can redesign or implement internal analogues. If that fails, we can go back to the HBase community with a stronger argument for the necessity of a supportable interface and prevail.",2014-12-04T18:37:30.181+0000,2015-02-05T05:34:42.031+0000,,Major
OOZIE-2179,Use HDFS INotify to track HDFS data dependencies instead of polling,OOZIE,New Feature,Open,[],3,"[<JIRA IssueLink: id='12426684'>, <JIRA IssueLink: id='12427489'>, <JIRA IssueLink: id='12411297'>]","Instead of polling the NN every minute for Coordinators, we should look into using the new INotify feature in HDFS-6634.  It allows you to get a stream of events from HDFS.  Internally, it still uses a polling mechanism for now, but even so, it would likely be more efficient and less heavy-handed than what we're doing.

We'd probably still have to check if the directory exists when a coordinator action starts in case we missed the event, but while waiting for an HDFS dependency to be available, we can use INotify.

For HCat dependencies we still have a backup polling of 10 minutes in case a JMS message is missed or lost.  I don't think we'll need to do this for INotify because you can view past events as long as you keep track of the event ID.  For example, if you restart Oozie and we kept track of the last ID Oozie looked at, we could resume from there without losing anything.

The INotify stream is asynchronous, so we won't receive a notification immediately.  We should look into the guarantees of how long it can take for the notification to show up.  ",2015-03-20T20:06:58.621+0000,2015-06-09T23:38:15.470+0000,,Major
HCATALOG-209,Support for partition-range queries from HCat command-line.,HCATALOG,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12347036'>, <JIRA IssueLink: id='12347064'>, <JIRA IssueLink: id='12360589'>, <JIRA IssueLink: id='12350841'>]","The HCat command-line does not allow for partition ranged queries, at the moment (on 0.1/0.2).

E.g. 

hcat -e ""show partitions myth_table partition(dt < '2011_08_01')"" 
Hive history file=/tmp/mithunr/hive_job_log_mithunr_201201121954_35712967.txt
FAILED: Parse Error: line 1:43 mismatched input '<' expecting ) in show statement

This is basic and would be valuable.",2012-01-12T19:57:54.766+0000,2013-10-24T04:14:53.694+0000,Fixed,Major
CALCITE-1784,SqlTypeName doesn't have timestamp with time zone type,CALCITE,New Feature,Open,[],2,"[<JIRA IssueLink: id='12512116'>, <JIRA IssueLink: id='12503130'>]",JDK 8 has TIMESTAMP_WITH_TIMEZONE in java.sql.Types. But seems Calcite doesn't support this type yet.,2017-05-10T03:33:21.486+0000,2019-02-22T01:20:51.724+0000,,Major
TEZ-3821,Ability to fail fast tasks that write too much to local disk,TEZ,Bug,Open,[],2,"[<JIRA IssueLink: id='12512571'>, <JIRA IssueLink: id='12577976'>]","It would be nice to have a configurable limit such that any task that wrote data to the local filesystem beyond that limit would fail quickly rather than waiting for the disk to fill much later, impacting other jobs on the cluster.

This is essentially asking for the Tez version of MAPREDUCE-6489.",2017-08-21T21:58:11.803+0000,2020-01-10T01:42:11.156+0000,,Major
KNOX-728,Don't encode Jobhistory URLs,KNOX,Bug,Closed,[],1,[<JIRA IssueLink: id='12494520'>],"I think this problem is similar to KNOX-709. Going to the Jobhistory log page:
https://hdp-node1:8443/gateway/hadoop-staging/jobhistory/joblogs/hdp-node2:45454/container_e696_1469407938027_0072_01_000011/attempt_1469407938027_0072_m_000009_0/hadoop
{noformat}
16/07/26 10:21:37 ||f4b6aeec-289f-4f96-9a81-fcf6df6cf762|audit|JOBHISTORYUI|akulikov|||dispatch|uri|http://hdp-node1:19888/jobhistory/logs/hdp-node2%3A45454/container_e696_1469407938027_0072_01_000011/attempt_1469407938027_0072_m_000009_0/hadoop/?user.name=akulikov|unavailable|Request method: GET
16/07/26 10:21:37 ||f4b6aeec-289f-4f96-9a81-fcf6df6cf762|audit|JOBHISTORYUI|akulikov|||dispatch|uri|http://hdp-node1:19888/jobhistory/logs/hdp-node2%3A45454/container_e696_1469407938027_0072_01_000011/attempt_1469407938027_0072_m_000009_0/hadoop/?user.name=akulikov|success|Response status: 200
16/07/26 10:21:37 ||f4b6aeec-289f-4f96-9a81-fcf6df6cf762|audit|JOBHISTORYUI|akulikov|||access|uri|/gateway/hadoop-staging/jobhistory/joblogs/hdp-node2:45454/container_e696_1469407938027_0072_01_000011/attempt_1469407938027_0072_m_000009_0/hadoop|success|Response status: 200
{noformat}

results in the error:

{noformat}
Cannot get container logs. Invalid nodeId: hdp-node2%3A45454
{noformat}

Jobhistory can't handle encoded paths.

Knox 0.8.0 doesn't have this problem.",2016-07-26T07:46:07.357+0000,2017-03-01T22:12:49.897+0000,Won't Fix,Major
SENTRY-2035,Metrics should move to destination atomically,SENTRY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12519486'>],"As explained in HIVE-17953, we need to use atomic move to write temp metric file to a destination.",2017-11-07T18:09:05.423+0000,2017-11-09T00:01:28.893+0000,Fixed,Major
SPARK-35321,Spark 3.x can't talk to HMS 1.2.x and lower due to get_all_functions Thrift API missing,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12624867'>, <JIRA IssueLink: id='12615134'>]","https://issues.apache.org/jira/browse/HIVE-10319 introduced a new API {{get_all_functions}} which is only supported in Hive 1.3.0/2.0.0 and up. This is called when creating a new {{Hive}} object:
{code}
  private Hive(HiveConf c, boolean doRegisterAllFns) throws HiveException {
    conf = c;
    if (doRegisterAllFns) {
      registerAllFunctionsOnce();
    }
  }
{code}

{{registerAllFunctionsOnce}} will reload all the permanent functions by calling the {{get_all_functions}} API from the megastore. In Spark, we always pass {{doRegisterAllFns}} as true, and this will cause failure:
{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: Invalid method name: 'get_all_functions'
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3897)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)
	... 96 more
Caused by: org.apache.thrift.TApplicationException: Invalid method name: 'get_all_functions'
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_functions(ThriftHiveMetastore.java:3845)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_functions(ThriftHiveMetastore.java:3833)
{code}

It looks like Spark doesn't really need to call {{registerAllFunctionsOnce}} since it loads the Hive permanent function directly from HMS API. The Hive {{FunctionRegistry}} is only used for loading Hive built-in functions.",2021-05-05T21:20:08.899+0000,2021-10-20T08:15:12.190+0000,Fixed,Major
BIGTOP-1543,hive-0.14 in bigtop,BIGTOP,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12402031'>, <JIRA IssueLink: id='12402946'>, <JIRA IssueLink: id='12402948'>]","* Needed for proper tez support.
* Seems to solve GetLog() impedenace mismatch with hue
",2014-11-21T12:35:54.262+0000,2015-03-18T22:47:23.408+0000,Fixed,Major
PHOENIX-3037,Setup proper security context in compaction/split coprocessor hooks,PHOENIX,Bug,Closed,[],4,"[<JIRA IssueLink: id='12472797'>, <JIRA IssueLink: id='12494029'>, <JIRA IssueLink: id='12474692'>, <JIRA IssueLink: id='12472996'>]",See HBASE-16115 for a discussion.,2016-06-29T04:53:42.076+0000,2019-11-19T16:44:27.942+0000,Fixed,Major
SPARK-24474,Cores are left idle when there are a lot of tasks to run,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12537825'>],"I've observed an issue happening consistently when:
 * A job contains a join of two datasets
 * One dataset is much larger than the other
 * Both datasets require some processing before they are joined

What I have observed is:
 * 2 stages are initially active to run processing on the two datasets
 ** These stages are run in parallel
 ** One stage has significantly more tasks than the other (e.g. one has 30k tasks and the other has 2k tasks)
 ** Spark allocates a similar (though not exactly equal) number of cores to each stage
 * First stage completes (for the smaller dataset)
 ** Now there is only one stage running
 ** It still has many tasks left (usually > 20k tasks)
 ** Around half the cores are idle (e.g. Total Cores = 200, active tasks = 103)
 ** This continues until the second stage completes
 * Second stage completes, and third begins (the stage that actually joins the data)
 ** This stage works fine, no cores are idle (e.g. Total Cores = 200, active tasks = 200)

Other interesting things about this:
 * It seems that when we have multiple stages active, and one of them finishes, it does not actually release any cores to existing stages
 * Once all active stages are done, we release all cores to new stages
 * I can't reproduce this locally on my machine, only on a cluster with YARN enabled
 * It happens when dynamic allocation is enabled, and when it is disabled
 * The stage that hangs (referred to as ""Second stage"" above) has a lower 'Stage Id' than the first one that completes
 * This happens with spark.shuffle.service.enabled set to true and false",2018-06-06T12:28:00.704+0000,2020-05-17T17:48:35.695+0000,Incomplete,Major
HTRACE-111,HTrace Java client API fixes for 3.2,HTRACE,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12408789'>, <JIRA IssueLink: id='12409925'>]","* Remove old MilliSpan constructors in favor of {{MilliSpan#Builder}}
* Restore {{Trace#startSpan(String, TraceInfo)}} because it's used in Hadoop
* Remove {{Span#getParentId}} API which assumes a single parent world, as well as {{ROOT_SPAN_ID}}.
* Use regular Random instead of SecureRandom.",2015-02-19T04:27:14.835+0000,2015-03-06T20:31:29.208+0000,Fixed,Critical
AMBARI-21730,Default of hive.llap.execution.mode=only on Hive Interactive breaks StorageHandler tables.,AMBARI,Bug,Open,[],1,[<JIRA IssueLink: id='12512027'>],"Ambari 2.5+ sets the following Hive Interactive flag by default to:

{{hive.llap.execution.mode=only}}

This mode breaks compatibility with StorageHander based tables. For example, HBase tables like these fail:

CREATE TABLE hbase_table_1(key int, value string) 
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES (""hbase.columns.mapping"" = "":key,cf1:val"")
    TBLPROPERTIES (""hbase.table.name"" = ""xyz"", ""hbase.mapred.output.outputtable"" = ""xyz"");

With a stack trace like:

0: jdbc:hive2://intel68bvt2.fyre.ibm.com:2181> INSERT OVERWRITE TABLE hbase_table_1 SELECT * FROM hbase_table_1;
    INFO  : Compiling command(queryId=hive_20170814172300_b01cbd64-f790-468b-855c-2ff8224f4fc0): INSERT OVERWRITE TABLE hbase_table_1 SELECT * FROM hbase_table_1
    ....
    INFO  : Dag name: INSERT OVERWRITE TABLE hbase...hbase_table_1(Stage-3)
    INFO  : Dag submit failed due to There is conflicting local resource (guava-14.0.1.jar) between dag local resource and vertex Map 1 local resource. 
    Resource of dag : resource { scheme: ""hdfs"" host: ""intel68bvt1.fyre.ibm.com"" port: 8020 file: ""/tmp/hive/hive/0ccc7843-8d48-474a-888a-96bea2fcc1cd/hive_2017-08-14_17-23-00_028_5196182473716985960-6/hive/_tez_scratch_dir/guava-14.0.1.jar"" } size: 2189117 timestamp: 1502756584191 type: FILE visibility: PRIVATE
    Resource of vertex: resource { scheme: ""hdfs"" host: ""intel68bvt1.fyre.ibm.com"" port: 8020 file: ""/tmp/hive/hive/_tez_session_dir/de9aa10e-39a4-4d76-a0d8-55493fc6ba00/guava-14.0.1.jar"" }

*When this flag is set to: {{hive.llap.execution.mode=auto}}, the issue is not seen anymore.*

I have reproed this on:
{{# cat HDP.repo | grep baseurl}}
{{baseurl=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.1.0}}
{{# cat ambari.repo | grep baseurl}}
{{baseurl=http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.5.1.0}}",2017-08-15T19:05:22.547+0000,2019-04-12T07:42:16.598+0000,,Major
TEZ-683,Diamond shape DAG fail,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12380157'>],"In Pig, we created a diamond shape DAG:
{code}
A  -------------->  C
     \---> B ---/
{code}

We see exception intermittently (90% of chance):
org.apache.tez.dag.app.dag.impl.VertexImpl: Can't handle Invalid event V_SOURCE_VERTEX_STARTED on vertex scope-31 with vertexId vertex_1387342253398_0018_1_01 at current state NEW
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: V_SOURCE_VERTEX_STARTED at NEW
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:946)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:142)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1252)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1244)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)
        at java.lang.Thread.run(Thread.java:695)",2013-12-18T19:18:07.931+0000,2014-03-01T04:19:28.736+0000,Fixed,Major
ZOOKEEPER-1467,Make server principal configurable at client side.,ZOOKEEPER,Improvement,Closed,[],7,"[<JIRA IssueLink: id='12352354'>, <JIRA IssueLink: id='12352376'>, <JIRA IssueLink: id='12352371'>, <JIRA IssueLink: id='12411238'>, <JIRA IssueLink: id='12353593'>, <JIRA IssueLink: id='12468357'>, <JIRA IssueLink: id='12436423'>]","Server principal on client side is derived using hostname.

org.apache.zookeeper.ClientCnxn.SendThread.startConnect()
{code}
           try {
                zooKeeperSaslClient = new ZooKeeperSaslClient(""zookeeper/""+addr.getHostName());
            }
{code}

This may have problems when admin wanted some customized principals like zookeeper/clusterid@HADOOP.COM where clusterid is the cluster identifier but not the host name.

IMO, server principal also should be configurable as hadoop is doing.",2012-05-16T11:57:15.453+0000,2020-02-14T15:23:40.698+0000,Fixed,Major
ZOOKEEPER-1469,Adding Cross-Realm support for secure Zookeeper client authentication,ZOOKEEPER,Improvement,Reopened,[],2,"[<JIRA IssueLink: id='12353089'>, <JIRA IssueLink: id='12352495'>]","There is a use case where one needs to support cross realm authentication for zookeeper cluster. One use case is HBase Replication: HBase supports replicating data to multiple slave clusters, where the later might be running in different realms. With current zookeeper security, the region server of master HBase cluster are not able to query the zookeeper quorum members of the slave cluster. This jira is about adding such Xrealm support.
",2012-05-20T06:13:02.978+0000,2022-02-03T08:50:13.977+0000,,Major
TEZ-3394,Record the last reporting task for fetch failure diagnostics,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12476682'>],"It would be nice if the diagnostics for a fetch-failed task attempt also recorded the last task attempt that reported a fetch failure against it.  Then the user can quickly find one of the readers that encountered an error and check its logs for the nature of the fetch failure.

This is essentially the Tez equivalent of MAPREDUCE-6384.",2016-08-02T19:59:04.251+0000,2016-08-02T19:59:31.921+0000,,Major
TEZ-1447,Provide a mechanism for InputInitializers to know about interesting Vertex state changes,TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12395926'>, <JIRA IssueLink: id='12395840'>, <JIRA IssueLink: id='12395639'>]","I'm trying to do dynamic partition pruning through input initializer events in Hive. That means that the initializer of a table scan vertex has to receive events from all tasks in another vertex (which contain the pruning info) before generating tasks to run.

The problem with the current API I ran into:

getNumTasks: I'm currently using a busy loop to wait for the num tasks for a vertex to be decided (-1 -> x). There's no way around it, because it's the only way to find out what number of events to expect (0 is a valid number of tasks - so I can't wait for the first to complete).

With auto-reducer parallelism I have to employ another busy loop. Because I might be initially expecting 10 events, which later get's knocked down to 5. Since there's no event associated with this, I have to periodically check whether I have enough events.

Versioning: Events have a version number, but I don't know which task they are coming from. Thus I can't de-dup events.",2014-08-18T20:39:20.245+0000,2014-10-02T21:40:59.707+0000,Fixed,Blocker
AMBARI-17353,First class support for YARN hosted services,AMBARI,Epic,Open,[],8,"[<JIRA IssueLink: id='12475441'>, <JIRA IssueLink: id='12475795'>, <JIRA IssueLink: id='12471183'>, <JIRA IssueLink: id='12471180'>, <JIRA IssueLink: id='12471184'>, <JIRA IssueLink: id='12471182'>, <JIRA IssueLink: id='12471181'>, <JIRA IssueLink: id='12491664'>]","YARN-896 and SLIDER-183 enabled running long running applications (services) on YARN. Apache Ambari Slider View provides us a way to deploy and manage long running services on YARN. 

However, the Slider View provides a limited functionality and does not provide a first class support for YARN hosted services similar to traditional services deployed directly on the hosts by Ambari. 

Besides while YARN-896 got the ball rolling for supporting services on YARN, the YARN team is working on major improvements and a first class support for YARN hosted services (YARN-4692). 

This initiative is for providing a first class support for YARN-hosted services and leverage all the YARN improvements planned and documented in YARN-4692. ",2016-06-21T22:45:53.683+0000,2017-03-15T21:03:46.572+0000,,Critical
SPARK-4834,Spark fails to clean up cache / lock files in local dirs,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12406599'>, <JIRA IssueLink: id='12426041'>]","This issue was caused by https://github.com/apache/spark/commit/7aacb7bfa.

That change shares downloaded jar / files among multiple executors running on the same host by using a lock file and a cache file for each file the executor needs to download. The problem is that these lock and cache files are never deleted.

On Yarn, the app's dir is automatically deleted when the app ends, so no files are left behind. But on standalone, there's no such thing as ""the app's dir""; files will end up in ""/tmp"" or in whatever place the user configure in ""SPARK_LOCAL_DIRS"", and will eventually start to fill that volume.

We should add a way to clean up these files. It's not as simple as ""hey, just call File.deleteOnExit()!"" because we're talking about multiple processes accessing these files, so to maintain the efficiency gains of the original change, the files should only be deleted when the application is finished.",2014-12-12T23:56:37.526+0000,2015-05-28T16:33:49.389+0000,Fixed,Major
INFRA-11700,mvn version for HIVE builds?,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12463871'>],"Could you offer the mvn version Hive builds run under?

http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/

I'm trying to pinpoint environmental differences between Jenkins and travis-ci.

https://issues.apache.org/jira/browse/HIVE-10293

Thanks",2016-04-18T01:07:15.585+0000,2017-06-17T06:28:41.793+0000,Fixed,Trivial
LUCENE-3245,Realtime terms dictionary,LUCENE,Improvement,Open,[],1,[<JIRA IssueLink: id='12340076'>],"For LUCENE-2312 we need a realtime terms dictionary.  While ConcurrentSkipListMap may be used, it has drawbacks in terms of high object overhead which can impact GC collection times and heap memory usage.  

If we implement a skip list that uses primitive backing arrays, we can hopefully have a data structure that is [as] fast and memory efficient.",2011-06-27T04:09:55.263+0000,2022-08-28T12:51:20.389+0000,,Minor
TEZ-1163,Tez Auto Reducer-parallelism throws Divide-by-Zero,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12389109'>],"When the per-reducer estimate is set to 256Mb and over-all data output is ~200Mb, gets a divide-by-zero.

{code}
2014-05-31 16:17:29,125 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.ArithmeticException: / by zero
	at org.apache.tez.dag.library.vertexmanager.ShuffleVertexManager.determineParallelismAndApply(ShuffleVertexManager.java:392)
	at org.apache.tez.dag.library.vertexmanager.ShuffleVertexManager.schedulePendingTasks(ShuffleVertexManager.java:446)
{code}

Because of incorrect value set for minTasks

{code}
vertexmanager.ShuffleVertexManager: Shuffle Vertex Manager: settings minFrac:0.25 maxFrac:0.75 auto:true desiredTaskIput:256000000 minTasks:0
{code}",2014-06-01T00:27:12.271+0000,2014-09-06T01:35:28.432+0000,Fixed,Minor
THRIFT-3769,Fix logic of THRIFT-2268,THRIFT,Bug,Closed,[],4,"[<JIRA IssueLink: id='12570304'>, <JIRA IssueLink: id='12554731'>, <JIRA IssueLink: id='12531731'>, <JIRA IssueLink: id='12462490'>]","THRIFT-2268 intended to reduce the logging noise of TSaslTransport enabled servers, but the commit doesn't help in doing so (its just adding more specific noise today).

This is because the transport factory overrides disallow throwing specific execution types (no TTransportException in signature), and thereby all implementations will throw a RuntimeException-wrapped exception, which the added catch clauses would never encounter.

https://github.com/apache/thrift/blob/master/lib/java/src/org/apache/thrift/transport/TSaslServerTransport.java#L217-L219
https://github.com/apache/thrift/blob/master/lib/java/src/org/apache/thrift/server/TThreadPoolServer.java#L290-L295

We'll need to unwrap the RuntimeException to perform an actual valid cause check.",2016-04-01T07:05:16.839+0000,2019-09-20T06:50:03.504+0000,Fixed,Minor
TEZ-1119,Support display of user payloads in Tez UI,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12398586'>],Applications like Pig rely heavily on configurations bundled in as part of the user payload for debugging. This requires each Input/Output/Processor's user payload to be converted into something that easily consumable by the UI layer to be displayed back to the user as meaningful data.,2014-05-15T18:27:41.281+0000,2014-10-08T22:36:19.858+0000,Fixed,Major
AMBARI-18961,Ambari HiveView does not create or upload script in UTF-8 format,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12494322'>],"- When we run an insert query with some special character in the Hive (Hive-next) View then we see that the values are not being reflected properly.  The special characters are being replaced with  ""?""

Example:
{code}
CREATE table test (id int, name string);

insert into test values (100, ""€"");
insert into test values (200, ""$"");
insert into test values (300, ""£"");
insert into test values (400, ""₡"");

SELECT * FROM test LIMIT 100;
{code}

- Once the above script is executed then we will see the output something like ""HiveView_UTF-8_Issue.png"" (attached)",2016-11-22T17:42:06.090+0000,2017-03-26T13:50:05.562+0000,Not A Bug,Major
IMPALA-3446,Materialized views,IMPALA,Epic,Open,[],2,"[<JIRA IssueLink: id='12535560'>, <JIRA IssueLink: id='12642504'>]","This JIRA is a placeholder for this big topic.
Some user stories I can imagine under the epic:
# materialized view is a CTAS where the SELECT is saved in the HMS and can be rerun by a simple command
# materialized view detects that the source data has changed and falls back to be a view instead of SELECT *
# materialized view detects that the source data has changed and reruns the CTAS automatically
# ... reruns only the necessary changes (e.g. if only some of the partitions change)
# Impala and Hive to have common semantics of materialized views (https://issues.apache.org/jira/browse/HIVE-10459)
# materialized view stores extra statistics that help the optimizer, storing the full resultset is optional
# query optimizer checks for every query whether part of the query plan can be covered by an existing materialized view",2016-04-28T13:04:42.000+0000,2022-06-24T09:36:47.513+0000,,Major
YETUS-271,findbugs.sh will -1 if there are no java source code in the module.,YETUS,Bug,Resolved,[],6,"[<JIRA IssueLink: id='12454122'>, <JIRA IssueLink: id='12457073'>, <JIRA IssueLink: id='12453474'>, <JIRA IssueLink: id='12453541'>, <JIRA IssueLink: id='12454560'>, <JIRA IssueLink: id='12488658'>]","In MAPREDUCE-6595, I'm seeing ""-1 findbugs"" in hadoop-mapreduce-project module. There are no java source code in the module and that's why {{mvn findbugs:findbugs}} does not output {{target/findbugsXml.xml}}.
{noformat}
-1	findbugs	3m 34s	patch/hadoop-mapreduce-project no findbugs output file (hadoop-mapreduce-project/target/findbugsXml.xml)
{noformat}",2016-01-06T06:32:53.970+0000,2016-12-09T01:54:21.786+0000,Fixed,Major
MAHOUT-1325,MapReduce unit tests cannot be run in parallel,MAHOUT,Bug,Closed,[],6,"[<JIRA IssueLink: id='12375446'>, <JIRA IssueLink: id='12378883'>, <JIRA IssueLink: id='12378882'>, <JIRA IssueLink: id='12375445'>, <JIRA IssueLink: id='12376310'>, <JIRA IssueLink: id='12375444'>]","Mahout-Quality build job execution #2225 failed because of this test.
Relevant build log:
{noformat}
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ mahout-integration ---
[INFO] Surefire report directory: /home/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/integration/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.mahout.utils.vectors.lucene.LuceneIterableTest
Running org.apache.mahout.utils.email.MailProcessorTest
Running org.apache.mahout.clustering.cdbw.TestCDbwEvaluator
Running org.apache.mahout.utils.regex.RegexMapperTest
Running org.apache.mahout.utils.vectors.io.VectorWriterTest
Running org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest
Running org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest
Running org.apache.mahout.clustering.TestClusterEvaluator
Running org.apache.mahout.utils.regex.RegexUtilsTest
Running org.apache.mahout.utils.vectors.lucene.DriverTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.17 sec - in org.apache.mahout.utils.regex.RegexUtilsTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.412 sec - in org.apache.mahout.utils.email.MailProcessorTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 sec - in org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest
Running org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest
Running org.apache.mahout.clustering.TestClusterDumper
Running org.apache.mahout.utils.vectors.VectorHelperTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.601 sec - in org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest
Running org.apache.mahout.utils.vectors.arff.ARFFTypeTest
Running org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.383 sec - in org.apache.mahout.utils.vectors.VectorHelperTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.332 sec - in org.apache.mahout.utils.vectors.lucene.LuceneIterableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.252 sec - in org.apache.mahout.utils.vectors.arff.ARFFTypeTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.369 sec - in org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.415 sec - in org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.5 sec - in org.apache.mahout.utils.regex.RegexMapperTest
Running org.apache.mahout.utils.vectors.arff.DriverTest
Running org.apache.mahout.utils.Bump125Test
Running org.apache.mahout.utils.SplitInputTest
Running org.apache.mahout.utils.TestConcatenateVectorsJob
Running org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.043 sec - in org.apache.mahout.utils.vectors.io.VectorWriterTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.362 sec - in org.apache.mahout.utils.Bump125Test
Running org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest
Running org.apache.mahout.text.SequenceFilesFromLuceneStorageTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.304 sec - in org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.887 sec - in org.apache.mahout.utils.vectors.arff.DriverTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.486 sec - in org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest
Running org.apache.mahout.text.TestSequenceFilesFromDirectory
Running org.apache.mahout.text.LuceneStorageConfigurationTest
Running org.apache.mahout.text.LuceneSegmentInputFormatTest
Running org.apache.mahout.text.SequenceFilesFromMailArchivesTest
Running org.apache.mahout.text.MailArchivesClusteringAnalyzerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.265 sec - in org.apache.mahout.text.MailArchivesClusteringAnalyzerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.8 sec - in org.apache.mahout.utils.TestConcatenateVectorsJob
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.097 sec - in org.apache.mahout.text.LuceneStorageConfigurationTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.902 sec - in org.apache.mahout.utils.vectors.lucene.DriverTest
Running org.apache.mahout.text.LuceneSegmentRecordReaderTest
Running org.apache.mahout.text.LuceneSegmentInputSplitTest
Running org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest
Running org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.237 sec - in org.apache.mahout.text.LuceneSegmentInputFormatTest
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.122 sec <<< FAILURE! - in org.apache.mahout.text.SequenceFilesFromMailArchivesTest
testMapReduce(org.apache.mahout.text.SequenceFilesFromMailArchivesTest)  Time elapsed: 2.333 sec  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.mahout.text.SequenceFilesFromMailArchivesTest.testMapReduce(SequenceFilesFromMailArchivesTest.java:153)

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.74 sec - in org.apache.mahout.text.TestSequenceFilesFromDirectory
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.172 sec - in org.apache.mahout.text.LuceneSegmentInputSplitTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.85 sec - in org.apache.mahout.text.LuceneSegmentRecordReaderTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.571 sec - in org.apache.mahout.clustering.TestClusterDumper
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.021 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.121 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.613 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.246 sec - in org.apache.mahout.utils.SplitInputTest
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.07 sec - in org.apache.mahout.clustering.cdbw.TestCDbwEvaluator
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.224 sec - in org.apache.mahout.clustering.TestClusterEvaluator

Results :

Failed tests: 
  SequenceFilesFromMailArchivesTest.testMapReduce:153->Assert.assertEquals:542->Assert.assertEquals:555->Assert.assertEquals:118->Assert.failNotEquals:743->Assert.fail:88 expected:<1> but was:<0>

Tests run: 95, Failures: 1, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Mahout Build Tools ................................ SUCCESS [8.296s]
[INFO] Apache Mahout ..................................... SUCCESS [2.731s]
[INFO] Mahout Math ....................................... SUCCESS [1:38.594s]
[INFO] Mahout Core ....................................... SUCCESS [4:44.809s]
[INFO] Mahout Integration ................................ FAILURE [26.458s]
[INFO] Mahout Examples ................................... SKIPPED
[INFO] Mahout Release Package ............................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 7:03.463s
[INFO] Finished at: Tue Sep 03 22:40:41 UTC 2013
[INFO] Final Memory: 62M/447M
[INFO] ------------------------------------------------------------------------
{noformat}
",2013-09-04T18:25:12.753+0000,2014-02-03T07:57:19.491+0000,Fixed,Minor
HTRACE-92,Thread local storing the currentSpan is never cleared,HTRACE,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12406658'>, <JIRA IssueLink: id='12406624'>, <JIRA IssueLink: id='12406872'>]","In Apache Phoenix, we use HTrace to provide request level trace information. The trace information (traceid, parentid, spanid, description) among other columns is stored in a Phoenix table. 

Spans that are MilliSpan get persisted to the phoenix table this way:
MilliSpans-> PhoenixTraceMetricsSource-> PhoenixMetricsSink->Phoenix Table

While inserting the traces to the phoenix table, we make sure that the upsert happening in sink is through a connection that doesn't have tracing on. So this way any spans that are created by executing these upsert statements on the client side are NullSpans. 

On server side too, when these batched up upsert statements are executed as batchMutate operations, they are not expected to have tracing on i.e. the current spans are null. However, we noticed that these current spans are not null which ends up resulting in an infinite loop. An example of such infinite loop is the following:

batchmutate -> fshlog.append -> check tracing on i.e. current span is not null -> yes -> create milli span -> do operation -> stop span -> publish to metrics source -> phoenix metrics sink -> upsert statement -> batchmutate -> fshlog.append......

My local cluster infact dies because of this infinite loop!!

On examining the thread local of the threads in the RPC thread pool, I saw that there were threads that had current spans that were closed at least an hour before. See the screenshot attached. 

The screenshot was taken at 11:20 PM and the thread had a current span whose stop time was 10:17 PM. 

These brought up a couple of design issues/limitations in the HTrace API:
1) There is no good way to set (reset) the value of the thread local current span to null. This is a huge issue especially if we we are reusing threads from a thread pool. 
2) Should we allow creating spans if the parent span is not running anymore i.e. Span.isRunning() is false. In the example I have shown in the screen shot, the current span stored in the thread local is already closed. Essentially making {code}
boolean isTracing() {
return currentSpan.get() != null && currentSpan.get().isRunning()
{code} 


",2015-01-28T08:41:16.227+0000,2015-02-03T00:06:01.155+0000,Not A Problem,Major
ORC-46,Reserve CompressionKind values for LZ4 and ZSTD,ORC,Improvement,Closed,[],1,[<JIRA IssueLink: id='12531579'>],We are going to start using ORC with LZ4 and ZSTD compression types in Presto (which has its own reader and soon its own writer). Registering these as official CompressionKind values now will avoid future compatibility issues.,2016-04-06T17:29:24.021+0000,2018-04-12T03:38:44.499+0000,Fixed,Major
ACCUMULO-652,support block-based filtering within RFile,ACCUMULO,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12353988'>],"If we keep some stats about what is in an RFile block, we might be able to efficiently [O(log N)], with high probability, implement filters that currently require linear table scans. Two use cases of this include timestamp range filtering (i.e. give me everything from last Tuesday) and cell-level security filtering (i.e. give me everything that I can see with my authorizations).

For the timestamp range filter, we can keep minimum and maximum timestamps across all keys used in a block within the index entry for that block. For the cell-level security filter, we can keep an aggregate label. This could be done using a simplified disjunction of all of the labels in the block. The extra block statistics information can propagate up the index hierarchy as well, giving nice performance characteristics for finding the next matching entry in a file.

In general, this is a heuristic technique that is good if data tends to naturally cluster in blocks with respect to the way it is queried. Testing its efficacy will require closely emulating real-world use cases -- tests like the continuous ingest test will not be sufficient. We will have to test for a few things:
# The cost for storing the extra stats in the index are not too expensive.
# The performance benefit for common use cases is significant.
# We shouldn't introduce any unacceptable worst-case behavior, like bloating the index to ridiculous proportions for any data set.

Eventually this will all need to be exposed through the Iterator API to be useful, which will be another ticket. ",2012-06-26T18:23:24.200+0000,2020-10-28T22:27:45.197+0000,Abandoned,Major
FLINK-9478,Use Yarn application proxy to communicate with Flink cluster,FLINK,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12535340'>, <JIRA IssueLink: id='12535339'>]","The Yarn AM proxy currently does not support PUT/POST/DELETE requests (see YARN-2084). Due to this limitation it is currently not possible for the Flink REST client to directly communicate with the Flink cluster through the Yarn proxy.

In order to circumvent this problem we could try to wrap all our REST calls in a GET request which is forwarded by the Yarn proxy. Maybe this can be achieved with X-HTTP methods override as described by http://fandry.blogspot.de/2012/03/x-http-header-method-override-and-rest.html.",2018-05-30T07:35:03.219+0000,2021-01-29T10:51:54.836+0000,Won't Do,Major
CALCITE-1334,Convert predicates on EXTRACT function calls into date ranges,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12478365'>, <JIRA IssueLink: id='12479237'>, <JIRA IssueLink: id='12479747'>]","We would like to convert predicates on date dimension columns into date ranges. This is particularly useful for Druid, which has a single timestamp column.

Consider the case of a materialized view

{code}
SELECT sales.*, product.*, time_by_day.*
FROM sales
JOIN product USING (product_id)
JOIN time_by_day USING (time_id)
{code}

that corresponds to a Druid table

{noformat}
sales_product_time(
  product_id int not null,
  time_id int not null,
  units int not null,
  the_year int not null,
  the_quarter int not null,
  the_month int not null,
  the_timestamp timestamp not null,
  product_name varchar(20) not null)
{noformat}

And suppose we have the following check constraints:
* {{CHECK the_year = EXTRACT(YEAR FROM the_timestamp)}}
* {{CHECK the_month = EXTRACT(MONTH FROM the_timestamp)}}

Given a query

{code}
SELECT product_id, count(*)
FROM sales
JOIN product USING (product_id)
JOIN time_by_day USING (time_id)
WHERE the_year = 2016
AND the_month IN (4, 5, 6)
{code}

we would like to transform it into the following query to be run against Druid:

{code}
SELECT product_id, count(*)
FROM sales_product_time
WHERE the_timestamp BETWEEN '2016-04-01' AND '2016-06-30'
{code}

Druid can handle timestamp ranges (or disjoint sets of ranges) very efficiently.

I believe we can write a rule that knows the check constraints and also knows the properties of the {{EXTRACT}} function:

1. Apply check constraints to convert {{WHERE year = ...}} to {{WHERE EXTRACT(YEAR FROM the_timestamp) = ...}}, etc.
2. {{EXTRACT(YEAR FROM ...)}} is monotonic, therefore we can deduce the range of the_timestamp values such that {{EXTRACT(YEAR FROM the_timestamp)}} returns 2016.
3. Then we need to use the fact that {{EXTRACT(MONTH FROM the_timestamp)}} is monotonic if {{the_timestamp}} is bounded within a particular year.
4. And we need to merge month ranges somehow.",2016-07-28T22:22:35.062+0000,2017-03-10T18:33:21.345+0000,Fixed,Major
SENTRY-2106,If Sentry is ahead do not trigger a full snapshot,SENTRY,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12525257'>, <JIRA IssueLink: id='12525258'>, <JIRA IssueLink: id='12524613'>]","After steady state, a full snapshot is triggered by mainly 2 cases
 # Sentry is ""ahead""
 # Sentry is behind
 Case 1 has a dependency on NOTIFICATION_SEQUENCE table. This is not reliable as it was observed that sometimes NOTIFICATION_SEQUENCE and NOTIFICATION_LOG are not in sync. As a result of this unnecessary full snapshots can be triggered.

The fix is to remove this dependency

 

PLEASE NOTE THAT THIS SHOULD BE COMMITTED WITH SENTRY-2109",2017-12-20T17:44:14.237+0000,2018-03-09T09:25:12.366+0000,Information Provided,Major
NIFI-7730,Jetty server does not start up when a keystore with multiple certificates is used,NIFI,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12608716'>, <JIRA IssueLink: id='12597245'>, <JIRA IssueLink: id='12597295'>, <JIRA IssueLink: id='12639830'>]","In the newer Jetty version (which is recently upgraded on the main branch), Jetty's `SslContextFactory()` has been deprecated, and we can use `SslContextFactory.Server()` or `SslContextFactory.Client()` instead. If we use `SslContextFactory()`, Jetty server does not start when we use keystores with multiple certificates, with the following error log.

In addition to that, we can remove `setEndpointIdentificationAlgorithm(null);` since it will be executed in the constructor of `SslContextFactory.Server()` if we replace with it.
 (See: [https://github.com/eclipse/jetty.project/blob/jetty-9.4.26.v20200117/jetty-util/src/main/java/org/eclipse/jetty/util/ssl/SslContextFactory.java#L2204])

 
{code:java}
2020-08-07 19:50:32,299 INFO [main] o.e.jetty.util.ssl.SslContextFactory x509=X509@3aac31b7(nifi-key,h=[****],w=[****]) for SslContextFactory@57def953[provider=null,keyStore=file:///****/keystore.jks,trustStore=file:///****/truststore.jks]
2020-08-07 19:50:32,308 WARN [main] org.apache.nifi.web.server.JettyServer Failed to start web server... shutting down.
java.lang.IllegalStateException: KeyStores with multiple certificates are not supported on the base class org.eclipse.jetty.util.ssl.SslContextFactory. (Use org.eclipse.jetty.util.ssl.SslContextFactory$Server or org.eclipse.jetty.util.ssl.SslContextFactory$Client instead)
        at org.eclipse.jetty.util.ssl.SslContextFactory.newSniX509ExtendedKeyManager(SslContextFactory.java:1275)
        at org.eclipse.jetty.util.ssl.SslContextFactory.getKeyManagers(SslContextFactory.java:1256)
        at org.eclipse.jetty.util.ssl.SslContextFactory.load(SslContextFactory.java:374)
        at org.eclipse.jetty.util.ssl.SslContextFactory.doStart(SslContextFactory.java:245)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:72)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
        at org.eclipse.jetty.server.SslConnectionFactory.doStart(SslConnectionFactory.java:92)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:72)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
        at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320)
        at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)
        at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:231)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:72)
        at org.eclipse.jetty.server.Server.doStart(Server.java:385)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:72)
        at org.apache.nifi.web.server.JettyServer.start(JettyServer.java:1060)
        at org.apache.nifi.NiFi.<init>(NiFi.java:160)
        at org.apache.nifi.NiFi.<init>(NiFi.java:72)
        at org.apache.nifi.NiFi.main(NiFi.java:303)
2020-08-07 19:50:32,309 INFO [Thread-1] org.apache.nifi.NiFi Initiating shutdown of Jetty web server...
{code}",2020-08-12T08:38:04.736+0000,2022-05-12T06:49:53.933+0000,Fixed,Blocker
SLIDER-1192,Slider Agent tarball should be uploaded to HDFS to avoid client side dependency,SLIDER,Bug,Open,[],2,"[<JIRA IssueLink: id='12493090'>, <JIRA IssueLink: id='12493089'>]","Trying to use Slider client from a Hive process that is shipped with slider client JAR
{noformat} 
Caused by: java.io.FileNotFoundException: File /grid/5/sershe/tez-autobuild/dist/hive/lib/slider-agent.tar.gz does not exist
   at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
   at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)
   at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)
   at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
   at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)
   at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1965)
   at org.apache.slider.providers.ProviderUtils.addAgentTar(ProviderUtils.java:128)
   at org.apache.slider.providers.agent.AgentClientProvider.prepareAMAndConfigForLaunch(AgentClientProvider.java:270){noformat}

I can see the gz file in the slider lib directory. However, using slider client one should not even be required to have slider on the box (Hive already has the jars); nor is there a good way to find where it is, if installed (given that jar is shipped and we are not aware of the slider installation).

The gz file should be included with client jars (perhaps as a resource?)

Update: setting slider.libdir and/or SLIDER_HOME actually doesn't work at all.
The code in addAgentTar basically looks for it in the directory of the client jar, no option to override it at all.
That makes slider client absolutely unusable",2017-01-30T21:41:11.314+0000,2017-02-13T21:36:24.892+0000,,Blocker
PHOENIX-992,Replace hTable.getRowOrBefore with ReverseScan to get maxKey in StatsManagerImpl,PHOENIX,Bug,Closed,[],1,[<JIRA IssueLink: id='12388522'>],"Below is the java doc of the function in HBase and we should remove it ASAP

{noformat}
   * @deprecated As of version 0.92 this method is deprecated without
   * replacement.   
   * getRowOrBefore is used internally to find entries in hbase:meta and makes
   * various assumptions about the table (which are true for hbase:meta but not
   * in general) to be efficient.
{noformat}",2014-05-21T01:28:33.703+0000,2015-11-21T02:17:07.609+0000,Fixed,Critical
FLUME-2191,HDFS Minicluster tests failing after protobuf upgrade.,FLUME,Bug,Resolved,[],1,[<JIRA IssueLink: id='12375231'>],"I ran the full build in hadoop-1 profile, but it looks like the protobuf upgrade broke the hadoop-2 profile. The HDFS Sink test on Minicluster fails with this:
{code}
Running org.apache.flume.sink.hdfs.TestHDFSEventSinkOnMiniCluster
2013-09-13 12:11:31.159 java[58566:1203] Unable to load realm info from SCDynamicStore
2013-09-13 12:11:31.208 java[58566:1203] Unable to load realm info from SCDynamicStore
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 4.238 sec <<< FAILURE!
simpleHDFSTest(org.apache.flume.sink.hdfs.TestHDFSEventSinkOnMiniCluster)  Time elapsed: 1979 sec  <<< ERROR!
java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.
	at com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto.getSerializedSize(ClientNamenodeProtocolProtos.java:21638)
	at com.google.protobuf.AbstractMessageLite.toByteString(AbstractMessageLite.java:49)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequest(ProtobufRpcEngine.java:137)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:181)
	at com.sun.proxy.$Proxy15.getDatanodeReport(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
	at com.sun.proxy.$Proxy15.getDatanodeReport(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDatanodeReport(ClientNamenodeProtocolTranslatorPB.java:488)
	at org.apache.hadoop.hdfs.DFSClient.datanodeReport(DFSClient.java:1642)
	at org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:1703)
	at org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:1722)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1066)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:929)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:588)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:527)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:398)
	at org.apache.flume.sink.hdfs.TestHDFSEventSinkOnMiniCluster.simpleHDFSTest(TestHDFSEventSinkOnMiniCluster.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

{code}
",2013-09-13T19:13:18.725+0000,2013-10-04T03:09:11.218+0000,Fixed,Blocker
HCATALOG-245,StorageHandler authorization providers ,HCATALOG,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12348133'>, <JIRA IssueLink: id='12348132'>]","As per the design in the parent issue, we will delegate the authorization checks to the storage handler (hdfs is considered as a storage handler as well). This jira will introduce HiveAuthorizationProviders for hbase + hdfs.",2012-01-31T01:12:00.568+0000,2012-05-17T01:20:10.475+0000,Fixed,Major
HTRACE-32,"Change span timeline annotations map to be a map<string, string>",HTRACE,Bug,Resolved,[],1,[<JIRA IssueLink: id='12408371'>],"We should consider changing the timeline annotations in Span.java to be a map<string, string> rather than a map<byte[], byte[]>.  It's very inconvenient to deal with byte arrays here, and every use-case we have currently puts string data into these fields.  One example is that you cannot look up fields in the map, since the map<byte[], byte[]> compares by byte array object equality (useless unless you have the original key array object you inserted).  It's also questionable if we could put non-string data in this map at all, since we are serializing them to JSON strings, which can't handle arbitrary byte strings.",2014-12-30T20:30:06.174+0000,2015-02-17T22:05:37.836+0000,Fixed,Major
HCATALOG-466,Update pig version used in unit tests to 0.10.0,HCATALOG,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12359126'>, <JIRA IssueLink: id='12356672'>]",Currently all the unit tests related to Pig use 0.8.0 version. The latest release version of Pig is 0.10.0. ,2012-08-14T20:57:37.832+0000,2013-09-19T14:18:09.248+0000,Duplicate,Major
THRIFT-4846,C++ generator should topologically sort struct definitions and error on cycles,THRIFT,Bug,Open,[],1,[<JIRA IssueLink: id='12559486'>],"C++ (and maybe other languages?) treat Thrift struct-typed fields as plain struct members of the containing struct. Currently, the generator outputs the types in the same order as they're defined in the underlying Thrift field, which means that a file like:

{code}
struct A {
  1: B foo;
}
struct B {
}
{code}

will generate C++ code that fails to compile.

We should topologically-sort the structs before outputting the definitions so that the generated code compiles correctly.",2019-04-05T18:56:57.101+0000,2020-03-18T12:51:41.472+0000,,Major
IMPALA-8721,Wrong result when Impala reads a Hive written parquet TimeStamp column,IMPALA,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12608285'>, <JIRA IssueLink: id='12608284'>, <JIRA IssueLink: id='12608287'>]"," 

Easy to repro on latest upstream:
{code:java}
hive> create table t1_hive(c1 timestamp) stored as parquet;
hive> insert into t1_hive values('2009-03-09 01:20:03.600000000');
hive> select * from t1_hive;
OK
2009-03-09 01:20:03.6
[localhost:21000] default> invalidate metadata t1_hive;
[localhost:21000] default> select * from t1_hive;
Query: select * from t1_hive
Query submitted at: 2019-06-24 09:55:36 (Coordinator: http://optimus-prime:25000)
Query progress can be monitored at: http://optimus-prime:25000/query_plan?query_id=b34f85cb5da29c26:d4dfcb2400000000
+-------------------------------+
| c1 |
+-------------------------------+
| 2009-03-09 09:20:03.600000000 | <<<<<UTC
+-------------------------------+
bin/start-impala-cluster.py --impalad_args='-convert_legacy_hive_parquet_utc_timestamps=true'
[localhost:21000] default> select * from t1_hive;
Query: select * from t1_hive
Query submitted at: 2019-06-24 10:00:22 (Coordinator: http://optimus-prime:25000)
Query progress can be monitored at: http://optimus-prime:25000/query_plan?query_id=d5428bb21fb259b9:7b10703400000000
+-------------------------------+
| c1 |
+-------------------------------+
| 2009-03-09 02:20:03.600000000 |. <<<<<<PST8PDT
+-------------------------------+
 
{code}
 

This issue is causing testcase test_hive_impala_interop to fail. Untill this issue is fixed, the testcase will be updated to not include a timestamp column. The test case should be updated to include a timestamp column once this issue is fixed.

 ",2019-06-27T18:53:08.364+0000,2021-02-10T00:40:29.783+0000,Fixed,Critical
TEZ-1190,Allow multiple edges between two vertices,TEZ,Bug,Open,"[<JIRA Issue: key='TEZ-3511', id='13018341'>, <JIRA Issue: key='TEZ-3512', id='13018342'>, <JIRA Issue: key='TEZ-3513', id='13018343'>, <JIRA Issue: key='TEZ-3514', id='13018344'>, <JIRA Issue: key='TEZ-3515', id='13018345'>, <JIRA Issue: key='TEZ-3516', id='13018346'>, <JIRA Issue: key='TEZ-3517', id='13018348'>, <JIRA Issue: key='TEZ-3518', id='13018350'>, <JIRA Issue: key='TEZ-3519', id='13018351'>, <JIRA Issue: key='TEZ-3520', id='13018352'>, <JIRA Issue: key='TEZ-3521', id='13018353'>, <JIRA Issue: key='TEZ-3522', id='13018354'>, <JIRA Issue: key='TEZ-3523', id='13018355'>, <JIRA Issue: key='TEZ-3524', id='13018358'>, <JIRA Issue: key='TEZ-3525', id='13018359'>, <JIRA Issue: key='TEZ-3526', id='13018360'>, <JIRA Issue: key='TEZ-3674', id='13059802'>]",3,"[<JIRA IssueLink: id='12412680'>, <JIRA IssueLink: id='12390086'>, <JIRA IssueLink: id='12507727'>]","This will be helpful in some scenario. In particular example, we can merge two small pipelines together in one pair of vertex. Note it is possible the edge type between the two vertexes are different.",2014-06-07T00:33:04.893+0000,2017-06-27T23:45:57.281+0000,,Major
AVRO-1241,improve trevni performance on string deserialization,AVRO,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12363844'>, <JIRA IssueLink: id='12363845'>, <JIRA IssueLink: id='12363846'>]","I have been trying to implement a storage function for Apache Pig that writes data in Trevni format. I found that the storage function was very slow when reading whole records.

I did some profiling (with Yourkit) and found that most of the CPU time was being spent in org.apache.trevni.InputBuffer$readString() (specifically in the String() method). I changed to java.nio.charset.CharsetDecoder.decode for deserialization and saw a big improvement. Changes are included in the patch.",2013-02-01T20:38:58.129+0000,2013-02-27T00:54:02.045+0000,Fixed,Major
SENTRY-498,Sentry integration with Hive authorization framework V2 ,SENTRY,New Feature,Resolved,"[<JIRA Issue: key='SENTRY-519', id='12753588'>, <JIRA Issue: key='SENTRY-765', id='12836735'>, <JIRA Issue: key='SENTRY-504', id='12751030'>, <JIRA Issue: key='SENTRY-861', id='12860631'>, <JIRA Issue: key='SENTRY-948', id='12909584'>, <JIRA Issue: key='SENTRY-506', id='12751032'>, <JIRA Issue: key='SENTRY-568', id='12760077'>, <JIRA Issue: key='SENTRY-505', id='12751031'>, <JIRA Issue: key='SENTRY-542', id='12757382'>, <JIRA Issue: key='SENTRY-532', id='12756314'>, <JIRA Issue: key='SENTRY-592', id='12763049'>, <JIRA Issue: key='SENTRY-603', id='12763799'>, <JIRA Issue: key='SENTRY-569', id='12760078'>]",7,"[<JIRA IssueLink: id='12402998'>, <JIRA IssueLink: id='12402997'>, <JIRA IssueLink: id='12403907'>, <JIRA IssueLink: id='12404084'>, <JIRA IssueLink: id='12453059'>, <JIRA IssueLink: id='12403750'>, <JIRA IssueLink: id='12403751'>]","Currently Sentry grant/revoke privileges via hook DDLTask, and do authorization via HiveSemanticAnalyzerHook. Now hive has a pluggable authorization framework via exposing some interfaces HiveAccessController and HiveAuthorizationValidator. HiveAccessController is used to grant/revoke roles and privileges. HiveAuthorizationValidator is used to do fine-grained authorization. 

Advantages to use this framework to grant/revoke privileges and do authorization:
- This framework is very convenient to use by external authorization system. 
- Using this framework will be better accepted by community.
- We don't need to take efforts to add so many hooks. 
- Some hooks has limitations. e.g. For column level security, we can't get accessed cloumns from query via HiveSemanticAnalyzerHook, so I extend the readEntity to put accessed columns into it(HIVE-7730). But if we use this framework, we don't need to extend the readEntity, we can just get accessed columns from ColumnAccessInfo directly. 

I will not remove the old sentry authorization framework, I will just add a new authorizationV2 via implement Hive authorization framework. If all the e2e tests passed, we can mark the old authorization deprecated.",2014-10-21T07:34:29.575+0000,2016-04-15T21:37:19.054+0000,Fixed,Major
BIGTOP-713,use newer debhelper and source format 3.0 (quilt) for Debian and Ubuntu packaging,BIGTOP,Improvement,Closed,"[<JIRA Issue: key='BIGTOP-690', id='12603628'>]",4,"[<JIRA IssueLink: id='12360612'>, <JIRA IssueLink: id='12360849'>, <JIRA IssueLink: id='12363647'>, <JIRA IssueLink: id='12363648'>]","debhelper can automate a lot of common things in debian package creation.

The current packages use an old style of debhelper, that often is unnecessarily complicated, making it harder to fix things.

For example, current Hadoop (0.23.3) does not compile on Debian because of the new GCC version. The fix is a simple ""include <unistd.h>"" in the HadoopPipes.cc file.

Modern Debian packaging with ""quilt"" has an excellent mechanism for managing such patches. However, in order to use this with the current Bigtop packaging, one has to 1. create debian/source/format to use ""3.0 (quilt)"" 2. manually add quilt patching to the debian/rules targets. 3. making sure the .debian.tar.gz is also copied instead of the old .diff.gz

You will be surprised how many things debhelper does well on its own with a rules file consisting just of little more than the automagic:

%:
        dh $@

Furthermore, ""java-wrappers"" is a Debian and Ubuntu package that helps with setting up classpaths and choosing the JVM. It can do all of bigtop-utils and more, and it is used by other Java packages. IMHO it should be preferred instead.

If the packaging would be more Debian-standard, it would be alot easier to get the packages at some point accepted into Debian mainline. It may even be desirable to build the various hadoop components (-commmon, -yarn etc.) independently if they are isolated well enough upstream.

Don't get me wrong. I think the packages are pretty good already. In particularly I like the split into namenode and datanode packages and the use of update-alternatives, for example. I just found it rather hard to get a grip of the process and to get my fixes into the package. For example, I had to manually set JAVA_HOME before building, some build dependencies were missing (cmake, but it probably is a new requirement), some paths have changed (probably the yarn promotion to a top level project?)
I understand that you want to have as much common code for all distributions as possible, as opposed to having per-distribution packaging. However, if every project uses its own specific version of java-wrappers and build process, things will not really be better than if it is at least consistent across the various distributions.
But ideally, there should be very little packaging code needed anyway, and most things be done by an appropriate installation process upstream.

And seriously, /usr/lib/hadoop/lib is a **mess**. There even is a package in there with a ""*"" in the file name. Plus, a lot of these jars are available in Debian, and could be shared across packages if the packages would accept them to be managed by the distribution instead of shipping their own...

Even within the bigtop packages this leads to a totally unnecessary overlap:

995720 Sep 25 14:18 /usr/lib/hadoop-hdfs/lib/snappy-java-1.0.3.2.jar
995720 Sep 25 14:18 /usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.3.2.jar
995720 Sep 25 14:18 /usr/lib/hadoop-yarn/lib/snappy-java-1.0.3.2.jar
[...]",2012-09-25T14:54:45.627+0000,2013-06-21T23:49:44.425+0000,Fixed,Minor
GIRAPH-453,Pure hive I/O ,GIRAPH,Bug,Resolved,[],1,[<JIRA IssueLink: id='12362108'>],"We have had a lot of issues with HCatalog and want to have our own pure-hive I/O. We can contribute this back to Hive folks as well if they're interested.

For starters this will be based on HCatalog's code, but we have a few ideas for Giraph-oriented improvements as well.

https://reviews.apache.org/r/8611/",2012-12-15T01:15:00.307+0000,2013-03-04T21:26:44.642+0000,Fixed,Major
IMPALA-8629,Adjust new KuduStorageHandler package,IMPALA,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12562528'>, <JIRA IssueLink: id='12562527'>]","Before releasing with the updated KuduStorageHandler, we should change the new KuduStorageHandler package from “org.apache.kudu.hive” to “org.apache.hadoop.hive.kudu”.

 

This should being done to ensure the stand-in storage handler can be a real storage handler when a Hive integration is added in the future. The “org.apache.hadoop.hive” package is the standard package all Hive storage handlers lives under.

 

Additionally the stand-in format details defined [here|https://github.com/apache/impala/blob/2bce974990e19788ec359deec50f06d44ec92048/fe/src/main/java/org/apache/impala/catalog/HdfsFileFormat.java#L70] should be updated as well. Values for those entries should be:
{noformat}
org.apache.hadoop.hive.kudu.KuduInputFormat
org.apache.hadoop.hive.kudu.KuduOutputFormat 
org.apache.hadoop.hive.kudu.KuduSerDe 
{noformat}
 

I have a WIP patch for HIVE-12971 and used that patch to validate that using ""correct"" stand-in values would allow Hive to read HMS tables/entries created by Impala. 

 

Note: This patch will need to be committed after the Kudu side patch is committed and the Kudu build/version may need to be update in Impala. A review for the Kudu side change is here: [https://gerrit.cloudera.org/#/c/13540/]",2019-06-06T17:01:13.219+0000,2019-06-27T22:38:12.668+0000,Fixed,Major
INFRA-11630,Enable travis-ci builds for Hive,INFRA,Task,Closed,[],3,"[<JIRA IssueLink: id='12463218'>, <JIRA IssueLink: id='12463217'>, <JIRA IssueLink: id='12549776'>]","Would you kindly enable the Travis-CI integration for Hive?

Thanks!",2016-04-09T19:25:33.893+0000,2018-12-11T04:34:01.782+0000,Fixed,Trivial
KNOX-1041,"High Availability Support For Apache SOLR, HBase & Kafka",KNOX,New Feature,Closed,[],4,"[<JIRA IssueLink: id='12515498'>, <JIRA IssueLink: id='12514843'>, <JIRA IssueLink: id='12515385'>, <JIRA IssueLink: id='12515320'>]","Provide high-availability/fail-over between Knox and SOLR/HBase/Kafka using the existing DefaultHaDispatch mechanism and a customized URLManager implementation with knowledge of active hosts in Zookeeper.

When SOLR Cloud is used the active hosts are stored in Zookeeper under the /live_nodes path.  The attached custom URLManager implementation queries Zookeeper for the active hosts upon startup.  In the event of fail-over, it updates the internal list of hosts.

The HS2ZookeeperURLManager implementation used to provide similar functionality for Hive was used as a starting point.",2017-09-14T15:19:00.780+0000,2019-03-28T14:07:21.131+0000,Fixed,Major
LOG4J2-1515,Help Apache HBase project to upgrade from Log4j 1.x to 2.x,LOG4J2,Sub-task,Open,[],1,[<JIRA IssueLink: id='12477563'>],,2016-08-13T13:29:30.699+0000,2017-10-04T19:27:25.841+0000,,Major
ORC-743,"Conversion of SArg into Filters, to take advantage of LazyIO",ORC,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12621141'>, <JIRA IssueLink: id='12607357'>, <JIRA IssueLink: id='12607358'>]","ORC-742 introduces lazy evaluation of the non-filter columns in the presence of filters. This builds further on that to convert SArg into filters.
h3. SArg to Filter

SArg to Filter converts the passed SArg into a filter. This enables automatic compatibility with both Spark and Hive as they already push down Search Arguments down to ORC.

The SArg is automatically converted into a Vector Filter. Which is applied during the read process.

The builder for search argument should allow skipping normalization during the [build|https://github.com/apache/hive/blob/storage-branch-2.7/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java#L491]. This has already been proposed as part of HIVE-24458.

Normalization is very poor in performance in the presence of multilevel predicates.
||Benchmark||(fSize)||(fType)||(normalize)||Mode||Cnt||Score||Error||Units||
|ComplexFilterBench.filter|2|vector|true|avgt|20|74.321|± 0.156|us/op|
|ComplexFilterBench.filter|2|vector|false|avgt|20|78.119|± 0.351|us/op|
|ComplexFilterBench.filter|4|vector|true|avgt|20|267.405|± 1.202|us/op|
|ComplexFilterBench.filter|4|vector|false|avgt|20|136.284|± 0.637|us/op|
|ComplexFilterBench.filter|8|vector|true|avgt|20|9907.765|± 49.208|us/op|
|ComplexFilterBench.filter|8|vector|false|avgt|20|247.714|± 0.651|us/op|

Explanation:
 * *fSize* identifies the size of the OR clause that will be normalized.
 * *normalize* identifies whether normalize was carried out on the Search Argument.

Observations:
 * Normalizing the search argument results in a significant performance penalty given the explosion of the operator tree
 ** In case where an AND includes 8 ORs, the unnormalized version is faster by *97.32%*",2021-01-25T22:25:22.164+0000,2021-09-21T20:44:14.659+0000,Fixed,Major
YETUS-538,set capacity on ArrayList init where possible,YETUS,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12512353'>],"we have one case of not giving the ArrayList constructor an initial capacity

{code}

Busbey-MBA:yetus busbey$ git grep ""new ArrayList""
audience-annotations-component/audience-annotations/src/main/java/org/apache/yetus/audience/tools/RootDocProcessor.java:      List<Object> list = new ArrayList<Object>(array.length);
audience-annotations-component/audience-annotations/src/main/java/org/apache/yetus/audience/tools/StabilityOptions.java:    List<String[]> optionsList = new ArrayList<String[]>();
Busbey-MBA:yetus busbey$ 
{code}

{{StabilityOptions}} should be updated to use {{options.length}}",2017-08-19T04:57:30.137+0000,2017-10-27T04:47:07.236+0000,Fixed,Minor
INFRA-20177,Github PR links are not linked automatically to jira tickets anymore,INFRA,Bug,Closed,[],2,"[<JIRA IssueLink: id='12587012'>, <JIRA IssueLink: id='12586988'>]","we had github pr links automatically added to all HIVE jiras after a few minutes it was opened (which is very handy)

and it was working fine until a few days ago....when it stopped creating PR links

is there something I should do - or something have stopped working in the background?

I think the last one which had a github pr auto-created was this:
https://issues.apache.org/jira/browse/HIVE-23235

this was the first one which didn't have it (so I've created it)
https://issues.apache.org/jira/browse/HIVE-23031


and the most recent one doesn't have either
https://issues.apache.org/jira/browse/HIVE-23269
corresponding PR:
https://github.com/apache/hive/pull/992

",2020-04-23T12:05:58.530+0000,2020-05-04T23:24:49.564+0000,Fixed,Major
INFRA-17597,Move HBase repositories to gitbox,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12551580'>],"Please move hbase related repositories to gitbox. We believe the list of outstanding repos to move is:

* hbase
* hbase-site
* hbase-thirdparty

Here's the summary post for the DISCUSS thread where we came to consensus:

https://lists.apache.org/thread.html/af761fa316cf05f3bf95a4eacb6e80528e73e774b04bfb447c12945c@%3Cdev.hbase.apache.org%3E

",2019-01-09T16:54:41.257+0000,2019-01-09T21:00:36.317+0000,Fixed,Major
AMBARI-16963,Hive View: Should use hive jdbc to connect to hiveserver2,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12474297'>],,2016-05-31T10:31:58.582+0000,2016-07-09T02:30:16.414+0000,Fixed,Major
TEZ-2710,TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION should be configurable at vertex level,TEZ,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12474061'>, <JIRA IssueLink: id='12433971'>]","  Was trying to set TezConfiguration.TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION used by WeightedScalingMemoryDistributor at vertex level.  But it failed with

""tez.task.scale.memory.reserve-fraction is set at the scope of VERTEX, but it is only valid in the scope of AM""

 Even otherwise [~sseth] was telling me that vertex.setConf() does not work as it is supposed to and not taken into account with container reuse.",2015-08-11T19:31:04.663+0000,2016-07-07T16:45:27.945+0000,Duplicate,Major
SAMZA-74,Use anti-affinity in YARN container requests,SAMZA,Wish,Open,[],3,"[<JIRA IssueLink: id='12380811'>, <JIRA IssueLink: id='12377827'>, <JIRA IssueLink: id='12486847'>]","Samza's YARN containers are currently allocated according to whatever default strategy the YARN RM uses. Sometimes, we get all of our containers allocated to a single NM, even when many NMs have available resources.

YARN is in the process of getting an anti-affinity feature (YARN-1042). When this is implemented, we should consider using it to prevent all the containers from ending up on a single host.

NOTE: This anti-affinity feature might conflict with a colocate-processor-with-data feature, if we were trying to colocate stream processors near their data.",2013-10-31T22:25:41.607+0000,2016-11-17T21:36:25.368+0000,,Major
CASSANDRA-9340,"Cassandra Hive throws ""Unable to find partitioner class 'org.apache.cassandra.dht.Murmur3Partitioner'""",CASSANDRA,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12424320'>, <JIRA IssueLink: id='12468336'>]","Using Hive trying to execute select statement on cassandra, but it throws error:
hive> select * from genericquantity;
OK
Failed with exception java.io.IOException:java.lang.RuntimeException: org.apache.cassandra.exceptions.ConfigurationException: Unable to find partitioner class 'org.apache.cassandra.dht.Murmur3Partitioner'
Time taken: 0.518 seconds
",2015-05-11T04:52:45.600+0000,2019-04-16T09:31:11.950+0000,Invalid,Normal
MRUNIT-86,"Configuration not passed to individual mappers/reducers in mapred MapReducerDriver, PipelineMapReduceDriver",MRUNIT,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12349302'>, <JIRA IssueLink: id='12349299'>]","The Configuration object is not passed to individual mappers/reducers in mapred MapReducerDriver, PipelineMapReduceDriver",2012-03-14T06:04:36.669+0000,2012-03-15T08:13:00.738+0000,Fixed,Minor
SPARK-30905,Execute the TIMESTAMP roadmap,SPARK,Task,Open,[],3,"[<JIRA IssueLink: id='12605366'>, <JIRA IssueLink: id='12605367'>, <JIRA IssueLink: id='12605364'>]","This issue is intended for tracking the addition and/or alteration of different TIMESTAMP types in order to eventually reach the desired state as specified in the [design doc|https://docs.google.com/document/d/1gNRww9mZJcHvUDCXklzjFEQGpefsuR_akCDfWsdE35Q/edit] for TIMESTAMP types.

It's a sister issue to HIVE-21348 & IMPALA-9408 - I found no comparable issue for Spark (and I was hoping to find out the status of this roadmap on the Spark-side) - and related to SPARK-26797.",2020-02-20T19:27:26.660+0000,2020-12-22T20:54:42.932+0000,,Major
FLINK-12623,Flink on yarn encountered AMRMClientImpl does not update AMRM token properly,FLINK,Bug,Closed,[],1,[<JIRA IssueLink: id='12565525'>],"Hi, all! When my task running on yarn dependency flink verison is 1.7.1, I encountered the following problem:
{panel}
[org.apache.hadoop.ipc.Client ] >>> msg=Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1547520251214_0582_000001sg [2019-05-25 03:31:04.163] [WARN] [AMRM Heartbeater thread] [org.apache.hadoop.ipc.Client ] >>> msg=Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1547520251214_0582_000001sg [2019-05-25 03:31:04.168] [INFO] [AMRM Heartbeater thread] [org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider] >>> msg=Failing over to rm1sg [2019-05-25 03:31:04.168] [INFO] [AMRM Heartbeater thread] [org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider] >>> msg=Failing over to rm1sg [2019-05-25 03:31:30.143] [INFO] [AMRM Heartbeater thread] [org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider] >>> msg=Failing over to rm2sg [2019-05-25 03:31:30.143] [INFO] [AMRM Heartbeater thread] [org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider] >>> msg=Failing over to rm2sg
{panel}
Refering to the yarn issue[AMRMClientImpl does not update AMRM token properly| https://issues.apache.org/jira/browse/YARN-3103] , I know this problem is 2.4.1 version yarn bug, this bug is fixed in 2.7.0 verison yarn. However, flink shaded hadoop version is 2.4.1, so I think this bug, flink should shade 2.7.0 version hadoop or a higher version, is my idea correct?

 ",2019-05-25T06:35:49.982+0000,2019-07-16T10:48:07.155+0000,Invalid,Major
THRIFT-64,ServerSocket should have the option to bind to a specific IP instead of all IPs,THRIFT,New Feature,Closed,[],1,[<JIRA IssueLink: id='12320761'>],"Sometimes, you only want to bind to a single interface. Java's ServerSocket supports it, so it's just a matter of changing the constructor to support it.",2008-06-26T16:58:07.617+0000,2011-11-01T02:54:28.618+0000,Fixed,Minor
YETUS-631,Add new command line parameter to define custom mvn argument,YETUS,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12532914'>, <JIRA IssueLink: id='12549184'>]","The HDDS subproject is turned off by default in Hadoop source tree. To make a workable Precommit-HDDS-Build we need a possibility to defined additional mvn arguments from the command line.

(I tried to use .mvn/maven.config but the current build forces a git clean build. Environment variable could not been used in dockerized environment )",2018-04-30T11:23:14.745+0000,2019-02-25T09:57:06.538+0000,Won't Fix,Major
OOZIE-1040,Revert OOZIE-1029 once YARN-140 is part of a Hadoop release,OOZIE,Bug,Open,[],2,"[<JIRA IssueLink: id='12359551'>, <JIRA IssueLink: id='12359550'>]",Revert OOZIE-1029 once YARN-140 is part of a Hadoop release.  OOZIE-1029 adds two properties related to Yarn that won't be needed once YARN-140 is in.  ,2012-10-26T23:23:51.330+0000,2016-08-03T18:40:12.712+0000,,Trivial
BIGTOP-2333,Make Apache Pig work with HBase 1.1 on Hadoop 2,BIGTOP,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12457356'>, <JIRA IssueLink: id='12457357'>]","To make Pig support HBase 1.1, we need a patch from PIG-4728   ",2016-02-16T06:52:24.235+0000,2017-03-29T18:45:57.866+0000,Fixed,Major
HDDS-7132,GetFileStatus returns NULL for some paths,HDDS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12645857'>],"Explored while integrating with Hive.

GetFileStatus by FS Javadoc, either returns the FileStatus or FNF if the path doesn't exist, or IOE in other cases.

Hive checks whether the WH path exists and is a directory by calling getFileStatus(), on FNF it goes and creates the directory.

But due to Ozone returning NULL, It leads to an NPE, and there is no way to find out the reason for NULL, but to debug.",2022-08-17T15:06:57.318+0000,2022-08-25T12:40:35.336+0000,Fixed,Major
TEZ-1278,TezClient#waitTillReady() should not swallow interrupts,TEZ,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12394007'>, <JIRA IssueLink: id='12392885'>]","Current code is:
{code}
  while (true) {
      TezAppMasterStatus status = getAppMasterStatus();
      if (status.equals(TezAppMasterStatus.SHUTDOWN)) {
        throw new SessionNotRunning(""TezSession has already shutdown"");
      }
      if (status.equals(TezAppMasterStatus.READY)) {
        return;
      }
      try {
        Thread.sleep(SLEEP_FOR_READY);
      } catch (InterruptedException e) {
        LOG.info(""Sleep interrupted"", e);
        continue;
      }
    }
{code}
That way you never can stop the wait call since all interrupts are caught and the wait logic just happily proceeds.

*Suggestion*: InterruptedException could be part of the method signature so the caller can handle this in a way which is adequate to the context.

Nice read on handling interrupts: http://www.ibm.com/developerworks/library/j-jtp05236/
",2014-07-15T09:38:27.340+0000,2014-09-06T01:35:18.405+0000,Fixed,Blocker
AVRO-432,add @Nullable annotation to specify union with null,AVRO,New Feature,Closed,[],1,[<JIRA IssueLink: id='12330488'>],"HDFS now includes a class (HdfsFileStatus) with a field (symlink) whose type is byte[] but which is null for non-symlinks.  For most other types, we could annotate this with something like @Union({Void.class, Foo.class}) to declare that the field may be null.  But there is unfortunately no way to refer to the class of byte[] in an annotation.  So, instead, I propose to add an annotation that permits the specification of an arbitrary schema.  In this case, the annotation would then be @AvroSchema(""[\""null\"",\bytes\"""").",2010-02-26T00:12:47.943+0000,2010-03-22T23:19:15.838+0000,Fixed,Major
YETUS-570,Report and optionally kill stale JVMs between unit test modules,YETUS,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12519025'>, <JIRA IssueLink: id='12518738'>]","YETUS-561  does a great job of preventing fork bombs and stale processes from destroying machines.  However, Yetus should do a better job of:

a) Reporting when that happens
b) Preventing that from happening

",2017-10-27T14:24:12.079+0000,2017-11-03T03:15:48.041+0000,Fixed,Major
PHOENIX-2033,PQS log environment details on launch,PHOENIX,Improvement,Closed,[],1,[<JIRA IssueLink: id='12427495'>],Follow HBase's lead and include environment details in log at process launch. There's already a handy utility for this in {{ServerCommandLine}}.,2015-06-09T23:37:13.389+0000,2015-11-21T02:17:44.635+0000,Fixed,Major
PHOENIX-3116,Support incompatible HBase 1.1.5 and HBase 1.2.2,PHOENIX,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12475956'>, <JIRA IssueLink: id='12479229'>, <JIRA IssueLink: id='12477777'>]",HBase 1.2.2 made a backwards incompatible change in HTableInterface that requires new overrides.,2016-07-25T17:06:26.599+0000,2019-11-19T16:44:36.659+0000,Won't Fix,Minor
DAEMON-364,Latest RHEL kernel update crashes jsvc,DAEMON,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12507268'>, <JIRA IssueLink: id='12507488'>]","After applying the latest RHEL kernel updates (CVE-2017-1000364), all jsvc instance crash on start up with the following:

 {quote}#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0x7) at pc=0x00007f0fcea07bdc, pid=28581, tid=0x00007f0fdfc0c700
#
# JRE version:  (8.0_121-b13) (build )
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# j  java.lang.Object.<clinit>()V+0
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# //hs_err_pid28581.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp{quote}

Manually starting the service does not exhibit the problem.",2017-06-21T20:51:26.927+0000,2017-07-04T08:47:00.616+0000,Duplicate,Major
PHOENIX-5881,Port MaxLookbackAge logic to 5.x,PHOENIX,Improvement,Closed,[],7,"[<JIRA IssueLink: id='12595224'>, <JIRA IssueLink: id='12595222'>, <JIRA IssueLink: id='12602817'>, <JIRA IssueLink: id='12596793'>, <JIRA IssueLink: id='12596794'>, <JIRA IssueLink: id='12597383'>, <JIRA IssueLink: id='12587404'>]","PHOENIX-5645 wasn't included in the master (5.x) branch because an HBase 2.x change prevented the logic from being useful in the case of deletes, since HBase 2.x no longer allows us to show deleted cells on an SCN query before the point of deletion. Unfortunately, PHOENIX-5645 wound up requiring a lot of follow-up work in the IndexTool and IndexScrutinyTool to deal with its implications, and because of that, the 4.x and 5.x codebases around indexes have diverged a good bit. 

This work item is to get them back in sync, even though the behavior in the face of deletes will be somewhat different, and so most likely some tests will have to be changed or Ignored. ",2020-05-01T17:09:23.032+0000,2021-02-10T10:01:04.233+0000,Fixed,Blocker
DATAFU-6,MonitoredUDF annotation does not work with AliasableEvalFunc,DATAFU,Bug,Closed,[],2,"[<JIRA IssueLink: id='12483599'>, <JIRA IssueLink: id='12484370'>]","This was reported by seregasheypak on GitHub (https://github.com/linkedin/datafu/issues/89).  We were able to reproduce this by adding the annotation to BagLeftOuterJoin and running its tests.  Simply adding the annotation causes problems.  In ContextualEvalFunc.getContextProperties, the properties retrieved for the class are empty.

seregasheypak:

Hi, If I use

{code}
@MonitoredUDF(timeUnit = TimeUnit.MINUTES, duration = 10, errorCallback = NplRecMatcherErrorCallback.class)
class NplRecFirstLevelMatcher extends AliasableEvalFunc<Tuple> implements DebuggableUDF{
//some cool stuff goes here!
}
{code}

I do get exception:
{noformat}
14/01/15 23:52:52 ERROR udf.NplRecFirstLevelMatcher: Class: class NplRecFirstLevelMatcher
14/01/15 23:52:52 ERROR udf.NplRecFirstLevelMatcher: Instance name: 30
14/01/15 23:52:52 ERROR udf.NplRecFirstLevelMatcher: Properties: {30={}}
*** ***A debug output from my handler method***  ***
NplRecMatcherErrorCallback.handleError

null
ERROR: java.lang.RuntimeException: Could not retrieve aliases from properties using aliasMap
java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not retrieve aliases from properties using aliasMap
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:232)
	at java.util.concurrent.FutureTask.get(FutureTask.java:91)
	at com.google.common.util.concurrent.ForwardingFuture.get(ForwardingFuture.java:69)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.MonitoredUDFExecutor.monitorExec(MonitoredUDFExecutor.java:183)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:335)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:376)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:354)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:372)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:297)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:241)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:95)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:465)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:433)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:413)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:164)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:610)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:444)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:449)
Caused by: java.lang.RuntimeException: Could not retrieve aliases from properties using aliasMap
	at datafu.pig.util.AliasableEvalFunc.getFieldAliases(AliasableEvalFunc.java:164)
	at datafu.pig.util.AliasableEvalFunc.getPosition(AliasableEvalFunc.java:171)
	at datafu.pig.util.AliasableEvalFunc.getBag(AliasableEvalFunc.java:253)
	at datafu.pig.util.AliasableEvalFunc$getBag.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:145)
	at NplRecFirstLevelMatcher.exec(NplRecFirstLevelMatcher.groovy:53)
	at NplRecFirstLevelMatcher.exec(NplRecFirstLevelMatcher.groovy)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.MonitoredUDFExecutor$1.apply(MonitoredUDFExecutor.java:95)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.MonitoredUDFExecutor$1.apply(MonitoredUDFExecutor.java:91)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.MonitoredUDFExecutor$2.call(MonitoredUDFExecutor.java:164)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

Exception happens on line:

{code}
itemsBag = getBag(input, ORDERED)
{code}

If I put away annotation @MonitoredUDF, it works fine, tests are passed.",2014-01-15T21:06:07.907+0000,2016-10-26T19:51:52.282+0000,Fixed,Major
CALCITE-1017,hive.mapred.mode=strict throws an error even if the final plan does not have cartesian product in it.,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12451520'>],"{code}
Vertex dependency in root stage
Reducer 10 <- Reducer 9 (SIMPLE_EDGE)
Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 11 (SIMPLE_EDGE)
Reducer 3 <- Map 12 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)
Reducer 4 <- Map 13 (SIMPLE_EDGE), Reducer 3 (SIMPLE_EDGE)
Reducer 5 <- Map 14 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)
Reducer 6 <- Map 15 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)
Reducer 7 <- Map 16 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE)
Reducer 8 <- Map 17 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)
Reducer 9 <- Reducer 8 (SIMPLE_EDGE)

Stage-0
   Fetch Operator
      limit:100
      Stage-1
         Reducer 10
         File Output Operator [FS_63]
            compressed:false
            Statistics:Num rows: 100 Data size: 143600 Basic stats: COMPLETE Column stats: NONE
            table:{""input format:"":""org.apache.hadoop.mapred.TextInputFormat"",""output format:"":""org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"",""serde:"":""org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe""}
            Limit [LIM_62]
               Number of rows:100
               Statistics:Num rows: 100 Data size: 143600 Basic stats: COMPLETE Column stats: NONE
               Select Operator [SEL_61]
               |  outputColumnNames:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col6"",""_col7"",""_col8"",""_col9"",""_col10"",""_col11"",""_col12"",""_col13"",""_col14""]
               |  Statistics:Num rows: 127050 Data size: 182479129 Basic stats: COMPLETE Column stats: NONE
               |<-Reducer 9 [SIMPLE_EDGE]
                  Reduce Output Operator [RS_60]
                     key expressions:_col0 (type: string), _col1 (type: string), _col2 (type: string)
                     sort order:+++
                     Statistics:Num rows: 127050 Data size: 182479129 Basic stats: COMPLETE Column stats: NONE
                     value expressions:_col3 (type: bigint), _col4 (type: double), _col5 (type: double), _col6 (type: double), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: bigint), _col12 (type: double), _col13 (type: double)
                     Select Operator [SEL_58]
                        outputColumnNames:[""_col0"",""_col1"",""_col10"",""_col11"",""_col12"",""_col13"",""_col2"",""_col3"",""_col4"",""_col5"",""_col6"",""_col7"",""_col8"",""_col9""]
                        Statistics:Num rows: 127050 Data size: 182479129 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator [GBY_57]
                        |  aggregations:[""count(VALUE._col0)"",""avg(VALUE._col1)"",""stddev_samp(VALUE._col2)"",""count(VALUE._col3)"",""avg(VALUE._col4)"",""stddev_samp(VALUE._col5)"",""count(VALUE._col6)"",""avg(VALUE._col7)"",""stddev_samp(VALUE._col8)""]
                        |  keys:KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string)
                        |  outputColumnNames:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col6"",""_col7"",""_col8"",""_col9"",""_col10"",""_col11""]
                        |  Statistics:Num rows: 127050 Data size: 182479129 Basic stats: COMPLETE Column stats: NONE
                        |<-Reducer 8 [SIMPLE_EDGE]
                           Reduce Output Operator [RS_56]
                              key expressions:_col0 (type: string), _col1 (type: string), _col2 (type: string)
                              Map-reduce partition columns:_col0 (type: string), _col1 (type: string), _col2 (type: string)
                              sort order:+++
                              Statistics:Num rows: 254100 Data size: 364958258 Basic stats: COMPLETE Column stats: NONE
                              value expressions:_col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,input:int>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: bigint), _col7 (type: struct<count:bigint,sum:double,input:int>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: bigint), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: struct<count:bigint,sum:double,variance:double>)
                              Group By Operator [GBY_55]
                                 aggregations:[""count(_col5)"",""avg(_col5)"",""stddev_samp(_col5)"",""count(_col10)"",""avg(_col10)"",""stddev_samp(_col10)"",""count(_col14)"",""avg(_col14)"",""stddev_samp(_col14)""]
                                 keys:_col22 (type: string), _col24 (type: string), _col25 (type: string)
                                 outputColumnNames:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col6"",""_col7"",""_col8"",""_col9"",""_col10"",""_col11""]
                                 Statistics:Num rows: 254100 Data size: 364958258 Basic stats: COMPLETE Column stats: NONE
                                 Select Operator [SEL_54]
                                    outputColumnNames:[""_col22"",""_col24"",""_col25"",""_col5"",""_col10"",""_col14""]
                                    Statistics:Num rows: 254100 Data size: 364958258 Basic stats: COMPLETE Column stats: NONE
                                    Merge Join Operator [MERGEJOIN_113]
                                    |  condition map:[{"""":""Inner Join 0 to 1""}]
                                    |  keys:{""0"":""_col1 (type: int)"",""1"":""_col0 (type: int)""}
                                    |  outputColumnNames:[""_col5"",""_col10"",""_col14"",""_col22"",""_col24"",""_col25""]
                                    |  Statistics:Num rows: 254100 Data size: 364958258 Basic stats: COMPLETE Column stats: NONE
                                    |<-Map 17 [SIMPLE_EDGE]
                                    |  Reduce Output Operator [RS_52]
                                    |     key expressions:_col0 (type: int)
                                    |     Map-reduce partition columns:_col0 (type: int)
                                    |     sort order:+
                                    |     Statistics:Num rows: 231000 Data size: 331780228 Basic stats: COMPLETE Column stats: NONE
                                    |     value expressions:_col1 (type: string), _col2 (type: string)
                                    |     Select Operator [SEL_18]
                                    |        outputColumnNames:[""_col0"",""_col1"",""_col2""]
                                    |        Statistics:Num rows: 231000 Data size: 331780228 Basic stats: COMPLETE Column stats: NONE
                                    |        Filter Operator [FIL_106]
                                    |           predicate:i_item_sk is not null (type: boolean)
                                    |           Statistics:Num rows: 231000 Data size: 331780228 Basic stats: COMPLETE Column stats: NONE
                                    |           TableScan [TS_17]
                                    |              alias:item
                                    |              Statistics:Num rows: 462000 Data size: 663560457 Basic stats: COMPLETE Column stats: NONE
                                    |<-Reducer 7 [SIMPLE_EDGE]
                                       Reduce Output Operator [RS_50]
                                          key expressions:_col1 (type: int)
                                          Map-reduce partition columns:_col1 (type: int)
                                          sort order:+
                                          Statistics:Num rows: 26735 Data size: 29919145 Basic stats: COMPLETE Column stats: NONE
                                          value expressions:_col5 (type: int), _col10 (type: int), _col14 (type: int), _col22 (type: string)
                                          Merge Join Operator [MERGEJOIN_112]
                                          |  condition map:[{"""":""Inner Join 0 to 1""}]
                                          |  keys:{""0"":""_col3 (type: int)"",""1"":""_col0 (type: int)""}
                                          |  outputColumnNames:[""_col1"",""_col5"",""_col10"",""_col14"",""_col22""]
                                          |  Statistics:Num rows: 26735 Data size: 29919145 Basic stats: COMPLETE Column stats: NONE
                                          |<-Map 16 [SIMPLE_EDGE]
                                          |  Reduce Output Operator [RS_47]
                                          |     key expressions:_col0 (type: int)
                                          |     Map-reduce partition columns:_col0 (type: int)
                                          |     sort order:+
                                          |     Statistics:Num rows: 852 Data size: 1628138 Basic stats: COMPLETE Column stats: NONE
                                          |     value expressions:_col1 (type: string)
                                          |     Select Operator [SEL_16]
                                          |        outputColumnNames:[""_col0"",""_col1""]
                                          |        Statistics:Num rows: 852 Data size: 1628138 Basic stats: COMPLETE Column stats: NONE
                                          |        Filter Operator [FIL_105]
                                          |           predicate:s_store_sk is not null (type: boolean)
                                          |           Statistics:Num rows: 852 Data size: 1628138 Basic stats: COMPLETE Column stats: NONE
                                          |           TableScan [TS_15]
                                          |              alias:store
                                          |              Statistics:Num rows: 1704 Data size: 3256276 Basic stats: COMPLETE Column stats: NONE
                                          |<-Reducer 6 [SIMPLE_EDGE]
                                             Reduce Output Operator [RS_45]
                                                key expressions:_col3 (type: int)
                                                Map-reduce partition columns:_col3 (type: int)
                                                sort order:+
                                                Statistics:Num rows: 24305 Data size: 27199223 Basic stats: COMPLETE Column stats: NONE
                                                value expressions:_col1 (type: int), _col5 (type: int), _col10 (type: int), _col14 (type: int)
                                                Merge Join Operator [MERGEJOIN_111]
                                                |  condition map:[{"""":""Inner Join 0 to 1""}]
                                                |  keys:{""0"":""_col11 (type: int)"",""1"":""_col0 (type: int)""}
                                                |  outputColumnNames:[""_col1"",""_col3"",""_col5"",""_col10"",""_col14""]
                                                |  Statistics:Num rows: 24305 Data size: 27199223 Basic stats: COMPLETE Column stats: NONE
                                                |<-Map 15 [SIMPLE_EDGE]
                                                |  Reduce Output Operator [RS_42]
                                                |     key expressions:_col0 (type: int)
                                                |     Map-reduce partition columns:_col0 (type: int)
                                                |     sort order:+
                                                |     Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                |     Select Operator [SEL_14]
                                                |        outputColumnNames:[""_col0""]
                                                |        Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                |        Filter Operator [FIL_104]
                                                |           predicate:((d_quarter_name) IN ('2000Q1', '2000Q2', '2000Q3') and d_date_sk is not null) (type: boolean)
                                                |           Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                |           TableScan [TS_12]
                                                |              alias:d1
                                                |              Statistics:Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: NONE
                                                |<-Reducer 5 [SIMPLE_EDGE]
                                                   Reduce Output Operator [RS_40]
                                                      key expressions:_col11 (type: int)
                                                      Map-reduce partition columns:_col11 (type: int)
                                                      sort order:+
                                                      Statistics:Num rows: 22096 Data size: 24726566 Basic stats: COMPLETE Column stats: NONE
                                                      value expressions:_col1 (type: int), _col3 (type: int), _col5 (type: int), _col10 (type: int), _col14 (type: int)
                                                      Merge Join Operator [MERGEJOIN_110]
                                                      |  condition map:[{"""":""Inner Join 0 to 1""}]
                                                      |  keys:{""0"":""_col6 (type: int)"",""1"":""_col0 (type: int)""}
                                                      |  outputColumnNames:[""_col1"",""_col3"",""_col5"",""_col10"",""_col11"",""_col14""]
                                                      |  Statistics:Num rows: 22096 Data size: 24726566 Basic stats: COMPLETE Column stats: NONE
                                                      |<-Map 14 [SIMPLE_EDGE]
                                                      |  Reduce Output Operator [RS_37]
                                                      |     key expressions:_col0 (type: int)
                                                      |     Map-reduce partition columns:_col0 (type: int)
                                                      |     sort order:+
                                                      |     Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                      |     Select Operator [SEL_11]
                                                      |        outputColumnNames:[""_col0""]
                                                      |        Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                      |        Filter Operator [FIL_103]
                                                      |           predicate:((d_quarter_name) IN ('2000Q1', '2000Q2', '2000Q3') and d_date_sk is not null) (type: boolean)
                                                      |           Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                      |           TableScan [TS_9]
                                                      |              alias:d1
                                                      |              Statistics:Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: NONE
                                                      |<-Reducer 4 [SIMPLE_EDGE]
                                                         Reduce Output Operator [RS_35]
                                                            key expressions:_col6 (type: int)
                                                            Map-reduce partition columns:_col6 (type: int)
                                                            sort order:+
                                                            Statistics:Num rows: 20088 Data size: 22478696 Basic stats: COMPLETE Column stats: NONE
                                                            value expressions:_col1 (type: int), _col3 (type: int), _col5 (type: int), _col10 (type: int), _col11 (type: int), _col14 (type: int)
                                                            Merge Join Operator [MERGEJOIN_109]
                                                            |  condition map:[{"""":""Inner Join 0 to 1""}]
                                                            |  keys:{""0"":""_col0 (type: int)"",""1"":""_col0 (type: int)""}
                                                            |  outputColumnNames:[""_col1"",""_col3"",""_col5"",""_col6"",""_col10"",""_col11"",""_col14""]
                                                            |  Statistics:Num rows: 20088 Data size: 22478696 Basic stats: COMPLETE Column stats: NONE
                                                            |<-Map 13 [SIMPLE_EDGE]
                                                            |  Reduce Output Operator [RS_32]
                                                            |     key expressions:_col0 (type: int)
                                                            |     Map-reduce partition columns:_col0 (type: int)
                                                            |     sort order:+
                                                            |     Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                            |     Select Operator [SEL_8]
                                                            |        outputColumnNames:[""_col0""]
                                                            |        Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                            |        Filter Operator [FIL_102]
                                                            |           predicate:((d_quarter_name = '2000Q1') and d_date_sk is not null) (type: boolean)
                                                            |           Statistics:Num rows: 18262 Data size: 20435178 Basic stats: COMPLETE Column stats: NONE
                                                            |           TableScan [TS_6]
                                                            |              alias:d1
                                                            |              Statistics:Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: NONE
                                                            |<-Reducer 3 [SIMPLE_EDGE]
                                                               Reduce Output Operator [RS_30]
                                                                  key expressions:_col0 (type: int)
                                                                  Map-reduce partition columns:_col0 (type: int)
                                                                  sort order:+
                                                                  Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                  value expressions:_col1 (type: int), _col3 (type: int), _col5 (type: int), _col6 (type: int), _col10 (type: int), _col11 (type: int), _col14 (type: int)
                                                                  Merge Join Operator [MERGEJOIN_108]
                                                                  |  condition map:[{"""":""Inner Join 0 to 1""}]
                                                                  |  keys:{""0"":""_col8 (type: int), _col7 (type: int)"",""1"":""_col1 (type: int), _col2 (type: int)""}
                                                                  |  outputColumnNames:[""_col0"",""_col1"",""_col3"",""_col5"",""_col6"",""_col10"",""_col11"",""_col14""]
                                                                  |  Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                  |<-Map 12 [SIMPLE_EDGE]
                                                                  |  Reduce Output Operator [RS_27]
                                                                  |     key expressions:_col1 (type: int), _col2 (type: int)
                                                                  |     Map-reduce partition columns:_col1 (type: int), _col2 (type: int)
                                                                  |     sort order:++
                                                                  |     Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                  |     value expressions:_col0 (type: int), _col3 (type: int)
                                                                  |     Select Operator [SEL_5]
                                                                  |        outputColumnNames:[""_col0"",""_col1"",""_col2"",""_col3""]
                                                                  |        Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                  |        Filter Operator [FIL_101]
                                                                  |           predicate:((cs_bill_customer_sk is not null and cs_item_sk is not null) and cs_sold_date_sk is not null) (type: boolean)
                                                                  |           Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                  |           TableScan [TS_4]
                                                                  |              alias:catalog_sales
                                                                  |              Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                  |<-Reducer 2 [SIMPLE_EDGE]
                                                                     Reduce Output Operator [RS_25]
                                                                        key expressions:_col8 (type: int), _col7 (type: int)
                                                                        Map-reduce partition columns:_col8 (type: int), _col7 (type: int)
                                                                        sort order:++
                                                                        Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                        value expressions:_col0 (type: int), _col1 (type: int), _col3 (type: int), _col5 (type: int), _col6 (type: int), _col10 (type: int)
                                                                        Merge Join Operator [MERGEJOIN_107]
                                                                        |  condition map:[{"""":""Inner Join 0 to 1""}]
                                                                        |  keys:{""0"":""_col2 (type: int), _col1 (type: int), _col4 (type: int)"",""1"":""_col2 (type: int), _col1 (type: int), _col3 (type: int)""}
                                                                        |  outputColumnNames:[""_col0"",""_col1"",""_col3"",""_col5"",""_col6"",""_col7"",""_col8"",""_col10""]
                                                                        |  Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                        |<-Map 1 [SIMPLE_EDGE]
                                                                        |  Reduce Output Operator [RS_20]
                                                                        |     key expressions:_col2 (type: int), _col1 (type: int), _col4 (type: int)
                                                                        |     Map-reduce partition columns:_col2 (type: int), _col1 (type: int), _col4 (type: int)
                                                                        |     sort order:+++
                                                                        |     Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                        |     value expressions:_col0 (type: int), _col3 (type: int), _col5 (type: int)
                                                                        |     Select Operator [SEL_1]
                                                                        |        outputColumnNames:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5""]
                                                                        |        Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                        |        Filter Operator [FIL_99]
                                                                        |           predicate:((((ss_customer_sk is not null and ss_item_sk is not null) and ss_ticket_number is not null) and ss_sold_date_sk is not null) and ss_store_sk is not null) (type: boolean)
                                                                        |           Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                        |           TableScan [TS_0]
                                                                        |              alias:store_sales
                                                                        |              Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                        |<-Map 11 [SIMPLE_EDGE]
                                                                           Reduce Output Operator [RS_22]
                                                                              key expressions:_col2 (type: int), _col1 (type: int), _col3 (type: int)
                                                                              Map-reduce partition columns:_col2 (type: int), _col1 (type: int), _col3 (type: int)
                                                                              sort order:+++
                                                                              Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                              value expressions:_col0 (type: int), _col4 (type: int)
                                                                              Select Operator [SEL_3]
                                                                                 outputColumnNames:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4""]
                                                                                 Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                                 Filter Operator [FIL_100]
                                                                                    predicate:(((sr_customer_sk is not null and sr_item_sk is not null) and sr_ticket_number is not null) and sr_returned_date_sk is not null) (type: boolean)
                                                                                    Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                                                                                    TableScan [TS_2]
                                                                                       alias:store_returns
                                                                                       Statistics:Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
{code}

The query is :
{code}
 explain select i_item_desc ,i_category ,i_class ,i_current_price ,i_item_id ,sum(ws_ext_sales_price) as itemrevenue ,sum(ws_ext_sales_price)*100/sum(sum(ws_ext_sales_price)) over (partition by i_class) as revenueratio from web_sales ,item ,date_dim where web_sales.ws_item_sk = item.i_item_sk and item.i_category in ('Jewelry', 'Sports', 'Books') and web_sales.ws_sold_date_sk = date_dim.d_date_sk and date_dim.d_date between '2001-01-12' and '2001-02-11' group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratio limit 100;
{code}

It seems that in SemanticAnalyzer.genJoinReduceSinkChild() we look for Join predicates only in 'ON' clause. If the join condition happens in 'WHERE' clause of the query, we aggressively throw an exception assuming this join is a cartesian product in strict mode. We should delay this check post physical optimizer until the plan is complete.",2015-12-10T20:36:46.097+0000,2015-12-10T21:26:47.465+0000,Invalid,Major
TEZ-4297,Hive CLI not working after upgrading from Oracle JDK 8u112 to 8u281 and have errors with TEZ,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12611286'>],"After upgrading Oracle JDK version from jdk-8u112 to jdk-8u281, Hive CLI is not working anymore and gives below error when logging in.
{code:java}
WARNING: Use ""yarn jar"" to launch YARN applications.
21/03/09 11:00:04 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not existLogging initialized using configuration in file:/etc/hive/2.4.3.0-227/0/hive-log4j.properties
Exception in thread ""main"" java.lang.RuntimeException: java.io.IOException: Previous writer likely failed to write hdfs://ppcontent-nn1.pp-content.dataplatform.com:8020/tmp/hive/hive/_tez_session_dir/96b21825-63f4-4316-9c43-20ebe641d9c9/hive-hcatalog-core.jar. Failing because I am unlikely to write too.
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:544)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:680)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.io.IOException: Previous writer likely failed to write hdfs://ppcontent-nn1.pp-content.dataplatform.com:8020/tmp/hive/hive/_tez_session_dir/96b21825-63f4-4316-9c43-20ebe641d9c9/hive-hcatalog-core.jar. Failing because I am unlikely to write too.
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982)
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862)
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFilesFromConf(DagUtils.java:805)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.refreshLocalResourcesFromConf(TezSessionState.java:233)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:158)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:117)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:541)
        ... 8 more
{code}
Version we are using:
 * Ambari 2.2.2
 * Hive 1.2.1
 * Hadoop 2.7
 * Spark 1.6
 * HDP 2.4
 * Tez 0.7.0.2.4",2021-03-23T10:19:32.231+0000,2021-04-30T16:00:26.596+0000,,Major
HDDS-550,Serialize ApplyTransaction calls per Container in ContainerStateMachine,HDDS,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12545095'>, <JIRA IssueLink: id='12560587'>]","As part of handling Node failures in Ozone, the block commit need to happen in order inside ContainerStateMachine per container. With RATIS-341, it is guaranteed that the  applyTransaction calls for committing the write chunks will be initiated only when the WriteStateMachine data for write Chunk operations finish. 

This Jira is aimed at making all the applyTransaction operations inside ContainerStateMachine serial per container with a single thread Executor per container handling all applyTransactions calls.",2018-09-25T10:57:07.239+0000,2019-05-11T06:03:04.341+0000,Fixed,Major
ZOOKEEPER-3601,introduce the fault injection framework: Byteman for ZooKeeper,ZOOKEEPER,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12573199'>, <JIRA IssueLink: id='12573198'>, <JIRA IssueLink: id='12609249'>]",[https://www.datastax.com/blog/2016/02/cassandra-unit-testing-byteman],2019-11-01T08:18:30.393+0000,2021-05-29T12:54:01.499+0000,Fixed,Major
ORC-154,add OrcFile.WriterOptions.clone(),ORC,Improvement,Closed,[],1,[<JIRA IssueLink: id='12496961'>],,2017-03-10T23:10:27.371+0000,2017-05-08T17:22:36.625+0000,Fixed,Major
HCATALOG-87,Newly added partition should inherit table properties.,HCATALOG,New Feature,Closed,[],1,[<JIRA IssueLink: id='12347027'>],"create table tbl (a string) partition by (b string) stored as inputformat <if> outputformat <of> inputstoragedriver <ult> outputstoragedriver <osd>;
alter table add partition <part_spec>;
select * from tbl;

Currently results in an exception.",2011-08-19T18:22:53.726+0000,2012-05-17T01:20:22.762+0000,Fixed,Major
SLIDER-425,Build broken against 2.6.0-SNAPSHOT due to new abstract methods added by YARN-2229,SLIDER,Task,Resolved,[],1,[<JIRA IssueLink: id='12396750'>],"Based on latest 2.6.0-SNAPSHOT, compilation fails because of the following:
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project slider-core: Compilation failure: Compilation failure:
[ERROR] /homes/hortonzy/slider/slider-core/src/test/groovy/org/apache/slider/server/appmaster/model/mock/MockContainerId.groovy:[24,7] 1. ERROR in /homes/hortonzy/slider/slider-core/src/test/groovy/org/apache/slider/server/appmaster/model/mock/MockContainerId.groovy (at line 24)
[ERROR] class MockContainerId extends ContainerId implements Cloneable {
[ERROR] ^^^^^^^^^^^^^^^
[ERROR] Groovy:Can't have an abstract method in a non-abstract class. The class 'org.apache.slider.server.appmaster.model.mock.MockContainerId' must be declared abstract or the method 'void setContainerId(long)' must be implemented.
[ERROR]
[ERROR] /homes/hortonzy/slider/slider-core/src/test/groovy/org/apache/slider/server/appmaster/model/mock/MockContainerId.groovy:[24,7] 2. ERROR in /homes/hortonzy/slider/slider-core/src/test/groovy/org/apache/slider/server/appmaster/model/mock/MockContainerId.groovy (at line 24)
[ERROR] class MockContainerId extends ContainerId implements Cloneable {
[ERROR] ^^^^^^^^^^^^^^^
[ERROR] Groovy:Can't have an abstract method in a non-abstract class. The class 'org.apache.slider.server.appmaster.model.mock.MockContainerId' must be declared abstract or the method 'long getContainerId()' must be implemented.
[ERROR]
[ERROR] Found 2 errors and 0 warnings.
{code}

This was due to new abstract methods added by YARN-2229",2014-09-12T23:14:48.573+0000,2014-09-16T12:42:57.208+0000,Fixed,Major
CALCITE-2343,PushProjector with OVER expression causing infinite loop,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12618022'>],"When applying the {{ProjectJoinTransposeRule}} on a project with a Rex expression containing a RexOver call (using HepPlanner), this might cause an infinite loop (and ultimately a StackOverflowError exception) as PushProjector identify the RexOver call as to be push down under the join, but doesn't not rewrite correctly the top Project expression. ",2018-05-31T03:17:56.353+0000,2021-06-24T05:49:49.105+0000,Fixed,Major
SLIDER-81,Support placement of containers on labeled YARN nodes,SLIDER,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12388391'>],"once YARN-796 adds labelled placement, Slider should support adding label values to container requests.

If this is queue-wide it is automatic; if it is a hint in requests (with a queue default) then slider needs to support a {{yarn.node.label}} attribute in the resources json, and propagate it to requests",2014-05-20T11:26:14.019+0000,2014-10-16T21:06:07.682+0000,Fixed,Major
PHOENIX-1473,Connecting with Phoenix client when Phoenix is not deployed on region server(s) takes down region server(s).,PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12402744'>],"When attempting to connect with Phoenix client when Phoenix server not deployed will take down region servers. It looks like problem with creating SYSTEM.CATALOG table causing it. It may be argued that there some improvements can be done on HBase side in regards to not allow to create table if coprocessor jar files could not be found but from Phoenix side I would think before doing anything it should check if required jar is in place (meaning Phoenix deployed properly).

here is log from region server

2014-11-20 14:29:48,411 ERROR [RS_OPEN_REGION-dn01:60020-1] handler.OpenRegionHandler: Failed open of region=SYSTEM.CATALOG,,1416493787900.35b7b3a19f75688fce382e8f1323b4ae., starting to roll back the global memstore size.
java.io.IOException: Unable to load configured region split policy 'org.apache.phoenix.schema.MetaDataSplitPolicy' for table 'SYSTEM.CATALOG'
        at org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.getSplitPolicyClass(RegionSplitPolicy.java:121)
        at org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.create(RegionSplitPolicy.java:101)
        at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:727)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:684)
       at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4550)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4520)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4492)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4448)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4399)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:465)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:139)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassNotFoundException: org.apache.phoenix.schema.MetaDataSplitPolicy
        at java.net.URLClassLoader$1.run(Unknown Source)
        at java.net.URLClassLoader$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Unknown Source)
        at org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.getSplitPolicyClass(RegionSplitPolicy.java:117)
        ... 14 more
2014-11-20 14:29:48,411 INFO  [RS_OPEN_REGION-dn01:60020-1] handler.OpenRegionHandler: Opening of region {ENCODED => 35b7b3a19f75688fce382e8f1323b4ae, NAME => 'SYSTEM.CATALOG,,1416493787900.35b7b3a19f75688fce382e8f1323b4ae.', STARTKEY => '', ENDKEY => ''} failed, transitioning from OPENING to FAILED_OPEN in ZK, expecting version 4
2014-11-20 14:29:48,412 DEBUG [RS_OPEN_REGION-dn01:60020-1] zookeeper.ZKAssign: regionserver:60020-0x349cd39697a00e5, quorum=nn02.abc.com:2181,nn01.abc.com:2181,jz01.abc.com:2181, baseZNode=/hbase Transitioning 35b7b3a19f75688fce382e8f1323b4ae from RS_ZK_REGION_OPENING to RS_ZK_REGION_FAILED_OPEN
2014-11-20 14:29:48,414 DEBUG [RS_OPEN_REGION-dn01:60020-1] zookeeper.ZKAssign: regionserver:60020-0x349cd39697a00e5, quorum=nn02.abc.com:2181,nn01.abc.com:2181,z01.abc.com:2181, baseZNode=/hbase Transitioned node 35b7b3a19f75688fce382e8f1323b4ae from RS_ZK_REGION_OPENING to RS_ZK_REGION_FAILED_OPEN
2014-11-20 14:29:48,528 INFO  [regionserver60020] regionserver.HRegionServer: stopping server dn01.abc.com,60020,1416493546924; all regions closed.
2014-11-20 14:29:48,528 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-11-20 14:29:48,528 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-11-20 14:29:48,529 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-11-20 14:29:48,529 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-11-20 14:29:48,529 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-11-20 14:29:48,529 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-11-20 14:29:48,529 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-11-20 14:29:48,529 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-11-20 14:29:48,529 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-11-20 14:29:48,529 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-11-20 14:29:48,530 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-11-20 14:29:48,530 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-11-20 14:29:48,530 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-11-20 14:29:48,530 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-11-20 14:29:48,530 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://mycluster/hbase/WALs/dn01.abc.com,60020,1416493546924
2014-11-20 14:29:48,547 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-11-20 14:29:48,547 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-11-20 14:29:48,915 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-11-20 14:29:48,915 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...
2014-11-20 14:29:48,915 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...
2014-11-20 14:29:48,915 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...
2014-11-20 14:29:48,916 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...
2014-11-20 14:29:48,920 INFO  [regionserver60020] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x349cd39697a00e6
2014-11-20 14:29:48,922 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-11-20 14:29:48,922 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x349cd39697a00e6 closed
2014-11-20 14:29:48,922 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-11-20 14:29:48,922 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-11-20 14:29:48,926 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-11-20 14:29:48,926 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x349cd39697a00e5 closed
2014-11-20 14:29:48,926 INFO  [regionserver60020] regionserver.HRegionServer: stopping server dn01.abc.com,60020,1416493546924; zookeeper connection closed.
2014-11-20 14:29:48,926 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-11-20 14:29:48,926 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2422)
2014-11-20 14:29:48,928 INFO  [Thread-9] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer@3e44f2a5
2014-11-20 14:29:48,928 INFO  [Thread-9] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-11-20 14:29:48,930 INFO  [Thread-9] regionserver.ShutdownHook: Shutdown hook finished.
 
",2014-11-20T15:53:16.636+0000,2014-12-04T21:42:54.158+0000,Won't Fix,Major
TEZ-4350,"Remove synchronized  from DAGAppMaster.serviceInit, serviceStart, serviceStop",TEZ,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12626420'>, <JIRA IssueLink: id='12631405'>, <JIRA IssueLink: id='12631404'>, <JIRA IssueLink: id='12631406'>]","according to AbstractService.serviceInit javadoc
{code}
   * This method will only ever be called once during the lifecycle of
   * a specific service instance.
   *
   * Implementations do not need to be synchronized as the logic
   * in {@link #init(Configuration)} prevents re-entrancy.
{code}

moreover, it generates findbugs alerts for every field that is accessed somewhere else in DAGAppMaster in an unsynchronized fashion:

{code}
Code	Warning
IS	Inconsistent synchronization of org.apache.tez.dag.app.DAGAppMaster.webUIService; locked 57% of time
Bug type IS2_INCONSISTENT_SYNC (click for details)
In class org.apache.tez.dag.app.DAGAppMaster
Field org.apache.tez.dag.app.DAGAppMaster.webUIService
Synchronized 57% of the time
Unsynchronized access at DAGAppMaster.java:[line 2623]
Unsynchronized access at DAGAppMaster.java:[line 2623]
Synchronized access at DAGAppMaster.java:[line 568]
Synchronized access at DAGAppMaster.java:[line 569]
Synchronized access at DAGAppMaster.java:[line 578]
Synchronized access at DAGAppMaster.java:[line 656]
{code}

I cannot see any value now in having it synchronized (most probably this wasn't the case 8 years ago at the time of TEZ-537)

UPDATE: double-checked, AbstractService lifecycle methods are protected with a lock since YARN-530",2021-11-14T12:08:16.081+0000,2022-01-23T14:49:12.215+0000,Fixed,Major
SPARK-22374,STS ran into OOM in a secure cluster,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12518871'>, <JIRA IssueLink: id='12518872'>]","In a secure cluster, FileSystem.CACHE grows indefinitely.

*ENVIRONMENT*
1. `spark.yarn.principal` and `spark.yarn.keytab` is used.
2. Spark Thrift Server run with `doAs` false.
{code}
<property>
  <name>hive.server2.enable.doAs</name>
  <value>false</value>
</property>
{code}

With 6GB (-Xmx6144m) options, `HiveConf` consumes 4GB inside FileSystem.CACHE.
{code}
20,030 instances of ""org.apache.hadoop.hive.conf.HiveConf"", loaded by ""sun.misc.Launcher$AppClassLoader @ 0x64001c160"" occupy 4,418,101,352 (73.42%) bytes. These instances are referenced from one instance of ""java.util.HashMap$Node[]"", loaded by ""<system class loader>""
{code}

Please see the attached images.",2017-10-27T23:50:06.793+0000,2021-05-25T01:55:10.916+0000,Incomplete,Major
FLINK-28939,Release Testing: Verify FLIP-241 ANALYZE TABLE,FLINK,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12646487'>, <JIRA IssueLink: id='12646471'>]","This issue aims to verify FLIP-240: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217386481

We can verify it in SQL client after we build the flink-dist package. 

1. create a partition table and a non-partition table (with/without compute column/metadata column, with different columns), and then insert some data
2. verify the different statements, please refer to the FLIP doc examples
3. verify the result in catalog. Currently, {{describe extended}} statement does not support show the statistics in catalog, we should write some code to get the statistics from catalog, or we can use hive cli if the catalog is hive catalog
4. verify the unsupported cases,
4.1  analyze non-existed table
4.2 analyze view
4.3 analyze a partition table with non-existed partition
4.4. analyze a non-partition table with a partition
4.5. analyze a non-existed column
4.6. analyze a computed column
4.6. analyze a metadata column
",2022-08-12T03:08:10.923+0000,2022-08-30T02:04:49.420+0000,Done,Blocker
SPARK-2978,Provide an MR-style shuffle transformation,SPARK,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12394081'>, <JIRA IssueLink: id='12394918'>, <JIRA IssueLink: id='12394573'>]","For Hive on Spark joins in particular, and for running legacy MR code in general, I think it would be useful to provide a transformation with the semantics of the Hadoop MR shuffle, i.e. one that
* groups by key: provides (Key, Iterator[Value])
* within each partition, provides keys in sorted order

A couple ways that could make sense to expose this:
* Add a new operator.  ""groupAndSortByKey"", ""groupByKeyAndSortWithinPartition"", ""hadoopStyleShuffle"", maybe?
* Allow groupByKey to take an ordering param for keys within a partition",2014-08-12T00:57:28.511+0000,2014-09-08T18:21:17.845+0000,Fixed,Major
TEZ-2660,Tez UI: need to show application page even if system metrics publish is disabled.,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12455510'>, <JIRA IssueLink: id='12433001'>]","if system metrics publish is disabled the application page is not shown currently. This means the following does not work. 
* history url from yarn ui will redirect to a page which will error
* we do not show the url from tez-ui to yarn app page.
This can be enhanced and fixed by using the data if its available through the yarn rest api. https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Applications_API
cc [~hitesh]",2015-07-30T12:21:43.310+0000,2016-01-26T18:47:03.778+0000,Fixed,Major
TEZ-1143,1-1 source split event should be handled in Vertex.RUNNING and Vertex.INITED state,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12388947'>],"One-one edge fail when the parallelism of source vertex changes dynamically (through a ShuffleVertexManager). Here is the stack:
{code}
2014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Vertex vertex_1400646157236_0012_1_03 parallelism set to 1 from 202014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_0000012014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_0000022014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_0000032014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_0000042014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_0000052014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000006
2014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_0000072014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000008
2014-05-21 00:05:55,284 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000009
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000010
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000011
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000012
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000013
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000014
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000015
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000016
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000017
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000018
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Removing task: task_1400646157236_0012_1_03_000019
2014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Replacing edge manager for source:scope-41 destination: vertex_1400646157236_0012_1_032014-05-21 00:05:55,285 INFO [AsyncDispatcher event handler] org.apache.tez.dag.history.HistoryEventHandler: [HISTORY][DAG:dag_1400646157236_0012_1][Event:VERTEX_PARALLELISM_UPDATED]: vertexId=vertex_1400646157236_0012_1_03, numTasks=1, vertexLocationHint=null, edgeManagersCount=12014-05-21 00:05:55,286 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.DAGImpl: Vertex vertex_1400646157236_0012_1_02 completed., numCompletedVertices=3, numSuccessfulVertices=3, numFailedVertices=0, numKilledVertices=0, numVertices=72014-05-21 00:05:55,287 ERROR [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Can't handle Invalid event V_ONE_TO_ONE_SOURCE_SPLIT on vertex scope-61 with vertexId vertex_1400646157236_0012_1_05 at current state RUNNINGorg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: V_ONE_TO_ONE_SOURCE_SPLIT at RUNNING
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1263)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:158)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1716)        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1702)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)        at java.lang.Thread.run(Thread.java:695)
{code}

Attached complete AM log. scope-42 is the source vertex and scope-61 is the destination vertex.

The issue is that the code assumed that the split event will come before the vertex starts. This may not be valid in all cases. E.g. if the event comes from 2 different paths in the DAG then the vertex can start after 1 path sets the parallelism and then the second path sends the event. Also if the previous vertex was a shuffle/reduce then its parallelism can change while its running, resulting in changing the current vertex parallelism while its running.",2014-05-21T18:20:06.685+0000,2014-09-06T01:35:10.680+0000,Fixed,Major
PHOENIX-4885,After HBASE-20940 any local index query will open all HFiles of every Region involved in the query,PHOENIX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12542217'>, <JIRA IssueLink: id='12542216'>]","See HBASE-20940.

[~vishk], [~apurtell]",2018-08-31T20:06:53.101+0000,2019-12-21T00:57:57.500+0000,Fixed,Major
PHOENIX-3659,Remove transitive OWASP esapi dependency,PHOENIX,Task,Resolved,[],1,[<JIRA IssueLink: id='12493862'>],"HBase accidentally let OWASP's ESAPI artifact slip into a few release which is not allowed (as there are GPL deps).

This was resolved in 1.1.6 and 1.2.3. A trivial fix would be to upgrade the 1.1 and 1.2 branches to these versions, but I don't know if there are other implications to doing that..

I'm not sure if there are runtime concerns if we just omit those dependencies. Would have to look at the suite of reverts that came in via HBASE-16317 to see if any of them would actually affect us in phoenix-landia.",2017-02-09T17:07:23.760+0000,2017-02-21T06:59:56.006+0000,Fixed,Blocker
SLIDER-1246,Application health should not be affected by faulty nodes,SLIDER,Bug,Resolved,"[<JIRA Issue: key='SLIDER-1250', id='13106180'>]",1,[<JIRA IssueLink: id='12513907'>],"In case of a faulty node, multiple container failures will be deemed as an application failure. 
Observed this in HIVE-16927, where container failures in certain nodes brings down entire application. Slider has to provide a way to not mark application as unhealthy if certain threshold of containers are running. Tuning failure threshold is not optimal as setting the correct default on large cluster is not trivial. Beyond certain failures, slider should mark the node as unhealthy and report that back to client/AM. Application could continue to run as long as container request is satisfied partially (example: 80% containers are running).",2017-09-05T22:20:18.639+0000,2017-10-04T07:54:20.333+0000,Fixed,Major
TEZ-1526,LoadingCache for TezTaskID slow for large jobs,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12629478'>],"Using the LoadingCache with default builder settings. 100,000 TezTaskIDs are created in 10 seconds on my setup. With a LoadingCache initialCapacity of 10,000 they are created in 300 ms. With no LoadingCache, they are created in 10 ms. A test case in attached to illustrate the condition I would like to be sped up.",2014-09-01T03:41:34.967+0000,2021-12-30T04:07:24.555+0000,Fixed,Major
THRIFT-889,Add SPNEGO support to Thrift HTTP transport,THRIFT,New Feature,Open,[],3,"[<JIRA IssueLink: id='12333693'>, <JIRA IssueLink: id='12333690'>, <JIRA IssueLink: id='12333689'>]","[SPNEGO|http://en.wikipedia.org/wiki/SPNEGO] is the standard mechanism for Kerberos authentication over HTTP. 

Thrift should give users the option of using a Thrift HTTP client or server that is capable of authentication using SPNEGO/Kerberos. This will be useful for Hadoop which has chosen Kerberos as its standard authentication protocol, as well as people who need to use Kerberos authentication with Thrift based services running in servlet containers like Tomcat.

References:
* [LGPL licensed SPNEGO library for Java|http://spnego.sourceforge.net/]
* [Glassfish Spnego project|https://spnego.dev.java.net/]
* [SPNEGO authentication scheme for HTTPClient|https://issues.apache.org/jira/browse/HTTPCLIENT-523]


",2010-09-02T01:28:01.682+0000,2017-10-10T18:19:54.495+0000,,Major
LUCENE-2919,IndexSplitter that divides by primary key term,LUCENE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12338052'>, <JIRA IssueLink: id='12339919'>]","Index splitter that divides by primary key term.  The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id.  I think this implementation is a fairly trivial change.",2011-02-15T03:33:38.936+0000,2022-08-28T12:41:07.073+0000,Fixed,Minor
TEZ-3302,Add a version of processorContext.waitForAllInputsReady and waitForAnyInputReady with a timeout,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12645214'>],"This is useful when a Processor needs to check on whether it has been aborted or not, and the interrupt that is sent in as part of the 'Task kill' process has been swallowed by some other entity.",2016-06-12T22:46:14.404+0000,2022-08-08T09:18:24.013+0000,Fixed,Major
IMPALA-4018,Add support for SQL:2016 datetime templates/patterns/masks to CAST(... AS ... FORMAT <template>),IMPALA,New Feature,Closed,"[<JIRA Issue: key='IMPALA-8160', id='13213789'>, <JIRA Issue: key='IMPALA-8703', id='13241411'>, <JIRA Issue: key='IMPALA-8704', id='13241413'>, <JIRA Issue: key='IMPALA-8705', id='13241414'>, <JIRA Issue: key='IMPALA-8706', id='13241415'>, <JIRA Issue: key='IMPALA-9141', id='13267179'>, <JIRA Issue: key='IMPALA-9219', id='13272582'>]",16,"[<JIRA IssueLink: id='12555960'>, <JIRA IssueLink: id='12511270'>, <JIRA IssueLink: id='12575673'>, <JIRA IssueLink: id='12575680'>, <JIRA IssueLink: id='12517662'>, <JIRA IssueLink: id='12560624'>, <JIRA IssueLink: id='12558323'>, <JIRA IssueLink: id='12552877'>, <JIRA IssueLink: id='12559144'>, <JIRA IssueLink: id='12511276'>, <JIRA IssueLink: id='12568327'>, <JIRA IssueLink: id='12561361'>, <JIRA IssueLink: id='12571381'>, <JIRA IssueLink: id='12562767'>, <JIRA IssueLink: id='12497193'>, <JIRA IssueLink: id='12514325'>]","*Summary*
The format masks/templates for currently are implemented using the [Java SimpleDateFormat patterns|http://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html], and although this is what Hive has implemented, it is not what most standard SQL systems implement.  For example see [Vertica|https://my.vertica.com/docs/7.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Formatting/TemplatePatternsForDateTimeFormatting.htm], [Netezza|http://www.ibm.com/support/knowledgecenter/SSULQD_7.2.1/com.ibm.nz.dbu.doc/r_dbuser_ntz_sql_extns_templ_patterns_date_time_conv.html],  [Oracle|https://docs.oracle.com/database/121/SQLRF/sql_elements004.htm#SQLRF00212], and [PostgreSQL|https://www.postgresql.org/docs/9.5/static/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE]. 

*Examples of incompatibilities*
{noformat}
-- PostgreSQL/Netezza/Vertica/Oracle
select to_timestamp('May 15, 2015 12:00:00', 'mon dd, yyyy hh:mi:ss');
-- Impala
select to_timestamp('May 15, 2015 12:00:00', 'MMM dd, yyyy HH:mm:ss');

-- PostgreSQL/Netezza/Vertica/Oracle
select to_timestamp('2015-02-14 20:19:07','yyyy-mm-dd hh24:mi:ss');
-- Impala
select to_timestamp('2015-02-14 20:19:07','yyyy-MM-dd HH:mm:ss');

-- Vertica/Oracle
select to_timestamp('2015-02-14 20:19:07.123456','yyyy-mm-dd hh24:mi:ss.ff');
-- Impala
select to_timestamp('2015-02-14 20:19:07.123456','yyyy-MM-dd HH:mm:ss.SSSSSS');
{noformat}

*Considerations*
Because this is a change in default behavior for to_timestamp(), if possible, having a feature flag to revert to the legacy Java SimpleDateFormat patterns should be strongly considered.  This would allow users to chose the behavior they desire and scope it to a session if need be.

SQL:2016 defines the following datetime templates

{noformat}
<datetime template> ::=
  { <datetime template part> }...
<datetime template part> ::=
    <datetime template field>
  | <datetime template delimiter>
<datetime template field> ::=
    <datetime template year>
  | <datetime template rounded year>
  | <datetime template month>
  | <datetime template day of month>
  | <datetime template day of year>
  | <datetime template 12-hour>
  | <datetime template 24-hour>
  | <datetime template minute>
  | <datetime template second of minute>
  | <datetime template second of day>
  | <datetime template fraction>
  | <datetime template am/pm>
  | <datetime template time zone hour>
  | <datetime template time zone minute>
<datetime template delimiter> ::=
    <minus sign>
  | <period>
  | <solidus>
  | <comma>
  | <apostrophe>
  | <semicolon>
  | <colon>
| <space>
<datetime template year> ::=
  YYYY | YYY | YY | Y
<datetime template rounded year> ::=
  RRRR | RR
<datetime template month> ::=
  MM
<datetime template day of month> ::=
  DD
<datetime template day of year> ::=
  DDD
<datetime template 12-hour> ::=
  HH | HH12
<datetime template 24-hour> ::=
  HH24
<datetime template minute> ::=
  MI
<datetime template second of minute> ::=
  SS
<datetime template second of day> ::=
  SSSSS
<datetime template fraction> ::=
  FF1 | FF2 | FF3 | FF4 | FF5 | FF6 | FF7 | FF8 | FF9
<datetime template am/pm> ::=
  A.M. | P.M.
<datetime template time zone hour> ::=
  TZH
<datetime template time zone minute> ::=
  TZM
{noformat}

SQL:2016 also introduced the FORMAT clause for CAST which is the standard way to do string <> datetime conversions
{noformat}
<cast specification> ::=
  CAST <left paren>
      <cast operand> AS <cast target>
      [ FORMAT <cast template> ]
      <right paren>
<cast operand> ::=
    <value expression>
  | <implicitly typed value specification>
<cast target> ::=
    <domain name>
| <data type>
<cast template> ::=
  <character string literal>
{noformat}
For example:
{noformat}
CAST(<datetime> AS <char string type> [FORMAT <template>])
CAST(<char string> AS <datetime type> [FORMAT <template>])
cast(dt as string format 'DD-MM-YYYY')
cast('01-05-2017' as date format 'DD-MM-YYYY')
{noformat}

*Update*
Here is the proposal for the new datetime patterns and their semantics:
https://docs.google.com/document/d/1V7k6-lrPGW7_uhqM-FhKl3QsxwCRy69v2KIxPsGjc1k/
",2016-08-25T03:14:09.000+0000,2020-01-09T14:36:12.072+0000,Fixed,Critical
AMBARI-12138,Set dfs.client.retry.policy.enabled property to true when HDFS HA is enabled,AMBARI,Improvement,Patch Available,[],1,[<JIRA IssueLink: id='12461263'>],"After enabling HDFS HA, hdfs-site.xml does not include property  dfs.client.retry.policy.enabled and 'hdfs getconf -confKey dfs.client.retry.policy.enabled' returns empty value. 

The property dfs.client.retry.policy.enabled is important when HA is enabled, as it enables HDFS client retry in case of NameNode failure. So, after enabling HDFS HA, Ambari should set this property to true in hdfs-site.xml.",2015-06-25T02:47:44.138+0000,2016-03-18T22:22:11.239+0000,,Major
THRIFT-5534,Backport removed TSocket java constructor to thrift 0.15,THRIFT,Improvement,Open,[],1,[<JIRA IssueLink: id='12640204'>],"It would be great if we can backport this commit https://github.com/apache/thrift/pull/2470 into thrift 0.15 as a new release 0.15.1.
The missing constructor breaks the Apache Hive JDBC driver which requires this. This unfortunately means that for a lot of teams the 0.15 version might be unusable where as the new 0.16 might not have been adopted yet by dependencies.

Hopefully this can be considered.",2022-03-04T01:49:50.954+0000,2022-08-30T21:09:46.730+0000,,Major
PHOENIX-6111,Jenkins jobs are unable to create new native thread,PHOENIX,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12599175'>, <JIRA IssueLink: id='12598657'>, <JIRA IssueLink: id='12598656'>]","Jenkins jobs are randomly failing  with 
{noformat}
java.lang.OutOfMemoryError: unable to create new native thread{noformat}
The ulimit on the slaves is 30000, and cannot be increased from within the job.

The typical thread count used by our test suite is 3-4000.

It is not clear yet if this is caused by our thread use spiking, or if a parallel job exhausts the thread limit.",2020-08-28T04:32:39.792+0000,2020-09-29T08:13:35.265+0000,Fixed,Major
AVRO-806,add a column-major file format,AVRO,New Feature,Closed,[],5,"[<JIRA IssueLink: id='12362730'>, <JIRA IssueLink: id='12362729'>, <JIRA IssueLink: id='12340439'>, <JIRA IssueLink: id='12340441'>, <JIRA IssueLink: id='12362727'>]",Define a column-major file format.  This would permit better compression and also permit efficient skipping of fields that are not of interest.,2011-04-19T23:24:21.991+0000,2013-05-02T02:30:56.762+0000,Fixed,Major
CALCITE-1358,Push filters on time dimension to Druid,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12478363'>, <JIRA IssueLink: id='12479745'>, <JIRA IssueLink: id='12479237'>]","Porting work done in HIVE-14217.

Logic should split the filter conditions into two parts: those predicates referred to the time dimension, and predicates referred to other columns.

Then, the predicates on the time dimension should be translated into Druid intervals, possibly consolidating those ranges e.g. to detect overlapping. The other predicates will go into the Druid filter field.",2016-08-23T19:17:43.135+0000,2016-09-21T14:00:40.126+0000,Fixed,Major
SLIDER-51,support GET operations to retrieve single-value key by name,SLIDER,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12388484'>, <JIRA IssueLink: id='12388483'>]","to aid in registration lookup, we should support URLs like

registry/hadoop-site.xml/yarn.lib.classpath

-this would resolve the value and return it, or return a 404. The path for the property key would have to be escaped, as we can't control what strings get published.
",2014-05-13T15:23:37.548+0000,2014-09-08T15:40:34.307+0000,Fixed,Major
ATLAS-4333,[MATERIALIZED VIEW]Column Lineage and hive_process missing in case of CREATE MATERIALIZED VIEW query at Hive,ATLAS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12617156'>],"Column Lineage and the hive_process are missing in case of materialised view
{code:java}
CREATE TABLE tbl1(id int, name string);  
CREATE MATERIALIZED VIEW tbl1_materialized_view as SELECT * from tbl1;

{code}
With above create materialized query, tbl1_view is created but with missing ""ddlQueries(hive_table_ddl)"" relationship and ""outputFromProcesses(hive_column_lineage)"" for columns.

HiveContext is not sending lineage info for columns of materialized view, once Hive fixes the issue it'll resolve missing lineage issue. Jira# HIVE-25236 created for Hive.",2021-06-10T16:40:03.237+0000,2021-06-23T22:05:04.668+0000,Fixed,Major
INFRA-22873,The hbase11-20 nodes do not have zip installed,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12633326'>],"We will compress the surefire output to save space but there is no zip command on hbase11-20.

See https://ci-hbase.apache.org/job/Test-Script/9/console",2022-02-11T04:01:53.323+0000,2022-02-11T14:45:56.889+0000,Fixed,Major
BIGTOP-860,To track HBase API changes affecting integration tests,BIGTOP,Bug,Closed,[],2,"[<JIRA IssueLink: id='12365065'>, <JIRA IssueLink: id='12365064'>]",To track HBASE-7973,2013-03-01T23:17:05.179+0000,2013-06-21T23:55:36.097+0000,Won't Fix,Major
DERBY-7132,SQLDataException when executing CAST inside a CASE WHEN clause,DERBY,Bug,Open,[],1,[<JIRA IssueLink: id='12633851'>],"{code:sql}
SELECT ""PARTITIONS"".""PART_ID""
FROM ""PARTITIONS""
         INNER JOIN ""TBLS"" ON ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID""
         INNER JOIN ""DBS"" ON ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID""
         INNER JOIN ""PARTITION_KEY_VALS"" ""FILTER0"" ON ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID""
WHERE ""DBS"".""CTLG_NAME"" = 'hive'
  AND ""TBLS"".""TBL_NAME"" = 'src_bucket_tbl'
  AND ""DBS"".""NAME"" = 'default'
  AND ""FILTER0"".""INTEGER_IDX"" = 0
  AND (((CASE
             WHEN ""FILTER0"".""PART_KEY_VAL"" <> '__HIVE_DEFAULT_PARTITION__'
                 AND ""TBLS"".""TBL_NAME"" = 'src_bucket_tbl'
                 AND ""DBS"".""NAME"" = 'default'
                 AND ""DBS"".""CTLG_NAME"" = 'hive'
                 AND ""FILTER0"".""INTEGER_IDX"" = 0 THEN cast(""FILTER0"".""PART_KEY_VAL"" AS decimal(21, 0))
    END) = 10))
{code}

The SQL query above fails with the following stacktrace when attempting to evaluate the CAST expression. Note that the condition inside the CASE WHEN clause guarantees that only legal values (numbers) should be passed inside the CAST function. Apparently, the operations are somehow re-ordered and the CAST is evaluated before the condition in the WHEN clause which has a result a non-number to be passed in the CAST and cause the exception below.

{noformat}
Exception in thread ""main"" java.sql.SQLDataException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(SQLExceptionFactory.java:84)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Util.java:230)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(TransactionResourceImpl.java:424)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(TransactionResourceImpl.java:353)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(EmbedConnection.java:2405)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(ConnectionChild.java:88)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1436)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1709)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeQuery(EmbedPreparedStatement.java:286)
	at com.github.zabetak.CaseProblem.main(CaseProblem.java:63)
Caused by: ERROR 22018: Invalid character string format for type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:290)
	at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:285)
	at org.apache.derby.iapi.types.DataType.invalidFormat(DataType.java:1280)
	at org.apache.derby.iapi.types.DataType.setValue(DataType.java:552)
	at org.apache.derby.exe.acf81e0010x017fx0812xbaa5x00003a07fe880.e3(Unknown Source)
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(ReflectGeneratedClass.java:107)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(ProjectRestrictResultSet.java:302)
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(NestedLoopJoinResultSet.java:119)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(JoinResultSet.java:149)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.openCore(ProjectRestrictResultSet.java:182)
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.open(BasicNoPutResultSetImpl.java:266)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:472)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:351)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1344)
{noformat}

The problem can be reproduced by running the query above in the derby database attached to the case.

{code:sql}
try (Connection c = DriverManager.getConnection(""jdbc:derby:;databaseName=repro_derby_db"")) {
        try (PreparedStatement ps = c.prepareStatement(sql)) {
          try (ResultSet rs = ps.executeQuery()) {
            while (rs.next()) {
              System.out.println(rs.getInt(1));
            }
          }
        }
      }
{code}

Unfortunately, I couldn't write a minimal reproducer cause slight changes to the order of performing the operations in the database has an impact on the plan and may hide the problem.
",2022-02-17T14:36:03.151+0000,2022-03-22T13:44:33.509+0000,,Major
CALCITE-1578,Druid adapter: wrong semantics of topN query limit with granularity,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12491609'>, <JIRA IssueLink: id='12491291'>]","Semantics of Druid topN query with limit and granularity is not equivalent to input SQL. In particular, limit is applied on each granularity value, not on the overall query.

Currently, the following query will be transformed into a topN query:
{code:sql}
SELECT i_brand_id, floor_day(`__time`), max(ss_quantity), sum(ss_wholesale_cost) as s
FROM store_sales_sold_time_subset
GROUP BY i_brand_id, floor_day(`__time`)
ORDER BY s DESC
LIMIT 10;
{code}

Previous query outputs at most 10 rows. In turn, the equivalent SQL query for a Druid topN query should be expressed as:
{code:sql}
SELECT rs.i_brand_id, rs.d, rs.m, rs.s
FROM (
    SELECT i_brand_id, floor_day(`__time`) as d, max(ss_quantity) as m, sum(ss_wholesale_cost) as s,
           ROW_NUMBER() OVER (PARTITION BY floor_day(`__time`) ORDER BY sum(ss_wholesale_cost) DESC ) AS rownum
    FROM store_sales_sold_time_subset
    GROUP BY i_brand_id, floor_day(`__time`)
) rs
WHERE rownum <= 10;
{code}",2017-01-16T15:31:59.057+0000,2017-03-24T03:20:01.850+0000,Fixed,Critical
SLIDER-1162,Create a Docker Provider,SLIDER,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12478060'>, <JIRA IssueLink: id='12478704'>]","If we create an agent-less Docker Provider, then we will solve the current problem of executing the agent's python code inside the docker container. Other problems will be created, as we will have to move some of the agent's tasks elsewhere. YARN-5430 will be helpful.",2016-08-01T17:21:20.877+0000,2016-09-07T20:38:00.896+0000,Won't Fix,Major
MSKINS-137,"Enable ""Hamburger menu"" with top-nav only",MSKINS,Improvement,Closed,[],1,[<JIRA IssueLink: id='12540575'>],"Revitalizing this old PR https://github.com/apache/maven-skins/pull/4

Still an issue for us down in HBase. Changes seem to apply and have worked in local testing on the HBase site.",2017-11-07T19:49:46.040+0000,2019-05-06T15:34:13.938+0000,Fixed,Major
SLIDER-665,Allow extensibility of the Slider AM Web UI to provide application specific end points,SLIDER,Task,Open,[],2,"[<JIRA IssueLink: id='12402764'>, <JIRA IssueLink: id='12401907'>]","Slider AppMaster UI provides a REST end point for various metadata related to the application and general purpose application status. Applications can also explicitly export config and URLs (any data for that matter) and that is also available through the REST end point.

What is not possible is for the application to report back custom data sets at regular intervals and have it available through the AM REST endpoint. The advantage of such a support would be for applications that do not need a comprehensive web service and can extend the AppMaster REST endpoint to provide application specific data. _Its worth investigating if the REST API should be readonly or it is also expected to have support for PUT and POST._

Such feature will mean:
* Allow agents to send custom stats to the AM (arbitrary JSON structure)
* Make per component instance JSON stats available through the Slider AM Web Service
",2014-11-20T01:53:01.012+0000,2014-12-05T00:59:48.800+0000,,Critical
SLIDER-377,slider MiniHDFSCluster tests failing on windows+branch2,SLIDER,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12395495'>],Tests that use the MiniHDFSCluster are failing on windows with link errors -datanodes are failing on JNI linkage errors calculating CRC32 checksums,2014-08-29T10:20:54.632+0000,2014-09-04T17:57:50.979+0000,Fixed,Major
AVRO-839,Implement builder pattern in generated record classes that sets default values when omitted,AVRO,Improvement,Closed,[],8,"[<JIRA IssueLink: id='12346963'>, <JIRA IssueLink: id='12347299'>, <JIRA IssueLink: id='12346832'>, <JIRA IssueLink: id='12345103'>, <JIRA IssueLink: id='12340494'>, <JIRA IssueLink: id='12340448'>, <JIRA IssueLink: id='12340449'>, <JIRA IssueLink: id='12340447'>]","This is an idea for an improvement to the SpecificCompiler-generated record classes.  There are two main issues to address:

# Default values specified in schemas are only used at read time, not when writing/serializing records.  For example, a NullPointerException is thrown when attempting to write a record that has an uninitialized array or string type.  I'm sure this was done for good reasons, like giving users maximum control and preventing unnecessary garbage collection, but I think it's also somewhat confusing and unintuitive for new users (myself included).
# Users have to create their own factory classes/methods for every record type, both to ensure that all non-primitive members are initialized and to facilitate the construction and initialization of record instances (i.e. constructing and setting values in a single statement).

These issues have been discussed previously here:
* [http://search-hadoop.com/m/iDVTn1JVeSR1]
* AVRO-726
* AVRO-770
* [http://search-hadoop.com/m/JuY1V16pwxh1]

I'd like to propose a solution that is used by at least one other messaging framework.  For each generated record class there will be a public static inner class called Builder.  The Builder inner class has the same fields as the record class, as well as accessors and mutators for each of these fields.  Whenever a mutator method is called, the Builder sets a boolean flag indicating that the field has been set.  All mutators return a reference to 'this', so it's possible to chain a series of setter invocations, which makes it really easy to construct records in a single statement.  The Builder also has a build() method which constructs a record instance using the values that were set in the Builder.  When the build() method is invoked, if there are any fields that have not been set but have default values as defined in the schema, the Builder will set the values of these fields using their defaults.

One nice thing about implementing the builder pattern in a static inner Builder class rather than in the record itself is that this enhancement will be completely backwards-compatible with existing code.  The record class itself would not change, and the public fields would still be there, so existing code would still work.  Users would have the option to use the Builder or continue constructing records manually.  Eventually the public fields could be phased out, and the record would be made immutable.  All changes would have to be done through the Builder.

Here is an example of what this might look like:
{code}
// Person.newBuilder() returns a new Person.Builder instance
// All Person.Builder setters return 'this' allowing us to chain set calls together for convenience
// Person.Builder.build() returns a Person instance after setting any uninitialized values that have defaults
Person me = Person.newBuilder().setName(""James"").setCountry(""US"").setState(""MA"").build();

// We still have direct access to Person's members, so the records are backwards-compatible
me.state = ""CA"";

// Person has accessor methods now so that the public fields can be phased out later
System.out.println(me.getState());

// No NPE here because the array<Person> field that stores this person's friends has been automatically 
// initialized by the Builder to a new java.util.ArrayList<Person> due to a @java_class annotation in the IDL
System.out.println(me.getFriends().size());
{code}

What do people think about this approach?  Any other ideas?",2011-06-16T04:35:39.806+0000,2012-01-24T04:56:04.703+0000,Fixed,Major
IMPALA-8068,A day may belong to a different year than the week it is a part of,IMPALA,New Feature,Open,[],1,[<JIRA IssueLink: id='12551691'>],"When using the year() and weekofyear() functions in a query, their result is 2018 and 1 (respectively) for the day '2018-12-31'.

The year() function returns 2018 for the input '2018-12-31', because that day belongs to the year 2018.

The weekofyear() functions returns 1 for the input '2018-12-31', because that day belongs to the first week of 2019.

Both functions provide sensible results on their own, but when combined, the result is wrong, because '2018-12-31' does not belong to week 1 of 2018.

I suggest adding a new function yearofweek() that would return 2019 for the input '2018-12-31' and adding a warning to the documentation of the weekofyear() function about this problem.",2019-01-11T14:55:42.536+0000,2020-12-22T21:05:40.735+0000,,Major
IMPALA-11013,Support migrating external tables to Iceberg tables,IMPALA,Bug,In Progress,[],1,[<JIRA IssueLink: id='12626218'>],"E.g. Hive supports migrating external tables to Iceberg tables via the following command:

{noformat}
ALTER TABLE t SET TBLPROPERTIES ('storage_handler'='org.apache.iceberg.mr.hive.HiveIcebergStorageHandler');
{noformat}

Maybe we could support table migration with the same command.",2021-11-10T09:42:40.323+0000,2022-08-29T06:00:56.230+0000,,Major
IMPALA-6985,Impala View Does Not Populate Table Input/Output Format Class,IMPALA,Improvement,Open,[],1,[<JIRA IssueLink: id='12533483'>],"When a view is created in Impala, the InputFormat and OutputFormat fields are set to NULL.  This is breaking some aspects of Hive. See: [HIVE-19424].  Perhaps Impala can play nice here and set these fields the same as Hive.

{code:sql}
-- hive
CREATE VIEW test_view_hive AS select * from sample_07;

-- impala
CREATE VIEW test_view_impala AS select * from sample_07;
{code}

{code:sql}
-- Impala
DESCRIBE extended test_view_impala;

InputFormat - (blank)
OutputFormat - (blank)

DESCRIBE extended test_view_hive;

InputFormat - org.apache.hadoop.mapred.TextInputFormat
OutputFormat - org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
{code}

You can see the difference in the Hive Metastore.

{code}
MariaDB [hive1]> SELECT TBLS.TBL_NAME FROM TBLS JOIN SDS ON TBLS.SD_ID=SDS.SD_ID WHERE SDS.INPUT_FORMAT IS NULL;

+------------------+
| TBL_NAME         |
+------------------+
| test_view_impala |
+------------------+
{code}",2018-05-07T21:13:01.282+0000,2018-05-08T13:06:38.743+0000,,Major
ORC-285,Empty vector batches of floats or doubles get  java.io.EOFException,ORC,Bug,Closed,[],3,"[<JIRA IssueLink: id='12527016'>, <JIRA IssueLink: id='12523129'>, <JIRA IssueLink: id='12526917'>]","The FloatTreeReader and DoubleTreeReader both fail if given a set of empty values. This often happens when the float or double data is inside a list where all of the values are empty.

The stack trace looks like:

{code}
java.io.IOException: Error reading file: /Users/piyush.mukati/tmp/11-task_1511779500016_298243_r_000031-r-00031.orc
	at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1191)
	at org.apache.orc.tools.ScanData.main(ScanData.java:67)
	at org.apache.orc.tools.Driver.main(Driver.java:109)
Caused by: java.io.EOFException: Read past EOF for compressed stream Stream for column 36 kind DATA position: 0 length: 0 range: 0 offset: 0 limit: 0
	at org.apache.orc.impl.SerializationUtils.readFully(SerializationUtils.java:119)
	at org.apache.orc.impl.SerializationUtils.readLongLE(SerializationUtils.java:102)
	at org.apache.orc.impl.SerializationUtils.readDouble(SerializationUtils.java:98)
	at org.apache.orc.impl.TreeReaderFactory$DoubleTreeReader.nextVector(TreeReaderFactory.java:763)
	at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextVector(TreeReaderFactory.java:1835)
	at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextVector(TreeReaderFactory.java:1835)
	at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextVector(TreeReaderFactory.java:1835)
	at org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector(TreeReaderFactory.java:2003)
	at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextVector(TreeReaderFactory.java:1835)
	at org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector(TreeReaderFactory.java:2003)
	at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextVector(TreeReaderFactory.java:1835)
	at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:1817)
	at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1184)
	... 2 more
{code}",2017-12-27T17:12:15.608+0000,2020-03-09T19:38:37.124+0000,Fixed,Major
RATIS-736,Add a new RetryPolicy similar to MultipleLinearRandomRetry in HDFS,RATIS,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12572763'>, <JIRA IssueLink: id='12573145'>]","In HDFS-3504, we first tried to implement exponential backoff retry policy but found that the later retries will need to wait for a very long time (since it is exponential).  We then implement a MultipleLinearRandomRetry.  We should do the same here.",2019-10-26T00:16:59.694+0000,2019-11-20T17:14:21.673+0000,Fixed,Major
TEZ-3983,VertexGroup ONE_TO_ONE edges do not produce the right graph,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12544892'>],"A VertexGroup is described to act as a union of multiple vertices.

Consider a VertexGroup composed of 2 vertices, each with 2 tasks. If this is a union, then the expectation is that the VertexGroup has 4 tasks. A ONE_TO_ONE edge, to a downstream vertex, must then require that vertex to also have 4 tasks.

This is not the case, as evidenced by the [following test|https://github.com/apache/tez/blob/261bbdd5929d562758deb31085b565db8e92d6a2/tez-api/src/test/java/org/apache/tez/dag/api/TestDAGVerify.java#L779-L831].

Rather, what happens is that implementation (of unrolling the VertexGroup and directly creating edges between the components of the VertexGroup and the downstream vertices) is presented as the behavior.",2018-08-24T21:06:36.495+0000,2018-10-04T20:23:51.961+0000,,Major
PHOENIX-5291,Ensure that Phoenix coprocessor close all scanners.,PHOENIX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12561401'>, <JIRA IssueLink: id='12561446'>]","With HBase 1.5 and later this is a disaster, as it causes the wrong reference counting of HFiles in HBase, and those subsequently will *never* be removed until the region closes and reopens for any reason.

We found at least two cases... See comments below.",2019-05-21T23:35:15.755+0000,2019-05-30T18:40:58.930+0000,Fixed,Critical
ORC-45,Support ZSTD Dictionary Compression ,ORC,Wish,Open,[],1,[<JIRA IssueLink: id='12531581'>],"ZSTD provides superior processing performance in comparison to deflate/gzip. Dictionary compression provides yet another opportunity for improvement, when applied to columns.

Implementing dictionary compression per-column may enable other code to be written, such as per-column encryption, which while mentioned in ORC-14, would be taking a different form.

",2016-04-04T19:45:21.921+0000,2018-04-12T03:38:45.083+0000,,Major
PARQUET-115,Pass a filter object to user defined predicate in filter2 api,PARQUET,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12399241'>, <JIRA IssueLink: id='12400390'>]","Currently for creating a user defined predicate using the new filter api,  no value can be passed to create a dynamic filter at runtime. This reduces the usefulness of the user defined predicate, and  meaningful predicates cannot be created. We can add a generic Object value that is passed through the api, which can internally be used in the keep function of the user defined predicate for creating many different types of filters.
For example, in spark sql, we can pass in a list of filter values for a where IN clause query and filter the row values based on that list.",2014-10-18T05:41:35.636+0000,2015-04-07T20:46:09.173+0000,Duplicate,Minor
SPARK-11416,Upgrade kryo package to version 3.0,SPARK,Wish,Resolved,[],7,"[<JIRA IssueLink: id='12462119'>, <JIRA IssueLink: id='12462920'>, <JIRA IssueLink: id='12448489'>, <JIRA IssueLink: id='12448498'>, <JIRA IssueLink: id='12457615'>, <JIRA IssueLink: id='12453409'>, <JIRA IssueLink: id='12448490'>]",Would like to have Apache Spark upgrade kryo package from 2.x (current) to 3.x.  ,2015-10-30T05:12:46.364+0000,2018-01-19T20:36:30.540+0000,Fixed,Major
TEZ-4347,Add some diagnostic endpoints to TezAM's WebUIService,TEZ,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12625335'>, <JIRA IssueLink: id='12625322'>]","-Currently, I cannot see an HTTP web server in DAGAppMaster, but sooner or later we'll need one. I'm attaching some tickets which are related.-
-As a reference, we can start with a simplified version of Hive's HTTPServer which runs in HiveServer2:- [-https://github.com/apache/hive/blob/master/common/src/java/org/apache/hive/http/HttpServer.java-]

 

I found WebUIService which can be reused to add some useful endpoints: jmx, conf, stacks",2021-10-27T20:19:26.800+0000,2022-05-04T04:45:46.532+0000,Fixed,Major
TEZ-168,Fix synchronization issue in use of container tokens when launching a container,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12369657'>],"2013-05-30 10:54:38,265 ERROR [ContainerLauncher #3] org.apache.tez.dag.app.launcher.ContainerLauncherImpl: Container launch failed for container_1369890958355_0004_01_000005 : java.nio.BufferUnderflowException        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)        at com.google.protobuf.ByteString.copyFrom(ByteString.java:108)
        at com.google.protobuf.ByteString.copyFrom(ByteString.java:117)        at org.apache.hadoop.yarn.util.ProtoUtils.convertToProtoFormat(ProtoUtils.java:156)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl.convertToProtoFormat(ContainerLaunchContextPBImpl.java:97)        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl.mergeLocalToBuilder(ContainerLaunchContextPBImpl.java:105)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl.mergeLocalToProto(ContainerLaunchContextPBImpl.java:124)        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl.getProto(ContainerLaunchContextPBImpl.java:66)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl.convertToProtoFormat(StartContainerRequestPBImpl.java:133)        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl.mergeLocalToBuilder(StartContainerRequestPBImpl.java:62)
        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl.mergeLocalToProto(StartContainerRequestPBImpl.java:72)        at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl.getProto(StartContainerRequestPBImpl.java:54)
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:105)        at org.apache.tez.dag.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:165)
        at org.apache.tez.dag.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:407)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)        at java.lang.Thread.run(Thread.java:680)
",2013-05-30T17:58:11.629+0000,2013-12-01T20:20:11.650+0000,Fixed,Major
FLUME-1669,Add support for columnar event serializer in HDFS,FLUME,New Feature,Open,[],1,[<JIRA IssueLink: id='12359565'>],"Motivation:
Columnar storage is preferred for better performance and compression for low-latency analytical workloads. Avro 1.7.2 supports column-major file format [1]
and we can implement {{AbstractTrevniAvroEventSerializer}} (as like {{AbstractAvroEventSerializer}}). {{HDFSSink}} can have serializer type to store events in Trevni column-major file format.

[1]    http://avro.apache.org/docs/current/trevni/spec.html
       https://github.com/cutting/trevni",2012-10-28T06:46:29.383+0000,2013-06-25T00:02:29.028+0000,,Major
CALCITE-1598,Pig adapter,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12507720'>],"Write an adapter that uses Pig to do processing. (This is different than Piglet, which is a Pig-Latin-like front-end to Calcite.)",2017-01-22T04:50:39.156+0000,2017-06-27T22:49:22.590+0000,Fixed,Major
SPARK-21101,Error running Hive temporary UDTF on latest Spark 2.2,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12508300'>],"I'm using temporary UDTFs on Spark 2.2, e.g.

CREATE TEMPORARY FUNCTION myudtf AS 'com.foo.MyUdtf' USING JAR 'hdfs:///path/to/udf.jar'; 

But when I try to invoke it, I get the following error:

{noformat}
17/06/14 19:43:50 ERROR SparkExecuteStatementOperation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.AnalysisException: No handler for Hive UDF 'com.foo.MyUdtf': java.lang.NullPointerException; line 1 pos 7
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:266)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Any help appreciated, thanks.",2017-06-15T00:00:35.553+0000,2017-10-25T06:00:55.769+0000,Fixed,Major
AVRO-137,Avro code generation should never cast an element to Object ,AVRO,Bug,Closed,[],2,"[<JIRA IssueLink: id='12327242'>, <JIRA IssueLink: id='12351771'>]","Avro code generation should never cast an element to Object. If casted, it results in redudant cast warning.",2009-10-07T06:35:31.471+0000,2012-05-11T21:19:59.238+0000,Fixed,Major
HDDS-45,Removal of old OzoneRestClient,HDDS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12515871'>],"Originally, Ozone used to have a REST client only, that is what was used in the oz CLI. Later we developed an RPC client and started using it Freon. The load generator for Ozone. Then we created a common interface and created factory methods for RestClient or RpcClient. That allows CLI or Freon or any program that is using the Ozone client SDK to trivially switch between REST and RPC.

This patch removes the old REST client from the code base, since the new code has better features and all the clients (CLI, Freon, Tests) have switched to using this new client with this patch.",2017-09-26T16:34:32.656+0000,2018-05-24T10:48:48.566+0000,Fixed,Major
SLIDER-1096,slider no longer bulding against Hadoop 2.8; MockNodeReport doesn't compile,SLIDER,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12469363'>, <JIRA IssueLink: id='12459144'>]","YARN-4293 has broken Slider's build, MockNodeReport needs a new field.

The datatype returned in the getter dates from YARN-1012; it's in Hadoop 2.8+ only. This means that we'd need to switch to 2.8 only...not likely for a while.

Otherwise, improve that mocking somehow, without going near the new method. Maybe, just maybe, groovy will let us get away with that",2016-03-02T14:45:57.534+0000,2016-06-13T13:32:34.673+0000,Duplicate,Major
PHOENIX-5422,Use Java8 DateTime APIs instead of joda-time APIs,PHOENIX,Task,Patch Available,[],1,[<JIRA IssueLink: id='12566990'>],"currently, phoenix-hive.jar bundles an old version of joda-time library. Java8 has new DateTime APIs are similar in to the joda-time APIs in features. It would make sense to eliminate a dependency on external library that may or may not be
a) Actively developed
b) not guaranteed to be backward compatible with older APIs and could involve code changes as well.

Its is better to move using JDK8 APIs.",2019-08-02T17:48:15.674+0000,2022-09-17T17:12:26.909+0000,,Minor
PHOENIX-1638,Avoid batched mutations via direct use of HRegion,PHOENIX,Sub-task,Open,[],2,"[<JIRA IssueLink: id='12407236'>, <JIRA IssueLink: id='12407241'>]","In some places we use HRegion directly to submit batch mutations, with HRegion#batchMutate or HRegion#mutateRowsWithLocks. We should replace this with batch mutations submitted via an HTableInterface from CoprocessorHConnection#getConnectionForEnvironment",2015-02-04T20:47:16.076+0000,2015-02-05T16:43:10.394+0000,,Major
PHOENIX-1452,Add Phoenix client-side logging and capture resource utilization metrics,PHOENIX,New Feature,Closed,[],2,"[<JIRA IssueLink: id='12409229'>, <JIRA IssueLink: id='12406294'>]","For performance testing and tuning of features that use Phoenix and for production monitoring it would be really helpful to easily be able to extract statistics about Phoenix's client-side Thread Pool and Queue Depth usage to help with tuning and being able to correlate the impact of tuning these 2 parameters to query performance.

For global per JVM logging one of the following would meet my needs, with a preference for #2:
1. A simple log line that that logs the data in ThreadPoolExecutor.toString() at a configurable interval
2. Exposing the ThreadPoolExecutor metrics in PhoenixRuntime or other global client exposed class and allow client to do their own logging.

In addition to this it would also be really valuable to have a single log line per query that provides statistics about the level of parallelism i.e. number of parallel scans being executed. I don't full explain plan level of data but a good heuristic to be able to track over time how queries are utilizing the thread pool as data size grows etc. 
",2014-11-13T19:04:38.944+0000,2015-11-21T02:15:51.180+0000,Fixed,Major
PHOENIX-3464,Pig doesn't handle hbase://query/ properly,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12486037'>],"{code}
A = load 'hbase://query/SELECT ID,NAME,DATE FROM HIRES WHERE DATE > TO_DATE('1990-12-21 05:55:00.000');
STORE A into 'output';
{code}

This will throw an exception in pig:
Caused by: Failed to parse: Pig script failed to parse: 
<line 1, column 23> pig script failed to validate: 
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative 
path in absolute URI...

Reason is that setHdfsServers method is called in pig, see PIG-4939",2016-11-08T08:41:14.455+0000,2016-11-08T08:45:33.804+0000,,Major
SPARK-4440,Enhance the job progress API to expose more information,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12401484'>, <JIRA IssueLink: id='12404769'>, <JIRA IssueLink: id='12402012'>]","The progress API introduced in SPARK-2321 provides a new way for user to monitor job progress. However the information exposed in the API is relatively limited. It'll be much more useful if we can enhance the API to expose more data.
Some improvement for example may include but not limited to:
1. Stage submission and completion time.
2. Task metrics.
The requirement is initially identified for the hive on spark project(HIVE-7292), other application should benefit as well.",2014-11-17T03:25:38.890+0000,2019-05-21T05:36:33.281+0000,Incomplete,Major
APEXCORE-679, Build fails with hadoop 2.7.x dependency,APEXCORE,Improvement,Open,[],2,"[<JIRA IssueLink: id='12498941'>, <JIRA IssueLink: id='12498928'>]","1. I downloaded apache-apex-core-3.5.0-source-release.tar.gz
2. Extracted the tar
3. Ran following command to build apex core:
mvn clean package -DskipTests -Dhadoop.version=2.7.3
(NOTE: I have overridden hadoop version to 2.7.3)

I get following compilation error:
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project apex-engine: Compilation failure: Compilation failure:
[ERROR] /home/chinmay/files/apache-apex-core-3.5.0/engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java:[159,87] cannot find symbol
[ERROR] symbol:   variable DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT
[ERROR] location: class org.apache.hadoop.yarn.conf.YarnConfiguration
[ERROR] /home/chinmay/files/apache-apex-core-3.5.0/engine/src/main/java/com/datatorrent/stram/client/StramAppLauncher.java:[586,120] cannot find symbol
[ERROR] symbol:   variable DELEGATION_TOKEN_MAX_LIFETIME_KEY
[ERROR] location: class org.apache.hadoop.yarn.conf.YarnConfiguration
[ERROR] /home/chinmay/files/apache-apex-core-3.5.0/engine/src/main/java/com/datatorrent/stram/client/StramAppLauncher.java:[586,173] cannot find symbol
[ERROR] symbol:   variable DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT
[ERROR] location: class org.apache.hadoop.yarn.conf.YarnConfiguration
[ERROR] -> [Help 1]

It should compile with 2.7.x version of hadoop.",2017-03-23T14:31:52.921+0000,2017-03-24T16:57:02.904+0000,,Minor
SENTRY-423,"Hive command ""SHOW TABLE EXTENDED LIKE... "" failed with NPE",SENTRY,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12397355'>, <JIRA IssueLink: id='12396230'>]"," show table extended in jira like 'sam*' will throw NPE with following error:
{code}
2014-09-05 07:21:12,921 ERROR org.apache.hadoop.hive.ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnescapedName(BaseSemanticAnalyzer.java:401)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnescapedName(BaseSemanticAnalyzer.java:397)
	at org.apache.sentry.binding.hive.HiveAuthzBindingHook.extractTable(HiveAuthzBindingHook.java:241)
	at org.apache.sentry.binding.hive.HiveAuthzBindingHook.preAnalyze(HiveAuthzBindingHook.java:167)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:452)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:988)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:98)
	at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:163)
	at org.apache.hive.service.cli.session.HiveSessionImpl.runOperationWithLogCapture(HiveSessionImpl.java:514)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:222)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:204)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:168)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:316)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1373)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1358)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:608)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}
In order to have a correct returned result, the HIVE-1363 needs to be resolved as well",2014-09-08T04:02:55.696+0000,2014-09-22T03:09:44.795+0000,Fixed,Major
AMBARI-22803,Update Hadoop RPC Encryption Properties During Kerberization,AMBARI,Task,Resolved,[],3,"[<JIRA IssueLink: id='12524505'>, <JIRA IssueLink: id='12527073'>, <JIRA IssueLink: id='12526560'>]","When *HDP 3.0.0* is installed, clients should have the ability to choose encrypted communication over RPC when talking to core hadoop components. Today, the properties that control this are:
 - {{core-site.xml : hadoop.rpc.protection = authentication}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication}}

The new value of {{privacy}} enables clients to choose an encrypted means of communication. By keeping {{authentication}} first, it will be taken as the default mechanism so that wire encryption is not automatically enabled by accident.

The following properties should be changed to add {{privacy}}:
 - {{core-site.xml : hadoop.rpc.protection = authentication,privacy}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication,privacy}}

The following are cases when this needs to be performed:
 - During Kerberization, the above two properties should be automatically reconfigured.
 - During a stack upgrade to any version of *HDP 3.0.0* is covered by AMBARI-22981

Blueprint deployment is not a scenario being covered here.",2018-01-17T18:27:29.485+0000,2018-03-09T16:00:21.545+0000,Fixed,Critical
CALCITE-4663,And predicate in one subtree of Join causes JoinPushTransitivePredicatesRule pulls up predicates infinitely and StackOverflowError,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12645885'>, <JIRA IssueLink: id='12617960'>]","the query is *select empid from hr.emps where deptno in (select deptno from hr.depts where deptno between 1000 and 2000)* and the logical plan tree after decorrelated optimization is 

LogicalProject(empid=[$0])
    LogicalJoin(condition=[=($1, $5)], joinType=[inner])
        LogicalTableScan(table=[[hr, emps]])
        LogicalAggregate(group=[\{0}])
            LogicalProject(deptno=[$0])
                LogicalFilter(condition=[*AND(>=($0, 1000), <=($0, 2000))*])
                     LogicalTableScan(table=[[hr, depts]])

the *AND* predicate behaves different with *Search* In JoinConditionBasedPredicateInference.

We can reproduce this problem through adding a test case like SortRemoveRuleTest.
{code:java}
public final class JoinPushTransitivePredicatesRuleTest {
  @Test
  void conjunctionTransitive() throws Exception {

    SchemaPlus rootSchema = Frameworks.createRootSchema(true);
    SchemaPlus defSchema = rootSchema.add(""hr"", new HrClusteredSchema());
    FrameworkConfig config = Frameworks.newConfigBuilder()
        .parserConfig(SqlParser.Config.DEFAULT)
        .defaultSchema(defSchema)
        .traitDefs(ConventionTraitDef.INSTANCE, RelCollationTraitDef.INSTANCE)
        .build();

    String sql = ""select \""empid\"" from \""hr\"".\""emps\"" where \""deptno\"" in (select \""deptno\"" from \""hr\"".\""depts\"" where \""deptno\"" between 1000 and 2000)"";
    Planner planner = Frameworks.getPlanner(config);
    SqlNode parse = planner.parse(sql);
    SqlNode validate = planner.validate(parse);
    RelRoot planRoot = planner.rel(validate);
    RelNode planBefore = planRoot.rel;
    HepProgram hepProgram = HepProgram.builder()
//        .addRuleInstance(CoreRules.FILTER_REDUCE_EXPRESSIONS)
        .addRuleInstance(CoreRules.JOIN_PUSH_TRANSITIVE_PREDICATES)
        .build();

    HepPlanner hepPlanner = new HepPlanner(hepProgram);
    hepPlanner.setRoot(planBefore);
    hepPlanner.findBestExp();
  }
}

{code}
{color:#ff0000}Exception in thread ""main"" java.lang.StackOverflowError{color}

The culprit is that the JoinPushTransitivePredicatesRule simplify pulledUpPredicates, otherwise JoinConditionBasedPredicateInference not, so that the JoinConditionBasedPredicateInference can infer predicates indefinitely.

Though we can add a *CoreRules.FILTER_REDUCE_EXPRESSIONS* before *CoreRules.JOIN_PUSH_TRANSITIVE_PREDICATES* as a workaround, but I think we can simplify left and right child predicates in JoinConditionBasedPredicateInference constructor and seems better.

 

I add simplification for child predicates and StackOverflowError disappears.
{code:java}
leftChildPredicates = simplify.simplify(leftPredicates.accept(
    new RexPermuteInputsShuttle(leftMapping, joinRel.getInput(0))));

rightChildPredicates = simplify.simplify(rightPredicates.accept(
    new RexPermuteInputsShuttle(rightMapping, joinRel.getInput(1))));

{code}
  ",2021-06-23T03:48:29.043+0000,2022-08-18T12:18:51.805+0000,Fixed,Major
SOLR-9515,Update to Hadoop 3,SOLR,Improvement,Closed,"[<JIRA Issue: key='SOLR-13630', id='13244794'>]",22,"[<JIRA IssueLink: id='12553119'>, <JIRA IssueLink: id='12553117'>, <JIRA IssueLink: id='12553116'>, <JIRA IssueLink: id='12553121'>, <JIRA IssueLink: id='12553120'>, <JIRA IssueLink: id='12557362'>, <JIRA IssueLink: id='12552996'>, <JIRA IssueLink: id='12571057'>, <JIRA IssueLink: id='12501126'>, <JIRA IssueLink: id='12498959'>, <JIRA IssueLink: id='12598493'>, <JIRA IssueLink: id='12553055'>, <JIRA IssueLink: id='12553146'>, <JIRA IssueLink: id='12553131'>, <JIRA IssueLink: id='12557001'>, <JIRA IssueLink: id='12480300'>, <JIRA IssueLink: id='12553129'>, <JIRA IssueLink: id='12556151'>, <JIRA IssueLink: id='12553128'>, <JIRA IssueLink: id='12552865'>, <JIRA IssueLink: id='12557043'>, <JIRA IssueLink: id='12553122'>]","Hadoop 3 is not out yet, but I'd like to iron out the upgrade to be prepared. I'll start up a dev branch.",2016-09-14T21:38:47.940+0000,2020-09-10T14:01:39.591+0000,Fixed,Blocker
TEZ-953,Port MAPREDUCE-5493,TEZ,Task,Closed,[],1,[<JIRA IssueLink: id='12384995'>],,2014-03-19T18:18:43.939+0000,2014-03-30T08:01:44.994+0000,Fixed,Major
SPARK-12014,Spark SQL query containing semicolon is broken in Beeline (related to HIVE-11100),SPARK,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12466844'>, <JIRA IssueLink: id='12450291'>, <JIRA IssueLink: id='12542972'>]","Actually it is known hive issue: https://issues.apache.org/jira/browse/HIVE-11100

patch available: https://reviews.apache.org/r/35907/diff/1

but Spark uses its own maven dependencies for hive (org.spark-project.hive), we can not use this patch to fix the problem, it would be better if you can fix this in spark's hive package.

In spark's beeline, the error message will be:

{code}
0: jdbc:hive2://host:10000/> CREATE TABLE beeline_tb (c1 int, c2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ';' LINES TERMINATED BY '\n';
Error: org.apache.spark.sql.AnalysisException: mismatched input '<EOF>' expecting StringLiteral near 'BY' in table row format's field separator; line 1 pos 87 (state=,code=0)

0: jdbc:hive2://host:10000/> CREATE TABLE beeline_tb (c1 int, c2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\;' LINES TERMINATED BY '\n';
Error: org.apache.spark.sql.AnalysisException: mismatched input '<EOF>' expecting StringLiteral near 'BY' in table row format's field separator; line 1 pos 88 (state=,code=0)

0: jdbc:hive2://host:10000/> SELECT str_to_map(other_data,';','=')['key_name'] FROM some_logs WHERE log_date = '20151125' limit 5;
Error: org.apache.spark.sql.AnalysisException: cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in select expression; line 1 pos 30 (state=,code=0)

0: jdbc:hive2://host:10000/> SELECT str_to_map(other_data,'\;','=')['key_name'] FROM some_logs WHERE log_date = '20151125' limit 5;
Error: org.apache.spark.sql.AnalysisException: cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in select expression; line 1 pos 31 (state=,code=0)
{code}",2015-11-26T13:30:27.412+0000,2019-10-12T02:38:57.484+0000,Incomplete,Minor
IMPALA-11234,impalad keeps reporting ShortCircuitCache slot release failures in heavy workload,IMPALA,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12637528'>, <JIRA IssueLink: id='12637529'>]","I keep seeing this error during a local perf test on my desktop machine:
{code:java}
E0410 07:04:10.691095   430 ShortCircuitCache.java:232] ShortCircuitCache(0x6e76c6a7): failed to release short-circuit shared memory slot Slot(slotIdx=0, shm=DfsClientShm(1effcf56a590fbc371938a368987f4e9)) by sending ReleaseShortCircuitAccessRequestProto to /var/lib/hadoop-hdfs/socket.31001.  Closing shared memory segment.
Java exception follows:
java.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId 1effcf56a590fbc371938a368987f4e9
        at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:214)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
 {code}
I can also find it in our Jenkins jobs, but it only happens in the data-loading phase. So I suspend it only happens in heavy workloads.

HDFS-14701 mentioned that this happens when the DataNode is stopped/restarted. But I didn't restart my HDFS cluster and I'm still able to see this error log.

It worth investigating if we are doing something wrong in short-circuit related stuffs.",2022-04-11T00:03:39.722+0000,2022-05-09T00:12:38.354+0000,Fixed,Major
BIGTOP-885,TestHiveSmokeBulk fails on Hive 0.9,BIGTOP,Bug,Closed,[],2,"[<JIRA IssueLink: id='12367873'>, <JIRA IssueLink: id='12367386'>]",TestHiveSmokeBulk fails with 17 failures and 1 error when smoking Hive 0.9,2013-03-26T03:22:35.787+0000,2013-06-21T23:49:47.738+0000,Fixed,Blocker
ZEPPELIN-2232,Support Spark SQL for Pig Interpreter ,ZEPPELIN,Bug,Open,[],1,[<JIRA IssueLink: id='12496569'>],,2017-03-08T00:00:09.096+0000,2018-06-09T07:24:39.071+0000,,Major
HDDS-7133,Upgrade reflections to 0.9.12 to avoid guava conflicts,HDDS,Improvement,Open,[],1,[<JIRA IssueLink: id='12645837'>],"Upgrade Reflections to 0.9.12 from 0.9.11,
We were on 0.9.12 in release 1.0.0 & we got downgraded to 0.9.11

0.9.12 solves the below problem:

https://github.com/ronmamo/reflections/issues/194#issuecomment-573390884

Rel: 1.0.0 with 0.9.12
https://github.com/apache/ozone/blob/ozone-1.0.0/hadoop-ozone/ozone-manager/pom.xml#L108-L113",2022-08-17T15:35:12.305+0000,2022-08-18T20:49:38.682+0000,,Major
SENTRY-905,Sentry should facilitate some means of authorizing jars outside of Hive's aux jars path,SENTRY,Bug,Open,[],1,[<JIRA IssueLink: id='12492394'>],"Sentry should add some means to authorize jars outside of hive aux jars. Currently users need to add jars to the aux path and restart HS2 in order to use them. 

",2015-10-07T18:02:22.054+0000,2018-12-10T04:41:20.048+0000,,Major
REEF-705,Support updating resources for Driver,REEF,New Feature,Open,[],1,[<JIRA IssueLink: id='12436069'>],With Driver HA we have the support to reliably run Driver for long running applications. This enables users to create and deploy never ending services built on REEF. A key missing element here though is deployment of new binaries/resources for Driver such that it can come back and reconnect with the evaluators seamlessly.,2015-09-01T00:14:11.474+0000,2015-09-01T00:32:15.069+0000,,Major
AVRO-1530,Java DataFileStream does not allow distinguishing between empty files and corrupt files,AVRO,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12390628'>, <JIRA IssueLink: id='12444599'>]","When writing data to HDFS, especially with Flume, it's possible to write empty files. When you run Hive queries over this data, the job fails with ""Not a data file."" from here https://github.com/apache/avro/blob/trunk/lang/java/avro/src/main/java/org/apache/avro/file/DataFileStream.java#L102",2014-06-23T19:54:27.474+0000,2015-10-02T20:07:06.989+0000,Won't Fix,Major
FLINK-28729,flink hive catalog don't support jdk11,FLINK,Bug,Open,[],1,[<JIRA IssueLink: id='12644724'>],"when I upgraded jdk to 11,I got the following error:
{code:java}
<dependency>
         <groupId>org.apache.flink</groupId>
         <artifactId>flink-sql-connector-hive-3.1.2_2.12</artifactId>
         <version>1.15.1</version>
     </dependency> {code}
{code:java}
// error
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1654)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:80)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:115)
    ... 84 more
Caused by: java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)
    ... 87 more
Caused by: MetaException(message:Got exception: java.lang.ClassCastException class [Ljava.lang.Object; cannot be cast to class [Ljava.net.URI; ([Ljava.lang.Object; and [Ljava.net.URI; are in module java.base of loader 'bootstrap'))
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1342)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:278)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:210)
    ... 92 more
Process finished with exit code -1
 {code}",2022-07-28T09:06:13.133+0000,2022-07-29T09:39:04.110+0000,,Major
DRILL-6569,Jenkins Regression: TPCDS query 19 fails with INTERNAL_ERROR ERROR: Can not read value at 2 in block 0 in file maprfs:///drill/testdata/tpcds_sf100/parquet/store_sales/1_13_1.parquet,DRILL,Bug,Closed,[],1,[<JIRA IssueLink: id='12541533'>],"This is TPCDS Query 19.

I am able to scan the parquet file using:

   select * from dfs.`/drill/testdata/tpcds_sf100/parquet/store_sales/1_13_1.parquet`

and I get 3,349,279 rows selected.

There are roughly 15 similar failures in the Advanced nightly run, out of 37 failures.  So this issue accounts for about half the failures.

Query: /root/drillAutomation/framework-master/framework/resources/Advanced/tpcds/tpcds_sf100/hive/parquet/query19.sql

SELECT i_brand_id              brand_id,
i_brand                 brand,
i_manufact_id,
i_manufact,
Sum(ss_ext_sales_price) ext_price
FROM   date_dim,
store_sales,
item,
customer,
customer_address,
store
WHERE  d_date_sk = ss_sold_date_sk
AND ss_item_sk = i_item_sk
AND i_manager_id = 38
AND d_moy = 12
AND d_year = 1998
AND ss_customer_sk = c_customer_sk
AND c_current_addr_sk = ca_address_sk
AND Substr(ca_zip, 1, 5) <> Substr(s_zip, 1, 5)
AND ss_store_sk = s_store_sk
GROUP  BY i_brand,
i_brand_id,
i_manufact_id,
i_manufact
ORDER  BY ext_price DESC,
i_brand,
i_brand_id,
i_manufact_id,
i_manufact
LIMIT 100;

Here is the stack trace:
2018-06-29 07:00:32 INFO  DrillTestLogger:348 - 
Exception:

java.sql.SQLException: INTERNAL_ERROR ERROR: Can not read value at 2 in block 0 in file maprfs:///drill/testdata/tpcds_sf100/parquet/store_sales/1_13_1.parquet

Fragment 4:26

[Error Id: 6401a71e-7a5d-4a10-a17c-16873fc3239b on atsqa6c88.qa.lab:31010]

  (hive.org.apache.parquet.io.ParquetDecodingException) Can not read value at 2 in block 0 in file maprfs:///drill/testdata/tpcds_sf100/parquet/store_sales/1_13_1.parquet
    hive.org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue():243
    hive.org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue():227
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():199
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():57
    org.apache.drill.exec.store.hive.readers.HiveAbstractReader.hasNextValue():417
    org.apache.drill.exec.store.hive.readers.HiveParquetReader.next():54
    org.apache.drill.exec.physical.impl.ScanBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.buildSchema():118
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.physical.impl.BaseRootExec.next():103
    org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext():93
    org.apache.drill.exec.physical.impl.BaseRootExec.next():93
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():294
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():281
    java.security.AccessController.doPrivileged():-2
    javax.security.auth.Subject.doAs():422
    org.apache.hadoop.security.UserGroupInformation.doAs():1595
    org.apache.drill.exec.work.fragment.FragmentExecutor.run():281
    org.apache.drill.common.SelfCleaningRunnable.run():38
    java.util.concurrent.ThreadPoolExecutor.runWorker():1149
    java.util.concurrent.ThreadPoolExecutor$Worker.run():624
    java.lang.Thread.run():748
  Caused By (java.lang.UnsupportedOperationException) org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$8$1
    hive.org.apache.parquet.io.api.PrimitiveConverter.addInt():101
    hive.org.apache.parquet.column.impl.ColumnReaderImpl$2$3.writeValue():254
    hive.org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter():371
    hive.org.apache.parquet.io.RecordReaderImplementation.read():405
    hive.org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue():218
    hive.org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue():227
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():199
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():57
    org.apache.drill.exec.store.hive.readers.HiveAbstractReader.hasNextValue():417
    org.apache.drill.exec.store.hive.readers.HiveParquetReader.next():54
    org.apache.drill.exec.physical.impl.ScanBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.buildSchema():118
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.physical.impl.BaseRootExec.next():103
    org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext():93
    org.apache.drill.exec.physical.impl.BaseRootExec.next():93
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():294
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():281
    java.security.AccessController.doPrivileged():-2
    javax.security.auth.Subject.doAs():422
    org.apache.hadoop.security.UserGroupInformation.doAs():1595
    org.apache.drill.exec.work.fragment.FragmentExecutor.run():281
    org.apache.drill.common.SelfCleaningRunnable.run():38
    java.util.concurrent.ThreadPoolExecutor.runWorker():1149
    java.util.concurrent.ThreadPoolExecutor$Worker.run():624
    java.lang.Thread.run():748

	at org.apache.drill.jdbc.impl.DrillCursor.nextRowInternally(DrillCursor.java:528)
	at org.apache.drill.jdbc.impl.DrillCursor.loadInitialSchema(DrillCursor.java:600)
	at org.apache.drill.jdbc.impl.DrillResultSetImpl.execute(DrillResultSetImpl.java:1904)
	at org.apache.drill.jdbc.impl.DrillResultSetImpl.execute(DrillResultSetImpl.java:64)
	at oadd.org.apache.calcite.avatica.AvaticaConnection$1.execute(AvaticaConnection.java:630)
	at org.apache.drill.jdbc.impl.DrillMetaImpl.prepareAndExecute(DrillMetaImpl.java:1109)
	at org.apache.drill.jdbc.impl.DrillMetaImpl.prepareAndExecute(DrillMetaImpl.java:1120)
	at oadd.org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:638)
	at org.apache.drill.jdbc.impl.DrillConnectionImpl.prepareAndExecuteInternal(DrillConnectionImpl.java:200)
	at oadd.org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:149)
	at oadd.org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:218)
	at org.apache.drill.jdbc.impl.DrillStatementImpl.executeQuery(DrillStatementImpl.java:110)
	at org.apache.drill.test.framework.DrillTestJdbc.executeQuery(DrillTestJdbc.java:210)
	at org.apache.drill.test.framework.DrillTestJdbc.run(DrillTestJdbc.java:115)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: oadd.org.apache.drill.common.exceptions.UserRemoteException: INTERNAL_ERROR ERROR: Can not read value at 2 in block 0 in file maprfs:///drill/testdata/tpcds_sf100/parquet/store_sales/1_13_1.parquet

Fragment 4:26

[Error Id: 6401a71e-7a5d-4a10-a17c-16873fc3239b on atsqa6c88.qa.lab:31010]

  (hive.org.apache.parquet.io.ParquetDecodingException) Can not read value at 2 in block 0 in file maprfs:///drill/testdata/tpcds_sf100/parquet/store_sales/1_13_1.parquet
    hive.org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue():243
    hive.org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue():227
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():199
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():57
    org.apache.drill.exec.store.hive.readers.HiveAbstractReader.hasNextValue():417
    org.apache.drill.exec.store.hive.readers.HiveParquetReader.next():54
    org.apache.drill.exec.physical.impl.ScanBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.buildSchema():118
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.physical.impl.BaseRootExec.next():103
    org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext():93
    org.apache.drill.exec.physical.impl.BaseRootExec.next():93
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():294
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():281
    java.security.AccessController.doPrivileged():-2
    javax.security.auth.Subject.doAs():422
    org.apache.hadoop.security.UserGroupInformation.doAs():1595
    org.apache.drill.exec.work.fragment.FragmentExecutor.run():281
    org.apache.drill.common.SelfCleaningRunnable.run():38
    java.util.concurrent.ThreadPoolExecutor.runWorker():1149
    java.util.concurrent.ThreadPoolExecutor$Worker.run():624
    java.lang.Thread.run():748
  Caused By (java.lang.UnsupportedOperationException) org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$8$1
    hive.org.apache.parquet.io.api.PrimitiveConverter.addInt():101
    hive.org.apache.parquet.column.impl.ColumnReaderImpl$2$3.writeValue():254
    hive.org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter():371
    hive.org.apache.parquet.io.RecordReaderImplementation.read():405
    hive.org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue():218
    hive.org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue():227
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():199
    org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next():57
    org.apache.drill.exec.store.hive.readers.HiveAbstractReader.hasNextValue():417
    org.apache.drill.exec.store.hive.readers.HiveParquetReader.next():54
    org.apache.drill.exec.physical.impl.ScanBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.sniffNonEmptyBatch():276
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.prefetchFirstBatchFromBothSides():238
    org.apache.drill.exec.physical.impl.join.HashJoinBatch.buildSchema():218
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.physical.impl.aggregate.HashAggBatch.buildSchema():118
    org.apache.drill.exec.record.AbstractRecordBatch.next():152
    org.apache.drill.exec.record.AbstractRecordBatch.next():119
    org.apache.drill.exec.record.AbstractRecordBatch.next():109
    org.apache.drill.exec.record.AbstractUnaryRecordBatch.innerNext():63
    org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext():147
    org.apache.drill.exec.record.AbstractRecordBatch.next():172
    org.apache.drill.exec.physical.impl.BaseRootExec.next():103
    org.apache.drill.exec.physical.impl.SingleSenderCreator$SingleSenderRootExec.innerNext():93
    org.apache.drill.exec.physical.impl.BaseRootExec.next():93
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():294
    org.apache.drill.exec.work.fragment.FragmentExecutor$1.run():281
    java.security.AccessController.doPrivileged():-2
    javax.security.auth.Subject.doAs():422
    org.apache.hadoop.security.UserGroupInformation.doAs():1595
    org.apache.drill.exec.work.fragment.FragmentExecutor.run():281
    org.apache.drill.common.SelfCleaningRunnable.run():38
    java.util.concurrent.ThreadPoolExecutor.runWorker():1149
    java.util.concurrent.ThreadPoolExecutor$Worker.run():624
    java.lang.Thread.run():748

	at oadd.org.apache.drill.exec.rpc.user.QueryResultHandler.resultArrived(QueryResultHandler.java:123)
	at oadd.org.apache.drill.exec.rpc.user.UserClient.handle(UserClient.java:422)
	at oadd.org.apache.drill.exec.rpc.user.UserClient.handle(UserClient.java:96)
	at oadd.org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:274)
	at oadd.org.apache.drill.exec.rpc.RpcBus$InboundHandler.decode(RpcBus.java:244)
	at oadd.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:88)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
	at oadd.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
	at oadd.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
	at oadd.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:312)
	at oadd.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:286)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
	at oadd.io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
	at oadd.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
	at oadd.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at oadd.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at oadd.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at oadd.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at oadd.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at oadd.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 1 more

The commit id is:
1.14.0-SNAPSHOT 140d09e69b65ac2cb1bed09a37fa5861d39a99b3 DRILL-6539: Record count not set for this vector container error 28.06.2018 @ 16:13:20 PDT Unknown 28.06.2018 @ 16:21:42 PDT",2018-06-30T01:11:03.857+0000,2018-11-21T00:47:29.595+0000,Won't Fix,Critical
SPARK-28841,"Spark cannot read a relative path containing "":""",SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12568256'>],"Reproducer:
{code}
spark.read.parquet(""test:test"")
{code}

Error:
{code}
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: test:test
{code}

This is actually a Hadoop issue since the error is thrown from ""new Path(""test:test"")"". I'm creating this ticket to see if we can work around this issue in Spark.",2019-08-21T17:59:35.073+0000,2021-05-25T01:50:11.902+0000,Incomplete,Major
TEZ-3633,Implement keep-alive timeout in tez shuffle handler,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12495367'>],"MAPREDUCE-5787 which added keep-alive to mapreduce shuffle handler was not fully functional as despite advertising keep-alive option and adding the  header to the response, all connections were closed immediately after write. This reduced the performance of certain fetches as now time is spent requesting a second get to the same serve, only for that server to reset the connection forcing the client to reestablish the connection on another port. The details of this is hidden behind HttpURLConnection and doesn't show in any log file at default logging level. However TCP sniffing does show errant behavior.",2017-02-21T21:15:32.151+0000,2017-08-22T00:03:07.823+0000,Fixed,Major
CALCITE-1655,Druid adapter: add IN filter,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12495382'>, <JIRA IssueLink: id='12495718'>]","The druid calcite adapter throw an exception when an IN filter is used.
This happens only in hive because in calcite project the IN filter is transformed to OR automatically. 
Since this rule does not kick in HIVE and it is better to use the native IN filter from druid instead having huge number of OR clauses i will send a patch that adds the IN filter.  ",2017-02-24T00:48:40.483+0000,2017-03-24T03:20:24.508+0000,Fixed,Major
CALCITE-1731,Rewriting of queries using materialized views with joins and aggregates,CALCITE,New Feature,Closed,[],8,"[<JIRA IssueLink: id='12513686'>, <JIRA IssueLink: id='12508489'>, <JIRA IssueLink: id='12503689'>, <JIRA IssueLink: id='12503922'>, <JIRA IssueLink: id='12504040'>, <JIRA IssueLink: id='12502117'>, <JIRA IssueLink: id='12502805'>, <JIRA IssueLink: id='12499515'>]","The idea is still to build a rewriting approach similar to:
ftp://ftp.cse.buffalo.edu/users/azhang/disc/SIGMOD/pdf-files/331/202-optimizing.pdf

I tried to build on CALCITE-1389 work. However, finally I ended up creating a new alternative rule. The main reason is that I wanted to follow the paper more closely and not rely on triggering rules within the MV rewriting to find whether expressions are equivalent. Instead, we extract information from the query plan and the MVs plans using the new metadata providers proposed in CALCITE-1682, and then we use that information to validate and execute the rewriting.

I also implemented new unifying/rewriting logic within the rule, since existing unifying rules for aggregates were assuming that aggregate inputs in the query and the MV needed to be equivalent (same Volcano node). That condition can be relaxed because we verify in the rule, by using the new metadata providers as stated above, that the result for the query is contained within the MV.

I added multiple tests, but any feedback pointing to new tests that could be added to check correctness/coverage is welcome.

Algorithm can trigger multiple rewritings for the same query node. In addition, support for multiple usages of tables in query/MVs is supported.

A few extensions that will follow this issue:
* Extend logic to filter relevant MVs for a given query node, so approach is scalable as number of MVs grows.
* Produce rewritings using Union operators, e.g., a given query could be partially answered from the MV (_year = 2014_) and from the query (_not(year=2014)_). If the MV is stored e.g. in Druid, this rewriting might be beneficial. As with the other rewritings, decision on whether to finally use the rewriting should be cost-based.",2017-03-30T17:56:53.368+0000,2017-09-02T00:59:56.773+0000,Fixed,Major
PHOENIX-6692,Add HBase 2.5 support,PHOENIX,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12638901'>, <JIRA IssueLink: id='12643649'>, <JIRA IssueLink: id='12640527'>]","I was talking with [~apurtell], who's RM for HBase 2.5, and he let me know that HBase 2.5 will be released very soon. Since we're also planning on releasing Phoenix 5.2 soon, we should make it sure it releases with HBase 2.5 support assuming this isn't too time-consuming / complicated. ",2022-04-22T18:27:31.349+0000,2022-09-09T04:35:22.131+0000,Fixed,Major
SENTRY-316,Users should be allowed to see tables in a db on which the user has authorization without having to switch to the db.,SENTRY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12390529'>],"Assume user has SELECT and INSERT privileges on say 'test_db'.'test_table'.

On connecting to Hive, user is placed in the 'default' .
If the user executes : SHOW TABLES in test_db
no rows are returned.

If the user does a : USE test_db
and then does a : SHOW TABLES in test_db
then, the table 'test_table' is returned.
",2014-06-27T21:17:06.685+0000,2014-09-19T20:13:51.656+0000,Fixed,Major
PHOENIX-6756,switch to using log4j2.properties instead of xml,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12644492'>, <JIRA IssueLink: id='12644491'>]","Hbase has switched, we should follow suit.",2022-07-26T11:47:12.202+0000,2022-08-04T12:06:23.280+0000,Fixed,Major
SPARK-1021,sortByKey() launches a cluster job when it shouldn't,SPARK,Sub-task,Resolved,[],6,"[<JIRA IssueLink: id='12401861'>, <JIRA IssueLink: id='12447682'>, <JIRA IssueLink: id='12391799'>, <JIRA IssueLink: id='12405576'>, <JIRA IssueLink: id='12412323'>, <JIRA IssueLink: id='12407939'>]","The sortByKey() method is listed as a transformation, not an action, in the documentation.  But it launches a cluster job regardless.

http://spark.incubator.apache.org/docs/latest/scala-programming-guide.html

Some discussion on the mailing list suggested that this is a problem with the rdd.count() call inside Partitioner.scala's rangeBounds method.

https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L102

Josh Rosen suggests that rangeBounds should be made into a lazy variable:

{quote}
I wonder whether making RangePartitoner .rangeBounds into a lazy val would fix this (https://github.com/apache/incubator-spark/blob/6169fe14a140146602fb07cfcd13eee6efad98f9/core/src/main/scala/org/apache/spark/Partitioner.scala#L95).  We'd need to make sure that rangeBounds() is never called before an action is performed.  This could be tricky because it's called in the RangePartitioner.equals() method.  Maybe it's sufficient to just compare the number of partitions, the ids of the RDDs used to create the RangePartitioner, and the sort ordering.  This still supports the case where I range-partition one RDD and pass the same partitioner to a different RDD.  It breaks support for the case where two range partitioners created on different RDDs happened to have the same rangeBounds(), but it seems unlikely that this would really harm performance since it's probably unlikely that the range partitioners are equal by chance.
{quote}

Can we please make this happen?  I'll send a PR on GitHub to start the discussion and testing.",2014-01-09T22:47:47.952+0000,2015-11-12T05:19:43.533+0000,Won't Fix,Major
IMPALA-3916,Reserve SQL:2016 keywords,IMPALA,Task,Resolved,"[<JIRA Issue: key='IMPALA-6462', id='13134940'>]",2,"[<JIRA IssueLink: id='12533762'>, <JIRA IssueLink: id='12518958'>]","Reserve common SQL keywords to match:
* HIVE-6617
* docs: http://www.cloudera.com/documentation/enterprise/latest/topics/impala_reserved_words.html

Should be a configuration option that is on by default in the next compatibility breaking release",2016-07-26T20:09:27.000+0000,2018-05-10T20:48:25.696+0000,Fixed,Minor
TEZ-2531,Support fuzzy search for dag name,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12426557'>],"Currently, on the tez-ui, search for dag name must be exactly match. Fuzzy match would be useful when user use tez for some iterative problem. In the iterative case, there would be a list of dags which may has the same prefix in the dag name. Search the dag name prefix would be helpful in this case. ",2015-06-03T04:58:35.296+0000,2015-06-03T05:16:48.198+0000,,Major
THRIFT-2805,0.9.3 release candidate,THRIFT,Bug,Closed,"[<JIRA Issue: key='THRIFT-3305', id='12858513'>, <JIRA Issue: key='THRIFT-3308', id='12858729'>]",1,[<JIRA IssueLink: id='12434618'>],,2014-11-08T13:26:00.170+0000,2015-10-12T02:52:01.020+0000,Fixed,Major
BIGTOP-162,downstream components need to be patched to be compatible with MR2,BIGTOP,Bug,Closed,[],5,"[<JIRA IssueLink: id='12345806'>, <JIRA IssueLink: id='12345741'>, <JIRA IssueLink: id='12345069'>, <JIRA IssueLink: id='12345047'>, <JIRA IssueLink: id='12345740'>]","Here's a list of downstream JIRAs:
    * SQOOP-354
    * PIG-2125, PIG-2277
    * OOZIE-565
    * HIVE-2468
    * MAHOUT-822",2011-10-25T22:30:44.020+0000,2013-06-21T23:55:12.706+0000,Fixed,Major
CALCITE-3609,Wrong query results in Hive due to wrong struct nullability ,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12579564'>, <JIRA IssueLink: id='12576917'>]","student table：
{code:java}
CREATE TABLE `student`(
  `id` int, 
  `info` struct<name:string,age:int>)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
WITH SERDEPROPERTIES ( 
  'colelction.delim'=':', 
  'field.delim'=',', 
  'serialization.format'=',', 
  'serialization.null.format'='NULL')  
{code}
the sql:

 
{code:java}
select * from student where info is not null;
{code}
result:

 
{code:java}
1       {""name"":""zhou"",""age"":30}
2       {""name"":""yan"",""age"":30}
3       {""name"":""chen"",""age"":20}
4       {""name"":""li"",""age"":80}
NULL    NULL
NULL    {""name"":null,""age"":null}
{code}
cause:calcite Ineffective optimization
{code:java}
HiveProject(id=[$0], info=[$1]) 
  HiveTableScan(table=[[default.student]], table:alias=[student])
{code}
 

after fix RelRecordType isNullable,the plan is
{code:java}
HiveProject(id=[$0], info=[$1])
  HiveFilter(condition=[IS NOT NULL($1)])
    HiveTableScan(table=[[default.student]], table:alias=[student])
{code}
then the result is:
{code:java}
1 {""name"":""zhou"",""age"":30} 
2 {""name"":""yan"",""age"":30} 
3 {""name"":""chen"",""age"":20} 
4 {""name"":""li"",""age"":80}
NULL    {""name"":null,""age"":null}

{code}
 ",2019-12-18T11:18:06.284+0000,2021-05-28T10:09:13.521+0000,Duplicate,Blocker
TEZ-1261,ShuffleVertexManager auto parallelism is broken after TEZ-1131,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12391024'>],A plain 3 vertex join hang forever when turn on auto-parallelism. AM log attached.,2014-07-07T21:43:20.862+0000,2014-09-06T01:35:12.663+0000,Fixed,Major
PHOENIX-4378,Unable to set KEEP_DELETED_CELLS to true on RS scanner,PHOENIX,Bug,Closed,[],7,"[<JIRA IssueLink: id='12524651'>, <JIRA IssueLink: id='12602817'>, <JIRA IssueLink: id='12525607'>, <JIRA IssueLink: id='12576896'>, <JIRA IssueLink: id='12520042'>, <JIRA IssueLink: id='12520043'>, <JIRA IssueLink: id='12523941'>]","[~jamestaylor], 
It seems we may need to fix PHOENIX-4277 differently for HBase 2.0 as we can only update TTL and maxVersions now in preStoreScannerOpen and cannot return a new StoreScanner with updated scanInfo.

for reference:
[1]https://issues.apache.org/jira/browse/PHOENIX-4318?focusedCommentId=16249943&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16249943",2017-11-14T10:32:33.208+0000,2021-02-10T10:01:42.694+0000,Duplicate,Blocker
TEZ-1069,Support ability to re-size a task attempt when previous attempts fail due to resource constraints,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12388606'>],"Consider a case where attempts for the final stage in a long DAG fails due to out of memory. In such a scenario, the framework  ( or via the base vertex manager ) should be able to change the task specifications on the fly to trigger a re-run with modified specs. 

Changes could be both java opts changes as well as container resource requirements. ",2014-04-18T17:45:52.467+0000,2019-02-28T17:30:44.200+0000,,Major
AMBARI-9085,Hive Metastore didn't start if ambari-agent is running with set environment variable DEBUG,AMBARI,Bug,Open,[],1,[<JIRA IssueLink: id='12405136'>],"When is ambari-agent started in terminal with environment variable *DEBUG* set to something, service *Hive Metastore* cannot start. (It is related to ambari-agent on the same server as Hive Metastore.)

How to reproduce:
1. stop Hive Metastore from Ambari web UI.
2. (re)start ambari-agent on the server with Hive Metastore with set env variable DEBUG. 
{noformat}
  DEBUG=1 ambari-agent restart
{noformat}
3. start Hive Metastore from Ambari web UI.

Result:
Task ""Hive Metastore Start"" fails with following output in log:
{noformat}
stderr:   /var/lib/ambari-agent/data/errors-148.txt

Python script has been killed due to timeout

stdout:   /var/lib/ambari-agent/data/output-148.txt

2015-01-12 10:57:55,900 - Execute['mkdir -p /tmp/HDP-artifacts/;     curl -kf -x """" --retry 10     http://dhcp-75-204.lab.eng.brq.redhat.com:8080/resources//UnlimitedJCEPolicyJDK7.zip -o /tmp/HDP-artifacts//UnlimitedJCEPolicyJDK7.zip'] {'environment': ..., 'not_if': 'test -e /tmp/HDP-artifacts//UnlimitedJCEPolicyJDK7.zip', 'ignore_failures': True, 'path': ['/bin', '/usr/bin/']}
2015-01-12 10:57:55,922 - Skipping Execute['mkdir -p /tmp/HDP-artifacts/;     curl -kf -x """" --retry 10     http://dhcp-75-204.lab.eng.brq.redhat.com:8080/resources//UnlimitedJCEPolicyJDK7.zip -o /tmp/HDP-artifacts//UnlimitedJCEPolicyJDK7.zip'] due to not_if
2015-01-12 10:57:56,061 - Directory['/etc/hadoop/conf.empty'] {'owner': 'root', 'group': 'root', 'recursive': True}
2015-01-12 10:57:56,063 - Link['/etc/hadoop/conf'] {'not_if': 'ls /etc/hadoop/conf', 'to': '/etc/hadoop/conf.empty'}
2015-01-12 10:57:56,087 - Skipping Link['/etc/hadoop/conf'] due to not_if
2015-01-12 10:57:56,105 - File['/etc/hadoop/conf/hadoop-env.sh'] {'content': Template('hadoop-env.sh.j2'), 'owner': 'hdfs'}
2015-01-12 10:57:56,106 - XmlConfig['core-site.xml'] {'owner': 'hdfs', 'group': 'hadoop', 'conf_dir': '/etc/hadoop/conf', 'configurations': ...}
2015-01-12 10:57:56,112 - Generating config: /etc/hadoop/conf/core-site.xml
2015-01-12 10:57:56,112 - File['/etc/hadoop/conf/core-site.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None}
2015-01-12 10:57:56,113 - Writing File['/etc/hadoop/conf/core-site.xml'] because contents don't match
2015-01-12 10:57:56,124 - Execute['/bin/echo 0 > /selinux/enforce'] {'only_if': 'test -f /selinux/enforce'}
2015-01-12 10:57:56,146 - Skipping Execute['/bin/echo 0 > /selinux/enforce'] due to only_if
2015-01-12 10:57:56,148 - Execute['mkdir -p /usr/lib/hadoop/lib/native/Linux-i386-32; ln -sf /usr/lib/libsnappy.so /usr/lib/hadoop/lib/native/Linux-i386-32/libsnappy.so'] {}
2015-01-12 10:57:56,178 - Execute['mkdir -p /usr/lib/hadoop/lib/native/Linux-amd64-64; ln -sf /usr/lib64/libsnappy.so /usr/lib/hadoop/lib/native/Linux-amd64-64/libsnappy.so'] {}
2015-01-12 10:57:56,204 - Directory['/var/log/hadoop'] {'owner': 'root', 'group': 'root', 'recursive': True}
2015-01-12 10:57:56,205 - Directory['/var/run/hadoop'] {'owner': 'root', 'group': 'root', 'recursive': True}
2015-01-12 10:57:56,205 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'recursive': True}
2015-01-12 10:57:56,213 - File['/etc/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}
2015-01-12 10:57:56,227 - File['/etc/hadoop/conf/health_check'] {'content': Template('health_check-v2.j2'), 'owner': 'hdfs'}
2015-01-12 10:57:56,228 - File['/etc/hadoop/conf/log4j.properties'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}
2015-01-12 10:57:56,264 - File['/etc/hadoop/conf/hadoop-metrics2.properties'] {'content': Template('hadoop-metrics2.properties.j2'), 'owner': 'hdfs'}
2015-01-12 10:57:56,265 - File['/etc/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
2015-01-12 10:57:56,289 - File['/etc/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2015-01-12 10:57:56,476 - Execute['hive mkdir -p /tmp/HDP-artifacts/ ; cp /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib//mysql-connector-java.jar'] {'creates': '/usr/lib/hive/lib//mysql-connector-java.jar', 'path': ['/bin', '/usr/bin/'], 'not_if': 'test -f /usr/lib/hive/lib//mysql-connector-java.jar'}
2015-01-12 10:57:56,500 - Skipping Execute['hive mkdir -p /tmp/HDP-artifacts/ ; cp /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib//mysql-connector-java.jar'] due to not_if
2015-01-12 10:57:56,500 - Directory['/etc/hive/conf.server'] {'owner': 'hive', 'group': 'hadoop', 'recursive': True}
2015-01-12 10:57:56,502 - XmlConfig['mapred-site.xml'] {'owner': 'hive', 'group': 'hadoop', 'mode': 0600, 'conf_dir': '/etc/hive/conf.server', 'configurations': ...}
2015-01-12 10:57:56,511 - Generating config: /etc/hive/conf.server/mapred-site.xml
2015-01-12 10:57:56,511 - File['/etc/hive/conf.server/mapred-site.xml'] {'owner': 'hive', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0600}
2015-01-12 10:57:56,513 - Writing File['/etc/hive/conf.server/mapred-site.xml'] because contents don't match
2015-01-12 10:57:56,513 - XmlConfig['hive-site.xml'] {'owner': 'hive', 'group': 'hadoop', 'mode': 0600, 'conf_dir': '/etc/hive/conf.server', 'configurations': ...}
2015-01-12 10:57:56,518 - Generating config: /etc/hive/conf.server/hive-site.xml
2015-01-12 10:57:56,518 - File['/etc/hive/conf.server/hive-site.xml'] {'owner': 'hive', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0600}
2015-01-12 10:57:56,520 - Writing File['/etc/hive/conf.server/hive-site.xml'] because contents don't match
2015-01-12 10:57:56,521 - Execute['/bin/sh -c 'cd /usr/lib/ambari-agent/ && curl -kf -x """" --retry 5 http://dhcp-75-204.lab.eng.brq.redhat.com:8080/resources/DBConnectionVerification.jar -o DBConnectionVerification.jar''] {'environment': ..., 'not_if': '[ -f DBConnectionVerification.jar]'}
2015-01-12 10:57:56,634 - File['/etc/hive/conf.server/hive-env.sh'] {'content': Template('hive-env.sh.j2'), 'owner': 'hive', 'group': 'hadoop'}
2015-01-12 10:57:56,645 - File['/tmp/start_metastore_script'] {'content': StaticFile('startMetastore.sh'), 'mode': 0755}
2015-01-12 10:57:56,655 - Execute['export HIVE_CONF_DIR=/etc/hive/conf.server ; /usr/lib/hive/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED]'] {'not_if': 'export HIVE_CONF_DIR=/etc/hive/conf.server ; /usr/lib/hive/bin/schematool -info -dbType mysql -userName hive -passWord [PROTECTED]'}
{noformat}

With no (or empty) variable DEBUG, everithing works as expected and ""Hive Metastore"" properly start.
{noformat}
DEBUG= ambari-agent restart
{noformat}

*UPDATE:*
It seems like the root of the issue is directly in Hive, because when I run following db schema check with env variable DEBUG, the command freeze.
{noformat}
export HIVE_CONF_DIR=/etc/hive/conf.server
DEBUG=1 /usr/lib/hive/bin/schematool -info -dbType mysql -userName hive -passWord $PASSWORD
{noformat}
So if you think it is completely Hive issue (or expected behaviour?) and there is nothink to do from Ambari point of view, fell free to close this jira or move it to the Hive.
",2015-01-12T11:49:17.151+0000,2015-01-12T13:24:37.059+0000,,Minor
HCATALOG-436,JSON SerDe column misnaming on CTAS,HCATALOG,Bug,Closed,[],3,"[<JIRA IssueLink: id='12354934'>, <JIRA IssueLink: id='12353997'>, <JIRA IssueLink: id='12357420'>]","Given an origin table as follows:

--
hive -e 'describe extended ttf'
OK
sterm	string	
count	bigint	
	 	 
Detailed Table Information	Table(tableName:ttf, dbName:default, owner:hive, createTime:1339518715, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:sterm, type:string, comment:null), FieldSchema(name:count, type:bigint, comment:null)], location:hdfs://localhost:54310/user/hive/warehouse/ttf, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{numPartitions=0, numFiles=1, transient_lastDdlTime=1339518715, totalSize=2155, numRows=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
--

On doing a CTAS, such as:

--
hive -e ""create table ttf_json row format serde 'org.apache.hcatalog.data.JsonSerDe' as select * from ttf;""
--

We get a resultant table ttf_json with schema similar to ttf, but on looking at the data present in the json file itself, we'd notice data like this:

--
{""_col0"":""S8.66045288732867"",""_col1"":103}
{""_col0"":""S8.66322678828148"",""_col1"":95}
--

This will then result in this table not being readable.

This is behaviour similar to the one fixed in HCATALOG-275, but we've obviously not fixed all the possibilities of that problem.",2012-06-26T21:17:53.883+0000,2013-02-15T21:32:54.824+0000,Fixed,Major
INFRA-3062,Enable Nexus Access For Apache pig,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12334345'>],"Project URL: http://pig.apache.org/

SVN URL: http://svn.apache.org/repos/asf/pig/

Maven Group Ids: org.apache.pig
 
Managed By This TLP Project: pig 


Pig had recently moved as a TLP and we want to publish pig artifacts with groupid: org.apache.pig. 
Kindly grant acess for the PIG PMC to publish artifacts to the nexus maven repo. 

Here is the discussion about PIG TLP

http://mail-archives.apache.org/mod_mbox/hadoop-general/201009.mbox/%3CAANLkTinHsBCjC9zpaN6RfxyREBXnSzas3YTGT4wx0a8M@mail.gmail.com%3E

",2010-10-12T00:40:01.070+0000,2014-01-22T23:19:45.359+0000,Fixed,Blocker
SENTRY-997,Update HiveAuthorizer of Sentry after HiveAuthorizer interface changes,SENTRY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12453061'>],HIVE-12698 make the HiveAuthorizer api changed. Sentry HiveAuthorizer need to adapt the change.,2015-12-30T07:45:05.649+0000,2016-03-11T07:26:41.876+0000,Fixed,Major
SPARK-1767,Prefer HDFS-cached replicas when scheduling data-local tasks,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12388461'>],,2014-05-08T20:44:06.384+0000,2014-10-02T07:32:36.366+0000,Fixed,Major
ORC-379,ConversionTreeReaders should handle Decimal64,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12536625'>],"ConversionTreeReaders does not seem to handle Decimal64 column vectors. Hive creates VRB with Decimal64ColumnVectors when possible which gets passed down to ConversionTreeReaders which results in following ClassCastException
{code:java}
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.Decimal64ColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector
at org.apache.orc.impl.ConvertTreeReaderFactory$DecimalFromAnyIntegerTreeReader.nextVector(ConvertTreeReaderFactory.java:1190) ~[orc-core-1.5.1.jar:1.5.1]
at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:2012) ~[orc-core-1.5.1.jar:1.5.1]
at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1276) ~[orc-core-1.5.1.jar:1.5.1]
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:88) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:104) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:253) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:228) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:360) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:167) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:52) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:229) ~[hive-shims-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:142) ~[hive-shims-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:205) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:191) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
at org.apache.hadoop.hive.ql.exec.mr.ExecMapRunner.run(ExecMapRunner.java:37) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_121]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_121]
at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_121]{code}",2018-06-15T21:41:00.294+0000,2018-06-29T21:15:53.502+0000,Fixed,Major
SPARK-5080,Expose more cluster resource information to user,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12404555'>, <JIRA IssueLink: id='12406937'>, <JIRA IssueLink: id='12404813'>]","It'll be useful if user can get detailed cluster resource info, e.g. granted/allocated executors, memory and CPU.
Such information is available via WebUI but seems SparkContext doesn't have these APIs.",2015-01-05T02:04:11.094+0000,2016-01-08T14:17:15.267+0000,Won't Fix,Major
CALCITE-1580,Druid adapter: Wrong semantics for ordering within groupBy queries,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12491293'>],"The scope of the ordering seems to be for _each different granularity value_ instead of _global_. 

It can be reproduce with the following kind of query, where there exist multiple values for {{floor_day(`__time`)}}:
{code:sql}
SELECT i_brand_id, floor_day(`__time`), max(ss_quantity), sum(ss_wholesale_cost) as s
FROM store_sales_sold_time_subset
GROUP BY i_brand_id, floor_day(`__time`)
ORDER BY i_brand_id
LIMIT 10;
{code}",2017-01-16T15:48:26.869+0000,2017-03-24T03:20:07.261+0000,Fixed,Critical
TEZ-3554,Add a link to get to all logs from Tez UI while job is running,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12491846'>],Currently the Tez UI shows a link for task attempt logs on multiple different pages. While the job is running this goes directly to the syslog of that particular task attempt. There is no easy way to get to the other logs such as stderr and stdout while the job is running. It would be useful to add a link so that we can get to both the aggregated page of all of the logs as well as having the link to go straight to the task attempt logs. ,2016-12-01T17:14:16.436+0000,2017-08-22T00:03:09.839+0000,Fixed,Major
THRIFT-5197,TSSLTransportFactory Do Not Wrap NOT_OPEN Exception Type for Client,THRIFT,Improvement,Closed,[],1,[<JIRA IssueLink: id='12587727'>],"The class TSSLTransportFactory is wrapping {{TTransportException}} which have a particular ""type"" in type-less {{TTransportException}} and therefore it becomes hard to find the status,... have to unwrap the exception to get the true cause (and the true type) and sometimes the type is not included at all.",2020-05-06T19:02:51.190+0000,2021-02-11T22:26:10.963+0000,Fixed,Major
PHOENIX-2883,Region close during automatic disabling of index for rebuilding can lead to RS abort,PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12466606'>],"(disclaimer: still performing due-diligence on this one)

I've been helping a user this week with what is thought to be a race condition in secondary index updates. This user has a relatively heavy write-based workload with a few tables that each have at least one index.

What we have seen is that when the region distribution is changing (concretely, we were doing a rolling restart of the cluster without the load balancer disabled in the hopes of retaining as much availability as possible), I've seen the following general outline in the logs:

* An index update fails (due to {{ERROR 2008 (INT10)}} the index metadata cache expired or is just missing)
* The index is taken offline to be asynchronously rebuilt
* A flush on the data table's region is queue for quite some time
* RS is asked to close a region (due to a move, commonly)
* RS aborts because the memstore for the data table's region is in an inconsistent state (e.g. {{Assertion failed while closing store <region> <colfam> flushableSize expected=0, actual= 193392. Current memstoreSize=-552208. Maybe a coprocessor operation failed and left the memstore in a partially updated state.}}

Some relevant HBase issues include HBASE-10514 and HBASE-10844.

Have been talking to [~ayingshu] and [~devaraj] about it, but haven't found anything definitively conclusive yet. Will dump findings here.",2016-05-06T22:00:33.327+0000,2018-02-07T07:54:58.765+0000,Incomplete,Major
OOZIE-1372,"When using uber mode, Oozie should also make the AM container size larger",OOZIE,Bug,Closed,[],5,"[<JIRA IssueLink: id='12369223'>, <JIRA IssueLink: id='12443174'>, <JIRA IssueLink: id='12369221'>, <JIRA IssueLink: id='12372614'>, <JIRA IssueLink: id='12368962'>]","OOZIE-1335 made Oozie use uber mode for the launcher job by default.  We should also increase the AM container size in case the launcher job needs more memory (e.g. a Pig job).  

from [~rohini]
{quote}
yarn.app.mapreduce.am.resource.mb and yarn.app.mapreduce.am.command-opts. yarn.app.mapreduce.am.resource.mb is easy and you can just set and overwrite. You will have to do some string manipulation with yarn.app.mapreduce.am.command-opts or append hoping the last values will take effect. Not sure if there are jvm's (IBM or openjdk) which will take the first value.
{quote}",2013-05-13T23:32:32.836+0000,2015-09-26T00:09:59.250+0000,Fixed,Major
PHOENIX-1636,Substitute HRegionInfo for HRegion where possible,PHOENIX,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12408956'>, <JIRA IssueLink: id='12407234'>, <JIRA IssueLink: id='12407246'>]","Some direct use of HRegion can be replaced by passing around and using HRegionInfo instead, we are looking at key ranges or region metadata (region name) or table metadata (HTD, HCD) only.",2015-02-04T20:34:45.753+0000,2015-02-24T19:07:06.644+0000,Duplicate,Major
AMBARI-24387,Support YARN Application timeout feature in Ambari Capacity Scheduler View,AMBARI,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12540524'>],"The CapacityScheduler supports the following parameters to lifetime of an application:

At high level, we need a mechanism to set timeout values for each leaf queue from Capacity Scheduler view. The sample configuration that should reflected in backend is


{code:java}
yarn.scheduler.capacity.root.default.maximum-application-lifetime=<values in seconds>
yarn.scheduler.capacity.root.default.default-application-lifetime=<values in seconds>
{code}


Capacity Scheduler view should support following tags.

description of this tags can be found in : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
",2018-07-31T10:56:32.038+0000,2018-08-08T09:39:38.340+0000,Fixed,Major
PHOENIX-1763,Support building with HBase-1.1.0 ,PHOENIX,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12420572'>, <JIRA IssueLink: id='12469099'>]","HBase-1.1 is in the works. However, due to HBASE-11544 and possibly HBASE-12972 and more, we need some changes for supporting HBase-1.1 even after PHOENIX-1642. 

We can decide on a plan to support (or not) HBase-1.1 on which branches by the time it comes out. Let's use subtasks to keep progress for build support for 1.1.0-SNAPSHOT. ",2015-03-20T21:01:29.567+0000,2016-06-09T05:15:03.201+0000,Fixed,Major
SQOOP-1190,Class HCatHadoopShims will be removed in HCatalog 0.12,SQOOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12374710'>],"I've noticed that recently the Hive/HCatalog trunk has removed class {{HCatHadoopShims}} via HIVE-4460. The functionality was merged into Hive Shim layer instead. We are using this class in case that Sqoop is running on hadoop 1 in local mode, so we should change he implementation accordingly.",2013-09-04T09:52:01.384+0000,2013-09-04T19:15:17.606+0000,Fixed,Major
BIGTOP-1808,hive 1.0.0 : kerberos does not work correctly,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12420941'>, <JIRA IssueLink: id='12420940'>]",See upstream bug HIVE-10240,2015-04-07T18:55:29.681+0000,2015-04-10T09:20:51.356+0000,Fixed,Major
SENTRY-2255,alter table set owner command can be executed only by user with proper privilege,SENTRY,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12538611'>, <JIRA IssueLink: id='12540583'>, <JIRA IssueLink: id='12540585'>]","Need to set sentry privilege mapping to make sure it can be done only by user  with ""all with grant option"" at table or parent obj, or user has owner privilege on this table or parent obj with grant option",2018-05-31T20:27:12.992+0000,2020-01-02T17:08:15.847+0000,Fixed,Major
KAFKA-1754,KOYA - Kafka on YARN,KAFKA,New Feature,Open,[],2,"[<JIRA IssueLink: id='12542344'>, <JIRA IssueLink: id='12400701'>]","YARN (Hadoop 2.x) has enabled clusters to be used for a variety of workloads, emerging as distributed operating system for big data applications. Initiatives are on the way to bring long running services under the YARN umbrella, leveraging it for centralized resource management and operations ([YARN-896] and examples such as HBase, Accumulo or Memcached through Slider). This JIRA is to propose KOYA (Kafka On Yarn), a YARN application master to launch and manage Kafka clusters running on YARN. Brokers will use resources allocated through YARN with support for recovery, monitoring etc. Please see attached for more details.",2014-11-05T00:29:40.707+0000,2018-09-04T15:15:44.843+0000,,Major
CALCITE-1589,"Druid adapter: timeseries query shows all days, even if no data",CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12491596'>, <JIRA IssueLink: id='12498609'>]","Following query is transformed into timeseries Druid query which yields different results in Calcite vs Druid, since it will show all values for the given time granularity, even if there is no data for the given _i\_brand\_id_.

{code:sql}
SELECT floor_day(`__time`) as `granularity`, max(ss_quantity), sum(ss_wholesale_cost)
FROM store_sales_sold_time_subset
WHERE i_brand_id = 10001009
GROUP BY floor_day(`__time`)
ORDER BY `granularity`;
OK
1999-11-01 00:00:00	45	37.47
1999-11-02 00:00:00	-9223372036854775808	0.0
1999-11-03 00:00:00	-9223372036854775808	0.0
1999-11-04 00:00:00	39	61.52
1999-11-05 00:00:00	74	145.84
1999-11-06 00:00:00	62	14.5
1999-11-07 00:00:00	-9223372036854775808	0.0
1999-11-08 00:00:00	5	34.08
1999-11-09 00:00:00	-9223372036854775808	0.0
1999-11-10 00:00:00	-9223372036854775808	0.0
1999-11-11 00:00:00	-9223372036854775808	0.0
1999-11-12 00:00:00	66	67.22
1999-11-13 00:00:00	-9223372036854775808	0.0
1999-11-14 00:00:00	-9223372036854775808	0.0
1999-11-15 00:00:00	-9223372036854775808	0.0
1999-11-16 00:00:00	60	96.37
1999-11-17 00:00:00	50	79.11
1999-11-18 00:00:00	-9223372036854775808	0.0
1999-11-19 00:00:00	-9223372036854775808	0.0
1999-11-20 00:00:00	-9223372036854775808	0.0
1999-11-21 00:00:00	-9223372036854775808	0.0
1999-11-22 00:00:00	-9223372036854775808	0.0
1999-11-23 00:00:00	57	17.69
1999-11-24 00:00:00	-9223372036854775808	0.0
1999-11-25 00:00:00	-9223372036854775808	0.0
1999-11-26 00:00:00	-9223372036854775808	0.0
1999-11-27 00:00:00	86	91.59
1999-11-28 00:00:00	-9223372036854775808	0.0
1999-11-29 00:00:00	93	136.48
1999-11-30 00:00:00	-9223372036854775808	0.0
SELECT floor_day(`__time`) as `granularity`, max(ss_quantity), sum(ss_wholesale_cost)
FROM store_sales_sold_time_subset_calcite
WHERE i_brand_id = 10001009
GROUP BY floor_day(`__time`)
ORDER BY `granularity`;
OK
1999-11-01 00:00:00	45	37.47
1999-11-04 00:00:00	39	61.52
1999-11-05 00:00:00	74	145.84
1999-11-06 00:00:00	62	14.5
1999-11-08 00:00:00	5	34.08
1999-11-12 00:00:00	66	67.22
1999-11-16 00:00:00	60	96.36999999999999
1999-11-17 00:00:00	50	79.11
1999-11-23 00:00:00	57	17.689999999999998
1999-11-27 00:00:00	86	91.59
1999-11-29 00:00:00	93	136.48
{code}",2017-01-18T20:03:10.904+0000,2017-03-24T03:20:13.128+0000,Fixed,Critical
BIGTOP-3545,Nexus configured by docker provisioner fails to download pentaho-aggdesigner-algorithm,BIGTOP,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12616464'>, <JIRA IssueLink: id='12616360'>]","https://ci.bigtop.apache.org/view/Packages/job/Bigtop-trunk-packages/704/COMPONENTS=hive,OS=centos-7/console

{code}
[ERROR] Failed to execute goal on project hive-upgrade-acid: Could not resolve dependencies for project org.apache.hive:hive-upgrade-acid:jar:3.1.2: Could not find artifact org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde in default (http://localhost:8081/nexus/content/repositories/central/) -> [Help 1]
{code}

Looks like the same cause as BIGTOP-3541.",2021-05-27T01:42:14.864+0000,2021-05-31T14:41:22.509+0000,Fixed,Major
PHOENIX-6305,Throttling decision does not take offheap memstore size into account,PHOENIX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12606230'>, <JIRA IssueLink: id='12606152'>]",This a regression from PHOENIX-6067,2021-01-07T11:55:02.626+0000,2021-02-10T10:00:37.658+0000,Fixed,Major
OOZIE-2797,Add a framework for cancellation of Delegation Tokens in Oozie AM,OOZIE,Bug,Open,[],2,"[<JIRA IssueLink: id='12515899'>, <JIRA IssueLink: id='12494350'>]","Pig and hive clients when run from command line take care of canceling HCatalog delegation tokens. But they do not get cancelled when run via Oozie.  RM only supports cancellation of hdfs and yarn tokens on job completion. So when running with Oozie, it is left to the HCatalog server to remove the token after the expiry. When there are too many jobs, the number of unexpired tokens can overwhelm the hcat server and cause outages. Oozie should take care of canceling these tokens after the job is done to avoid that.

  We should add a token cancellation framework in the new Oozie AM to support implementations for token cancellation for different credential types similar to CredentialsProvider.

   Attaching a reference patch that we used internally for directly canceling hcat tokens for just pig and hive actions that could be used by anyone who needs it till OOZIE-1770 is done and the new token cancellation framework is added by this jira.",2017-02-13T23:37:42.208+0000,2019-12-09T08:34:31.674+0000,,Major
TEZ-1386,Users should not need to setup TezGroupedInputFormat to enable grouping,TEZ,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12393863'>, <JIRA IssueLink: id='12393795'>]","To enable grouping via Tez, users should not need to change the underlying InputFormat. A simple enable / disable option should be sufficient.
MRInputConfigurer does this.
Many of the methods in MRHelpers, however, require an InputFormat to be specified. The main objective of this JIRA is to get rid of this requirement in favor of a simple enableGrouping flag.
Marking this as a blocker for TEZ-1347, since it should simplify the set of APIs required. Also, making all the changes in TEZ-1347 would just lead to a very large patch.",2014-08-07T08:31:57.900+0000,2014-09-06T01:35:51.613+0000,Fixed,Major
PHOENIX-6174,Phoenix Metrics Initial Unification,PHOENIX,Improvement,Open,[],1,[<JIRA IssueLink: id='12647179'>],"I see this as an incremental improvement toward a larger more powerful metrics framework in phoenix-server, phoenix-client, and phoenix-connectors.  Today we describe our available metrics as https://phoenix.apache.org/metrics.html

Long term a tag based multi-dimensional data model (similar to prometheus/open tsdb) may be superior but for now this targets simple generalization of the existing metrics in phoenix.  Today much of the metrics are built on hbase/hadoop offering.

 

Some initial discussion/brainstorming:


Metric Types:

  Phoenix uses/provides sufficient verbosity of metrics, but does not define the metric types cleanly.  Typical counter, gauge, histogram

 

Monitoring Models:

  Events (Event Metrics), metrics with no summarization or aggregation.  

     Example of this will be request metrics.  On this request you used 5 scanners.

  Metrics (Summary/Aggregated Metrics), 

    Example of this might be total queries

Reporting:
  Today JMX is one of the main approaches used in phoenix for metrics.  For events most of them are retrievable from PhoenixRuntime.  Long term these should be pluggable.

 

Storage:

  End user based on reporting is envisioned.

  May consider storing in HBase/Phoenix as an option?  The query log could store events with that query log entry.",2020-10-03T08:13:18.775+0000,2022-09-07T15:48:08.542+0000,,Major
SLIDER-1107,Generate app configuration files in AM,SLIDER,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12478703'>, <JIRA IssueLink: id='12478704'>]","Currently, each container generates its own application configuration files.  Instead, we could do this in the AM and have YARN localize the configuration files.  Having some basic config generation in the AM may allow us to simplify the config generation code in the app packages.  Also, it would be much better in the case of Docker containers, where we would prefer not to have to execute our own code inside the container.",2016-04-06T16:03:14.372+0000,2017-03-10T07:05:20.499+0000,Fixed,Major
PARQUET-177,MemoryManager ensure minimum Column Chunk size,PARQUET,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12407616'>],"The memory manager currently has no limit to how small it will make row groups.  This is problematic because jobs that have a large number of writers can result in tiny row groups that hurt performance.

The following patch will allow a configurable minimum size before killing the job.  Default is currently no limit.

",2015-02-04T00:56:03.152+0000,2015-04-07T20:46:08.569+0000,Fixed,Minor
TEZ-3581,Add different logger to enable suppressing logs for specific lines.,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12495199'>],"The following classes are heavy loggers, add a way to suppress these specific lines:

* https://github.com/apache/tez/blob/master/tez-runtime-library/src/main/java/org/apache/tez/http/HttpConnection.java#L233
* https://github.com/apache/tez/blob/master/tez-runtime-library/src/main/java/org/apache/tez/runtime/library/common/shuffle/ShuffleUtils.java#L541
* https://github.com/apache/tez/blob/master/tez-dag/src/main/java/org/apache/tez/dag/history/HistoryEventHandler.java#L125
",2017-01-18T07:08:19.032+0000,2017-08-22T00:02:22.628+0000,Fixed,Major
AVRO-2817,Avro file generated  using avro-1.8.2  is  not readable from  avro-1.9.2  for certian type  of schemas,AVRO,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12588783'>, <JIRA IssueLink: id='12608891'>, <JIRA IssueLink: id='12601122'>]","Hello,

We observed   avro file generated using avro-1.8.2  is not readable while using avro-1.9.2   if schema contains a field ""default:null"".   Please see below.

 

--------------------------------------------------------------------------

[mxj142:Jars]$ ls -ltr
total 168520
-rwx------@ 1 mxj142 staff 34798932 Jan 16 14:45 avro-tools-1.8.2.jar
-rwxr-xr-x@ 1 mxj142 staff 51303364 Mar 6 17:43 avro-tools-1.9.2.jar
-rw-r--r-- 1 mxj142 staff 715 Apr 24 11:19 records.avro
[mxj142:Jars]$ java -jar avro-tools-1.8.2.jar tojson records.avro
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{""enrichmentHeader"":\{""correlationId"":""1"",""sourceId"":""09b5d770-7373-462d-9517-8e5ce957793f""},""cx"":\{""dateFirst"":""20200419"",""dateLast"":""20200419""}}
{""enrichmentHeader"":\{""correlationId"":""2"",""sourceId"":""0285dc49-cc5c-4717-8ea2-45e50de2f65f""},""cx"":\{""dateFirst"":""20200420"",""dateLast"":""20200420""}}
[mxj142:Jars]$
[mxj142:Jars]$
[mxj142:Jars]$
[mxj142:Jars]$
[mxj142:Jars]$
[mxj142:Jars]$ java -jar avro-tools-1.9.2.jar tojson records.avro
20/04/24 11:20:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Exception in thread ""main"" org.apache.avro.AvroTypeException: Invalid default for field enrichmentHeader: null not a \{""type"":""record"",""name"":""EnrichmentHeader"",""namespace"":""com.mxj142.commons.domain"",""fields"":[{""name"":""correlationId"",""type"":{""type"":""string"",""avro.java.string"":""String""}},\{""name"":""sourceId"",""type"":{""type"":""string"",""avro.java.string"":""String""}}]}
 at org.apache.avro.Schema.validateDefault(Schema.java:1540)
 at org.apache.avro.Schema.access$500(Schema.java:87)
 at org.apache.avro.Schema$Field.<init>(Schema.java:521)
 at org.apache.avro.Schema.parse(Schema.java:1647)
 at org.apache.avro.Schema$Parser.parse(Schema.java:1394)
 at org.apache.avro.Schema$Parser.parse(Schema.java:1382)
 at org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:130)
 at org.apache.avro.file.DataFileStream.<init>(DataFileStream.java:90)
 at org.apache.avro.tool.DataFileReadTool.run(DataFileReadTool.java:93)
 at org.apache.avro.tool.Main.run(Main.java:66)
 at org.apache.avro.tool.Main.main(Main.java:55)
[mxj142:Jars]$

--------------------------------------------------------------------------

 

The file ""records.avro""  is generated using avro-1.8.2 .  It contains two  records  and using ""avro-tools-1.8.2.jar""  one can read it.  If   we use   ""avro-tools-1.9.2.jar"", then ""records.avro""  is not readable  (as shown above).  The file ""records.avro""  is also  attached for your convenience. 

 

Let me know if you need any more information.

 

Thanks,

Manoj

 

 ",2020-04-24T15:44:50.917+0000,2021-02-20T09:40:29.018+0000,Fixed,Blocker
THRIFT-1308,libfb303-0.7.0.jar missing in maven repository,THRIFT,Bug,Closed,[],1,[<JIRA IssueLink: id='12342633'>],"It looks like libfb303 was published to maven repositories for the 0.6.1 release,
but I don't see any libfb303 artifacts available for the 0.7.0 release.",2011-08-29T23:36:50.184+0000,2011-08-30T00:26:03.846+0000,Fixed,Major
TEZ-661,Implement a non-sorted partitioned output,TEZ,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12384099'>, <JIRA IssueLink: id='12383408'>, <JIRA IssueLink: id='12381869'>]","When implementing Pig union, we need to gather data from two or more upstream vertexes without sorting. The vertex itself might consists of several tasks. Ideally, it should use OnFileUnorderedKVOutput with DataMovementType.SCATTER_GATHER. However, this combination does not work according to [~hitesh]. We need to implement that. Also, key is meaningless in this scenario, we just want to evenly distribute the output records to tasks.",2013-12-05T00:06:42.140+0000,2014-09-06T01:35:26.398+0000,Fixed,Major
BIGTOP-3778,Hive 3.1.3 WebHCat service is not compatible with Hadoop 3.3.x,BIGTOP,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12645714'>],,2022-08-16T02:10:13.234+0000,2022-08-16T07:12:16.072+0000,Fixed,Major
TEZ-3138,Consider ignoring successful empty partition fetches in Shuffle failure handling,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12561102'>],"Partitions complete as a result of empty partitions don't really provide any information on whether the source is health or not. These could be skipped from the failure handling computations which looks at total successful fetches.

cc [~rajesh.balamohan]. This may not provide a lot of benefit after the change in TEZ-2882, but could be considered for future improvements - especially if there is a large rewrite.",2016-02-25T07:21:46.270+0000,2020-08-11T21:01:06.161+0000,,Major
TEZ-2383,Cleanup input/output/processor contexts in LogicalIOProcessorRuntimeTask,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12423223'>],"Currently they get released when sorter object gets GC-ed, but it might be good to explicitly release them on close as well.",2015-04-29T01:49:13.675+0000,2015-09-21T23:04:33.547+0000,Fixed,Major
NIFI-7818,PutHDFS results in EOF due to HDFS-15191,NIFI,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12599025'>, <JIRA IssueLink: id='12599024'>]","Nifi 1.11.4, 1.12.0, and 1.12.1-RC1 are using HDFS client libraries 3.2.1 which are impacted by https://issues.apache.org/jira/browse/HDFS-15191. This Jira is to request upgrading Nifi client hadoop libraries to 3.2.2 or higher where the issue has been fixed.",2020-09-18T16:10:42.597+0000,2021-02-09T17:51:42.075+0000,Fixed,Major
CALCITE-2444,Handle IN expressions when converting SqlNode to SQL,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12546959'>, <JIRA IssueLink: id='12596671'>]","It is not possible to convert an Rel tree into an sql string if it contains a filter which has an IN:

example: `select * from emp where empno in (10,20)`
note: Calcite's sql parser translates INs into either ORs or into a subquery.


The following testcase can be added to RelToSqlConverterTest to reproduce the issue:
{code}
  @Test public void testHiveIn() {
    // this can't be tested using ""sql"" because Calcite's sql parser replaces INs with ORs or subqueries.
    final RelBuilder builder = RelBuilder.create(RelBuilderTest.config().build());
    RexBuilder rexBuilder = builder.getRexBuilder();
    RelNode root =
            builder.scan(""EMP"")
                    .filter(rexBuilder.makeCall(SqlStdOperatorTable.IN, builder.field(""DEPTNO""),
                            builder.literal(20), builder.literal(21)))
                    .build();
    RelNode rel = root;

    SqlDialect dialect = SqlDialect.DatabaseProduct.HIVE.getDialect();
    final RelToSqlConverter converter = new RelToSqlConverter(dialect);
    final SqlNode sqlNode = converter.visitChild(0, rel).asStatement();
    String sqlStr = sqlNode.toSqlString(dialect).getSql();
    assertEquals(""select * from emp where deptno in (10,20)"", sqlStr);
  }
{code}

The exception is raised because calcite expects that every IN is a subquery(because of sql2rel rewrites them)
{code}
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_171]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_171]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
        at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:524) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.dispatch(RelToSqlConverter.java:103) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visitChild(RelToSqlConverter.java:109) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visit(RelToSqlConverter.java:173) ~[calcite-core-1.17.0.jar:1.17.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_171]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_171]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
        at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:524) ~[calcite-core-1.17.0.jar:1.17.0]
        ... 61 more
Caused by: java.lang.ClassCastException: org.apache.calcite.rex.RexCall cannot be cast to org.apache.calcite.rex.RexSubQuery
        at org.apache.calcite.rel.rel2sql.SqlImplementor$Context.toSql(SqlImplementor.java:540) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visit(RelToSqlConverter.java:166) ~[calcite-core-1.17.0.jar:1.17.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_171]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_171]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
        at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:524) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.dispatch(RelToSqlConverter.java:103) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visitChild(RelToSqlConverter.java:109) ~[calcite-core-1.17.0.jar:1.17.0]
        at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visit(RelToSqlConverter.java:173) ~[calcite-core-1.17.0.jar:1.17.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_171]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_171]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
        at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:524) ~[calcite-core-1.17.0.jar:1.17.0]
{code}

",2018-08-03T08:23:11.075+0000,2020-08-13T07:52:12.992+0000,Fixed,Major
CALCITE-4163,Unify the code creating single and multi column semijoin reducers,CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12594950'>],"In HIVE-21196, we add a transformation capable of merging single column semijoin reducers to multi column semijoin reducer. 

The code for creating multi-column semijoin reducers in {{SemiJoinReductionMerge}} presents some similarities with the code creating single-column semijoin reducers in {{DynamicPartitionPruningOptimization}}. 

Possibly we could refactor the respective parts to unify the creation logic of semijoin reducers.

",2020-08-06T13:26:31.522+0000,2020-08-06T17:34:23.414+0000,Invalid,Major
MPIR-374,Unknown packaging: bundle when creating report,MPIR,Bug,Closed,[],5,"[<JIRA IssueLink: id='12568042'>, <JIRA IssueLink: id='12627785'>, <JIRA IssueLink: id='12541324'>, <JIRA IssueLink: id='12545477'>, <JIRA IssueLink: id='12539975'>]","After I upgraded the plugin to 3.0.0 I get the following errors:
{noformat}
[INFO] Generating ""Dependency Management"" report --- maven-project-info-reports-plugin:3.0.0:dependency-management
[WARNING] Unable to create Maven project for com.fasterxml.jackson.module:jackson-module-scala_2.10:jar:2.9.5 from repository.
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[ERROR] Unknown packaging: bundle @ line 6, column 16

	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:192)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:327)
	at org.apache.maven.report.projectinfo.dependencies.RepositoryUtils.getMavenProjectFromRepository(RepositoryUtils.java:125)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.getDependencyRow(DependencyManagementRenderer.java:253)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForScope(DependencyManagementRenderer.java:202)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForAllScopes(DependencyManagementRenderer.java:151)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderSectionProjectDependencies(DependencyManagementRenderer.java:144)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderBody(DependencyManagementRenderer.java:130)
	at org.apache.maven.reporting.AbstractMavenReportRenderer.render(AbstractMavenReportRenderer.java:80)
	at org.apache.maven.report.projectinfo.DependencyManagementReport.executeReport(DependencyManagementReport.java:107)
	at org.apache.maven.reporting.AbstractMavenReport.generate(AbstractMavenReport.java:251)
	at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument(ReportDocumentRenderer.java:230)
	at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render(DefaultSiteRenderer.java:349)
	at org.apache.maven.plugins.site.render.SiteMojo.renderLocale(SiteMojo.java:198)
	at org.apache.maven.plugins.site.render.SiteMojo.execute(SiteMojo.java:147)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:305)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:956)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:290)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:194)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.model.building.ModelBuildingException: 1 problem was encountered while building the effective model for com.fasterxml.jackson.module:jackson-module-scala_2.10:2.9.5
[ERROR] Unknown packaging: bundle @ line 6, column 16

	at org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildingException(DefaultModelProblemCollector.java:197)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:482)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:424)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:414)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:158)
	... 36 more
[WARNING] Unable to create Maven project for com.fasterxml.jackson.module:jackson-module-scala_2.11:jar:2.9.5 from repository.
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[ERROR] Unknown packaging: bundle @ line 6, column 16

	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:192)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:327)
	at org.apache.maven.report.projectinfo.dependencies.RepositoryUtils.getMavenProjectFromRepository(RepositoryUtils.java:125)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.getDependencyRow(DependencyManagementRenderer.java:253)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForScope(DependencyManagementRenderer.java:202)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForAllScopes(DependencyManagementRenderer.java:151)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderSectionProjectDependencies(DependencyManagementRenderer.java:144)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderBody(DependencyManagementRenderer.java:130)
	at org.apache.maven.reporting.AbstractMavenReportRenderer.render(AbstractMavenReportRenderer.java:80)
	at org.apache.maven.report.projectinfo.DependencyManagementReport.executeReport(DependencyManagementReport.java:107)
	at org.apache.maven.reporting.AbstractMavenReport.generate(AbstractMavenReport.java:251)
	at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument(ReportDocumentRenderer.java:230)
	at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render(DefaultSiteRenderer.java:349)
	at org.apache.maven.plugins.site.render.SiteMojo.renderLocale(SiteMojo.java:198)
	at org.apache.maven.plugins.site.render.SiteMojo.execute(SiteMojo.java:147)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:305)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:956)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:290)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:194)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.model.building.ModelBuildingException: 1 problem was encountered while building the effective model for com.fasterxml.jackson.module:jackson-module-scala_2.11:2.9.5
[ERROR] Unknown packaging: bundle @ line 6, column 16

	at org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildingException(DefaultModelProblemCollector.java:197)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:482)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:424)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:414)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:158)
	... 36 more
[WARNING] Unable to create Maven project for com.fasterxml.jackson.module:jackson-module-scala_2.12:jar:2.9.5 from repository.
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[ERROR] Unknown packaging: bundle @ line 6, column 16

	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:192)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:327)
	at org.apache.maven.report.projectinfo.dependencies.RepositoryUtils.getMavenProjectFromRepository(RepositoryUtils.java:125)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.getDependencyRow(DependencyManagementRenderer.java:253)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForScope(DependencyManagementRenderer.java:202)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForAllScopes(DependencyManagementRenderer.java:151)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderSectionProjectDependencies(DependencyManagementRenderer.java:144)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderBody(DependencyManagementRenderer.java:130)
	at org.apache.maven.reporting.AbstractMavenReportRenderer.render(AbstractMavenReportRenderer.java:80)
	at org.apache.maven.report.projectinfo.DependencyManagementReport.executeReport(DependencyManagementReport.java:107)
	at org.apache.maven.reporting.AbstractMavenReport.generate(AbstractMavenReport.java:251)
	at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument(ReportDocumentRenderer.java:230)
	at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render(DefaultSiteRenderer.java:349)
	at org.apache.maven.plugins.site.render.SiteMojo.renderLocale(SiteMojo.java:198)
	at org.apache.maven.plugins.site.render.SiteMojo.execute(SiteMojo.java:147)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:305)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:956)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:290)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:194)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.model.building.ModelBuildingException: 1 problem was encountered while building the effective model for com.fasterxml.jackson.module:jackson-module-scala_2.12:2.9.5
[ERROR] Unknown packaging: bundle @ line 6, column 16

	at org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildingException(DefaultModelProblemCollector.java:197)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:482)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:424)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:414)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:158)
	... 36 more
[WARNING] Unable to create Maven project for org.postgresql:postgresql:jar:42.2.2 from repository.
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[ERROR] Unknown packaging: bundle @ line 11, column 14

	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:192)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:327)
	at org.apache.maven.report.projectinfo.dependencies.RepositoryUtils.getMavenProjectFromRepository(RepositoryUtils.java:125)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.getDependencyRow(DependencyManagementRenderer.java:253)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForScope(DependencyManagementRenderer.java:202)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderDependenciesForAllScopes(DependencyManagementRenderer.java:151)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderSectionProjectDependencies(DependencyManagementRenderer.java:144)
	at org.apache.maven.report.projectinfo.dependencies.renderer.DependencyManagementRenderer.renderBody(DependencyManagementRenderer.java:130)
	at org.apache.maven.reporting.AbstractMavenReportRenderer.render(AbstractMavenReportRenderer.java:80)
	at org.apache.maven.report.projectinfo.DependencyManagementReport.executeReport(DependencyManagementReport.java:107)
	at org.apache.maven.reporting.AbstractMavenReport.generate(AbstractMavenReport.java:251)
	at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument(ReportDocumentRenderer.java:230)
	at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render(DefaultSiteRenderer.java:349)
	at org.apache.maven.plugins.site.render.SiteMojo.renderLocale(SiteMojo.java:198)
	at org.apache.maven.plugins.site.render.SiteMojo.execute(SiteMojo.java:147)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:305)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:956)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:290)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:194)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.model.building.ModelBuildingException: 1 problem was encountered while building the effective model for org.postgresql:postgresql:42.2.2
[ERROR] Unknown packaging: bundle @ line 11, column 14

	at org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildingException(DefaultModelProblemCollector.java:197)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:482)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:424)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:414)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:158)
	... 36 more
{noformat}

You can find the project that exhibits the problem here: https://github.com/Serranya/ballot/tree/mpir-bug-report
Build with {noformat}mvn -Pfatjar,dev clean package site:site{noformat}",2018-07-10T10:41:08.382+0000,2021-12-02T17:15:30.799+0000,Fixed,Minor
PHOENIX-4852,IndexHalfStoreFileReaderGenerator.getLocalIndexScanners creates an scanner which would store file reference count to -1,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12541108'>],"Actual fix of this bug is in the link HBASE-21047. 

Object to LocalIndexStoreFileScanner and StoreFileScanner in class IndexHalfStoreFileReaderGenerator would leave the fileReference in inconsistent state. Opening this bug here to make sure above HBASE patch needs to be picked up for running any instance of phoenix local indexes",2018-08-16T04:23:24.022+0000,2018-08-16T04:24:31.886+0000,,Major
RANGER-772,Hive plugin: Update Ranger authorizer to mimic changes made by Hive standard authorizer for the case when IMPORT can end up creating a table,RANGER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12451394'>],Ranger authorizer should mimic the changes made by Hive Std authorizer via HIVE-11988.,2015-12-07T22:59:21.180+0000,2015-12-09T18:18:51.133+0000,Fixed,Critical
AVRO-1965,Reparsing an existing schema mutates the schema,AVRO,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12634894'>, <JIRA IssueLink: id='12487761'>, <JIRA IssueLink: id='12487762'>]","h2. Overview

In certain scenarios re-parsing a schema constructed via {{SchemaBuilder}} or {{Schema.createRecord}} produces a schema that is no longer equal to the original. In the below example, after parsing the existing schema, the namespace of the top level record is cascaded down to the bottom level record. Note that the way the top level record is being created with the namespace as part of the name {{default.a}} is the same way Hive's {{AvroSerDe}} constructs a schema from a Hive table's metadata.

h2. Failing test case

{code}
Schema d = SchemaBuilder.builder().intType();
Schema c = SchemaBuilder.record(""c"").fields().name(""d"").type(d).noDefault().endRecord();
Schema b = SchemaBuilder.record(""b"").fields().name(""c"").type(c).noDefault().endRecord();

Schema a1 = SchemaBuilder.record(""default.a"").fields().name(""b"").type(b).noDefault().endRecord();
Schema a2 = new Schema.Parser().parse(a1.toString());

// a1 = {""type"":""record"",""name"":""a"",""namespace"":""default"",""fields"":[{""name"":""b"",""type"":{""type"":""record"",""name"":""b"",""namespace"":"""",""fields"":[{""name"":""c"",""type"":{""type"":""record"",""name"":""c"",""fields"":[{""name"":""d"",""type"":""int""}]}}]}}]}
// a2 = {""type"":""record"",""name"":""a"",""namespace"":""default"",""fields"":[{""name"":""b"",""type"":{""type"":""record"",""name"":""b"",""namespace"":"""",""fields"":[{""name"":""c"",""type"":{""type"":""record"",""name"":""c"",""namespace"":""default"",""fields"":[{""name"":""d"",""type"":""int""}]}}]}}]}

assertThat(a2, is(a1));
{code}",2016-11-30T11:23:03.550+0000,2022-03-03T14:27:36.665+0000,Fixed,Major
SPARK-17398,Failed to query on external JSon Partitioned table,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12576531'>, <JIRA IssueLink: id='12576530'>]","1. Create External Json partitioned table 
with SerDe in hive-hcatalog-core-1.2.1.jar, download fom
https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core/1.2.1
2. Query table meet exception, which works in spark1.5.2
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task
 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.hive.hcatalog.data.HCatRecord
        at org.apache.hive.hcatalog.data.HCatRecordObjectInspector.getStructFieldData(HCatRecordObjectInspector.java:45)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:430)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
 

3. Test Code

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object JsonBugs {

  def main(args: Array[String]): Unit = {
    val table = ""test_json""
    val location = ""file:///g:/home/test/json""
    val create = s""""""CREATE   EXTERNAL  TABLE  ${table}
             (id string,  seq string )
              PARTITIONED BY(index int)
              ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
              LOCATION ""${location}"" 
          """"""
    val add_part = s""""""
         ALTER TABLE ${table} ADD 
         PARTITION (index=1)LOCATION '${location}/index=1'
    """"""

    val conf = new SparkConf().setAppName(""scala"").setMaster(""local[2]"")
    conf.set(""spark.sql.warehouse.dir"", ""file:///g:/home/warehouse"")
    val ctx = new SparkContext(conf)

    val hctx = new HiveContext(ctx)
    val exist = hctx.tableNames().map { x => x.toLowerCase() }.contains(table)
    if (!exist) {
      hctx.sql(create)
      hctx.sql(add_part)
    } else {
      hctx.sql(""show partitions "" + table).show()
    }
    hctx.sql(""select * from test_json"").show()
  }
}
",2016-09-05T05:27:38.429+0000,2019-12-20T23:23:35.523+0000,Fixed,Major
SENTRY-1750,HMSFollower does not handle view update correctly,SENTRY,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12504604'>, <JIRA IssueLink: id='12505693'>]","The location of a view is null, and it should be accepted by Sentry. However, when there is view update such ""create view testView as select * from test"" in TestHDFSIntegrationEnd2End.testViews, the location is null, and HMSFollower throws exception when processing this notification.

The notification uses HCatEventMessage.EventType. Should we add types for view? So sentry can process view and table differently.

The call stack of exception

2017-05-04 23:24:00,198 (pool-5-thread-1) [ERROR - org.apache.sentry.service.thrift.HMSFollower.run(HMSFollower.java:300)] Encounter SentryInvalidInputException|SentryInvalidHMSEventException while processing notification log
org.apache.sentry.core.common.exception.SentryInvalidHMSEventException: Create table event has incomplete information. dbName = default, tableName = testView, location = null
	at org.apache.sentry.service.thrift.HMSFollower.processNotificationEvents(HMSFollower.java:415)
	at org.apache.sentry.service.thrift.HMSFollower.run(HMSFollower.java:287)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",2017-05-05T14:49:24.928+0000,2017-10-25T18:53:48.138+0000,Not A Problem,Minor
PHOENIX-6658,Replace HRegion.get() calls,PHOENIX,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12635872'>, <JIRA IssueLink: id='12634640'>, <JIRA IssueLink: id='12634641'>]","HBASE-26036 made a change where Region.get() always clones Off-Heap cells before returning them.

In HBase all non-test occurances of this code were changed to create their own RegionScanner to avoid copying the off-heap cells. We should do the same.

This would also let Phoenix run correctly with HBase 2.4.5-2.4.10 , as the fix in HBASE-26036  introduced another bug, HBASE-26777, which causes failures in Phoenix.

Without this change, Phoenix needs at least HBase version 2.4.11, that has the fix for HBASE-26777.",2022-02-28T16:43:21.546+0000,2022-03-21T07:00:53.231+0000,Fixed,Major
ACCUMULO-4004,open WALs prevent DN decommissioning,ACCUMULO,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12442581'>, <JIRA IssueLink: id='12466046'>, <JIRA IssueLink: id='12442582'>, <JIRA IssueLink: id='12446319'>]","It should be possible to manually roll WALs so that files on decommissioning datanodes are closed and the decommissioning process can complete. At the very least, the logs could be closed after an elapsed period of time, such as an hour.
",2015-09-18T19:02:57.218+0000,2016-10-31T13:34:06.954+0000,Fixed,Major
ZOOKEEPER-1401,Extract generally useful client utilities from CLI code  ,ZOOKEEPER,Improvement,Open,[],1,[<JIRA IssueLink: id='12348596'>],"There are a bunch of things that would be useful/reusable from ZK Java client, such as ACL parsing. Also, it would be nice to see other utilities for dealing with path creation (""mkdir -p ..."") readily available for clients rather than implementing in downstream projects. Some of this can be seen in HIVE-2712.
",2012-02-24T18:12:54.919+0000,2012-03-17T19:59:26.138+0000,,Major
SPARK-21433,Spark SQL should support higher version of Hive metastore,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12509312'>, <JIRA IssueLink: id='12509342'>, <JIRA IssueLink: id='12509340'>]","Now. Spark SQL supports Hive metastore versions ranging from 0.12.0 to 1.2.1 (inclusive). 
When I start the Thrift JDBC/ODBC server and test with JDBC codes, I will discover that some running Spark Jobs is very slowly. I want to stop running code to stop this running Spark Job. But the running code stopped and the running Spark Job does not stop. The system resource can not be released. 
Is running spark jobs stopped when a query is cancelled? I think yes.
I checked same Hive issues and Hive will resolve this issue.

This is My test code:
{code:java}
object App_2017071100 {

  private[this] val DRIER = ""org.apache.hive.jdbc.HiveDriver""
  private[this] val URL = ""jdbc:hive2://192.168.50.3:10000/default""
  private[this] val USERNAME = ""hive""
  private[this] val PASSWORD = ""hive""

  def main(args: Array[String]): Unit = {
    var connect: Connection = null
    var statement: PreparedStatement = null
    var result: ResultSet = null
    try {
      Class.forName(DRIER)
      connect = DriverManager.getConnection(URL, USERNAME, PASSWORD)
      statement = connect.prepareStatement(""select count(1) from ip"")
      //  statement.setQueryTimeout(30)
      result = statement.executeQuery
      while (result.next) {
        println(result.getString(1))
      }
    } catch {
      case e: LinkageError => e.printStackTrace
      case e: ExceptionInInitializerError => e.printStackTrace
      case e: ClassNotFoundException => e.printStackTrace
      case e: SQLException => e.printStackTrace
      case e: SQLTimeoutException => e.printStackTrace
    } finally {
      if (result != null) result.close
      if (statement != null) statement.close
      if (connect != null) connect.close
    }
  }

}
{code}
",2017-07-17T03:29:04.351+0000,2017-07-19T08:57:38.630+0000,Invalid,Major
SQOOP-413,"Port files org.apache.hadoop.mapreduce.lib.{db,input}.* from CDH3/Hadoop-0.21 to sqoop",SQOOP,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12346585'>],"We need to port following files from hadoop-0.21 (or CDH3) to sqoop to support hadoop-0.20 that is still in use:

org.apache.hadoop.mapreduce.lib.db.DBWritable 
org.apache.hadoop.mapreduce.lib.input.CombineFileSplit 
org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader",2011-12-19T08:07:13.079+0000,2011-12-29T06:16:44.392+0000,Fixed,Major
SENTRY-2143,Table renames should synchronize with Sentry,SENTRY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12527863'>],"Currently table renames are not synchronized from Hive (while table creates/drops are). This creates a problem since the renamed table doesn't have correct privileges for a bit until it is processed by Sentry. So perfectly valid scripts that rename tables and expect the rename table to retain the privileges are going to fail.

The fix is to update {{SentrySyncHMSNotificationsPostEventListener}} to synchronize table renames as well.
",2018-02-16T18:47:52.984+0000,2020-01-02T17:07:42.798+0000,Fixed,Major
SPARK-32145,ThriftCLIService.GetOperationStatus should include exception's stack trace to the error message,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12592471'>],"In https://issues.apache.org/jira/browse/SPARK-29283, we only show the error message of root cause to end-users through JDBC client. In some cases, it erases the straightaway messages that we intentionally make to help them for better understanding.

We should do as Hive does in https://issues.apache.org/jira/browse/HIVE-14368",2020-07-01T07:09:11.756+0000,2020-07-17T02:28:40.859+0000,Fixed,Major
SPARK-17076,Cardinality estimation of join operator,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12486548'>],"support cardinality estimates for equi-join, Cartesian product join, and outer join, etc. ",2016-08-16T06:36:14.708+0000,2020-05-17T17:58:48.315+0000,Fixed,Major
ARROW-13011,[Python] HadoopFileSystem crash when called twice and Java was misconfigured,ARROW,Bug,Open,[],1,[<JIRA IssueLink: id='12620476'>],"See [https://github.com/dask/dask/pull/7752#issuecomment-856231163] and discussion below.

 

Didn't investigate yet (and I also think dask cannot yet use the new filesystem, since it has a different API, but it should nonetheless not crash ..), there is a docker workflow to reproduce the tests I can try.",2021-06-08T15:42:38.116+0000,2021-08-04T09:21:08.928+0000,,Major
SPARK-28688,Incompatible java.util.ArrayList,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12567529'>],"{{2 TESTS FAILED}} from {{org.apache.spark.sql.hive.client.VersionsSuite}}
{noformat}
- 3.0: sql read hive materialized view *** FAILED *** (1 second, 292 milliseconds)
- 3.1: sql read hive materialized view *** FAILED *** (1 second, 312 milliseconds)
{noformat}
Logs:
{noformat}
06:54:07.608 WARN org.apache.hadoop.hive.ql.exec.Utilities: Cannot create class org.apache.hive.storage.jdbc.JdbcInputFormat for hive.vectorized.row.serde.inputformat.excludes checks
06:54:07.670 WARN org.apache.hadoop.hive.ql.Driver: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
java.lang.RuntimeException: java.lang.NoSuchFieldException: parentOffset
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$ArrayListSubListSerializer.<init>(SerializationUtilities.java:389)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$1.create(SerializationUtilities.java:235)
	at org.apache.hive.com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:51)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.borrowKryo(SerializationUtilities.java:279)
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:571)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:563)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:555)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:362)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:149)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2479)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2150)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1567)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1556)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:786)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:310)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:244)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:293)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:766)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:753)
	at org.apache.spark.sql.hive.client.VersionsSuite.$anonfun$new$72(VersionsSuite.scala:626)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.NoSuchFieldException: parentOffset
	at java.base/java.lang.Class.getDeclaredField(Class.java:2412)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$ArrayListSubListSerializer.<init>(SerializationUtilities.java:383)
	... 68 more
06:54:07.780 ERROR org.apache.hadoop.hive.ql.exec.Task: Job Submission failed with exception 'java.lang.RuntimeException(java.lang.NoSuchFieldException: parentOffset)'
java.lang.RuntimeException: java.lang.NoSuchFieldException: parentOffset
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$ArrayListSubListSerializer.<init>(SerializationUtilities.java:389)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$1.create(SerializationUtilities.java:235)
	at org.apache.hive.com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:51)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.borrowKryo(SerializationUtilities.java:279)
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:571)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:563)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:555)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:362)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:149)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2479)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2150)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1567)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1556)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:786)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:310)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:244)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:293)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:766)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:753)
	at org.apache.spark.sql.hive.client.VersionsSuite.$anonfun$new$72(VersionsSuite.scala:626)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.NoSuchFieldException: parentOffset
	at java.base/java.lang.Class.getDeclaredField(Class.java:2412)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$ArrayListSubListSerializer.<init>(SerializationUtilities.java:383)
	... 68 more

06:54:07.780 ERROR org.apache.hadoop.hive.ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. java.lang.NoSuchFieldException: parentOffset
06:54:07.792 ERROR org.apache.spark.sql.hive.client.HiveClientImpl:
======================
HIVE FAILURE OUTPUT
======================
SET spark.sql.test.key=1
OK
Query ID = root_20190811065407_baceadf3-30bb-4884-a815-72cbb02d429d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Job Submission failed with exception 'java.lang.RuntimeException(java.lang.NoSuchFieldException: parentOffset)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. java.lang.NoSuchFieldException: parentOffset

======================
END HIVE FAILURE OUTPUT
======================

[info] - 3.0: sql read hive materialized view *** FAILED *** (1 second, 292 milliseconds)
[info]   org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. java.lang.NoSuchFieldException: parentOffset
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:790)
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:310)
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:244)
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:293)
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:766)
[info]   at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:753)
[info]   at org.apache.spark.sql.hive.client.VersionsSuite.$anonfun$new$72(VersionsSuite.scala:626)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
[info]   at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
[info]   at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
[info]   at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
[info]   at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
[info]   at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
[info]   at org.scalatest.Suite.run(Suite.scala:1147)
[info]   at org.scalatest.Suite.run$(Suite.scala:1129)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
[info]   at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
[info]   at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
[info]   at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[info]   at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}

https://github.com/apache/hive/blob/ae4df627952610dbec029b099f0964908b3a4f25/ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java#L381-L383

 !screenshot-2.png! 
 !screenshot-1.png! ",2019-08-11T14:34:45.875+0000,2019-08-12T12:10:07.002+0000,Fixed,Major
SLIDER-149,Support a YARN service registry,SLIDER,New Feature,Resolved,"[<JIRA Issue: key='SLIDER-320', id='12734078'>, <JIRA Issue: key='SLIDER-222', id='12726386'>, <JIRA Issue: key='SLIDER-365', id='12737150'>, <JIRA Issue: key='SLIDER-478', id='12745620'>, <JIRA Issue: key='SLIDER-492', id='12746463'>, <JIRA Issue: key='SLIDER-500', id='12747350'>, <JIRA Issue: key='SLIDER-504', id='12747463'>, <JIRA Issue: key='SLIDER-506', id='12747523'>, <JIRA Issue: key='SLIDER-509', id='12747874'>, <JIRA Issue: key='SLIDER-516', id='12748158'>, <JIRA Issue: key='SLIDER-531', id='12748644'>, <JIRA Issue: key='SLIDER-602', id='12752324'>, <JIRA Issue: key='SLIDER-608', id='12752446'>]",7,"[<JIRA IssueLink: id='12398864'>, <JIRA IssueLink: id='12398865'>, <JIRA IssueLink: id='12390015'>, <JIRA IssueLink: id='12395339'>, <JIRA IssueLink: id='12398164'>, <JIRA IssueLink: id='12398246'>, <JIRA IssueLink: id='12398248'>]",YARN-913 proposes a YARN-wide service registry; we must integrate with this as it is implemented.,2014-06-18T20:17:52.591+0000,2014-11-24T17:08:08.466+0000,Fixed,Major
ACCUMULO-404,Support running on-top of Kerberos-enabled HDFS,ACCUMULO,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12348292'>],"Hadoop 0.20.20x, 1.0.x and 0.23.x all support requiring kerberos for strong authentication in order to talk to HDFS. It would be useful if Accumulo could be configured with keytab files for the TabletServers, Master, etc. so that it can be run on a Kerberos-enabled cluster.",2012-02-15T16:53:20.483+0000,2012-12-18T18:39:49.353+0000,Fixed,Major
BIGTOP-635,"Implement a cluster-abstraction, discovery and manipulation framework for iTest",BIGTOP,New Feature,Open,[],3,"[<JIRA IssueLink: id='12353879'>, <JIRA IssueLink: id='12358119'>, <JIRA IssueLink: id='12354078'>]","We've come to a point where our tests need to have a uniform way of interfacing with the cluster under test. It is no longer ok to assume that the test can be executed on a particular node (and thus have access to services running on it). It is also less than ideal for tests to assume a particular type of interaction with the services since it tends to break in different deployment scenarios. 

A framework that needs to be put in place has to be capable of (regardless of where a test using it is executed on):
  # representing the abstract configuration of the cluster
  # representing the abstract topology of the entire cluster (services running on a cluster, nodes hosting the daemons, racks, etc).
  # giving tests an ability to query this topology
  # giving tests an ability to affect the nodes in that topology in a particular way (refreshing configuration, restarting services, etc.)

Of course, the ideal solution here would be to give Bigtop tests a programmatic access to a Hadoop cluster management framework such as Cloudera's CM or Apache Ambari. 

As with any ideal solutions I don't think it is realistic though. Hence we have to cook something up. At this point I'm really focused on getting the API right and I'm totally fine with an implementation of that API to be something as silly as a bunch of ssh-based scripts or something.

This JIRA is primarily focused on coming up with such an API. Anybody who's willing to help is welcome to.",2012-06-18T23:10:12.931+0000,2013-03-26T18:22:16.406+0000,,Major
SENTRY-1963,Sentry JSON reporter should use regular implementation for local file system,SENTRY,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12515542'>],See https://issues.apache.org/jira/browse/HIVE-17563 for the description of issues with FileSystem.getLocal(). This should be updated to use local filesystem operations.,2017-09-22T07:28:07.627+0000,2017-10-04T00:20:09.797+0000,Fixed,Major
SPARK-1693,"Dependent on multiple versions of servlet-api jars lead to throw an SecurityException when Spark built for hadoop 2.3.0 , 2.4.0 ",SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12453762'>],"{code}mvn test -Pyarn -Dhadoop.version=2.4.0 -Dyarn.version=2.4.0 > log.txt{code}

The log: 
{code}
UnpersistSuite:
- unpersist RDD *** FAILED ***
  java.lang.SecurityException: class ""javax.servlet.FilterRegistration""'s signer information does not match signer information of other classes in the same package
  at java.lang.ClassLoader.checkCerts(ClassLoader.java:952)
  at java.lang.ClassLoader.preDefineClass(ClassLoader.java:666)
  at java.lang.ClassLoader.defineClass(ClassLoader.java:794)
  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
  at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
  at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
{code}",2014-05-01T07:43:47.859+0000,2017-03-02T21:42:28.065+0000,Fixed,Blocker
SPARK-14631,"""drop database cascade"" needs to unregister functions for HiveExternalCatalog",SPARK,Bug,Closed,[],1,[<JIRA IssueLink: id='12463659'>],"as HIVE-12304, drop database cascade of hive did not drop functions as well. We need to fix this when call `dropDatabase` in HiveExternalCatalog.",2016-04-14T11:42:32.315+0000,2016-12-12T20:19:30.829+0000,Not A Problem,Major
KNOX-1524,"Hive ""select *"" performance evaluation",KNOX,Task,Closed,[],6,"[<JIRA IssueLink: id='12545757'>, <JIRA IssueLink: id='12545663'>, <JIRA IssueLink: id='12545756'>, <JIRA IssueLink: id='12546230'>, <JIRA IssueLink: id='12546145'>, <JIRA IssueLink: id='12545682'>]","While looking at WebHDFS performance in KNOX-1221, I decided to look a bit more into performance for common use cases. Hive performance is another area that could use some research.

Use ""select * ... limit"" to get a comparison of raw return speed from HiveServer2. This should show how fast results can be streamed through HiveServer2 and Knox. Compare the results to ""hdfs dfs -text"" since this will render the data directly from HDFS. This should give comparisons for the difference in overhead between HDFS, HiveServer2 binary, HiveServer2 HTTP, and HiveServer2 HTTP with Knox.",2018-10-15T13:51:52.955+0000,2019-03-28T13:57:16.327+0000,Information Provided,Major
BIGTOP-1450,Eliminate broken hive test artifacts in favor of smoke-tests.,BIGTOP,Improvement,Resolved,"[<JIRA Issue: key='BIGTOP-1392', id='12731604'>, <JIRA Issue: key='BIGTOP-1461', id='12743918'>]",2,"[<JIRA IssueLink: id='12397484'>, <JIRA IssueLink: id='12399467'>]","*Overall: The hive tests in {{test-artifacts}} are prone to failures from missing data sets and generally need a thorough review*

When testing bigtop 0.8.0 release candidate, I found that I got some errors 
{noformat}

[--- /dev/fd/63  2014-09-16 10:12:54.579647323 +0000, +++ /dev/fd/62     2014-09-16 10:12:54.579647323 +0000, @@ -14,4 +14,4 @@,  INSERT OVERWRITE DIRECTORY '/tmp/count',  SELECT COUNT(1) FROM u_data,  dfs -cat /tmp/count/*, -0, +100000] err=[14/09/16 10:12:17 WARN mapred.JobConf: The variable mapred.child.ulimit is no longer used., , Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties, OK, Time taken: 2.609 seconds, OK, Time taken: 0.284 seconds, Total jobs = 1, Launching Job 1 out of 1, Number of reduce tasks determined at compile time: 1, In order to change the average load for a reducer (in bytes):,   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;, In order to limit the maximum number of reducers:,   set hive.exec.reducers.max=&lt;number&gt;, In order to set a constant number of reducers:,   set mapreduce.job.reduces=&lt;number&gt;, Starting Job = job_1410830363557_0019, Tracking URL = http://bigtop1.vagrant:20888/proxy/application_1410830363557_0019/, Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1410830363557_0019, Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1, 2014-09-16 10:12:38,870 Stage-1 map = 0%,  reduce = 0%, 2014-09-16 10:12:45,516 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.81 sec, 2014-09-16 10:12:53,036 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.73 sec, MapReduce Total cumulative CPU time: 1 seconds 730 msec, Ended Job = job_1410830363557_0019, Moving data to: /tmp/count, MapReduce Jobs Launched: , Job 0: Map: 1  Reduce: 1   Cumulative CPU: 1.73 sec   HDFS Read: 272 HDFS Write: 2 SUCCESS, Total MapReduce CPU Time Spent: 1 seconds 730 msec, OK, Time taken: 24.594 seconds

{noformat}

I know there is a diff error in here - some kind of diff is going on , but I forgot how the actual,output,and filter are working.  

In any case, I think these tests can be simplified to just grep for a output string and check error code, or else, at least add some very clear assertions as to what failures may be. ",2014-09-16T10:38:23.377+0000,2015-03-18T22:47:56.560+0000,Fixed,Major
PHOENIX-3507,HBase secure wal log cash with Phoenix secondary index,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12487372'>],"When I used wal.encryption and phoenix secondary indexes, I got the error message
2016-11-24 17:11:20,361 INFO [regionserver/host-172-31-0-68/172.31.0.68:16020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/host-172-31-0-68,16020,1479976404182/host-172-31-0-68%2C16020%2C1479976404182.default.1479978679455 with entries=0, filesize=242 B; new WAL /hbase/WALs/host-172-31-0-68,16020,1479976404182/host-172-31-0-68%2C16020%2C1479976404182.default.1479978680140
2016-11-24 17:11:20,363 INFO [regionserver/host-172-31-0-68/172.31.0.68:16020.logRoller] wal.FSHLog: Archiving hdfs://data/hbase/WALs/host-172-31-0-68,16020,1479976404182/host-172-31-0-68%2C16020%2C1479976404182.default.1479978679455 to hdfs://data/hbase/oldWALs/host-172-31-0-68%2C16020%2C1479976404182.default.1479978679455
2016-11-24 17:11:20,654 WARN [regionserver/host-172-31-0-68/172.31.0.68:16020.append-pool1-t1] wal.FSHLog: Append sequenceId=9, requesting roll of WAL
java.lang.NullPointerException
at org.apache.hadoop.hbase.util.Bytes.toInt(Bytes.java:801)
at org.apache.hadoop.hbase.util.Bytes.toInt(Bytes.java:788)
at org.apache.hadoop.hbase.KeyValue.getKeyLength(KeyValue.java:1324)
at org.apache.hadoop.hbase.KeyValue.getTagsLength(KeyValue.java:1638)
at org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder.write(SecureWALCellCodec.java:199)
at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:122)
at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1932)
at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1794)
at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1704)
at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
2016-11-24 17:11:20,810 WARN [regionserver/host-172-31-0-68/172.31.0.68:16020.logRoller] wal.FSHLog: Failed sync-before-close but no outstanding appends; closing WAL: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=9, requesting roll of WAL
And my hbase-site.xml is containing the settings.
<property>
<name>hbase.regionserver.wal.encryption</name>
<value>true</value>
</property>

<property>
<name>hbase.regionserver.hlog.reader.impl</name>
<value>org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader</value>
</property>

<property>
<name>hbase.regionserver.hlog.writer.impl</name>
<value>org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter</value>
</property>

<property>
<name>hbase.regionserver.wal.codec</name>
<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
</property>
And my phoenix table sql
CREATE SCHEMA TEST;
CREATE TABLE TEST.TEST (ID BIGINT NOT NULL PRIMARY KEY, TEST VARCHAR);
UPSERT INTO TEST.TEST (ID, TEST) VALUES (1, 'test');
CREATE INDEX TEST_INDEX ON TEST.TEST (TEST);",2016-11-24T10:38:58.695+0000,2019-01-08T20:59:51.045+0000,,Major
TEZ-591,Provide mode specific diagnostic information to the Tez client,TEZ,Wish,Closed,[],1,[<JIRA IssueLink: id='12377752'>],"While developing Pig on Tez, I found it's hard to debug DAG failures due to lack of diagnostic information. Currently, the MR Pig client reports the backend error message when there is a job failure. For example, if I have a UDF that throws a runtime exception, I will see the following stack trace in the front-end log file-
{code}
Pig Stack Trace
---------------
ERROR 1066: Unable to open iterator for alias b. Backend error : FAIL IT NOW!
...
Caused by: java.lang.RuntimeException: FAIL IT NOW!
    at Kill.exec(Kill.java:9)
    at Kill.exec(Kill.java:6)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:334)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:383)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:346)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:372)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:297)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:283)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:771)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:375)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)
{code}
Basically, I'd like to do something similar in Tez Pig.

If there are multiple failed vertices and tasks, it may be not possible to propagate all the backend exceptions to the frontend. But would it be possible to propagate some of first ones at least? Perhaps one per failed vertex? Given that DAGStatus.getDiagnostics() returns a list of Strings, it seems feasible.



",2013-10-30T20:01:08.772+0000,2013-12-01T20:21:45.043+0000,Fixed,Minor
INFRA-3567,Enable Nexus Access For Hive,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12338476'>],"Project URL: http://hive.apache.org

SVN URL: http://svn.apache.org/repos/asf/hive/

Maven Group Ids: org.apache.hive

Managed By This TLP Project:-
",2011-04-07T03:21:32.175+0000,2014-01-22T23:19:34.955+0000,Fixed,Major
ZOOKEEPER-1080,Provide a Leader Election framework based on Zookeeper recipe,ZOOKEEPER,New Feature,Resolved,[],5,"[<JIRA IssueLink: id='12340495'>, <JIRA IssueLink: id='12340488'>, <JIRA IssueLink: id='12340678'>, <JIRA IssueLink: id='12388260'>, <JIRA IssueLink: id='12339536'>]","Currently Hadoop components such as NameNode and JobTracker are single point of failure.
If Namenode or JobTracker goes down, there service will not be available until they are up and running again. If there was a Standby Namenode or JobTracker available and ready to serve when Active nodes go down, we could have reduced the service down time. Hadoop already provides a Standby Namenode implementation which is not fully a ""hot"" Standby. 
The common problem to be addressed in any such Active-Standby cluster is Leader Election and Failure detection. This can be done using Zookeeper as mentioned in the Zookeeper recipes.
http://zookeeper.apache.org/doc/r3.3.3/recipes.html


+Leader Election Service (LES)+

Any Node who wants to participate in Leader Election can use this service. They should start the service with required configurations. The service will notify the nodes whether they should be started as Active or Standby mode. Also they intimate any changes in the mode at runtime. All other complexities can be handled internally by the LES.
",2011-05-30T13:24:16.032+0000,2014-05-17T12:03:02.335+0000,Duplicate,Major
PHOENIX-1071,Provide integration for exposing Phoenix tables as Spark RDDs,PHOENIX,New Feature,Closed,"[<JIRA Issue: key='PHOENIX-1816', id='12818667'>, <JIRA Issue: key='PHOENIX-1818', id='12818710'>]",3,"[<JIRA IssueLink: id='12412912'>, <JIRA IssueLink: id='12391133'>, <JIRA IssueLink: id='12420408'>]","A core concept of Apache Spark is the resilient distributed dataset (RDD), a ""fault-tolerant collection of elements that can be operated on in parallel"". One can create a RDDs referencing a dataset in any external storage system offering a Hadoop InputFormat, like PhoenixInputFormat and PhoenixOutputFormat. There could be opportunities for additional interesting and deep integration. 

Add the ability to save RDDs back to Phoenix with a {{saveAsPhoenixTable}} action, implicitly creating necessary schema on demand.

Add support for {{filter}} transformations that push predicates to the server.

Add a new {{select}} transformation supporting a LINQ-like DSL, for example:
{code}
// Count the number of different coffee varieties offered by each
// supplier from Guatemala
phoenixTable(""coffees"")
    .select(c =>
        where(c.origin == ""GT""))
    .countByKey()
    .foreach(r => println(r._1 + ""="" + r._2))
{code} 

Support conversions between Scala and Java types and Phoenix table data.",2014-07-08T23:04:32.532+0000,2016-02-04T01:53:54.219+0000,Fixed,Major
TEZ-135,Let there be hive,TEZ,Bug,Closed,"[<JIRA Issue: key='TEZ-152', id='12648784'>, <JIRA Issue: key='TEZ-146', id='12648371'>, <JIRA Issue: key='TEZ-147', id='12648372'>, <JIRA Issue: key='TEZ-155', id='12649242'>, <JIRA Issue: key='TEZ-181', id='12651188'>, <JIRA Issue: key='TEZ-182', id='12651190'>, <JIRA Issue: key='TEZ-189', id='12651733'>, <JIRA Issue: key='TEZ-195', id='12651827'>, <JIRA Issue: key='TEZ-248', id='12653101'>, <JIRA Issue: key='TEZ-249', id='12653102'>]",13,"[<JIRA IssueLink: id='12373825'>, <JIRA IssueLink: id='12376437'>, <JIRA IssueLink: id='12373774'>, <JIRA IssueLink: id='12373780'>, <JIRA IssueLink: id='12377330'>, <JIRA IssueLink: id='12378401'>, <JIRA IssueLink: id='12380149'>, <JIRA IssueLink: id='12369197'>, <JIRA IssueLink: id='12369198'>, <JIRA IssueLink: id='12373513'>, <JIRA IssueLink: id='12369950'>, <JIRA IssueLink: id='12370196'>, <JIRA IssueLink: id='12373745'>]",Changes tracking Hive to run on Tez,2013-05-17T07:57:17.344+0000,2014-10-16T04:31:17.801+0000,Fixed,Major
ZOOKEEPER-1569,"support upsert: setData if the node exists, otherwise, create a new node",ZOOKEEPER,Improvement,Open,[],1,[<JIRA IssueLink: id='12359344'>],"Currently, ZooKeeper supports setData and create.  If it can support upsert like in SQL, it will be great.",2012-10-23T17:47:47.657+0000,2013-12-20T20:03:57.493+0000,,Major
TEZ-1151,Vertex should stay in initializing state until custom vertex manager sets the parallelism,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12388949'>],"In TEZ-1145, application is allowed to set -1 parallelism on a non 1-1 input vertex, and expect VertexManager to set the actual parallelism dynamically. However, when I does that, I hit the following exception:
{code}
org.apache.tez.dag.api.TezUncheckedException: vertex_1400869346120_0005_1_04 has -1 tasks but neither input initializers nor 1-1 uninited sources
        at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.handleInitEvent(VertexImpl.java:2485)
        at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.transition(VertexImpl.java:2431)
        at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.transition(VertexImpl.java:2412)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1267)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:158)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1716)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1702)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)
        at java.lang.Thread.run(Thread.java:695)
{code}",2014-05-23T22:13:15.189+0000,2014-09-06T01:35:44.599+0000,Fixed,Major
HCATALOG-623,Understanding how to use the HBase bulk import feature,HCATALOG,Documentation,Resolved,[],2,"[<JIRA IssueLink: id='12365943'>, <JIRA IssueLink: id='12365942'>]","I'm working through use of the HBaseBulkOutputFormat and I'm getting stuck. I have a simple example that replicates the [ImportTsv example|http://hbase.apache.org/book/ops_mgt.html#importtsv] from the HBase documentation. The end result is the ImportSequenceFile job failing due to jars missing from its classpath. Presumably I've not configured something correctly. In this example I'm using Pig.

Here's the error message and also the command files and commands I use to run them.

{noformat}
$ hadoop fs -put simple.tsv /tmp/
$ HCAT_CLASSPATH=$(hbase classpath) hcat -f simple.ddl
$ PIG_CLASSPATH=$(hbase classpath) pig -v -useHCatalog simple.bulkload.pig
{noformat}

Error message:

{noformat}
2013-02-19 19:55:30,354 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.zookeeper.ZooKeeper in order to ship it to the cluster.
2013-02-19 19:55:30,355 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hadoop.hbase.client.HTable in order to ship it to the cluster.
2013-02-19 19:55:30,357 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hadoop.hive.ql.metadata.HiveException in order to ship it to the cluster.
2013-02-19 19:55:30,358 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hcatalog.mapreduce.HCatOutputFormat in order to ship it to the cluster.
2013-02-19 19:55:30,359 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hcatalog.hbase.HBaseHCatStorageHandler in order to ship it to the cluster.
2013-02-19 19:55:30,360 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hadoop.hive.hbase.HBaseSerDe in order to ship it to the cluster.
2013-02-19 19:55:30,361 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hadoop.hive.metastore.api.Table in order to ship it to the cluster.
2013-02-19 19:55:30,363 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class interface org.apache.thrift.TBase in order to ship it to the cluster.
2013-02-19 19:55:30,364 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class org.apache.hadoop.hbase.util.Bytes in order to ship it to the cluster.
2013-02-19 19:55:30,365 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class com.facebook.fb303.FacebookBase in order to ship it to the cluster.
2013-02-19 19:55:30,366 WARN org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil: Could not find jar for class class com.google.common.util.concurrent.ThreadFactoryBuilder in order to ship it to the cluster.
{noformat}",2013-02-21T18:08:01.592+0000,2013-03-20T21:16:37.466+0000,Fixed,Major
FLINK-1439,Enable all YARN tests on Travis,FLINK,Improvement,Closed,[],1,[<JIRA IssueLink: id='12406269'>],,2015-01-23T17:42:34.452+0000,2019-02-26T15:24:35.513+0000,Done,Minor
SENTRY-540,Fix Sentry test validating special chars in username due to HIVE-8916,SENTRY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12402002'>],"Hive was updated to always strip the domain portion of the username of the connecting user. This causes sentry.tests.e2e.hive.TestUserManagement.testGroup7 to fail because the test is validating special characters in usernames. One of the special chars is an @ symbol so when the test case gets executed via Hive the connecting user and the user in the Sentry policy file will not match, causing the unit test to fail.",2014-11-23T07:18:26.486+0000,2014-11-23T18:16:13.955+0000,Fixed,Major
SPARK-2741,Publish version of spark assembly which does not contain Hive,SPARK,Task,Resolved,[],1,[<JIRA IssueLink: id='12392897'>],"The current spark assembly contains Hive. This conflicts with Hive + Spark which is attempting to use it's own version of Hive.

We'll need to publish a version of the assembly which does not contain the Hive jars.",2014-07-30T02:21:22.906+0000,2014-07-31T00:08:56.108+0000,Fixed,Major
OOZIE-2874,Make the Launcher Mapper map-only job's InputFormat class pluggable,OOZIE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12502348'>, <JIRA IssueLink: id='12502347'>]","In order to prepare for YARN's [*rack-schedulable AM feature*|YARN-6050], and [*its MapReduce counterpart*|MAPREDUCE-6871], let's have the possibility of making the Launcher Mapper map-only job's [{{InputFormat}}|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/InputFormat.java] class pluggable in order to provide alternative implementations that can behave differently from [{{OozieLauncherInputFormat}}|https://github.com/apache/oozie/blob/master/sharelib/oozie/src/main/java/org/apache/oozie/action/hadoop/OozieLauncherInputFormat.java] that does not request any HDFS resources.",2017-05-02T12:00:33.168+0000,2018-01-25T21:00:37.720+0000,Fixed,Major
BIGTOP-632,Release itest-common as an artifact,BIGTOP,Improvement,Closed,[],1,[<JIRA IssueLink: id='12353782'>],"For HBase-managed system tests interacting with the cluster, we may need the itest-common artifact to be released, so that HBase can consume it.

In HBase-land, we are discussing how to do more integration/system tests under HBASE-6201. 

As I have explained in the comments there, HBase and Bigtop can share different parts of the testing matrix. Feel free to leave your comments here or HBASE-6201.  ",2012-06-18T20:51:47.167+0000,2013-06-21T23:55:24.505+0000,Invalid,Major
CALCITE-4574,Wrong/Invalid plans when using RelBuilder#join with correlations,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12612589'>],"RelBuilder#join produces wrong/invalid relational expressions when correlated variables are passed as a parameter along with different join types and non trivial (always true) conditions.

*Wrong plans* exist already in the code base where the {{requiredColumns}} attribute in {{LogicalCorrelate}} is missing some columns.

Consider for instance the middle plan in {{RelOptRulesTest#testSelectNotInCorrelated}}:
{noformat}
LogicalProject(SAL=[$5], EXPR$1=[IS NULL($10)])
  LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{2}]) <-- PROBLEM
    LogicalTableScan(table=[[CATALOG, SALES, EMP]])
    LogicalFilter(condition=[=($cor0.EMPNO, $0)]) <-- $cor0.EMPNO refers to column 0 in EMP relation
      LogicalProject(DEPTNO=[$0], i=[true])
        LogicalFilter(condition=[=($cor0.JOB, $1)]) <-- $cor0.JOB refers to column 2 in EMP relation 
          LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
{noformat}

{{EMPNO}} column (index 0) that is referenced in the correlation in the right input is not present in the {{requiredColumns}} attribute.

*Invalid plans* are created when the join type is SEMI or ANTI and the join condition uses columns from the right side. Currently, the join condition is added after the {{Correlate}} and columns from right side no longer exist thus the filter does not reference valid inputs. 
If we are lucky we will get an {{AssertionError}} when constructing the {{Filter}} operator:

{noformat}
RexInputRef index 8 out of range 0..7
java.lang.AssertionError: RexInputRef index 8 out of range 0..7
	at org.apache.calcite.util.Litmus$1.fail(Litmus.java:32)
	at org.apache.calcite.rex.RexChecker.visitInputRef(RexChecker.java:125)
	at org.apache.calcite.rex.RexChecker.visitInputRef(RexChecker.java:61)
	at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:114)
	at org.apache.calcite.rex.RexChecker.visitCall(RexChecker.java:144)
	at org.apache.calcite.rex.RexChecker.visitCall(RexChecker.java:61)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
	at org.apache.calcite.rel.core.Filter.isValid(Filter.java:127)
	at org.apache.calcite.rel.logical.LogicalFilter.<init>(LogicalFilter.java:72)
	at org.apache.calcite.rel.logical.LogicalFilter.create(LogicalFilter.java:116)
	at org.apache.calcite.rel.core.RelFactories$FilterFactoryImpl.createFilter(RelFactories.java:345)
	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:1349)
	at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:1307)
	at org.apache.calcite.tools.RelBuilder.join(RelBuilder.java:2407)
{noformat}

otherwise (assertions disabled) we will end up with an invalid plan.

{code:java}
RelNode root = builder
        .scan(""EMP"")
        .variable(v)
        .scan(""DEPT"")
        .join(type,
            builder.equals(
                builder.field(2, 0, ""DEPTNO""),
                builder.field(2, 1, ""DEPTNO"")), ImmutableSet.of(v.get().id))
        .build();
{code}

+Actual plan+
{noformat}
LogicalFilter(condition=[=($7, $8)]) <- PROBLEM I
  LogicalCorrelate(correlation=[$cor0], joinType=[semi], requiredColumns=[{}]) <- PROBLEM II
    LogicalTableScan(table=[[scott, EMP]])
    LogicalTableScan(table=[[scott, DEPT]])
{noformat}

+Expected plan+
{noformat}
LogicalCorrelate(correlation=[$cor0], joinType=[semi], requiredColumns=[{7}])
  LogicalTableScan(table=[[scott, EMP]])
  LogicalFilter(condition=[=($cor0.DEPTNO, $0)])
    LogicalTableScan(table=[[scott, DEPT]])
{noformat}
",2021-04-09T16:56:19.230+0000,2021-06-03T22:31:40.505+0000,Fixed,Major
KNOX-2150,Unable to submit Hive jobs using  Knox JWTProvider,KNOX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12577703'>, <JIRA IssueLink: id='12577704'>]","While trying to submit Hive Job over JDBC with JWTProvider  ,  I faced 2 issues
 # Space not supported between ""{{Bearer ey..""}}
 # {{if we remove space then http }}{{Authorization}} header is overwritten by default Basic token .

 

{{beeline -u ""jdbc:hive2://localhost:8443/;ssl=true;AllowSelfSignedCerts=1;AllowAllHostNames=1;sslTrustStore=/Users/abc/knox/install/knox-1.3.0/data/security/keystores/gateway.jks;;AuthMech=0;trustStorePassword=knox;transportMode=http;httpPath=gateway/tokenbased/hive;http.header.Authorization=Bearer eyJraWQiOiJp...df""}}

 

 ",2019-12-18T01:53:43.215+0000,2020-06-30T18:17:13.059+0000,Won't Fix,Major
CALCITE-1058,"Add method RelBuilder.empty, and rewrite LIMIT 0 to it",CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12456171'>],"Add a method RelBuilder.empty(), a new method that returns an empty relation. The default implementation returns a Values with no rows, but schema-on-query systems such as Drill might override the empty method to read schema from an input at query time.

RelBuilder.filter(false) and RelBuilder.limit(0, 0) should simplify to empty().",2016-01-14T05:37:13.936+0000,2016-02-02T19:08:38.395+0000,Fixed,Major
PARQUET-110,Some schemas without column projection cause Pig failures,PARQUET,Bug,Resolved,[],1,[<JIRA IssueLink: id='12398109'>],"Parquet stores and loads the Pig schema in the Configuration. Along the way, Pig changes that Schema:

{code:java}
// This schema is converted from Parquet and written in Configuration
String schemaStr = ""my_list: {array: (array_element: (num1: int,num2: int))}"";
// Reparsed using org.apache.pig.impl.util.Utils
Schema schema = Utils.getSchemaFromString(schemaStr);
// But no longer matches the original structure
schema.toString();
// => {my_list: {array_element: (num1: int,num2: int)}}
{code}

Note that the intermediate bag, named either ""bag"" or ""array"", is removed when Pig reparses the Schema. I can work around this to an extent in the Parquet code, but the Pig behavior gets more strange. If there are two of these, the second is preserved but renamed to ""bag_0"". Something funny is going on there.",2014-09-30T21:25:16.796+0000,2015-01-14T17:14:05.967+0000,Fixed,Major
CALCITE-4560,Wrong plan when decorrelating EXISTS subquery with COALESCE in the predicate,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12611892'>],"The problem can be seen by adding the following test in {{SqlToRelConverterTest}}.
{code:java}
  @Test void testExistsCorrelatedDecorrelate01() {
    final String sql = ""select e1.empno from empnullables e1 where exists (\n""
        + ""  select 1 from empnullables e2 where COALESCE(e1.ename,'M')=COALESCE(e2.ename,'M'))"";
    sql(sql).decorrelate(true).ok();
  }
{code}

The plan after decorrelation is shown below:

{noformat}
LogicalProject(EMPNO=[$0])
  LogicalProject(EMPNO=[$0], ENAME=[$1], JOB=[$2], MGR=[$3], HIREDATE=[$4], SAL=[$5], COMM=[$6], DEPTNO=[$7], SLACKER=[$8], ENAME0=[$9], $f1=[CAST($10):BOOLEAN])
    LogicalJoin(condition=[=($1, $9)], joinType=[inner])
      LogicalTableScan(table=[[CATALOG, SALES, EMPNULLABLES]])
      LogicalAggregate(group=[{0}], agg#0=[MIN($1)])
        LogicalProject(ENAME0=[$9], $f0=[true])
          LogicalJoin(condition=[=(CASE(IS NOT NULL($9), $9, 'M':VARCHAR(20)), CASE(IS NOT NULL($1), CAST($1):VARCHAR(20) NOT NULL, 'M':VARCHAR(20)))], joinType=[inner])
            LogicalTableScan(table=[[CATALOG, SALES, EMPNULLABLES]])
            LogicalAggregate(group=[{0}])
              LogicalProject(ENAME=[$1])
                LogicalTableScan(table=[[CATALOG, SALES, EMPNULLABLES]])
{noformat}

The problem lies in the {{LogicalJoin(condition=[=($1, $9)], joinType=[inner])}} operator. If there are rows with {{NULL}} values in the {{ENAME}} column these are going to be incorrectly removed from the result set. The COALESCE operator is present in the SQL query to ensure that rows with NULL values are retained in the result.",2021-03-31T10:41:55.733+0000,2021-06-03T22:31:24.473+0000,Fixed,Major
TEZ-2233,Allow EdgeProperty of an edge to be changed by VertexManager,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12411879'>],"Currently, all edge managers set during setParallelism end up becoming custom edges. However, just like during dag creation, it should be possible to specify standard edge types like scatter_gather if that is what the final user decision is. More broadly, allowing the complete EdgeProperty to be specified at runtime would make that action at par with compile time.",2015-03-25T21:05:15.055+0000,2015-06-30T04:52:32.674+0000,Fixed,Major
HCATALOG-231,hcat error does not indicate that metaserver connection failed due to authentication issues (in secure mode),HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12347340'>],"If the kerberos ticket-granting ticket has not been correctly set (using kinit), the error that hcat gives is not very useful -

FAILED: Error in metadata: MetaException(message:Could not connect to meta store using any of the URIs provided)

It will be useful to print a message that authentication with meta store had failed.
 When it fails because of some other reason, it should also print the URI it failed to connect to. That will make it easier to figure out if is a configuration issue. 
",2012-01-25T00:19:07.248+0000,2013-05-02T02:29:48.971+0000,Fixed,Major
AVRO-1537,Make it easier to set up a multi-language build environment,AVRO,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12463298'>, <JIRA IssueLink: id='12403285'>, <JIRA IssueLink: id='12403284'>]","It's currently quite tedious to set up an environment in which the Avro test suites for all supported languages can be run, and in which release candidates can be built. This is especially so when we need to test against several different versions of a programming language or VM (e.g. JDK6/JDK7/JDK8, Ruby 1.8.7/1.9.3/2.0/2.1).

Our shared Hudson server isn't an ideal solution, because it only runs tests on changes that are already committed, and maintenance of the server can't easily be shared across the community.

I think a Docker image might be a good solution, since it could be set up by one person, shared with all Avro developers, and maintained by the community on an ongoing basis. But other VM solutions (Vagrant, for example?) might work just as well. Suggestions welcome.

Related resources:

* Using AWS (setting up an EC2 instance for Avro build and release): https://cwiki.apache.org/confluence/display/AVRO/How+To+Release#HowToRelease-UsingAWSforAvroBuildandRelease
* Testing multiple versions of Ruby in CI: AVRO-1515
",2014-07-02T15:06:11.465+0000,2019-10-24T08:25:36.915+0000,Fixed,Major
SENTRY-1885,Remove unused NOTIFICATION_ID from the SENTRY_PATH_CHANGE table,SENTRY,Bug,Open,[],1,[<JIRA IssueLink: id='12512848'>],"I noticed while working on SENTRY-1803 that the SENTRY_PATH_CHANGE table has an unused column named NOTIFICATION_ID. I think this was used before to get the latest notification id processed, but now that process was moved to the SENTRY_HMS_NOTIFICATION_ID table.

I think we should remove this unused column to avoid confusions.",2017-08-16T19:50:40.151+0000,2017-08-24T18:32:02.075+0000,,Major
TEZ-1223,Shuffle errors at 10 TB scale,TEZ,Bug,Open,"[<JIRA Issue: key='TEZ-1224', id='12723265'>, <JIRA Issue: key='TEZ-1225', id='12723266'>, <JIRA Issue: key='TEZ-1226', id='12723267'>]",1,[<JIRA IssueLink: id='12390347'>],"When running a job with the following DAG at 10 TB scale, different shuffle exceptions occurred.  Creating this as umbrella ticket for tracking these errors.  Most of them are related to ShuffleHeader parsing.

DAG:
=====
digraph rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1
{ graph [ label=""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1"", fontsize=24, fontname=Helvetica]; node [fontsize=12, fontname=Helvetica]; edge [fontsize=9, fontcolor=blue, fontname=Arial]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_5"" [ label = ""Map_5[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_5"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileUnorderedKVOutput,\n output=ShuffledUnorderedKVInput,\n dataMovement=BROADCAST,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_9"" [ label = ""Reducer_9[ReduceTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_9"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileSortedOutput,\n output=ShuffledMergedInputLegacy,\n dataMovement=SCATTER_GATHER,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_11_store_returns"" [ label = ""Map_11[store_returns]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_11_store_returns"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_11"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_4_out_Reducer_4"" [ label = ""Reducer_4[out_Reducer_4]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_10"" [ label = ""Map_10[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_10"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileUnorderedKVOutput,\n output=ShuffledUnorderedKVInput,\n dataMovement=BROADCAST,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_8"" [ label = ""Map_8[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_8"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_9"" [ label = ""[input=OnFileSortedOutput,\n output=ShuffledMergedInputLegacy,\n dataMovement=SCATTER_GATHER,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_1"" [ label = ""Map_1[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_1"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileUnorderedKVOutput,\n output=ShuffledUnorderedKVInput,\n dataMovement=BROADCAST,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_6"" [ label = ""Map_6[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_6"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileUnorderedKVOutput,\n output=ShuffledUnorderedKVInput,\n dataMovement=BROADCAST,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_10_item"" [ label = ""Map_10[item]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_10_item"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_10"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_6_d3"" [ label = ""Map_6[d3]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_6_d3"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_6"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_2_catalog_sales"" [ label = ""Map_2[catalog_sales]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_2_catalog_sales"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_2"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_2"" [ label = ""Map_2[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_2"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileSortedOutput,\n output=ShuffledMergedInputLegacy,\n dataMovement=SCATTER_GATHER,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""Reducer_3[ReduceTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_4"" [ label = ""[input=OnFileSortedOutput,\n output=ShuffledMergedInputLegacy,\n dataMovement=SCATTER_GATHER,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_8_store_sales"" [ label = ""Map_8[store_sales]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_8_store_sales"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_8"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_7_store"" [ label = ""Map_7[store]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_7_store"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_7"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_5_d2"" [ label = ""Map_5[d2]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_5_d2"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_5"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_4"" [ label = ""Reducer_4[ReduceTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_4"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_4_out_Reducer_4"" [ label = ""Output [outputClass=MROutput,\n initializer=]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_11"" [ label = ""Map_11[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_11"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_9"" [ label = ""[input=OnFileSortedOutput,\n output=ShuffledMergedInputLegacy,\n dataMovement=SCATTER_GATHER,\n schedulingType=SEQUENTIAL]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_1_d1"" [ label = ""Map_1[d1]"", shape = ""box"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_1_d1"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_1"" [ label = ""Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_7"" [ label = ""Map_7[MapTezProcessor]"" ]; ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Map_7"" -> ""rajesh_20140622203232_d3d3d3ce_3d7a_4f04_ad05_31df915a74fd_1.Reducer_3"" [ label = ""[input=OnFileUnorderedKVOutput,\n output=ShuffledUnorderedKVInput,\n dataMovement=BROADCAST,\n schedulingType=SEQUENTIAL]"" ]; } ",2014-06-24T03:37:35.896+0000,2014-07-15T06:02:47.671+0000,,Major
CALCITE-785,"Add ""Piglet"", a subset of Pig Latin on top of Calcite algebra",CALCITE,Bug,Closed,[],8,"[<JIRA IssueLink: id='12436071'>, <JIRA IssueLink: id='12613576'>, <JIRA IssueLink: id='12613574'>, <JIRA IssueLink: id='12613575'>, <JIRA IssueLink: id='12613455'>, <JIRA IssueLink: id='12613457'>, <JIRA IssueLink: id='12429800'>, <JIRA IssueLink: id='12429801'>]","Implement a subset of Pig Latin on top of Calcite, mapping it onto Calcite algebra (via RelBuilder) that can then be optimized and executed on any back-end.

This serves several purposes:
* Illustrate how to implement a data language on Calcite
* Clear technical roadblocks so that the real Pig can be implemented on Calcite
* Strengthen Calcite's support for nested collections and related operations such as cogroup",2015-07-05T07:15:33.528+0000,2021-04-14T04:43:15.608+0000,Fixed,Major
YETUS-630,Add HDDS subproject to the Hadoop personality,YETUS,Task,Resolved,[],2,"[<JIRA IssueLink: id='12532916'>, <JIRA IssueLink: id='12554165'>]","The current hadop precommit builds use the hadoop personality from yetus repository.

As a new subproject is opened in hadoop (https://issues.apache.org/jira/projects/HDDS) it should be added to the personality.",2018-04-30T11:18:45.226+0000,2019-02-15T07:00:30.979+0000,Fixed,Major
TEZ-8,TEZ UI for progress tracking and history,TEZ,Bug,Closed,"[<JIRA Issue: key='TEZ-1066', id='12709113'>, <JIRA Issue: key='TEZ-1119', id='12714566'>, <JIRA Issue: key='TEZ-1594', id='12742542'>, <JIRA Issue: key='TEZ-1595', id='12742543'>, <JIRA Issue: key='TEZ-1600', id='12742810'>, <JIRA Issue: key='TEZ-1603', id='12742813'>, <JIRA Issue: key='TEZ-1604', id='12742814'>, <JIRA Issue: key='TEZ-1605', id='12742816'>, <JIRA Issue: key='TEZ-1606', id='12742817'>, <JIRA Issue: key='TEZ-1615', id='12743582'>, <JIRA Issue: key='TEZ-1617', id='12743584'>, <JIRA Issue: key='TEZ-1655', id='12747477'>, <JIRA Issue: key='TEZ-1708', id='12750850'>, <JIRA Issue: key='TEZ-1709', id='12750863'>, <JIRA Issue: key='TEZ-1720', id='12751467'>, <JIRA Issue: key='TEZ-1741', id='12753105'>, <JIRA Issue: key='TEZ-1751', id='12753676'>, <JIRA Issue: key='TEZ-1753', id='12753787'>, <JIRA Issue: key='TEZ-1756', id='12753894'>, <JIRA Issue: key='TEZ-1757', id='12753895'>, <JIRA Issue: key='TEZ-1765', id='12754001'>, <JIRA Issue: key='TEZ-1768', id='12754372'>, <JIRA Issue: key='TEZ-1781', id='12755662'>, <JIRA Issue: key='TEZ-1782', id='12755665'>, <JIRA Issue: key='TEZ-1783', id='12755719'>, <JIRA Issue: key='TEZ-1784', id='12755721'>, <JIRA Issue: key='TEZ-1791', id='12756363'>, <JIRA Issue: key='TEZ-1792', id='12756370'>, <JIRA Issue: key='TEZ-1794', id='12756662'>, <JIRA Issue: key='TEZ-1799', id='12757495'>, <JIRA Issue: key='TEZ-1801', id='12757610'>, <JIRA Issue: key='TEZ-1809', id='12758652'>, <JIRA Issue: key='TEZ-1810', id='12758734'>, <JIRA Issue: key='TEZ-1813', id='12758888'>, <JIRA Issue: key='TEZ-1817', id='12759138'>, <JIRA Issue: key='TEZ-1820', id='12759417'>, <JIRA Issue: key='TEZ-1823', id='12759693'>]",3,"[<JIRA IssueLink: id='12389308'>, <JIRA IssueLink: id='12397130'>, <JIRA IssueLink: id='12392313'>]",,2013-04-19T00:46:52.236+0000,2015-01-29T20:25:55.791+0000,Fixed,Major
THRIFT-1465,Visibility of methods in generated java code,THRIFT,Improvement,Closed,[],1,[<JIRA IssueLink: id='12346336'>],Need to resort to reflection to invoke private methods in generated java code.,2011-12-15T00:03:57.773+0000,2012-01-27T04:57:55.850+0000,Fixed,Major
TEZ-3880,do not count rejected tasks as killed in vertex progress,TEZ,Task,Closed,[],2,"[<JIRA IssueLink: id='12523515'>, <JIRA IssueLink: id='12523512'>]","Tasks rejected from LLAP because the cluster is full are shown as killed tasks in the commandline query UI (CLI and beeline). This shouldn't really happen; killed tasks in the container case means something else, and this scenario doesn't exist because AM doesn't continuously try to queue tasks. We could change LLAP queue to use sort of a pull model (would also allow for better duplicate scheduling), but for now we should fix the UI",2017-11-15T22:12:58.919+0000,2019-04-19T19:21:11.002+0000,Fixed,Major
SLIDER-365,"slider ""resolve"" command to retrieve service record",SLIDER,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12398155'>, <JIRA IssueLink: id='12395338'>, <JIRA IssueLink: id='12395339'>, <JIRA IssueLink: id='12398165'>]","Slider registry client can list an entry, but it needs to export the information to for other applications ... e.g. storm launcher client.

Proposed: allow the client to ask slider to save the ServiceRecord to a file.",2014-08-27T17:18:21.252+0000,2014-10-10T01:39:48.685+0000,Fixed,Major
IMPALA-8751,Kudu tables cannot be found after created,IMPALA,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12600800'>, <JIRA IssueLink: id='12600802'>, <JIRA IssueLink: id='12603263'>, <JIRA IssueLink: id='12600857'>]","For example in some kudu integration test as:
TestKuduHMSIntegration.test_drop_db_cascade in  custom_cluster/test_kudu.py
It failed with:
custom_cluster/test_kudu.py:239: in test_drop_db_cascade
    assert not kudu_client.table_exists(kudu_table.name)
/usr/lib/python2.7/contextlib.py:35: in __exit__
    self.gen.throw(type, value, traceback)
common/kudu_test_suite.py:165: in temp_kudu_table
    kudu.delete_table(name)
kudu/client.pyx:392: in kudu.client.Client.delete_table (kudu/client.cpp:7106)
    ???
kudu/errors.pyx:56: in kudu.errors.check_status (kudu/errors.cpp:904)
    ???
E   KuduNotFound: failed to drop Hive Metastore table: TException - service has thrown: NoSuchObjectException(message=s7mo1z)

And when trying to add default capabilities to kudu tables, it is sometime effective, sometimes not:
For example after enable default kudu OBJCAPABILITIES,
./run-tests.py metadata/test_ddl.py -k ""create_kudu""  will succeed while same test in
./run-tests.py custom_cluster/test_kudu.py :
{noformat}
 TestKuduHMSIntegration.test_create_managed_kudu_tables[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none] 
custom_cluster/test_kudu.py:147: in test_create_managed_kudu_tables
    self.run_test_case('QueryTest/kudu_create', vector, use_db=unique_database)
common/impala_test_suite.py:563: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:500: in __exec_in_impala
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:798: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:184: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:187: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:362: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:356: in execute_query_async
    handle = self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:519: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: AnalysisException: Write not supported. Table test_create_managed_kudu_tables_a8d11828.add  access type is: READONLY

{noformat}

",2019-07-11T14:40:10.370+0000,2022-08-08T23:37:01.718+0000,Fixed,Major
TEZ-1390,Replace byte[] with ByteBuffer as the type of user payload in the API,TEZ,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12393673'>, <JIRA IssueLink: id='12394523'>]","This is just and API change. Internally we can continue to use byte[] since thats a much bigger change.
The translation from ByteBuffer to byte[] in the API layer should not have perf impact.",2014-08-07T22:00:24.825+0000,2014-09-06T01:35:14.725+0000,Fixed,Blocker
CALCITE-5282,JdbcValues should add CAST on NULL values,CALCITE,Bug,Open,[],2,"[<JIRA IssueLink: id='12647478'>, <JIRA IssueLink: id='12647986'>]","The following unit test in {{JdbcAdapterTest.java}} is working fine


{code:java}
  @Test void testNullValuesPlan() {
    final String sql = ""select empno, ename, e.deptno, dname\n""
        + ""from scott.emp e left outer join (select * from scott.dept where 0 = 1) d\n""
        + ""on e.deptno = d.deptno"";
    final String explain = ""PLAN=JdbcToEnumerableConverter\n"" +
        ""  JdbcProject(EMPNO=[$0], ENAME=[$1], DEPTNO=[$2], DNAME=[$4])\n"" +
        ""    JdbcJoin(condition=[=($2, $3)], joinType=[left])\n"" +
        ""      JdbcProject(EMPNO=[$0], ENAME=[$1], DEPTNO=[$7])\n"" +
        ""        JdbcTableScan(table=[[SCOTT, EMP]])\n"" +
        ""      JdbcValues(tuples=[[]])\n\n"";
    final String jdbcSql = ""SELECT \""t\"".\""EMPNO\"", \""t\"".\""ENAME\"", \""t\"".\""DEPTNO\"", \""t0\"".\""DNAME\""\n"" +
        ""FROM (SELECT \""EMPNO\"", \""ENAME\"", \""DEPTNO\""\n"" +
        ""FROM \""SCOTT\"".\""EMP\"") AS \""t\""\n"" +
        ""LEFT JOIN (SELECT *\n"" +
        ""FROM (VALUES (NULL, NULL)) AS \""t\"" (\""DEPTNO\"", \""DNAME\"")\n"" +
        ""WHERE 1 = 0) AS \""t0\"" ON \""t\"".\""DEPTNO\"" = \""t0\"".\""DEPTNO\"""";
    CalciteAssert.model(JdbcTest.SCOTT_MODEL)
        .query(sql)
        .explainContains(explain)
        .runs();
  }
{code}

The problem is that {{JdbcValues}} is loosing the type information for each {{NULL}} column
and postgres complains about that. Inside the join condition {{t.DEPTNO = to.DEPTNO}}. postgres doesn't know the type of {{t.DEPTNO}}, assumes it's of type {{TEXT}} and raises an error like {{ERROR: operator does not exist: text = integer,  Hint: No operator matches the given name and argument types. You might need to add explicit type casts.}}

Would it be possible to add a {{CAST}} in case of {{NULL}} values in {{JdbcValues}}. 
Changing {{VALUES (NULL, NULL)}} to  {{VALUES (CAST(NULL AS ...), CAST(NULL AS ...))}} in the resulting SQL statement.

If it is appreciated, we could provide a PR.

If you are asking yourself, why we are doing something strange like {{WHERE 1 = 0}}: We are applying row level access policies as WHERE condition. In this case the user has no access to the table at all.",2022-09-12T09:07:55.350+0000,2022-09-21T14:46:21.098+0000,,Major
PHOENIX-3485,CSVBulkLoadToolIT failing consistently because of HBASE-17108,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12486735'>, <JIRA IssueLink: id='12486669'>]","Stuck threads

Client side:

Thread 1
{code}
""main"" prio=5 tid=0x00007fbcc1800000 nid=0x1903 waiting on condition [0x0000000102f5e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007abe8f720> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:425)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.hadoop.hbase.client.HTable.coprocessorService(HTable.java:1659)
	at org.apache.hadoop.hbase.client.HTable.coprocessorService(HTable.java:1616)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.metaDataCoprocessorExec(ConnectionQueryServicesImpl.java:1276)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.metaDataCoprocessorExec(ConnectionQueryServicesImpl.java:1256)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1441)
	at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:2275)
	at org.apache.phoenix.schema.MetaDataClient.createIndex(MetaDataClient.java:1428)
	at org.apache.phoenix.compile.CreateIndexCompiler$1.execute(CreateIndexCompiler.java:85)
	at org.apache.phoenix.jdbc.PhoenixStatement$3.call(PhoenixStatement.java:355)
	at org.apache.phoenix.jdbc.PhoenixStatement$3.call(PhoenixStatement.java:1)
	at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
	at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:337)
	at org.apache.phoenix.jdbc.PhoenixStatement.execute(PhoenixStatement.java:1459)
	at org.apache.phoenix.end2end.CsvBulkLoadToolIT.testImportOneIndexTable(CsvBulkLoadToolIT.java:309)
	at org.apache.phoenix.end2end.CsvBulkLoadToolIT.testImportOneLocalIndexTable(CsvBulkLoadToolIT.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
{code}

Client side Thread 2 

{code}

""hconnection-0x480a20e3-shared--pool18-t9"" daemon prio=5 tid=0x00007fbcc61b5800 nid=0x1f50b in Object.wait() [0x00000001212c2000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007ada86798> (a org.apache.hadoop.hbase.ipc.RpcClient$Call)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1484)
	- locked <0x00000007ada86798> (a org.apache.hadoop.hbase.ipc.RpcClient$Call)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1750)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:31660)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1621)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:93)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:90)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:115)
	- locked <0x00000007ada86130> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:91)
	- locked <0x00000007ada86130> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:96)
	at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callMethod(CoprocessorRpcChannel.java:57)
	at org.apache.phoenix.coprocessor.generated.MetaDataProtos$MetaDataService$Stub.createTable(MetaDataProtos.java:16499)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl$7.call(ConnectionQueryServicesImpl.java:1458)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl$7.call(ConnectionQueryServicesImpl.java:1)
	at org.apache.hadoop.hbase.client.HTable$17.call(HTable.java:1647)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


Server side:
{code}
""MetadataRpcServer.handler=3,queue=0,port=56678"" daemon prio=5 tid=0x00007fbcc18f0000 nid=0x15f03 waiting on condition [0x000000011c573000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007af048a38> (a com.google.common.util.concurrent.AbstractFuture$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:280)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:132)
	at com.google.common.util.concurrent.Futures.getUnchecked(Futures.java:999)
	at org.apache.twill.zookeeper.ForwardingZKClientService.startAndWait(ForwardingZKClientService.java:51)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.initTxServiceClient(ConnectionQueryServicesImpl.java:383)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.openConnection(ConnectionQueryServicesImpl.java:397)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.access$8(ConnectionQueryServicesImpl.java:390)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl$13.call(ConnectionQueryServicesImpl.java:2373)
	- locked <0x00000007ae4c5bf8> (a org.apache.phoenix.query.ConnectionQueryServicesImpl)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl$13.call(ConnectionQueryServicesImpl.java:1)
	at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76)
	at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:2351)
	at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:232)
	at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:147)
	at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:202)
	at java.sql.DriverManager.getConnection(DriverManager.java:571)
	at java.sql.DriverManager.getConnection(DriverManager.java:187)
	at org.apache.phoenix.util.QueryUtil.getConnection(QueryUtil.java:341)
	at org.apache.phoenix.util.QueryUtil.getConnectionOnServer(QueryUtil.java:328)
	at org.apache.phoenix.util.QueryUtil.getConnectionOnServer(QueryUtil.java:318)
	at org.apache.phoenix.coprocessor.MetaDataEndpointImpl.createTable(MetaDataEndpointImpl.java:1489)
	at org.apache.phoenix.coprocessor.generated.MetaDataProtos$MetaDataService.callMethod(MetaDataProtos.java:16282)
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:6041)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.execServiceOnRegion(HRegionServer.java:3520)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.execService(HRegionServer.java:3502)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:31194)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2149)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:104)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)
	at java.lang.Thread.run(Thread.java:745)

{code}


On the server side, the MetadataEndPointImpl#createTable thread is stuck trying to initiate the transaction service client.",2016-11-15T22:27:43.497+0000,2016-11-16T19:31:22.713+0000,Fixed,Major
SUREFIRE-1852,ReporterException: When writing xml report stdout/stderr,SUREFIRE,Bug,Open,[],1,[<JIRA IssueLink: id='12610897'>],"I'm seeing some junit xml errors lately on ci.hive.apache.org (we are using 3.0.0-M4)

jenkins is unable to parse the junit result xml because its not a valid xml.
I was able to take a look at one of the problematic xmls and it doesn't ""end well""
{code}
<system-err><![CDATA[
{code}

there is an exception in a dump file:
{code}
# Created at 2020-10-09T11:56:23.638
When writing xml report stdout/stderr
org.apache.maven.surefire.report.ReporterException: When writing xml report stdout/stderr
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.addOutputStreamElement(StatelessXmlReporter.java:508)
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.createOutErrElements(StatelessXmlReporter.java:484)
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.serializeTestClassWithoutRerun(StatelessXmlReporter.java:227)
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.serializeTestClass(StatelessXmlReporter.java:213)
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.testSetCompleted(StatelessXmlReporter.java:152)
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.testSetCompleted(StatelessXmlReporter.java:51)
        at org.apache.maven.plugin.surefire.report.TestSetRunListener.testSetCompleted(TestSetRunListener.java:193)
        at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$TestSetCompletedListener.handle(ForkClient.java:166)
        at org.apache.maven.plugin.surefire.booterclient.output.ForkClient$TestSetCompletedListener.handle(ForkClient.java:155)
        at org.apache.maven.plugin.surefire.booterclient.output.ForkedChannelDecoder.handleEvent(ForkedChannelDecoder.java:275)
        at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.processLine(ForkClient.java:409)
        at org.apache.maven.plugin.surefire.booterclient.output.ForkClient.consumeLine(ForkClient.java:379)
        at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:88)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /tmp/stderr6375771388417760677deferred (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at org.apache.maven.surefire.shade.common.org.apache.commons.io.output.DeferredFileOutputStream.writeTo(DeferredFileOutputStream.java:321)
        at org.apache.maven.plugin.surefire.report.Utf8RecodingDeferredFileOutputStream.writeTo(Utf8RecodingDeferredFileOutputStream.java:85)
        at org.apache.maven.plugin.surefire.report.StatelessXmlReporter.addOutputStreamElement(StatelessXmlReporter.java:502)
        ... 13 more
{code}

I've found some very old 2-3 years old ticket (SUREFIRE-1239) which was fixed...it seems like a similar issue have resurfaced
",2020-10-09T12:39:54.746+0000,2021-03-17T11:15:50.027+0000,,Major
HCATALOG-485,Document that storage-based security ignores GRANT/REVOKE statements,HCATALOG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12357178'>, <JIRA IssueLink: id='12357179'>]","In HCatalog's authorization doc, explain that the SQL statements GRANT and REVOKE don't work for storage-based security, which uses the read/write permissions of underlying storage entities to determine a user's permissions for databases, tables, and partitions.  (This is different from Hive's default authorization, which has RDBMS-style permissions for users, groups, and roles as described in [Hive Authorization|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization].) ",2012-08-30T07:38:08.993+0000,2013-02-15T21:32:44.374+0000,Fixed,Major
PHOENIX-1769,Exception thrown when TO_DATE function used as LHS in WHERE-clause,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12456023'>, <JIRA IssueLink: id='12456884'>]","I want to compare DATE type that converted from VARCHAR type using TO_DATE().

Query :
{quote}
select BIRTH from yunsh where TO_DATE(BIRTH) > TO_DATE('2001-01-01');
{quote}

BIRTH field is VARCHAR(50) type.

But,  it thrown exception below:
{quote}
Mon Mar 23 15:37:53 KST 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@4ef6823, java.io.IOException: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1362)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.toScan(ProtobufUtil.java:917)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3078)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29497)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2027)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:114)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:94)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1360)
        ... 8 more
Caused by: org.apache.hadoop.hbase.exceptions.DeserializationException: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.filter.FilterList.parseFrom(FilterList.java:406)
        ... 12 more
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1362)
        at org.apache.hadoop.hbase.filter.FilterList.parseFrom(FilterList.java:403)
        ... 12 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1360)
        ... 13 more
Caused by: org.apache.hadoop.hbase.exceptions.DeserializationException: org.apache.hadoop.hbase.DoNotRetryIOException: BooleanExpressionFilter failed during reading: Could not initialize class org.apache.phoenix.util.DateUtil$ISODateFormatParser
        at org.apache.phoenix.filter.SingleCQKeyValueComparisonFilter.parseFrom(SingleCQKeyValueComparisonFilter.java:55)
        ... 17 more
Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: BooleanExpressionFilter failed during reading: Could not initialize class org.apache.phoenix.util.DateUtil$ISODateFormatParser
        at org.apache.phoenix.util.ServerUtil.createIOException(ServerUtil.java:84)
        at org.apache.phoenix.util.ServerUtil.throwIOException(ServerUtil.java:52)
        at org.apache.phoenix.filter.BooleanExpressionFilter.readFields(BooleanExpressionFilter.java:108)
        at org.apache.phoenix.filter.SingleKeyValueComparisonFilter.readFields(SingleKeyValueComparisonFilter.java:136)
        at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:131)
        at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:101)
        at org.apache.phoenix.filter.SingleCQKeyValueComparisonFilter.parseFrom(SingleCQKeyValueComparisonFilter.java:53)
        ... 17 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.phoenix.util.DateUtil$ISODateFormatParser
        at org.apache.phoenix.util.DateUtil$ISODateFormatParserFactory.getParser(DateUtil.java:242)
        at org.apache.phoenix.util.DateUtil.getDateTimeParser(DateUtil.java:132)
        at org.apache.phoenix.expression.function.ToDateFunction.init(ToDateFunction.java:69)
        at org.apache.phoenix.expression.function.ToDateFunction.readFields(ToDateFunction.java:155)
        at org.apache.phoenix.expression.BaseCompoundExpression.readFields(BaseCompoundExpression.java:110)
        at org.apache.phoenix.expression.ComparisonExpression.readFields(ComparisonExpression.java:341)
        at org.apache.phoenix.filter.BooleanExpressionFilter.readFields(BooleanExpressionFilter.java:106)
        ... 21 more


        at sqlline.IncrementalRows.hasNext(IncrementalRows.java:73)
        at sqlline.TableOutputFormat.print(TableOutputFormat.java:33)
        at sqlline.SqlLine.print(SqlLine.java:1653)
        at sqlline.Commands.execute(Commands.java:833)
        at sqlline.Commands.sql(Commands.java:732)
        at sqlline.SqlLine.dispatch(SqlLine.java:808)
        at sqlline.SqlLine.begin(SqlLine.java:681)
        at sqlline.SqlLine.start(SqlLine.java:398)
        at sqlline.SqlLine.main(SqlLine.java:292)
{quote}

I tested below cases and that succeeded
- TO_DATE used in SELECT-caluse 
- TO_DATE used at RHS in WHERE-caluse. (In this case, BIRTH field was DATE type)

Does anybody have a solution? or is it a bug?",2015-03-23T06:56:42.067+0000,2016-02-09T19:25:42.876+0000,Fixed,Major
SPARK-23717,Leverage docker support in Hadoop 3,SPARK,Improvement,Open,[],5,"[<JIRA IssueLink: id='12529735'>, <JIRA IssueLink: id='12529736'>, <JIRA IssueLink: id='12529738'>, <JIRA IssueLink: id='12529739'>, <JIRA IssueLink: id='12530106'>]","The introduction of docker support in Apache Hadoop 3 can be leveraged by Apache Spark for resolving multiple long standing shortcoming - particularly related to package isolation.
It also allows for network isolation, where applicable, allowing for more sophisticated cluster configuration/customization.

This jira will track the various tasks for enhancing spark to leverage container support.",2018-03-17T01:49:19.648+0000,2020-03-16T22:50:29.531+0000,,Major
ZOOKEEPER-2219,ZooKeeper server should better handle SessionMovedException.,ZOOKEEPER,Bug,Open,[],1,[<JIRA IssueLink: id='12428734'>],"ZooKeeper server should better handle SessionMovedException.
We hit the SessionMovedException. the following is the reason for the SessionMovedException we find:
1. ZK client tried to connect to Leader L. Network was very slow, so before leader processed the request, client disconnected.
2. Client then re-connected to Follower F reusing the same session ID. It was successful.
3. The request in step 1 went into leader. Leader processed it and invalidated the connection created in step 2. But client didn't know the connection it used is invalidated.
4. Client got SessionMovedException when it used the connection invalidated by leader for any ZooKeeper operation.

The following are logs: c045dkh is the Leader, c470udy is the Follower and the sessionID is 0x14be28f50f4419d.
1. ZK client try to initiate session to Leader at 015-03-16 10:59:40,735 and timeout after 10/3 seconds.
logs from ZK client 
{code}
2015-03-16 10:59:40,078 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6670ms for sessionid 0x14be28f50f4419d, closing socket connection and attempting reconnect
015-03-16 10:59:40,735 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server c045dkh/?.?.?.67:2181. Will not attempt to authenticate using SASL (unknown error)
2015-03-16 10:59:40,735 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to c045dkh/?.?.?.67:2181, initiating session
2015-03-16 10:59:44,071 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 3336ms for sessionid 0x14be28f50f4419d, closing socket connection and attempting reconnect
{code}

2. ZK client initiated session to Follower successfully at 2015-03-16 10:59:44,688
logs from ZK client
{code}
2015-03-16 10:59:44,673 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server c470udy/?.?.?.65:2181. Will not attempt to authenticate using SASL (unknown error)
2015-03-16 10:59:44,673 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to c470udy/?.?.?.65:2181, initiating session
2015-03-16 10:59:44,688 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server c470udy/?.?.?.65:2181, sessionid = 0x14be28f50f4419d, negotiated timeout = 10000
{code}
logs from ZK Follower server
{code}
2015-03-16 10:59:44,673 INFO org.apache.zookeeper.server.NIOServerCnxnFactory: Accepted socket connection from /?.?.?.65:42777
2015-03-16 10:59:44,674 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to renew session 0x14be28f50f4419d at /?.?.?.65:42777
2015-03-16 10:59:44,674 INFO org.apache.zookeeper.server.quorum.Learner: Revalidating client: 0x14be28f50f4419d
2015-03-16 10:59:44,675 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x14be28f50f4419d with negotiated timeout 10000 for client /?.?.?.65:42777
{code}

3. At 2015-03-16 10:59:45,668, Leader processed the delayed request which is sent by Client at 2015-03-16 10:59:40,735, right after session was established, it close the socket connection/session because client was already disconnected due to timeout.
logs from ZK Leader server
{code}
2015-03-16 10:59:45,668 INFO org.apache.zookeeper.server.ZooKeeperServer: Client attempting to renew session 0x14be28f50f4419d at /?.?.?.65:50271
2015-03-16 10:59:45,668 INFO org.apache.zookeeper.server.ZooKeeperServer: Established session 0x14be28f50f4419d with negotiated timeout 10000 for client /?.?.?.65:50271
2015-03-16 10:59:45,670 WARN org.apache.zookeeper.server.NIOServerCnxn: Exception causing close of session 0x14be28f50f4419d due to java.io.IOException: Broken pipe
2015-03-16 10:59:45,671 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /?.?.?.65:50271 which had sessionid 0x14be28f50f4419d
{code}

4. Client got SessionMovedException at 2015-03-16 10:59:45,693
logs from ZK Leader server
{code}
2015-03-16 10:59:45,693 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14be28f50f4419d type:multi cxid:0x86e3 zxid:0x1c002a4e53 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:null Error:KeeperErrorCode = Session moved
2015-03-16 10:59:45,695 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14be28f50f4419d type:multi cxid:0x86e5 zxid:0x1c002a4e56 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:null Error:KeeperErrorCode = Session moved
2015-03-16 10:59:45,700 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14be28f50f4419d type:multi cxid:0x86e7 zxid:0x1c002a4e57 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:null Error:KeeperErrorCode = Session moved
{code}

5. At 2015-03-16 10:59:45,710, we close the session 0x14be28f50f4419d but the socket connection between ZK client and ZK Follower is closed at 2015-03-16 10:59:45,715 after session termination.
logs from ZK Leader server:
{code}
2015-03-16 10:59:45,710 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x14be28f50f4419d
{code}
logs from ZK Follower server:
{code}
2015-03-16 10:59:45,715 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /?.?.?.65:42777 which had sessionid 0x14be28f50f4419d
{code}

It looks like Zk client is out-of-sync with ZK server.
My question is how ZK client can recover from this error. It looks like the ZK Client won't be disconnected from Follower until session is closed and any ZooKeeper operation will fail with SessionMovedException before session is closed.
Also since ZK Leader already closed the socket connection/session to ZK Client at step 3, why it still reject the ZooKeeper operation from client with SessionMovedException. Will it be better to endorse the session/connection between ZK client and ZK Follower? This seems like a bug to me. ",2015-06-24T06:39:28.746+0000,2018-11-22T02:09:49.977+0000,,Major
ATLAS-1045,"HiveHook fails to post metadata for ""Create view"" with ""Relative path in absolute URI: NULL::character%20varying"" exception",ATLAS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12505259'>],"*Impact: No metadata for ""create view"" on atlas.*

Steps to repro:
{noformat}
* create table sample (abc String);
* create view sample_view_1 (url COMMENT 'URL of Referring page') AS  SELECT * from sample;
{noformat}


Hiveserver2.log snapshot
{noformat}
2016-07-23 17:28:24,371 INFO  [HiveServer2-Background-Pool: Thread-9870]: hooks.ATSHook (ATSHook.java:<init>(90)) - Created ATS Hook
2016-07-23 17:28:24,371 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogBegin(148)) - <PERFLOG method=PostHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,371 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogEnd(176)) - </PERFLOG method=PostHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1469294904371 end=1469294904371 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,372 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogBegin(148)) - <PERFLOG method=PostHook.org.apache.atlas.hive.hook.HiveHook from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,372 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogEnd(176)) - </PERFLOG method=PostHook.org.apache.atlas.hive.hook.HiveHook start=1469294904372 end=1469294904372 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,372 INFO  [HiveServer2-Background-Pool: Thread-9870]: ql.Driver (Driver.java:execute(1635)) - Resetting the caller context to HIVE_SSN_ID:8cc6511f-5454-431b-aa2f-d7a1c11159a7
2016-07-23 17:28:24,373 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogEnd(176)) - </PERFLOG method=Driver.execute start=1469294904326 end=1469294904373 duration=47 from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,373 INFO  [HiveServer2-Background-Pool: Thread-9870]: ql.Driver (SessionState.java:printInfo(939)) - OK
2016-07-23 17:28:24,373 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogBegin(148)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,373 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogEnd(176)) - </PERFLOG method=releaseLocks start=1469294904373 end=1469294904373 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,373 INFO  [HiveServer2-Background-Pool: Thread-9870]: log.PerfLogger (PerfLogger.java:PerfLogEnd(176)) - </PERFLOG method=Driver.run start=1469294904233 end=1469294904373 duration=140 from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,373 INFO  [Atlas Logger 1]: hook.HiveHook (HiveHook.java:fireAndForget(202)) - Entered Atlas hook for hook type POST_EXEC_HOOK operation CREATEVIEW
2016-07-23 17:28:24,374 INFO  [Atlas Logger 1]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 6: get_database: default
2016-07-23 17:28:24,374 INFO  [Atlas Logger 1]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(391)) - ugi=hive/nat-s11-4-kuls-atlas-1.openstacklocal@HWQE.HORTONWORKS.COM	ip=unknown-ip-addr	cmd=get_database: default
2016-07-23 17:28:24,380 INFO  [Atlas Logger 1]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 6: get_database: default
2016-07-23 17:28:24,380 INFO  [Atlas Logger 1]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(391)) - ugi=hive/nat-s11-4-kuls-atlas-1.openstacklocal@HWQE.HORTONWORKS.COM	ip=unknown-ip-addr	cmd=get_database: default
2016-07-23 17:28:24,385 INFO  [Atlas Logger 1]: bridge.HiveMetaStoreBridge (HiveMetaStoreBridge.java:createOrUpdateDBInstance(161)) - Importing objects from databaseName : default
2016-07-23 17:28:24,385 INFO  [Atlas Logger 1]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 6: get_table : db=default tbl=sample
2016-07-23 17:28:24,385 INFO  [Atlas Logger 1]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(391)) - ugi=hive/nat-s11-4-kuls-atlas-1.openstacklocal@HWQE.HORTONWORKS.COM	ip=unknown-ip-addr	cmd=get_table : db=default tbl=sample
2016-07-23 17:28:24,387 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: session.HiveSessionImpl (HiveSessionImpl.java:acquireAfterOpLock(332)) - We are setting the hadoop caller context to 8cc6511f-5454-431b-aa2f-d7a1c11159a7 for thread HiveServer2-HttpHandler-Pool: Thread-62
2016-07-23 17:28:24,387 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: session.HiveSessionImpl (HiveSessionImpl.java:releaseBeforeOpLock(356)) - We are resetting the hadoop caller context for thread HiveServer2-HttpHandler-Pool: Thread-62
2016-07-23 17:28:24,392 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: session.HiveSessionImpl (HiveSessionImpl.java:acquireAfterOpLock(332)) - We are setting the hadoop caller context to 8cc6511f-5454-431b-aa2f-d7a1c11159a7 for thread HiveServer2-HttpHandler-Pool: Thread-62
2016-07-23 17:28:24,392 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: session.HiveSessionImpl (HiveSessionImpl.java:releaseBeforeOpLock(356)) - We are resetting the hadoop caller context for thread HiveServer2-HttpHandler-Pool: Thread-62
2016-07-23 17:28:24,396 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: session.HiveSessionImpl (HiveSessionImpl.java:acquireAfterOpLock(332)) - We are setting the hadoop caller context to 8cc6511f-5454-431b-aa2f-d7a1c11159a7 for thread HiveServer2-HttpHandler-Pool: Thread-62
2016-07-23 17:28:24,416 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: log.PerfLogger (PerfLogger.java:PerfLogBegin(148)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,416 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: log.PerfLogger (PerfLogger.java:PerfLogEnd(176)) - </PERFLOG method=releaseLocks start=1469294904416 end=1469294904416 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2016-07-23 17:28:24,416 INFO  [HiveServer2-HttpHandler-Pool: Thread-62]: session.HiveSessionImpl (HiveSessionImpl.java:releaseBeforeOpLock(356)) - We are resetting the hadoop caller context for thread HiveServer2-HttpHandler-Pool: Thread-62
2016-07-23 17:28:24,424 INFO  [Atlas Logger 1]: bridge.HiveMetaStoreBridge (HiveMetaStoreBridge.java:createOrUpdateTableInstance(411)) - Importing objects from default.sample
2016-07-23 17:28:24,424 INFO  [Atlas Logger 1]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 6: get_database: default
2016-07-23 17:28:24,424 INFO  [Atlas Logger 1]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(391)) - ugi=hive/nat-s11-4-kuls-atlas-1.openstacklocal@HWQE.HORTONWORKS.COM	ip=unknown-ip-addr	cmd=get_database: default
2016-07-23 17:28:24,439 INFO  [Atlas Logger 1]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 6: get_database: default
2016-07-23 17:28:24,440 INFO  [Atlas Logger 1]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(391)) - ugi=hive/nat-s11-4-kuls-atlas-1.openstacklocal@HWQE.HORTONWORKS.COM	ip=unknown-ip-addr	cmd=get_database: default
2016-07-23 17:28:24,445 INFO  [Atlas Logger 1]: bridge.HiveMetaStoreBridge (HiveMetaStoreBridge.java:createOrUpdateDBInstance(161)) - Importing objects from databaseName : default
2016-07-23 17:28:24,446 INFO  [Atlas Logger 1]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 6: get_table : db=default tbl=sample_view_1
2016-07-23 17:28:24,446 INFO  [Atlas Logger 1]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(391)) - ugi=hive/nat-s11-4-kuls-atlas-1.openstacklocal@HWQE.HORTONWORKS.COM	ip=unknown-ip-addr	cmd=get_table : db=default tbl=sample_view_1
2016-07-23 17:28:24,463 ERROR [Atlas Logger 1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(195)) - java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.hive.ql.metadata.Table.getPath(Table.java:242)
	at org.apache.hadoop.hive.ql.metadata.Table.getDataLocation(Table.java:251)
	at org.apache.hadoop.hive.ql..authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:178)
	at org.apache.hadoop.hive.ql..authorization.AuthorizationPreEventListener.authorizeReadTable(AuthorizationPreEventListener.java:188)
	at org.apache.hadoop.hive.ql..authorization.AuthorizationPreEventListener.onEvent(AuthorizationPreEventListener.java:149)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:2113)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1877)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:139)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:97)
	at com.sun.proxy.$Proxy13.get_table(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1234)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:131)
	at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:159)
	at com.sun.proxy.$Proxy14.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1157)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1131)
	at org.apache.atlas.hive.hook.HiveHook.createOrUpdateEntities(HiveHook.java:513)
	at org.apache.atlas.hive.hook.HiveHook.createOrUpdateEntities(HiveHook.java:535)
	at org.apache.atlas.hive.hook.HiveHook.processHiveEntity(HiveHook.java:623)
	at org.apache.atlas.hive.hook.HiveHook.registerProcess(HiveHook.java:603)
	at org.apache.atlas.hive.hook.HiveHook.fireAndForget(HiveHook.java:227)
	at org.apache.atlas.hive.hook.HiveHook.access$200(HiveHook.java:82)
	at org.apache.atlas.hive.hook.HiveHook$2.run(HiveHook.java:186)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	... 35 more

2016-07-23 17:28:24,464 ERROR [Atlas Logger 1]: hook.HiveHook (HiveHook.java:run(188)) - Atlas hook failed due to error
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table sample_view_1. java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1165)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1131)
	at org.apache.atlas.hive.hook.HiveHook.createOrUpdateEntities(HiveHook.java:513)
	at org.apache.atlas.hive.hook.HiveHook.createOrUpdateEntities(HiveHook.java:535)
	at org.apache.atlas.hive.hook.HiveHook.processHiveEntity(HiveHook.java:623)
	at org.apache.atlas.hive.hook.HiveHook.registerProcess(HiveHook.java:603)
	at org.apache.atlas.hive.hook.HiveHook.fireAndForget(HiveHook.java:227)
	at org.apache.atlas.hive.hook.HiveHook.access$200(HiveHook.java:82)
	at org.apache.atlas.hive.hook.HiveHook$2.run(HiveHook.java:186)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.hive.ql.metadata.Table.getPath(Table.java:242)
	at org.apache.hadoop.hive.ql.metadata.Table.getDataLocation(Table.java:251)
	at org.apache.hadoop.hive.ql..authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:178)
	at org.apache.hadoop.hive.ql..authorization.AuthorizationPreEventListener.authorizeReadTable(AuthorizationPreEventListener.java:188)
	at org.apache.hadoop.hive.ql..authorization.AuthorizationPreEventListener.onEvent(AuthorizationPreEventListener.java:149)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:2113)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1877)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:139)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:97)
	at com.sun.proxy.$Proxy13.get_table(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1234)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:131)
	at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:159)
	at com.sun.proxy.$Proxy14.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1157)
	... 13 more
Caused by: java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	... 35 more
{noformat}",2016-07-23T17:45:23.809+0000,2017-06-01T04:40:57.227+0000,Duplicate,Critical
CALCITE-1651,Difference in DATE cast between Hive and Calcite,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12494992'>, <JIRA IssueLink: id='12495586'>, <JIRA IssueLink: id='12495155'>]","Hive {{CAST('<literal>' as DATE)}} is handled by Java's {{Date.valueOf}} and thus expects {{yyyy-mm-dd}}. In Hive {{SELECT CAST('1' as DATE);}} yields {{NULL}} but in Calcite is {{0001-12-31}}.

I'm opening this as Calcite issue to track HIVE-15708 (migrate to Calcite 1.12) blockers, not sure if solution will require Calcite changes.",2017-02-21T17:20:59.272+0000,2017-02-26T10:41:02.626+0000,Duplicate,Major
SPARK-2447,"Add common solution for sending upsert actions to HBase (put, deletes, and increment)",SPARK,New Feature,Closed,[],4,"[<JIRA IssueLink: id='12392964'>, <JIRA IssueLink: id='12391594'>, <JIRA IssueLink: id='12429238'>, <JIRA IssueLink: id='12391290'>]","Going to review the design with Tdas today.  

But first thoughts is to have an extension of VoidFunction that handles the connection to HBase and allows for options such as turning auto flush off for higher through put.

Need to answer the following questions first.
- Can it be written in Java or should it be written in Scala?
- What is the best way to add the HBase dependency? (will review how Flume does this as the first option)
- What is the best way to do testing? (will review how Flume does this as the first option)
- How to support python? (python may be a different Jira it is unknown at this time)

Goals:
- Simple to use
- Stable
- Supports high load
- Documented (May be in a separate Jira need to ask Tdas)
- Supports Java, Scala, and hopefully Python
- Supports Streaming and normal Spark",2014-07-11T11:00:19.812+0000,2015-06-30T20:59:40.425+0000,Won't Fix,Major
PHOENIX-3062,JMXCacheBuster restarting the metrics system causes PhoenixTracingEndToEndIT to hang,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12496050'>, <JIRA IssueLink: id='12474577'>]","With some recent fixes in the hbase metrics system, we are now affectively restarting the metrics system (in HBase-1.3.0, probably not affecting 1.2.0). Since we use a custom sink in the PhoenixTracingEndToEndIT, restarting the metrics system loses the registered sink thus causing a hang. 

We need a fix in HBase, and Phoenix so that we will not restart the metrics during tests. 

Thanks to [~sergey.soldatov] for analyzing the initial root cause of the hang. 

See HBASE-14166 and others. 

Please continue to PHOENIX-3752 for further discussions.",2016-07-11T21:48:56.648+0000,2017-06-10T01:39:26.603+0000,Duplicate,Major
PHOENIX-6702,ConcurrentMutationsExtendedIT and PartialIndexRebuilderIT fail on Hbase 2.4.11+,PHOENIX,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12641633'>, <JIRA IssueLink: id='12639062'>, <JIRA IssueLink: id='12640190'>, <JIRA IssueLink: id='12641546'>]","On my local machine

ConcurrentMutationsExtendedIT.testConcurrentUpserts failed 6 out 10 times 
while PartialIndexRebuilderIT.testConcurrentUpsertsWithRebuild failed 10 out of 10 times with HBase 2.4.11 (the default build)

 The same tests succeeded 3 out of 3 times with HBase 2.3.7.

Either HBase 2.4 has a bug, or our compatibility modules need to be fixed.",2022-04-29T12:20:22.371+0000,2022-06-13T03:33:23.691+0000,Not A Bug,Blocker
PARQUET-211,Release parquet-mr 1.6.0,PARQUET,Bug,Resolved,[],11,"[<JIRA IssueLink: id='12411743'>, <JIRA IssueLink: id='12412740'>, <JIRA IssueLink: id='12412742'>, <JIRA IssueLink: id='12420422'>, <JIRA IssueLink: id='12411741'>, <JIRA IssueLink: id='12412739'>, <JIRA IssueLink: id='12412741'>, <JIRA IssueLink: id='12412817'>, <JIRA IssueLink: id='12412383'>, <JIRA IssueLink: id='12412412'>, <JIRA IssueLink: id='12421599'>]",Need to determine a list of tasks that should be done before release. Please add issues as sub-tasks.,2015-03-09T20:06:06.815+0000,2015-04-17T00:51:11.856+0000,Fixed,Major
KNOX-1244,YARNUIV2 support requires a custom dispatch to handle redirects from passive to active endpoint,KNOX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12531438'>, <JIRA IssueLink: id='12533653'>]","If discovery queries the YARN config to identify the endpoint for the new YARNUIV2 service, it may find that the configured value is actually the standby endpoint. interactions with this endpoint yield a redirect notice, which needs to be handled in exactly the same way as is done for the RESOURCEMANAGER (RMHaDispatch) and YARNUI (RMUIHaDispatch) services.

The RMUIHaDispatch may be able to be leveraged directly or extended.",2018-04-10T13:53:21.470+0000,2019-03-28T14:05:17.167+0000,Fixed,Major
BIGTOP-3297,Bump HBase to 1.5.0,BIGTOP,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12579614'>, <JIRA IssueLink: id='12579406'>]",,2020-01-30T13:19:20.213+0000,2020-02-06T06:20:08.500+0000,Fixed,Major
SUBMARINE-399,Support Hadoop 2.10,SUBMARINE,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12582098'>],"Hadoop 2.8 and 2.9 will soon be deprecated. Before we drop Hadoop 2 support entirely, we should add Hadoop 2.10 support.

2.10 release adds a number of great features that Submarine depends on, including YARN GPU scheduling.",2020-02-29T15:48:45.446+0000,2020-06-26T14:22:12.242+0000,Fixed,Major
CALCITE-794,Detect cycles when computing statistics,CALCITE,Bug,Closed,[],8,"[<JIRA IssueLink: id='12520366'>, <JIRA IssueLink: id='12431238'>, <JIRA IssueLink: id='12451697'>, <JIRA IssueLink: id='12454344'>, <JIRA IssueLink: id='12431225'>, <JIRA IssueLink: id='12450058'>, <JIRA IssueLink: id='12453749'>, <JIRA IssueLink: id='12431222'>]","The graph of RelNodes is allowed to be cyclic. This causes problems when evaluating certain metadata, for example RelMetataQuery.areColumnsUnique. While computing the value for RelNode r, it might recurse through say a Project and hit r again. This causes a stack overflow.

We solve this by adding a map or set of active RelNodes. The map is stored within RelMetadataQuery, which can now be instantiated, and its methods are no longer static. The first call should instantiate a RelMetadataQuery, but all subsequent calls for metadata (perhaps several kinds of metadata) will use the same RelMetadataQuery instance, hence the same map.

Also add a RelMetadataQuery argument to the static ""handler"" methods in RelMdColumnUniqueness and similar classes.

This is a breaking change for people who have written a metadata handler, and might be subtle to detect, because the methods are invoked via reflection.

For code that is just using RelMetadataQuery methods, the change is still breaking, but the break points and remedy will be obvious: the methods are no longer static, so they need to change  RelMetadataQuery.foo() to  RelMetadataQuery.instance().foo().",2015-07-11T22:37:43.815+0000,2017-11-17T01:08:08.361+0000,Fixed,Major
OOZIE-1544,Remove references of PigStatsUtil from OoziePigStats,OOZIE,Bug,Open,[],2,"[<JIRA IssueLink: id='12375448'>, <JIRA IssueLink: id='12375449'>]",Details in PIG-3457,2013-09-18T16:36:45.611+0000,2015-01-12T06:08:10.931+0000,,Major
CALCITE-1577,Druid adapter: Incorrect result - limit on timestamp disappears,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12491284'>, <JIRA IssueLink: id='12491285'>]","This can be observed with the following query:

{code:sql}
SELECT DISTINCT `__time`
FROM store_sales_sold_time_subset
ORDER BY `__time` ASC
LIMIT 10;
{code}

Query is translated to Druid _timeseries_, but _limit_ operator disappears.",2017-01-16T14:33:03.679+0000,2017-03-24T03:20:02.510+0000,Fixed,Critical
OOZIE-1665,Oozie 3.3.2 build error with Hive 0.11.0,OOZIE,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12388453'>, <JIRA IssueLink: id='12381285'>]","Oozie 3.3.2 build failed with Hive 0.11.0.

Failure to find org.apache.hive:hive-builtins:jar:0.11.0 in
http://repo1.maven.org/maven2",2014-01-09T09:18:37.233+0000,2014-05-20T21:39:08.012+0000,Duplicate,Major
SPARK-20161,Default log4j properties file should print thread-id in ConversionPattern,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12499511'>],"The default log4j file in {{spark/conf/log4j.properties.template}} doesn't display the thread-id when printing out the logs. It would be very useful to add this, especially for YARN. Currently, logs from all the different threads in a single executor are sent to the same log file. This makes debugging difficult as it is hard to filter out what logs come from what thread.",2017-03-30T17:42:18.401+0000,2020-05-17T18:13:47.126+0000,Won't Fix,Major
SPARK-2984,FileNotFoundException on _temporary directory,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12508199'>, <JIRA IssueLink: id='12484072'>]","We've seen several stacktraces and threads on the user mailing list where people are having issues with a {{FileNotFoundException}} stemming from an HDFS path containing {{_temporary}}.

I ([~aash]) think this may be related to {{spark.speculation}}.  I think the error condition might manifest in this circumstance:

1) task T starts on a executor E1
2) it takes a long time, so task T' is started on another executor E2
3) T finishes in E1 so moves its data from {{_temporary}} to the final destination and deletes the {{_temporary}} directory during cleanup
4) T' finishes in E2 and attempts to move its data from {{_temporary}}, but those files no longer exist!  exception

Some samples:

{noformat}
14/08/11 08:05:08 ERROR JobScheduler: Error running job streaming job 1407744300000 ms.0
java.io.FileNotFoundException: File hdfs://hadoopc/user/csong/output/human_bot/-1407744300000.out/_temporary/0/task_201408110805_0000_m_000007 does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:654)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:712)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:708)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:708)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:360)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:310)
        at org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)
        at org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:126)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:841)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:724)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:643)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1068)
        at org.apache.spark.streaming.dstream.DStream$$anonfun$8.apply(DStream.scala:773)
        at org.apache.spark.streaming.dstream.DStream$$anonfun$8.apply(DStream.scala:771)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:41)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.streaming.scheduler.Job.run(Job.scala:32)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:172)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
-- Chen Song at http://apache-spark-user-list.1001560.n3.nabble.com/saveAsTextFiles-file-not-found-exception-td10686.html



{noformat}
I am running a Spark Streaming job that uses saveAsTextFiles to save results into hdfs files. However, it has an exception after 20 batches

result-1406312340000/_temporary/0/task_201407251119_0000_m_000003 does not exist.
{noformat}
and
{noformat}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /apps/data/vddil/real-time/checkpoint/temp: File does not exist. Holder DFSClient_NONMAPREDUCE_327993456_13 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2946)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2766)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor146.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1439)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1261)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
14/07/25 14:45:12 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://gnosis-01-01-01.crl.samsung.com/apps/data/vddil/real-time/checkpoint/checkpoint-1406324700000
{noformat}
-- Bill Jay at http://apache-spark-user-list.1001560.n3.nabble.com/saveAsTextFiles-file-not-found-exception-td10686.html




{noformat}
scala> d3.sample(false,0.01,1).map( pair => pair._2 ).saveAsTextFile(""10000.txt"")


14/06/09 22:06:40 ERROR TaskSetManager: Task 0.0:0 failed 4 times; aborting job
org.apache.spark.SparkException: Job aborted: Task 0.0:0 failed 4 times (most recent failure: Exception failure: java.io.IOException: The temporary job-output directory file:/data/spark-0.9.1-bin-hadoop1/10000.txt/_temporary doesn't exist!)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}
-- Oleg Proudnikov at http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201406.mbox/%3CCAFEaGwKYSXz2Gyviqu44-DvP-V1JzbVFAPd1x5dLHCQBOgTnjg@mail.gmail.com%3E




{noformat}
[INFO] 11 Dec 2013 12:00:33 - org.apache.spark.Logging$class - Loss was due to org.apache.hadoop.util.Shell$ExitCodeException
org.apache.hadoop.util.Shell$ExitCodeException: chmod: getting attributes of `/cygdrive/c/somepath/_temporary/_attempt_201312111200_0000_m_000000_0/part-00000': No such file or directory
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)
        at org.apache.hadoop.util.Shell.run(Shell.java:188)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:467)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:450)
        at org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:593)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:584)
        at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:427)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:465)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:781)
        at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:118)
        at org.apache.hadoop.mapred.SparkHadoopWriter.open(SparkHadoopWriter.scala:86)
        at org.apache.spark.rdd.PairRDDFunctions.writeToFile$1(PairRDDFunctions.scala:667)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:680)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:680)
        at org.apache.spark.scheduler.ResultTask.run(ResultTask.scala:99)
        at org.apache.spark.scheduler.local.LocalScheduler.runTask(LocalScheduler.scala:198)
        at org.apache.spark.scheduler.local.LocalActor$$anonfun$launchTask$1$$anon$1.run(LocalScheduler.scala:68)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
[INFO] 11 Dec 2013 12:00:33 - org.apache.spark.Logging$class - Remove TaskSet 0.0 from pool
[INFO] 11 Dec 2013 12:00:33 - org.apache.spark.Logging$class - Failed to run saveAsTextFile at Test.scala:19
Exception in thread ""main"" org.apache.spark.SparkException: Job failed: Task 0.0:0 failed more than 4 times; aborting job org.apache.hadoop.util.Shell$ExitCodeException: chmod: getting attributes of `/cygdrive/c/somepath/_temporary/_attempt_201312111200_0000_m_000000_0/part-00000': No such file or directory
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:760)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:758)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:758)
        at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:379)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:441)
        at org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:149)
{noformat}
On a Windows box!
-- Nathan Kronenfeld at http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCAEpWh49EvUEWdnsfKJGvU5MB9V5QsR=HQ=wHpufUmEetU19frg@mail.gmail.com%3E




{noformat}
14/07/29 16:16:57 ERROR executor.Executor: Exception in task ID 6087
 java.io.IOException: The temporary job-output directory hdfs://mybox:8020/path/to/a/dir/_temporary doesn't exist!
         at org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
         at org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:240)
         at org.apache.avro.mapred.AvroOutputFormat.getRecordWriter(AvroOutputFormat.java:154)
         at org.apache.hadoop.mapred.SparkHadoopWriter.open(SparkHadoopWriter.scala:90)
         at org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:728)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
         at org.apache.spark.scheduler.Task.run(Task.scala:53)
         at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
         at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
         at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
         at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)
{noformat}
and
{noformat}
14/07/29 16:16:57 ERROR executor.Executor: Exception in task ID 6158
 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /path/to/a/dir/_temporary/_attempt_201407291616_0000_m_0002
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2445)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2437)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2503)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2480)
         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:556)
         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:337)
         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44958)
         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1751)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1747)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1745)

         at org.apache.hadoop.ipc.Client.call(Client.java:1225)
         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
         at com.sun.proxy.$Proxy13.complete(Unknown Source)
         at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:606)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
         at com.sun.proxy.$Proxy13.complete(Unknown Source)
         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:329)
         at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:1769)
         at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1756)
         at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:66)
         at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:99)
         at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
         at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
         at org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:376)
         at org.apache.avro.mapred.AvroOutputFormat$1.close(AvroOutputFormat.java:163)
         at org.apache.hadoop.mapred.SparkHadoopWriter.close(SparkHadoopWriter.scala:102)
         at org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:737)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)
         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
         at org.apache.spark.scheduler.Task.run(Task.scala:53)
         at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
         at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
{noformat}
and
{noformat}
14/07/29 21:01:33 ERROR executor.Executor: Exception in task ID 150
 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /path/to/dir/main/_temporary/_attempt_201407292101_0000_m_000125_150/part-0012
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2445)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2437)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2503)
         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2480)
         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:556)
         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:337)
         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44958)
         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1751)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1747)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1745)
{noformat}
-- Andrew Ash",2014-08-12T06:08:14.853+0000,2020-01-07T18:55:25.857+0000,Fixed,Critical
BOOKKEEPER-219,Active Namenode shutdown after throwing 'java.io.IOException: BookKeeper error during close'.,BOOKKEEPER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12351415'>],"scenario:

1. I am started Active namenode and backup namenode. 
2. write some file. 

Result:

ANN throwing following Error

{noformat} 
2012-04-18 10:46:04,001 WARN org.apache.bookkeeper.client.LedgerHandle: Conditional write failed: BADVERSION
2012-04-18 10:46:04,001 FATAL org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: finalize log segment 1, 8 failed for required journal (JournalAndStream(mgr=org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager@1ec3362f, stream=org.apache.hadoop.contrib.bkjournal.BookKeeperEditLogOutputStream@221a5770))
java.io.IOException: BookKeeper error during close
	at org.apache.hadoop.contrib.bkjournal.BookKeeperEditLogOutputStream.close(BookKeeperEditLogOutputStream.java:90)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream.closeStream(JournalSet.java:79)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet$2.apply(JournalSet.java:180)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:322)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.finalizeLogSegment(JournalSet.java:176)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.endCurrentLogSegment(FSEditLog.java:925)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.rollEditLog(FSEditLog.java:855)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.rollEditLog(FSImage.java:971)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4092)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:714)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:113)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:8068)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:417)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1661)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1657)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1205)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1655)
Caused by: org.apache.bookkeeper.client.BKException$ZKException
	at org.apache.bookkeeper.client.BKException.create(BKException.java:64)
	at org.apache.bookkeeper.client.LedgerHandle.close(LedgerHandle.java:216)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperEditLogOutputStream.close(BookKeeperEditLogOutputStream.java:86)
	... 19 more
{noformat} 



BNN throwing following Error


{noformat} 
2012-04-18 10:39:09,421 ERROR org.apache.bookkeeper.client.PendingReadOp: Error: No such entry while reading entry: 1 ledgerId: 5 from bookie: /10.18.52.157:3183
2012-04-18 10:39:09,423 ERROR org.apache.bookkeeper.client.PendingReadOp: Error: No such entry while reading entry: 1 ledgerId: 5 from bookie: /10.18.52.157:3181
2012-04-18 10:39:09,457 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edits tailer failed to find any streams. Will try again later.
java.io.IOException: No ledger for fromTxnId -12344 found.
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.getInputStream(BookKeeperJournalManager.java:329)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.getInputStream(JournalSet.java:246)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1100)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.access$700(EditLogTailer.java:59)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:318)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:276)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:293)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:504)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:289)

{noformat} 


",2012-04-18T06:42:03.714+0000,2012-05-08T13:30:50.088+0000,Invalid,Major
TEZ-4035,Tez master breaks with YARN 3.2.0 ApplicationReport API change,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12589797'>, <JIRA IssueLink: id='12589796'>]","{noformat}

tez/tez-mapreduce/src/main/java/org/apache/tez/mapreduce/client/NotRunningJob.java:[89,29] no suitable method found for newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,<nulltype>,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,int,int,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,<nulltype>,java.lang.String,float,java.lang.String,<nulltype>)
[ERROR] method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token,java.util.Set<java.lang.String>,boolean,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] method org.apache.hadoop.yarn.api.records.ApplicationReport.newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token,java.util.Set<java.lang.String>,boolean,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,java.lang.String) is not applicable{noformat}",2019-02-02T06:48:42.520+0000,2020-08-25T15:00:09.589+0000,Fixed,Minor
TEZ-3894,Tez intermediate outputs implicitly rely on permissive umask for shuffle,TEZ,Bug,Closed,[],4,"[<JIRA IssueLink: id='12561310'>, <JIRA IssueLink: id='12525834'>, <JIRA IssueLink: id='12588846'>, <JIRA IssueLink: id='12588851'>]","Tez does not explicitly set the permissions of intermediate output files for shuffle. In a secure cluster the shuffle service is running as a different user than the task, so the output files require group readability in order to serve up the data during the shuffle phase. If the umask is too restrictive (e.g.: 077) then the task's file.out and file.out.index permissions can be too restrictive to allow the shuffle handler to access them.",2018-01-31T21:00:38.930+0000,2020-07-23T13:42:03.253+0000,Fixed,Major
ZOOKEEPER-1475,Messages about missing JAAS configuration should not be logged at WARN level,ZOOKEEPER,Improvement,Open,[],1,[<JIRA IssueLink: id='12353177'>],"Messages about unconfigured JAAS settings probably should not be logged at WARN level because it's intentional if the user is not using any SASL based security features. The user may conclude that security is not optional, or that the missing JAAS configuration is behind failures that have an unrelated cause. Perhaps INFO level instead.",2012-05-31T19:08:39.935+0000,2012-09-10T04:26:11.505+0000,,Major
HDDS-818,OzoneConfiguration uses an existing XMLRoot value,HDDS,Bug,In Progress,[],2,"[<JIRA IssueLink: id='12547636'>, <JIRA IssueLink: id='12547641'>]","OzoneConfiguration and ConfInfo have 
@XmlRootElement(name = ""configuration"")

This makes REST Client crash for XML calls.",2018-11-07T19:59:04.188+0000,2021-10-20T20:36:16.608+0000,,Major
TEZ-3296,Tez job can hang if two vertices at the same root distance have different task requirements,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12469209'>, <JIRA IssueLink: id='12470578'>]","When two vertices have the same distance from the root Tez will schedule containers with the same priority.  However those vertices could have different task requirements and therefore different capabilities.  As documented in YARN-314, YARN currently doesn't support requests for multiple sizes at the same priority.  In practice this leads to one vertex allocation requests clobbering the other, and that can result in a situation where the Tez AM is waiting on containers it will never receive from the RM.
",2016-06-09T20:59:35.776+0000,2016-07-09T19:15:59.618+0000,Fixed,Critical
SENTRY-855,support DROP ROLE IF EXISTS or CREATE ROLE IF NOT EXISTS,SENTRY,New Feature,Open,[],1,[<JIRA IssueLink: id='12447715'>],"For drop database/table/view, we support syntax such as,

{code}
DROP DATABASE/TABLE/VIEW [IF EXISTS]
{code}

Only for role, there is no support for IF EXISTS,

We might need to consider to add it.",2015-08-20T22:34:22.697+0000,2017-08-21T07:06:13.255+0000,,Major
THRIFT-1195,Allow users to act on client connects/disconnects,THRIFT,New Feature,Closed,[],2,"[<JIRA IssueLink: id='12353133'>, <JIRA IssueLink: id='12353132'>]","As it stands, the Java library doesn't provide any hooks to detect exactly when a client got connected/disconnected. For many services, this information is both useful and required for things like cleaning up state.

There are of course many possible ways to address this. Here are some thoughts from my initial mail on the topic:

{noformat}
Suppose I have a stateful service and I'd like to clean up some state
when a client disconnects. IIUC, there's no straight forward way to do
this with Thrift. I'd love to hear what others have done in similar
situations.

I'm trying to figure out if there's a way to support this without
modifying Thrift core (this is all with the Thrift Java library):

* my first instinct was to extend TFramedTransport with a custom
factory that allows adding ""listeners"" that can be fired on a close.
Unfortunately it seems like TFramedTransport.close is either never
called, or not called when a client disconnects. The actual socket
close is wrapped up inside a TNonblockingSocket within the FrameBuffer
managed by TNonblockingServer. So this approach doesn't work.

* Since the client socket is generated by
TNonblockingServerSocket.accept, I next considered overriding
accemptImpl() in a custom ServerSocket. This poses other problems --
because much of the state in TNonblockingServerSocket is private, I
need to use super.acceptImpl() to obtain the TNonblockingSocket (or
reimplement everything). This in turn is not helpful because I then
need to wrap the returned TNonblockingSocket in another ""forwarding
object"" such that the listeners can be fired when the socket is
closed.
{noformat}

Unfortunately I did not find any way to do this without modifying Thrift itself, hence this issue. After evaluating several different alternatives, I decided to go with the least intrusive approach, which is implemented in the attached patch. Basically users can add open/close handlers as part of the Args supplied to TNonblockingServer. The server code then invokes the callbacks when appropriate. I realize this approach is not perfect so I'm eager to get some feedback and hear some alternatives.",2011-06-03T17:49:43.823+0000,2012-06-22T04:20:01.270+0000,Fixed,Major
SPARK-15302,"Implement FK/PK ""rely novalidate"" constraints for better CBO",SPARK,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12496433'>, <JIRA IssueLink: id='12466386'>]","Oracle has ""RELY NOVALIDATE"" option for constraints.. Could be easier for Hive to start with something like that for PK/FK constraints. So CBO has more information for optimizations. It does not have to actually check if that constraint is relationship is true; it can just ""rely"" on that constraint.

RELY clause:
https://docs.oracle.com/database/121/SQLRF/clauses002.htm#SQLRF52223

""Overview of Constraint States"":
https://docs.oracle.com/database/121/DWHSG/schemas.htm#DWHSG9053
- Enforcement
- Validation
- Belief

So FK/PK with ""rely novalidate"" will have Enforcement&Validate disabled but Belief = RELY as it is possible to do in Oracle and now in Hive (HIVE-13076).

It opens a lot of ways to do additional ways to optimize execution plans.
As explained in Tom Kyte's ""Metadata matters""
http://www.peoug.org/wp-content/uploads/2009/12/MetadataMatters_PEOUG_Day2009_TKyte.pdf
pp.30 - ""Tell us how the tables relate and we can remove them from the plan..."".
pp.35 - ""Tell us how the tables relate and we have more access paths available..."".",2016-05-12T18:41:36.110+0000,2019-05-21T04:11:13.168+0000,Incomplete,Major
CALCITE-2074,Simplification of point ranges that are open above or below yields wrong results,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12521279'>, <JIRA IssueLink: id='12521328'>]","Discovered while testing 1.15.0 RC0 with Hive. It seems this regression was introduced by CALCITE-1995.

Consider the following query:
{code}
select * from (
select a.*,b.d d1,c.d d2 from
  t1 a join t2 b on (a.id1 = b.id)
       join t2 c on (a.id2 = b.id) where b.d <= 1 and c.d <= 1
) z where d1 > 1 or d2 > 1
{code}

We end up generating the following plan:
{code}
HiveProject(id1=[$0], id2=[$1], d1=[$3], d2=[$4])
  HiveJoin(condition=[OR(=($3, 1), =($4, 1))], joinType=[inner], algorithm=[none], cost=[not available])
    HiveJoin(condition=[AND(=($0, $2), =($1, $2))], joinType=[inner], algorithm=[none], cost=[not available])
      HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($1))])
        HiveProject(id1=[$0], id2=[$1])
          HiveTableScan(table=[[default.t1]], table:alias=[a])
      HiveFilter(condition=[AND(<=($1, 1), IS NOT NULL($0))])
        HiveProject(id=[$0], d=[$1])
          HiveTableScan(table=[[default.t2]], table:alias=[b])
    HiveFilter(condition=[<=($0, 1)])
      HiveProject(d=[$1])
        HiveTableScan(table=[[default.t2]], table:alias=[c])
{code}
Observe that the condition in the top join is not correct.

I can reproduce this in {{RexProgramTest.simplifyFilter}} with the following example:
{code}
    // condition ""a > 5 or b > 5""
    // with pre-condition ""a <= 5 and b <= 5""
    // should yield ""false"" but yields ""a = 5 or b = 5""
    checkSimplifyFilter(or(gt(aRef, literal5),gt(bRef, literal5)),
        RelOptPredicateList.of(rexBuilder,
            ImmutableList.of(le(aRef, literal5), le(bRef, literal5))),
        ""false"");
{code}

",2017-12-01T07:33:13.427+0000,2017-12-09T18:21:28.837+0000,Fixed,Major
SLIDER-868,Ability to put a Slider application on cruise control,SLIDER,New Feature,Open,[],1,[<JIRA IssueLink: id='12435449'>],"You create a Slider application package, deploy it to a YARN cluster and manage it. From the management perspective it is primarily flexing. Based on needs (and the architecture of the application) you grow or shrink specific components of your long running application from time to time. Of course you can set some constraints like affinity, anti-affinity, and strict placement (for data locality or other reasons). Some of these are handled very well by Slider, others are best efforts.

However long running applications have an inherent need to be auto (or even self) managed. This can be achieved by a custom management tool, interacting with Slider client based on constant feedback on the health of the application (metrics, alerts, etc.). This is primarily reactive management. 

There is also proactive management, where the application owner is aware of the usage pattern of the application over time. For example, a financial application usage peaks between 8am to 4pm Mon to Sat (local time), and slows down at other times. A tax application usage peaks for a few months prior to April 15 and then slows down for several months. Certain healthcare applications peak during flu season. You get the point!

It should be possible to declaratively define such an application usage skyline, which can be fed to Slider and put an application on cruise control. The specification can be modified dynamically and Slider should honor the modified version for (reasonably acceptable) future state of the application.

This kind of feature would need support from YARN. There should be a way for Slider to provide details to YARN for guaranteed future capacity planning. 

Note: It is the negotiation that Slider will do with YARN, to ensure the guaranteed (or best effort) future capacity planning to maintain the skyline is what makes this pro-active management useful. Nothing stops an application owner to write pro-active tools to manage a skyline.
",2015-05-08T23:58:12.251+0000,2015-08-25T19:14:22.984+0000,,Major
SPARK-24493,Kerberos Ticket Renewal is failing in Hadoop 2.8+ and Hadoop 3,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12561464'>],"Kerberos Ticket Renewal is failing on long running spark job. I have added below 2 kerberos properties in the HDFS configuration and ran a spark streaming job ([hdfs_wordcount.py|https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/hdfs_wordcount.py])
{noformat}
dfs.namenode.delegation.token.max-lifetime=1800000 (30min)
dfs.namenode.delegation.token.renew-interval=900000 (15min)
{noformat}
 

Spark Job failed at 15min with below error:
{noformat}
18/06/04 18:56:51 INFO DAGScheduler: ShuffleMapStage 10896 (call at /usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py:2381) failed in 0.218 s due to Job aborted due to stage failure: Task 0 in stage 10896.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10896.0 (TID 7290, <GatewayNodeHostname>, executor 1): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (token for abcd: HDFS_DELEGATION_TOKEN owner=abcd@EXAMPLE.COM, renewer=yarn, realUser=, issueDate=1528136773875, maxDate=1528138573875, sequenceNumber=38, masterKeyId=6) is expired, current time: 2018-06-04 18:56:51,276+0000 expected renewal time: 2018-06-04 18:56:13,875+0000
at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
at org.apache.hadoop.ipc.Client.call(Client.java:1445)
at org.apache.hadoop.ipc.Client.call(Client.java:1355)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
at com.sun.proxy.$Proxy18.getBlockLocations(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:317)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy19.getBlockLocations(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:856)
at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:845)
at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:834)
at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:998)
at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)
at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:322)
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:334)
at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:950)
at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:86)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:189)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:186)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:141)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:70)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:64)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:99)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
{noformat}
*Steps to Reproduce:*
 # Add {{dfs.namenode.delegation.token.max-lifetime}} and {{dfs.namenode.delegation.token.renew-interval}} properties in the HDFS config and restart the affected services.
 # Run spark streaming job on gateway node of the cluster in one terminal tab
{noformat}
/bin/spark-submit --master yarn --principal <SPN> --keytab <Keytab File Full Path> hdfs_wordcount.py ""/tmp/streaming_input"" 2>&1 | tee driver.log{noformat}

After 15min the spark application is terminated with the above mentioned error.

 ",2018-06-08T05:53:26.733+0000,2019-05-23T23:14:37.333+0000,Later,Major
SPARK-20277,Allow Spark on YARN to be launched with Docker,SPARK,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12506832'>],"Currently YARN is going to support Docker(YARN-3611). 
We want to empower Spark to support launching Executors via a Docker image that resolving the user's dependencies.",2017-04-10T08:40:10.941+0000,2021-05-25T01:53:51.925+0000,Incomplete,Major
BIGTOP-1813,Create /user/hbase HDFS directory,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12420860'>],"There is a need to run ExportSnapshot (part of HBase) MR job on behalf of hbase user, 
to avoid problems with permissions on data/snapshot files in `/hbase` HDFS directory.

But the problem is there is no `/user/hbase` directory that ExportSnapshot tries to use as a working directory.

Going to add `/user/hbase` in `init-hcfs.json`.
",2015-04-09T17:42:33.543+0000,2015-04-10T19:52:49.640+0000,Fixed,Major
TEZ-2317,Event processing backlog can result in task failures for short tasks,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12421378'>, <JIRA IssueLink: id='12421437'>]",,2015-04-14T08:44:59.683+0000,2015-06-30T04:53:15.179+0000,Fixed,Major
BIGTOP-2281,Add HIVE-12875 to Bigtop,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12455471'>],We really need to add HIVE-12875 ,2016-01-26T10:27:12.866+0000,2016-01-28T18:21:54.434+0000,Fixed,Major
TEZ-699,Have sensible defaults for java opts,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12391377'>],"Its a burden to have to specify them all the time. API's already exist if users want to set special values. Also, for vertex memory would be good to have a TezUtils.getDefaultVertexResource() with a sensible resource size.",2013-12-29T13:21:27.013+0000,2014-09-06T01:35:49.307+0000,Fixed,Major
INFRA-22849,need an ARM worker node for the ci-hbase controller,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12632940'>],"the HBase community is in the process of migrating from ci-hadoop to ci-hbase.

We have an ARM specific build that needs an appropriate node to run on.

Could you please either:

* move one of the two ARM nodes from ci-hadoop to ci-hbase
* make the two ARM nodes a pool that is leased as needed to the ci-hadoop and ci-hbase controllers",2022-02-07T16:07:30.145+0000,2022-02-15T18:45:07.008+0000,Fixed,Major
SPARK-12297,Add work-around for Parquet/Hive int96 timestamp bug.,SPARK,Task,Resolved,[],2,"[<JIRA IssueLink: id='12493287'>, <JIRA IssueLink: id='12498073'>]","Spark copied Hive's behavior for parquet, but this was inconsistent with other file formats, and inconsistent with Impala (which is the original source of putting a timestamp as an int96 in parquet, I believe).  This made timestamps in parquet act more like timestamps with timezones, while in other file formats, timestamps have no time zone, they are a ""floating time"".

The easiest way to see this issue is to write out a table with timestamps in multiple different formats from one timezone, then try to read them back in another timezone.  Eg., here I write out a few timestamps to parquet and textfile hive tables, and also just as a json file, all in the ""America/Los_Angeles"" timezone:

{code}
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val tblPrefix = args(0)
val schema = new StructType().add(""ts"", TimestampType)
val rows = sc.parallelize(Seq(
  ""2015-12-31 23:50:59.123"",
  ""2015-12-31 22:49:59.123"",
  ""2016-01-01 00:39:59.123"",
  ""2016-01-01 01:29:59.123""
).map { x => Row(java.sql.Timestamp.valueOf(x)) })
val rawData = spark.createDataFrame(rows, schema).toDF()

rawData.show()

Seq(""parquet"", ""textfile"").foreach { format =>
  val tblName = s""${tblPrefix}_$format""
  spark.sql(s""DROP TABLE IF EXISTS $tblName"")
  spark.sql(
    raw""""""CREATE TABLE $tblName (
          |  ts timestamp
          | )
          | STORED AS $format
     """""".stripMargin)
  rawData.write.insertInto(tblName)
}

rawData.write.json(s""${tblPrefix}_json"")
{code}

Then I start a spark-shell in ""America/New_York"" timezone, and read the data back from each table:

{code}
scala> spark.sql(""select * from la_parquet"").collect().foreach{println}
[2016-01-01 02:50:59.123]
[2016-01-01 01:49:59.123]
[2016-01-01 03:39:59.123]
[2016-01-01 04:29:59.123]

scala> spark.sql(""select * from la_textfile"").collect().foreach{println}
[2015-12-31 23:50:59.123]
[2015-12-31 22:49:59.123]
[2016-01-01 00:39:59.123]
[2016-01-01 01:29:59.123]

scala> spark.read.json(""la_json"").collect().foreach{println}
[2015-12-31 23:50:59.123]
[2015-12-31 22:49:59.123]
[2016-01-01 00:39:59.123]
[2016-01-01 01:29:59.123]

scala> spark.read.json(""la_json"").join(spark.sql(""select * from la_textfile""), ""ts"").show()
+--------------------+
|                  ts|
+--------------------+
|2015-12-31 23:50:...|
|2015-12-31 22:49:...|
|2016-01-01 00:39:...|
|2016-01-01 01:29:...|
+--------------------+

scala> spark.read.json(""la_json"").join(spark.sql(""select * from la_parquet""), ""ts"").show()
+---+
| ts|
+---+
+---+
{code}

The textfile and json based data shows the same times, and can be joined against each other, while the times from the parquet data have changed (and obviously joins fail).

This is a big problem for any organization that may try to read the same data (say in S3) with clusters in multiple timezones.  It can also be a nasty surprise as an organization tries to migrate file formats.  Finally, its a source of incompatibility between Hive, Impala, and Spark.

HIVE-12767 aims to fix this by introducing a table property which indicates the ""storage timezone"" for the table.  Spark should add the same to ensure consistency between file formats, and with Hive & Impala.",2015-12-12T00:11:45.826+0000,2018-02-02T12:35:56.598+0000,Fixed,Major
SPARK-2243,Support multiple SparkContexts in the same JVM,SPARK,New Feature,Resolved,[],7,"[<JIRA IssueLink: id='12393086'>, <JIRA IssueLink: id='12393149'>, <JIRA IssueLink: id='12399232'>, <JIRA IssueLink: id='12401347'>, <JIRA IssueLink: id='12408276'>, <JIRA IssueLink: id='12402142'>, <JIRA IssueLink: id='12395145'>]","We're developing a platform where we create several Spark contexts for carrying out different calculations. Is there any restriction when using several Spark contexts? We have two contexts, one for Spark calculations and another one for Spark Streaming jobs. The next error arises when we first execute a Spark calculation and, once the execution is finished, a Spark Streaming job is launched:

{code}
14/06/23 16:40:08 ERROR executor.Executor: Exception in task ID 0
java.io.FileNotFoundException: http://172.19.0.215:47530/broadcast_0
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
	at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)
	at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:63)
	at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:139)
	at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:45)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/06/23 16:40:08 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)
14/06/23 16:40:08 WARN scheduler.TaskSetManager: Loss was due to java.io.FileNotFoundException
java.io.FileNotFoundException: http://172.19.0.215:47530/broadcast_0
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
	at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)
	at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:63)
	at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:139)
	at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:45)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
14/06/23 16:40:08 ERROR scheduler.TaskSetManager: Task 0.0:0 failed 1 times; aborting job
14/06/23 16:40:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
14/06/23 16:40:08 INFO scheduler.DAGScheduler: Failed to run runJob at NetworkInputTracker.scala:182
[WARNING] 
org.apache.spark.SparkException: Job aborted: Task 0.0:0 failed 1 times (most recent failure: Exception failure: java.io.FileNotFoundException: http://172.19.0.215:47530/broadcast_0)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/06/23 16:40:09 INFO dstream.ForEachDStream: metadataCleanupDelay = 3600
{code}

So far, we are working on localhost. Any clue about where this error is coming from? Any workaround to solve the issue?",2014-06-23T14:51:04.490+0000,2018-11-12T11:24:24.011+0000,Won't Fix,Major
AVRO-1229,Trevni should support Boolean fields,AVRO,Improvement,Closed,[],1,[<JIRA IssueLink: id='12362966'>],"Trevni currently provides no native support for Boolean types. As part of the Avro support, Booleans should be available. The Impala code recognizes this as well: https://github.com/cloudera/impala/blob/master/be/src/exec/trevni-def.h",2013-01-10T01:12:25.589+0000,2013-02-27T00:54:00.538+0000,Fixed,Major
PHOENIX-1402,Don't recalculate stats on split,PHOENIX,Bug,Closed,[],1,[<JIRA IssueLink: id='12400499'>],"Rather than scan the new regions on a split (which is potentially expensive, and might be causing the timeouts you're seeing [~jfernando_sfdc]), we should instead just split up the existing guideposts between new two new regions based on the split point.

",2014-11-04T08:32:56.165+0000,2015-11-21T02:17:48.738+0000,Fixed,Major
SPARK-3691,Provide a mini cluster for testing system built on Spark,SPARK,Test,Closed,[],2,"[<JIRA IssueLink: id='12397680'>, <JIRA IssueLink: id='12397790'>]","Most Hadoop components such MR, DFS, Tez, and Yarn provide a mini cluster that can be used to test the external systems that rely on those frameworks, such as Pig and Hive. While Spark's local mode can be used to do such testing and is friendly for debugging, it's too far from a real Spark cluster and a lot of problems cannot be discovered. Thus, an equivalent of Hadoop MR mini cluster in Spark would be very helpful in testing system such as Hive/Pig on Spark.

Spark's local-cluster is considered for this purpose but it doesn't fit well because it requires a Spark installation on the box where the tests run. Also, local-cluster isn't exposed.",2014-09-25T18:14:19.173+0000,2015-02-27T22:34:14.936+0000,Won't Fix,Major
SLIDER-717,Migrate slider client to slider REST API,SLIDER,Sub-task,Open,[],4,"[<JIRA IssueLink: id='12408333'>, <JIRA IssueLink: id='12408331'>, <JIRA IssueLink: id='12408332'>, <JIRA IssueLink: id='12408334'>]","Assuming client-side REST operations have been implemented, move the client to issuing REST calls to the AM.

# YARN-2031 is a pre-requisite
# This will not be compatible with Hadoop 2.6 due to the YARN proxy

This *may* involve significant changes to the client, hence estimate of 4d. Hopefully it will not.",2014-12-10T14:42:51.013+0000,2015-04-29T16:56:36.230+0000,,Major
TEZ-4040,Upgrade RoaringBitmap version to avoid NoSuchMethodError,TEZ,Task,Closed,[],1,[<JIRA IssueLink: id='12553952'>],a common request is to use the runOptimize function which is present is later versions of roaringbitmap,2019-02-12T22:48:47.727+0000,2020-08-25T15:00:11.910+0000,Fixed,Major
IMPALA-6212,Re-enable test_hdfs_safe_mode_error_255 after HBASE-18738 has been fixed,IMPALA,Bug,Open,[],1,[<JIRA IssueLink: id='12520477'>],"IMPALA-6109 made it necessary to xfail test_hdfs_safe_mode_error_255. Once HBASE-18738 has been fixed, we should re-enable the test. On the other hand, newer versions of HBase may return a proper error code for failed writes during safe mode (e.g. ""Error(30): Read-only file system""), so we may need a better way to trigger ""Unknown Error 255"" then.",2017-11-17T23:21:54.226+0000,2017-11-17T23:22:19.512+0000,,Major
CALCITE-1357,Recognize Druid Timeseries and TopN queries in DruidQuery,CALCITE,Bug,Closed,[],4,"[<JIRA IssueLink: id='12478364'>, <JIRA IssueLink: id='12478304'>, <JIRA IssueLink: id='12479746'>, <JIRA IssueLink: id='12491285'>]","Port the work done in HIVE-14217 to recognize Timeseries and TopN queries to Calcite.

This includes a rule to push SortLimit into DruidQuery, which can lead to creating TopN queries. This rule can help to push sorting and limit into GroupBy queries, and limit to Select queries.",2016-08-23T19:14:10.520+0000,2017-01-16T14:34:04.581+0000,Fixed,Major
TWILL-23,Log to a separate file,TWILL,Bug,Open,[],1,[<JIRA IssueLink: id='12407036'>],"Currently, log messages get written to stdout, which means long running containers will have an ever growing local stdout file.  It would be better to write to a separate log file so it can be rotated.",2013-12-12T19:10:52.850+0000,2015-06-12T22:12:42.180+0000,,Major
IMPALA-1652,Fix CHAR datatype: Incorrect results with basic predicate on CHAR typed column.,IMPALA,Bug,Open,[],3,"[<JIRA IssueLink: id='12596807'>, <JIRA IssueLink: id='12640702'>, <JIRA IssueLink: id='12497232'>]","Repro:
{code}
create table foo(col1 char(10));
insert into foo values (cast('test1' as char(10)));
select * from foo where col1 = 'test1'; <-- returns an empty result set
select * from foo where col1 = cast('test1' as char(10)); <-- correctly returns 1 row
{code}",2015-01-09T20:16:05.000+0000,2022-05-25T16:15:52.096+0000,,Major
TEZ-1017,Add API to get TezSession diagnostics,TEZ,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12403316'>],"It is useful to get the Yarn Application diagnostics message and display.

For eg: Application appattempt_1396562772944_0011_000001 submitted by user rohinip to unknown queue: wrong",2014-04-03T23:26:15.606+0000,2014-12-11T18:25:52.854+0000,Invalid,Major
CALCITE-4196,Avatica server responds with HTTP/401 prior to consuming all data written by client,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12597548'>],"First off, big thanks to [~krisden] for pointing me to HIVE-22231 which was the similar problem on the Hive side.

Symptoms: the client, when sending a large HTTP request to the Avatica server which is configured for SPNEGO authentication, e.g. an ExecuteBatchRequest with 100's to 1000's of rows to execute, will receive an HTTP/401 response as a part of the normal SPNEGO negotiation (described in [https://tools.ietf.org/html/rfc4559#section-5]). The client will observe an error similar to the following, indicate ""Broken pipe"".
{noformat}
2020-08-24 17:21:54,512 DEBUG http.wire: http-outgoing-1 >> ""[write] I/O error: Broken pipe (Write failed)""
2020-08-24 17:21:54,512 DEBUG conn.DefaultManagedHttpClientConnection: http-outgoing-1: Close connection
2020-08-24 17:21:54,512 DEBUG conn.DefaultManagedHttpClientConnection: http-outgoing-1: Shutdown connection
2020-08-24 17:21:54,512 DEBUG execchain.MainClientExec: Connection discarded
2020-08-24 17:21:54,512 DEBUG conn.PoolingHttpClientConnectionManager: Connection released: [id: 1][route: {}->http://avatica-server:8765][total kept alive: 0; route allocated: 0 of 25; total allocated: 0 of 100]
2020-08-24 17:21:54,512 INFO execchain.RetryExec: I/O exception (java.net.SocketException) caught when processing request to {}->http://avatica-server:8765: Broken pipe (Write failed)
2020-08-24 17:21:54,512 DEBUG execchain.RetryExec: Broken pipe (Write failed)
java.net.SocketException: Broken pipe (Write failed)
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
        at org.apache.calcite.avatica.org.apache.http.impl.conn.LoggingOutputStream.write(LoggingOutputStream.java:74)
        at org.apache.calcite.avatica.org.apache.http.impl.io.SessionOutputBufferImpl.streamWrite(SessionOutputBufferImpl.java:124)
        at org.apache.calcite.avatica.org.apache.http.impl.io.SessionOutputBufferImpl.write(SessionOutputBufferImpl.java:160)
        at org.apache.calcite.avatica.org.apache.http.impl.io.ContentLengthOutputStream.write(ContentLengthOutputStream.java:113)
        at org.apache.calcite.avatica.org.apache.http.entity.ByteArrayEntity.writeTo(ByteArrayEntity.java:112)
        at org.apache.calcite.avatica.org.apache.http.impl.DefaultBHttpClientConnection.sendRequestEntity(DefaultBHttpClientConnection.java:156)
        at org.apache.calcite.avatica.org.apache.http.impl.conn.CPoolProxy.sendRequestEntity(CPoolProxy.java:152)
        at org.apache.calcite.avatica.org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:238)
        at org.apache.calcite.avatica.org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:123)
        at org.apache.calcite.avatica.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)
        at org.apache.calcite.avatica.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
        at org.apache.calcite.avatica.org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)
        at org.apache.calcite.avatica.org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
        at org.apache.calcite.avatica.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
        at org.apache.calcite.avatica.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
        at org.apache.calcite.avatica.remote.AvaticaCommonsHttpClientSpnegoImpl.send(AvaticaCommonsHttpClientSpnegoImpl.java:129)
        at org.apache.calcite.avatica.remote.DoAsAvaticaHttpClient$1.run(DoAsAvaticaHttpClient.java:39)
        at org.apache.calcite.avatica.remote.DoAsAvaticaHttpClient$1.run(DoAsAvaticaHttpClient.java:37)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:360)
        at org.apache.calcite.avatica.remote.DoAsAvaticaHttpClient.send(DoAsAvaticaHttpClient.java:37)
        at org.apache.calcite.avatica.remote.RemoteProtobufService._apply(RemoteProtobufService.java:44)
        at org.apache.calcite.avatica.remote.ProtobufService.apply(ProtobufService.java:117)
        at org.apache.calcite.avatica.remote.RemoteMeta$20.call(RemoteMeta.java:430)
        at org.apache.calcite.avatica.remote.RemoteMeta$20.call(RemoteMeta.java:427)
        at org.apache.calcite.avatica.AvaticaConnection.invokeWithRetries(AvaticaConnection.java:793)
        at org.apache.calcite.avatica.remote.RemoteMeta.executeBatch(RemoteMeta.java:427)
        at org.apache.calcite.avatica.AvaticaConnection.executeBatchUpdateInternal(AvaticaConnection.java:593)
        at org.apache.calcite.avatica.AvaticaPreparedStatement.executeLargeBatch(AvaticaPreparedStatement.java:266)
        at org.apache.calcite.avatica.AvaticaPreparedStatement.executeBatch(AvaticaPreparedStatement.java:259)
        at TestThinClient.main(TestThinClient.java:62) {noformat}
What happens: as a result of how Jetty operates, the request will be dispatched into the Avatica Handler classes before all of the data has been read off of the wire. Jetty will notice that the request does not come with the WWW-Authenticate challenge response, so it will immediately trigger the code to respond to the client and begin the handshake process.

Why Avatica/Jetty do this, the client is still in the process of sending the data over the wire, but Avatica/Jetty is ""done"" processing this request and closes the socket. As the client continues to write the rest of the data (for the request which Avatica has already responded to with HTTP/401), Jetty will send back a TCP reset and close the socket.

The result is that the client sees an exception like above. Again, note, this will only happen with SPNEGO being enabled. It would not happen for no authentication or Basic/Digest authentication.

All this considered, the solution is simple: read all of the data the client is sending prior to replying back with the HTTP/401.",2020-08-26T17:37:06.642+0000,2021-05-18T11:23:00.193+0000,Fixed,Critical
SPARK-16632,Vectorized parquet reader fails to read certain fields from Hive tables,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12475505'>],"The vectorized parquet reader fails to read certain tables created by Hive. When the tables have type ""tinyint"" or ""smallint"", Catalyst converts those to ""ByteType"" and ""ShortType"" respectively. But when Hive writes those tables in parquet format, the parquet schema in the files contains ""int32"" fields.

To reproduce, run these commands in the hive shell (or beeline):

{code}
create table abyte (value tinyint) stored as parquet;
create table ashort (value smallint) stored as parquet;
insert into abyte values (1);
insert into ashort values (1);
{code}

Then query them with Spark 2.0:

{code}
spark.sql(""select * from abyte"").show();
spark.sql(""select * from ashort"").show();
{code}

You'll see this exception (for the byte case):

{noformat}
16/07/13 12:24:23 ERROR datasources.InsertIntoHadoopFsRelationCommand: Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, scm-centos71-iqalat-2.gce.cloudera.com): org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getByte(OnHeapColumnVector.java:159)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)
	... 8 more
{noformat}

This works when you point Spark directly at the files (instead of using the metastore data), or when you disable the vectorized parquet reader.

The root cause seems to be that Hive creates these tables with a not-so-complete schema:

{noformat}
$ parquet-tools schema /tmp/byte.parquet 
message hive_schema {
  optional int32 value;
}
{noformat}

There's no indication that the field is a 32-bit field used to store 8-bit values. When the ParquetReadSupport code tries to consolidate both schemas, it just chooses whatever is in the parquet file for primitive types (see ParquetReadSupport.clipParquetType); the vectorized reader uses the catalyst schema, which comes from the Hive metastore, and says it's a byte field, so when it tries to read the data, the byte data stored in ""OnHeapColumnVector"" is null.

I have tested a small change to {{ParquetReadSupport.clipParquetType}} that fixes this particular issue, but I haven't run any other tests, so I'll do that while I wait for others to chime in and maybe tell me that's not the right place to fix this.
",2016-07-19T22:11:32.799+0000,2016-10-14T05:28:11.938+0000,Fixed,Major
ZEPPELIN-2040,ClusterManager to support launching interpreter in a cluster,ZEPPELIN,New Feature,In Progress,"[<JIRA Issue: key='ZEPPELIN-2041', id='13039484'>, <JIRA Issue: key='ZEPPELIN-2898', id='13099629'>, <JIRA Issue: key='ZEPPELIN-3181', id='13132549'>]",5,"[<JIRA IssueLink: id='12493260'>, <JIRA IssueLink: id='12509598'>, <JIRA IssueLink: id='12506399'>, <JIRA IssueLink: id='12506865'>, <JIRA IssueLink: id='12502523'>]","Zeppelin's interpreter launches in a same machine of Zeppelin server only. This umbrella issue covers to support launching interpreters in remote machine including Yarn, Mesos and AWS.",2017-02-01T14:25:01.373+0000,2018-04-23T08:22:53.317+0000,,Major
OOZIE-3062,Set HADOOP_CONF_DIR for spark action,OOZIE,Bug,Patch Available,[],1,[<JIRA IssueLink: id='12515463'>],"OOZIE-2569 created core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml for spark action. But for spark to consider these files ([spark code ref|https://github.com/apache/spark/blob/83fe3b5e10f8dc62245ea37143abb96be0f39805/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala#L284]), we need to set HADOOP_CONF_DIR environmental variable, otherwise it will point to default HADOOP_CONF_DIR configured on the node. Therefore, Oozie should set HADOOP_CONF_DIR.
Due to a bug YARN-4727, yarn was not considering user specified value for whiltelisted environment variables including HADOOP_CONF_DIR. So, this will work on specified hadoop fix versions in YARN-4727.

This fix in Oozie will be no-op on Hadoop without YARN-4727 fix and Oozie will behave as it is.

To verify this on one node machine. 
1. In hadoop mapred-site.xml, set 
{code}
<property>
   <name>mapreduce.fileoutputcommitter.marksuccessfuljobs</name>
   <value>false</value>
</property>
{code}
2. Run any spark file copy example from Oozie. Check output directory, _SUCCESS flag is missing. It should be there because we explicitly set     conf.set(""mapreduce.fileoutputcommitter.marksuccessfuljobs"", ""true""); in https://github.com/apache/oozie/blob/master/core/src/main/java/org/apache/oozie/action/hadoop/JavaActionExecutor.java#L293",2017-09-20T00:56:59.186+0000,2018-10-15T08:01:48.301+0000,,Major
YETUS-693,Add warning about requirements for XML test-patch test,YETUS,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12599846'>, <JIRA IssueLink: id='12570688'>, <JIRA IssueLink: id='12544616'>]","When running Test-Patch with some JVM's (currently seen in Azul JDK7 over in HBase) the ""xml"" test always votes -1

{code}
...

15:51:42 ============================================================================
15:51:42 ============================================================================
15:51:42                           XML verification: patch
15:51:42 ============================================================================
15:51:42 ============================================================================
15:51:42 
15:51:42 
15:51:42 
15:51:42 
15:51:42 ============================================================================
...
17:24:45 |  -1  |           xml  |   0m  0s   | The patch has 1 ill-formed XML file(s). 
...
17:24:45               Reason | Tests
17:24:45                 XML  |  Parsing Error(s): 
17:24:45                      |  pom.xml 
...
{code}

The actual output indicates the problem is the JDK we have doesn't support javascript scripting:

{code}
pom.xml:

script engine for language js can not be found
{code}

Maybe we could do a quick known-pass and known-fail check in setup for the plugin?",2018-09-14T16:15:36.256+0000,2020-10-06T15:44:27.385+0000,Fixed,Minor
INFRA-16681,Hive precommit jobs error out when run on H19 node.,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12537193'>],"Hi,

While investigating HIVE-19988 I noticed that Hive pre-commit jobs error out due to maven not being able to download dependencies when the job runs on H19 node. It works fine when it runs on other nodes. Can you please check if there are any issues with respect to downloading artifacts from maven central repo. Easiest way to reproduce it would be to run the script which is run by https://builds.apache.org/job/PreCommit-HIVE-Build/

Here are the examples of failed jobs:

https://builds.apache.org/job/PreCommit-HIVE-Build/12124/console
https://builds.apache.org/job/PreCommit-HIVE-Build/12123/console
https://builds.apache.org/job/PreCommit-HIVE-Build/12122/console

Here is the example of job which works (ran on H9 node and H4 node respectively):

https://builds.apache.org/job/PreCommit-HIVE-Build/12113/console
https://builds.apache.org/job/PreCommit-HIVE-Build/12112/console",2018-06-25T22:26:54.692+0000,2018-08-25T07:40:27.422+0000,Fixed,Blocker
TWILL-194,KMS delegation token is not included if the FileContextLocationFactory is ued,TWILL,Bug,Resolved,[],1,[<JIRA IssueLink: id='12480910'>],It is caused by HDFS-10296 and Twill can provide a workaround for that by acquiring the KMS delegation token explicitly when the FileContextLocationFactory is used.,2016-09-21T06:24:21.090+0000,2017-03-27T19:42:38.245+0000,Fixed,Major
SPARK-10793,Make spark's use/subclassing of hive more maintainable,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12448499'>, <JIRA IssueLink: id='12448498'>]","The latest spark/hive integration round has closed the gap with Hive versions, but the integration is still pretty complex
# SparkSQL has deep hooks into the parser
# hivethriftserver uses ""aggressive reflection"" to inject spark classes into the Hive base classes.
# there's a separate org.sparkproject.hive JAR to isolate Kryo versions while avoiding the hive uberjar with all its dependencies getting into the spark uberjar.

We can improve this with some assistance from the other projects, even though no guarantees of stability of things like the parser and thrift server APIs are likely in the near future",2015-09-24T10:19:18.988+0000,2019-05-21T04:36:33.410+0000,Incomplete,Major
SLIDER-1130,Hadoop 2.8 YARN changes have broken slider,SLIDER,Bug,Resolved,[],6,"[<JIRA IssueLink: id='12467296'>, <JIRA IssueLink: id='12471287'>, <JIRA IssueLink: id='12467294'>, <JIRA IssueLink: id='12467293'>, <JIRA IssueLink: id='12467295'>, <JIRA IssueLink: id='12467343'>]","Changes in YARN (YARN-2882, YARN-3866, YARN-4293 have broken the slider MockYarnCluster). That's despite the classes being tagged as {{@Public, @Stable}}.",2016-05-23T21:13:49.700+0000,2016-06-22T18:57:43.075+0000,Fixed,Blocker
FLINK-12054,HBaseConnectorITCase fails on Java 9,FLINK,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12567889'>, <JIRA IssueLink: id='12565098'>, <JIRA IssueLink: id='12572338'>]","An issue in hbase.

{code}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.83 sec <<< FAILURE! - in org.apache.flink.addons.hbase.HBaseConnectorITCase
org.apache.flink.addons.hbase.HBaseConnectorITCase  Time elapsed: 21.829 sec  <<< FAILURE!
java.lang.AssertionError: We should get a URLClassLoader
	at org.apache.flink.addons.hbase.HBaseConnectorITCase.activateHBaseCluster(HBaseConnectorITCase.java:81)
{code}",2019-03-28T13:09:14.267+0000,2019-10-20T10:24:57.083+0000,Won't Fix,Major
FLUME-1954,HBase sink does not work when zoo.cfg is on agent classpath,FLUME,Bug,Open,[],1,[<JIRA IssueLink: id='12365805'>],,2013-03-16T23:25:23.531+0000,2013-03-16T23:26:29.589+0000,,Major
PARQUET-397,Pig Predicate Pushdown using Filter2 API,PARQUET,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12450599'>, <JIRA IssueLink: id='12450597'>]",Use the pushdown API from Pig to build filter predicates for parquet.  See PIG-3760.,2015-12-01T19:41:31.940+0000,2016-02-26T18:29:01.972+0000,Fixed,Major
TEZ-2460,Temporary solution for issue due to YARN-2560,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12425376'>],"Due to YARN-2560, DAGClient can't get the correct diagnostics, this jira is to make a temporary solution for it before YARN-2560 is resolved.  Here's one example that dag is failed due to version incompatible.
{noformat}
15/05/18 17:34:18 INFO client.TezClient: Tez Client Version: [ component=tez-api, version=0.7.0.2.3.0.0-1986, revision=7a048b526519e53de5d6464493697e92b15718fc, SCM-URL=scm:git:https://git-wip-us.apache.org/repos/asf/tez.git, buildTime=20150514-0857 ]
15/05/18 17:34:19 INFO impl.TimelineClientImpl: Timeline service address: http://c6402.ambari.apache.org:8188/ws/v1/timeline/
15/05/18 17:34:19 INFO client.RMProxy: Connecting to ResourceManager at c6402.ambari.apache.org/192.168.64.102:8050
15/05/18 17:34:19 INFO client.TezClient: Using org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager to manage Timeline ACLs
15/05/18 17:34:19 INFO impl.TimelineClientImpl: Timeline service address: http://c6402.ambari.apache.org:8188/ws/v1/timeline/
15/05/18 17:34:21 INFO examples.OrderedWordCount: Running OrderedWordCount
15/05/18 17:34:21 INFO client.TezClient: Submitting DAG application with id: application_1431967480765_0017
15/05/18 17:34:21 INFO client.TezClientUtils: Using tez.lib.uris value from configuration: /hdp/apps/2.3.0.0-1986/tez/tez.tar.gz
15/05/18 17:34:21 INFO client.TezClient: Tez system stage directory hdfs://c1ha/tmp/ambari-qa/staging/.tez/application_1431967480765_0017 doesn't exist and is created
15/05/18 17:34:21 INFO acls.ATSHistoryACLPolicyManager: Created Timeline Domain for History ACLs, domainId=Tez_ATS_application_1431967480765_0017
15/05/18 17:34:22 INFO client.TezClient: Submitting DAG to YARN, applicationId=application_1431967480765_0017, dagName=OrderedWordCount
15/05/18 17:34:22 INFO impl.YarnClientImpl: Submitted application application_1431967480765_0017
15/05/18 17:34:22 INFO client.TezClient: The url to track the Tez AM: http://c6402.ambari.apache.org:8088/proxy/application_1431967480765_0017/
15/05/18 17:34:22 INFO impl.TimelineClientImpl: Timeline service address: http://c6402.ambari.apache.org:8188/ws/v1/timeline/
15/05/18 17:34:22 INFO client.RMProxy: Connecting to ResourceManager at c6402.ambari.apache.org/192.168.64.102:8050
15/05/18 17:34:22 INFO client.DAGClientImpl: Waiting for DAG to start running
15/05/18 17:34:34 INFO client.DAGClientImpl: DAG initialized: CurrentState=Running
15/05/18 17:34:38 INFO client.DAGClientImpl: DAG completed. FinalState=FAILED
15/05/18 17:34:38 INFO examples.OrderedWordCount: DAG diagnostics: []
{noformat}

{noformat}
2015-05-18 16:57:46,807 INFO [main] app.DAGAppMaster: Created DAGAppMaster for application appattempt_1431967480765_0008_000001, versionInfo=[ component=tez-dag, version=0.5.2.2.2.3.0-2611, revision=2d3c6b639d5b1048bd20aad5736823a35edd2485, SCM-URL=scm:git:https://git-wip-us.apache.org/repos/asf/tez.git, buildTIme=20150314-1805 ]
2015-05-18 16:57:47,931 INFO [main] app.DAGAppMaster: Comparing client version with AM version, clientVersion=0.7.0.2.3.0.0-1986, AMVersion=0.5.2.2.2.3.0-2611
2015-05-18 16:57:47,932 FATAL [main] app.DAGAppMaster: Incompatible versions found, clientVersion=0.7.0.2.3.0.0-1986, AMVersion=0.5.2.2.2.3.0-2611
2015-05-18 16:57:49,348 INFO [main] app.DAGAppMaster: Adding session token to jobTokenSecretManager for application
2015-05-18 16:57:49,354 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.container.AMContainerEventType for class org.apache.tez.dag.app.rm.container.AMContainerMap
2015-05-18 16:57:49,355 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.node.AMNodeEventType for class org.apache.tez.dag.app.rm.node.AMNodeTracker
2015-05-18 16:57:49,356 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.DAGAppMasterEventType for class org.apache.tez.dag.app.DAGAppMaster$DAGAppMasterEventHandler
2015-05-18 16:57:49,356 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.DAGEventType for class org.apache.tez.dag.app.DAGAppMaster$DagEventDispatcher
2015-05-18 16:57:49,357 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.VertexEventType for class org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher
2015-05-18 16:57:49,358 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.TaskEventType for class org.apache.tez.dag.app.DAGAppMaster$TaskEventDispatcher
2015-05-18 16:57:49,358 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.TaskAttemptEventType for class org.apache.tez.dag.app.DAGAppMaster$TaskAttemptEventDispatcher
2015-05-18 16:57:49,391 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.AMSchedulerEventType for class org.apache.tez.dag.app.rm.TaskSchedulerEventHandler
2015-05-18 16:57:49,394 INFO [main] event.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.NMCommunicatorEventType for class org.apache.tez.dag.app.launcher.ContainerLauncherImpl
2015-05-18 16:57:49,397 INFO [main] node.AMNodeTracker: blacklistDisablePercent is 33, blacklistingEnabled: true, maxTaskFailuresPerNode: 10
2015-05-18 16:57:49,397 ERROR [main] web.WebUIService: Tez UI History URL is not set
2015-05-18 16:57:49,398 INFO [main] launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500
2015-05-18 16:57:49,398 INFO [main] history.HistoryEventHandler: Initializing HistoryEventHandler
2015-05-18 16:57:49,407 INFO [main] ats.ATSHistoryLoggingService: Initializing ATSService
2015-05-18 16:57:50,051 INFO [main] impl.TimelineClientImpl: Timeline service address: http://c6402.ambari.apache.org:8188/ws/v1/timeline/
2015-05-18 16:57:50,054 INFO [main] recovery.RecoveryService: Initializing RecoveryService
2015-05-18 16:57:50,089 INFO [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] history.HistoryEventHandler: Starting HistoryEventHandler
2015-05-18 16:57:50,089 INFO [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] ats.ATSHistoryLoggingService: Starting ATSService
2015-05-18 16:57:50,091 INFO [ServiceThread:org.apache.tez.dag.app.launcher.ContainerLauncherImpl] impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2015-05-18 16:57:50,103 INFO [ServiceThread:org.apache.tez.dag.app.TaskAttemptListenerImpTezDag] ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-05-18 16:57:50,105 INFO [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] recovery.RecoveryService: Starting RecoveryService
2015-05-18 16:57:50,108 INFO [ServiceThread:DAGClientRPCServer] ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-05-18 16:57:50,123 INFO [Socket Reader #1 for port 53947] ipc.Server: Starting Socket Reader #1 for port 53947
2015-05-18 16:57:50,140 INFO [Socket Reader #1 for port 53906] ipc.Server: Starting Socket Reader #1 for port 53906
2015-05-18 16:57:50,178 INFO [IPC Server listener on 53906] ipc.Server: IPC Server listener on 53906: starting
2015-05-18 16:57:50,178 INFO [ServiceThread:DAGClientRPCServer] client.DAGClientServer: Instantiated DAGClientRPCServer at c6403.ambari.apache.org/192.168.64.103:53906
2015-05-18 16:57:50,178 INFO [IPC Server Responder] ipc.Server: IPC Server Responder: starting
2015-05-18 16:57:50,186 INFO [IPC Server Responder] ipc.Server: IPC Server Responder: starting
2015-05-18 16:57:50,186 INFO [IPC Server listener on 53947] ipc.Server: IPC Server listener on 53947: starting
2015-05-18 16:57:50,235 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-05-18 16:57:50,239 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpRequestLog: Http request log for http.requests. is not defined
2015-05-18 16:57:50,249 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-05-18 16:57:50,254 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context 
2015-05-18 16:57:50,254 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static
2015-05-18 16:57:50,257 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: adding path spec: /*
2015-05-18 16:57:50,276 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Jetty bound to port 39924
2015-05-18 16:57:50,276 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: jetty-6.1.26.hwx
2015-05-18 16:57:50,325 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: Extract jar:file:/hadoop/yarn/local/filecache/15/tez.tar.gz/lib/hadoop-yarn-common-2.6.0.2.2.3.0-2611.jar!/webapps/ to /tmp/Jetty_0_0_0_0_39924_webapps____jab5s/webapp
2015-05-18 16:57:50,468 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: NO JSP Support for /, did not find org.apache.jasper.servlet.JspServlet
2015-05-18 16:57:50,663 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:39924
2015-05-18 16:57:50,669 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] webapp.WebApps: Web app / started at 39924
2015-05-18 16:57:51,023 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] webapp.WebApps: Registered webapp guice modules
2015-05-18 16:57:51,098 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] rm.YarnTaskSchedulerService: TaskScheduler initialized with configuration: maxRMHeartbeatInterval: 250, containerReuseEnabled: true, reuseRackLocal: true, reuseNonLocal: false, localitySchedulingDelay: 250, preemptionPercentage: 10, numHeartbeatsBetweenPreemptions3, idleContainerMinTimeout=10000, idleContainerMaxTimeout=20000, sessionMinHeldContainers=0
2015-05-18 16:57:51,231 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] client.RMProxy: Connecting to ResourceManager at c6402.ambari.apache.org/192.168.64.102:8030
2015-05-18 16:57:51,467 INFO [main] rm.TaskSchedulerEventHandler: TaskScheduler notified that it should unregister from RM
2015-05-18 16:57:51,467 INFO [main] app.DAGAppMaster: DAGAppMasterShutdownHandler invoked
2015-05-18 16:57:51,467 INFO [main] app.DAGAppMaster: Handling DAGAppMaster shutdown
2015-05-18 16:57:51,473 INFO [AMShutdownThread] app.DAGAppMaster: Sleeping for 5 seconds before shutting down
2015-05-18 16:57:51,553 INFO [AMRM Callback Handler Thread] rm.YarnTaskSchedulerService: App total resource memory: 682 cpu: 1 taskAllocations: 0
2015-05-18 16:57:51,556 INFO [AsyncDispatcher event handler] node.AMNodeTracker: Num cluster nodes = 1
2015-05-18 16:57:52,002 INFO [IPC Server handler 0 on 53906] ipc.Server: IPC Server handler 0 on 53906, call org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 192.168.64.103:36267 Call#136 Retry#0
org.apache.tez.dag.api.TezException: No running dag at present
	at org.apache.tez.dag.api.client.DAGClientHandler.getDAG(DAGClientHandler.java:84)
	at org.apache.tez.dag.api.client.DAGClientHandler.getACLManager(DAGClientHandler.java:151)
	at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPBServerImpl.getDAGStatus(DAGClientAMProtocolBlockingPBServerImpl.java:94)
	at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolRPC$DAGClientAMProtocol$2.callBlockingMethod(DAGClientAMProtocolRPC.java:7375)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
...
...
2015-05-18 16:57:56,517 INFO [IPC Server handler 0 on 53906] ipc.Server: IPC Server handler 0 on 53906, call org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 192.168.64.103:36885 Call#1936 Retry#0
org.apache.tez.dag.api.TezException: No running dag at present
	at org.apache.tez.dag.api.client.DAGClientHandler.getDAG(DAGClientHandler.java:84)
	at org.apache.tez.dag.api.client.DAGClientHandler.getACLManager(DAGClientHandler.java:151)
	at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPBServerImpl.getDAGStatus(DAGClientAMProtocolBlockingPBServerImpl.java:94)
	at org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolRPC$DAGClientAMProtocol$2.callBlockingMethod(DAGClientAMProtocolRPC.java:7375)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
2015-05-18 16:57:56,616 INFO [AMShutdownThread] rm.YarnTaskSchedulerService: Successfully unregistered application from RM
2015-05-18 16:57:56,616 INFO [AMRM Callback Handler Thread] impl.AMRMClientAsyncImpl: Interrupted while waiting for queue
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2052)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:274)
2015-05-18 16:57:56,642 INFO [AMShutdownThread] mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:0
2015-05-18 16:57:56,644 INFO [AMShutdownThread] ipc.Server: Stopping server on 53947
2015-05-18 16:57:56,646 INFO [IPC Server listener on 53947] ipc.Server: Stopping IPC Server listener on 53947
2015-05-18 16:57:56,647 INFO [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
2015-05-18 16:57:56,647 INFO [AMShutdownThread] ipc.Server: Stopping server on 53906
2015-05-18 16:57:56,647 INFO [IPC Server listener on 53906] ipc.Server: Stopping IPC Server listener on 53906
2015-05-18 16:57:56,665 INFO [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
2015-05-18 16:57:56,676 INFO [AMShutdownThread] app.DAGAppMaster: Completed deletion of tez scratch data dir, path=hdfs://c1ha/tmp/ambari-qa/staging/.tez/application_1431967480765_0008
2015-05-18 16:57:56,676 INFO [AMShutdownThread] app.DAGAppMaster: Exiting DAGAppMaster..GoodBye!
2015-05-18 16:57:56,677 INFO [Thread-1] app.DAGAppMaster: DAGAppMasterShutdownHook invoked
{noformat}",2015-05-19T00:50:39.208+0000,2016-05-18T04:54:59.151+0000,Fixed,Major
SQOOP-433,Tests are failing on current 0.23 version,SQOOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12347801'>],"We're currently failing most of out test cases on hadoop version 0.23 on following Exception:

java.lang.IllegalStateException: Variable substitution depth too large: 20 ${fs.default.name}
	at org.apache.hadoop.conf.Configuration.substituteVars(Configuration.java:561)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:579)
	at org.apache.hadoop.conf.Configuration.getStrings(Configuration.java:1057)
	at org.apache.hadoop.mapreduce.JobSubmitter.populateTokenCache(JobSubmitter.java:564)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:353)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1221)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1218)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1239)
	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:141)
	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:201)
	at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:413)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:380)
	at org.apache.sqoop.tool.ImportAllTablesTool.run(ImportAllTablesTool.java:64)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:145)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:181)
	at com.cloudera.sqoop.Sqoop.runSqoop(Sqoop.java:45)
	at com.cloudera.sqoop.testutil.ImportJobTestCase.runImport(ImportJobTestCase.java:215)
	at com.cloudera.sqoop.TestAllTables.testMultiTableImport(TestAllTables.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785)",2012-02-02T14:32:44.712+0000,2012-02-03T16:19:04.689+0000,Fixed,Blocker
HDDS-7129,Shade & Relocate Guava dependencies ,HDDS,Improvement,Open,[],2,"[<JIRA IssueLink: id='12645747'>, <JIRA IssueLink: id='12645751'>]",Explore using Guava from Thirdparty to avoid Guava conflicts with downstream projects,2022-08-16T08:30:26.210+0000,2022-08-16T08:41:06.390+0000,,Major
SPARK-31675,Fail to insert data to a table with remote location which causes by hive encryption check,SPARK,Bug,In Progress,[],2,"[<JIRA IssueLink: id='12588035'>, <JIRA IssueLink: id='12587968'>]","Before this fix https://issues.apache.org/jira/browse/HIVE-14380 in Hive 2.2.0, when moving files from staging dir to the final table dir, Hive will do encryption check for the srcPaths and destPaths


{code:java}
// Some comments here
     if (!isSrcLocal) {
        // For NOT local src file, rename the file
        if (hdfsEncryptionShim != null && (hdfsEncryptionShim.isPathEncrypted(srcf) || hdfsEncryptionShim.isPathEncrypted(destf))
            && !hdfsEncryptionShim.arePathsOnSameEncryptionZone(srcf, destf))
        {
          LOG.info(""Copying source "" + srcf + "" to "" + destf + "" because HDFS encryption zones are different."");
          success = FileUtils.copy(srcf.getFileSystem(conf), srcf, destf.getFileSystem(conf), destf,
              true,    // delete source
              replace, // overwrite destination
              conf);
        } else {
{code}

The hdfsEncryptionShim instance holds a global FileSystem instance belong to the default fileSystem. It causes failures when checking a path that belongs to a remote file system.

For example, I 
{code:sql}
key	int	NULL

# Detailed Table Information
Database	bdms_hzyaoqin_test_2
Table	abc
Owner	bdms_hzyaoqin
Created Time	Mon May 11 15:14:15 CST 2020
Last Access	Thu Jan 01 08:00:00 CST 1970
Created By	Spark 2.4.3
Type	MANAGED
Provider	hive
Table Properties	[transient_lastDdlTime=1589181255]
Location	hdfs://cluster2/user/warehouse/bdms_hzyaoqin_test.db/abc
Serde Library	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat	org.apache.hadoop.mapred.TextInputFormat
OutputFormat	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties	[serialization.format=1]
Partition Provider	Catalog
Time taken: 0.224 seconds, Fetched 18 row(s)
{code}

The table abc belongs to the remote hdfs 'hdfs://cluster2', and when we run command below via a spark sql job with default fs is ' 'hdfs://cluster1'
{code:sql}
insert into bdms_hzyaoqin_test_2.abc values(1);
{code}


{code:java}

Error in query: java.lang.IllegalArgumentException: Wrong FS: hdfs://cluster2/user/warehouse/bdms_hzyaoqin_test.db/abc/.hive-staging_hive_2020-05-11_17-10-27_123_6306294638950056285-1/-ext-10000/part-00000-badf2a31-ab36-4b60-82a1-0848774e4af5-c000, expected: hdfs://cluster1
{code}

",2020-05-11T09:15:33.238+0000,2022-04-05T07:05:57.909+0000,,Major
ORC-54,Evolve schemas based on field name rather than index,ORC,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12482184'>, <JIRA IssueLink: id='12482183'>]","Schema evolution as it stands today allows adding fields to the end of schemas or removing them from the end. However, because it is based on the index of the column, you can only ever add or remove -- not both.

ORC files have the full schema information of their contents, so there's actually enough metadata to support changing columns anywhere in the schema.",2016-05-20T22:43:15.242+0000,2016-10-06T17:31:45.539+0000,Fixed,Major
MENFORCER-300,Enforcer somewhat is too sensitive,MENFORCER,Bug,Closed,[],2,"[<JIRA IssueLink: id='12569353'>, <JIRA IssueLink: id='12535006'>]","I am building library with maven settings:
{code:java}
    <maven.compiler.source>1.7</maven.compiler.source>
    <maven.compiler.target>1.7</maven.compiler.target>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.7.0</version>
        <configuration>
          <executable>${jvm.path}/bin/javac</executable>
          <source>1.7</source>
          <target>1.7</target>
        </configuration>
      </plugin>
{code}
And our customer require that we provide JDK 1.7 compatible SW.

Thanks to the help [How to confiugure maven-enforcer-plugin to exclude some rule in test scope?|https://stackoverflow.com/questions/49531075/how-to-confiugure-maven-enforcer-plugin-to-exclude-some-rule-in-test-scope/49534564#49534564] I had setup:
{code:java}
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-enforcer-plugin</artifactId>
        <version>3.0.0-M1</version>
        <executions>
          <execution>
            <id>enforce-bytecode-version</id>
            <goals>
              <goal>enforce</goal>
            </goals>
            <configuration>
              <rules>
                <enforceBytecodeVersion>
                  <maxJdkVersion>1.7</maxJdkVersion>
                  <ignoredScopes>
                    <ignoreScope>test</ignoreScope>
                  </ignoredScopes>
                </enforceBytecodeVersion>
              </rules>
              <fail>true</fail>
            </configuration>
          </execution>
        </executions>
        <dependencies>
          <dependency>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>extra-enforcer-rules</artifactId>
            <version>1.0-beta-7</version>
          </dependency>
        </dependencies>
      </plugin>
{code}
Compiler plugin settings guarantee that **our** code is 1.7, but enforce plugin proves also dependencies.

Our library is using Log4j2 v. 2.10.0 and apparently enforce plugin complains that:
{code:java}
    log4j-api:jar:2.10.0:compile contains module-info.class targeted to JDK 1.9
{code}
Exactly I got:
{code:java}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-bytecode-version) @ clj-log4j2-appender ---
[INFO] Restricted to JDK 1.7 yet org.apache.logging.log4j:log4j-api:jar:2.10.0:compile contains module-info.class targeted to JDK 1.9
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.EnforceBytecodeVersion failed with message:
Found Banned Dependency: org.apache.logging.log4j:log4j-api:jar:2.10.0
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
{code}
However
 * our library is working very well on JDK 1.7 with given Log4j 2.10.0
 * dependency tree did not show this module-info, as it is single class and not a package

Is then Enforcerer too sensitive? Handles Enforcerer correctly classes like module-info?",2018-03-29T14:55:10.537+0000,2019-09-05T14:37:02.352+0000,Not A Problem,Major
REEF-1776,Use YARN proxy user to register REEF Driver in Unmanaged AM mode,REEF,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12500779'>, <JIRA IssueLink: id='12500773'>, <JIRA IssueLink: id='12501143'>]","When launching REEF application in Unmanaged AM mode from another YARN application, two Drivers access the same global object {{UserGroupInformation}} from YARN API. That issue prevents us from running REEF applications on YARN in REEF-on-REEF or REEF-on-Spark settings. We have to implement a mechanism to create proxy YARN users and register Unmanaged AM Drivers in that proxy user context and update the API so that REEF client can pass the user credentials into the driver setup.",2017-04-13T21:15:49.601+0000,2017-04-18T23:57:26.249+0000,Fixed,Major
OOZIE-1722,"When an ApplicationMaster restarts, it restarts the launcher job",OOZIE,Improvement,Closed,"[<JIRA Issue: key='OOZIE-1728', id='12698764'>, <JIRA Issue: key='OOZIE-1729', id='12698765'>, <JIRA Issue: key='OOZIE-1733', id='12699250'>]",5,"[<JIRA IssueLink: id='12387469'>, <JIRA IssueLink: id='12387496'>, <JIRA IssueLink: id='12383747'>, <JIRA IssueLink: id='12383745'>, <JIRA IssueLink: id='12383746'>]","When using Yarn, there are some situations in which the ApplicationMaster can be restarted (e.g. RM failover, the AM dies and another attempt is made, etc).  

When this happens, it starts the launcher job again, which will start over.  So, if that launcher has already launched a job, we'll end up with two instances of the same job, which can be problematic.  For example, if you have a Pig action, the Pig client might run a job, but then the launcher gets restarted by an AM restart and launches that same job again.  

We don't have a way of ""re-attaching"" to previously launched jobs; however, with YARN-1461 and MAPREDUCE-5699, we can use yarn tags to find anything the launcher previously launched that's running and kill them.  We still have to start over, but at least we're not running two instances of a job at the same time.

Here's what we can do for each action type:
- Pig, Sqoop, Hive
-- Kill previously launched jobs and start over
- MapReduce (different because of the optimization)
-- Exit launcher if a previously launched job already exists
- Java, Shell
-- No out-of-the-box support for this
-- Like with other things, the Java action can take advantage of this like Pig, Sqoop, and Hive if the user adds some code
- DistCp
-- Not supported
- SSH, Email
-- N/A

The yarn tags won't be available until Hadoop 2.4.0, but is in the nightly (i.e. Hadoop 3.0.0-SNAPSHOT); and its obviously not in Hadoop 1.x.  To be able to use the Yarn methods and the new methods for tagging, we can add a new type of Hadooplib called ""Hadoop Utils"" where we can put classes that are specific to a specific version of Hadoop; the other implementations can have dummy versions.  For example, in the Hadoop-2 Hadoop Utils, we can put a method foo() that calls some yarn stuff but in the Hadoop-1 Hadoop Utils, the foo() method would either do the equivalent in MR1 or a no-op.  So for now, I put some methods in the Hadoop-3 Hadoop Utils that use the tags and the Hadoop-1, Hadoop-2, and Hadoop-23 Hadoop Utils all have dummy implementations that don't do anything (so the existing behavior is preserved).  The Hadoop Utils modules will allow us to take advantage of Hadoop 2 only features in the future, while still being able to compile against Hadoop 1; so it's not just limited to this feature.  ",2014-02-28T21:46:43.962+0000,2015-05-18T07:11:31.542+0000,Fixed,Major
TEZ-1540,Improvements to make InputInitializerEvents easier to use,TEZ,Bug,Closed,[],4,"[<JIRA IssueLink: id='12395926'>, <JIRA IssueLink: id='12395927'>, <JIRA IssueLink: id='12395928'>, <JIRA IssueLink: id='12395929'>]",Replacement for TEZ-1447 as a tracker jira - which has evolved into a jira for one of the features required to make InputInitializers usable. See comments there on what the problems are.,2014-09-04T08:22:28.101+0000,2014-10-02T21:41:05.403+0000,Done,Major
IMPALA-8178,Tests failing with “Could not allocate memory while trying to increase reservation” on EC filesystem,IMPALA,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12582831'>, <JIRA IssueLink: id='12554642'>]","In tests run against an Erasure Coding filesystem, multiple tests failed with memory allocation errors.

In total 10 tests failed:
 * query_test.test_scanners.TestParquet.test_decimal_encodings
 * query_test.test_scanners.TestTpchScanRangeLengths.test_tpch_scan_ranges
 * query_test.test_exprs.TestExprs.test_exprs [enable_expr_rewrites: 0]
 * query_test.test_exprs.TestExprs.test_exprs [enable_expr_rewrites: 1]
 * query_test.test_hbase_queries.TestHBaseQueries.test_hbase_scan_node
 * query_test.test_scanners.TestParquet.test_def_levels
 * query_test.test_scanners.TestTextSplitDelimiters.test_text_split_across_buffers_delimiterquery_test.test_hbase_queries.TestHBaseQueries.test_hbase_filters
 * query_test.test_hbase_queries.TestHBaseQueries.test_hbase_inline_views
 * query_test.test_hbase_queries.TestHBaseQueries.test_hbase_top_n

The first failure looked like this on the client side:

{quote}
F query_test/test_scanners.py::TestParquet::()::test_decimal_encodings[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
 query_test/test_scanners.py:717: in test_decimal_encodings
     self.run_test_case('QueryTest/parquet-decimal-formats', vector, unique_database)
 common/impala_test_suite.py:472: in run_test_case
     result = self.__execute_query(target_impalad_client, query, user=user)
 common/impala_test_suite.py:699: in __execute_query
     return impalad_client.execute(query, user=user)
 common/impala_connection.py:174: in execute
     return self.__beeswax_client.execute(sql_stmt, user=user)
 beeswax/impala_beeswax.py:183: in execute
     handle = self.__execute_query(query_string.strip(), user=user)
 beeswax/impala_beeswax.py:360: in __execute_query
     self.wait_for_finished(handle)
 beeswax/impala_beeswax.py:381: in wait_for_finished
     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
 E    Query aborted:ExecQueryFInstances rpc query_id=6e44c3c949a31be2:f973c7ff00000000 failed: Failed to get minimum memory reservation of 8.00 KB on daemon xxx.com:22001 for query 6e44c3c949a31be2:f973c7ff00000000 due to following error: Memory limit exceeded: Could not allocate memory while trying to increase reservation.
 E   Query(6e44c3c949a31be2:f973c7ff00000000) could not allocate 8.00 KB without exceeding limit.
 E   Error occurred on backend xxx.com:22001
 E   Memory left in process limit: 1.19 GB
 E   Query(6e44c3c949a31be2:f973c7ff00000000): Reservation=0 ReservationLimit=9.60 GB OtherMemory=0 Total=0 Peak=0
 E   Memory is likely oversubscribed. Reducing query concurrency or configuring admission control may help avoid this error.
{quote}


On the server side log:

{quote}
I0207 18:25:19.329311  5562 impala-server.cc:1063] 6e44c3c949a31be2:f973c7ff00000000] Registered query query_id=6e44c3c949a31be2:f973c7ff00000000 session_id=93497065f69e9d01:8a3bd06faff3da5
I0207 18:25:19.329434  5562 Frontend.java:1242] 6e44c3c949a31be2:f973c7ff00000000] Analyzing query: select score from decimal_stored_as_int32
I0207 18:25:19.329583  5562 FeSupport.java:285] 6e44c3c949a31be2:f973c7ff00000000] Requesting prioritized load of table(s): test_decimal_encodings_28d99c0e.decimal_stored_as_int32
I0207 18:25:30.776041  5562 Frontend.java:1282] 6e44c3c949a31be2:f973c7ff00000000] Analysis finished.
I0207 18:25:35.919486 10418 admission-controller.cc:608] 6e44c3c949a31be2:f973c7ff00000000] Schedule for id=6e44c3c949a31be2:f973c7ff00000000 in pool_name=default-pool per_host_mem_estimate=16.02 MB PoolConfig: max_requests=-1 max_queued=200 max_mem=-1.00 B
I0207 18:25:35.919528 10418 admission-controller.cc:613] 6e44c3c949a31be2:f973c7ff00000000] Stats: agg_num_running=2, agg_num_queued=0, agg_mem_reserved=24.13 MB,  local_host(local_mem_admitted=1.99 GB, num_admitted_running=2, num_queued=0, backend_mem_reserved=8.06 MB)
I0207 18:25:35.919549 10418 admission-controller.cc:645] 6e44c3c949a31be2:f973c7ff00000000] Admitted query id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:35.920532 10418 coordinator.cc:93] 6e44c3c949a31be2:f973c7ff00000000] Exec() query_id=6e44c3c949a31be2:f973c7ff00000000 stmt=select score from decimal_stored_as_int32
I0207 18:25:35.930855 10418 coordinator.cc:359] 6e44c3c949a31be2:f973c7ff00000000] starting execution on 2 backends for query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:35.938108 21110 impala-internal-service.cc:50] 6e44c3c949a31be2:f973c7ff00000000] ExecQueryFInstances(): query_id=6e44c3c949a31be2:f973c7ff00000000 coord=xxx.com:22000 #instances=1
I0207 18:25:36.037228 10571 query-state.cc:624] 6e44c3c949a31be2:f973c7ff00000000] Executing instance. instance_id=6e44c3c949a31be2:f973c7ff00000000 fragment_idx=0 per_fragment_instance_idx=0 coord_state_idx=0 #in-flight=5
I0207 18:25:48.149771 12581 coordinator-backend-state.cc:209] ExecQueryFInstances rpc query_id=6e44c3c949a31be2:f973c7ff00000000 failed: Failed to get minimum memory reservation of 8.00 KB on daemon xxx.com:22001 for query 6e44c3c949a31be2:f973c7ff00000000 due to following error: Memory limit exceeded: Could not allocate memory while trying to increase reservation.
Query(6e44c3c949a31be2:f973c7ff00000000) could not allocate 8.00 KB without exceeding limit.
Query(6e44c3c949a31be2:f973c7ff00000000): Reservation=0 ReservationLimit=9.60 GB OtherMemory=0 Total=0 Peak=0
I0207 18:25:48.149895 10418 coordinator.cc:373] 6e44c3c949a31be2:f973c7ff00000000] started execution on 2 backends for query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.152803 10418 coordinator.cc:527] 6e44c3c949a31be2:f973c7ff00000000] ExecState: query id=6e44c3c949a31be2:f973c7ff00000000 finstance=N/A on host=xxx.com (EXECUTING -> ERROR) status=ExecQueryFInstances rpc query_id=6e44c3c949a31be2:f973c7ff00000000 failed: Failed to get minimum memory reservation of 8.00 KB on daemon xxx.com:22001 for query 6e44c3c949a31be2:f973c7ff00000000 due to following error: Memory limit exceeded: Could not allocate memory while trying to increase reservation.
Query(6e44c3c949a31be2:f973c7ff00000000) could not allocate 8.00 KB without exceeding limit.
Query(6e44c3c949a31be2:f973c7ff00000000): Reservation=0 ReservationLimit=9.60 GB OtherMemory=0 Total=0 Peak=0
I0207 18:25:48.152827 10418 coordinator-backend-state.cc:453] 6e44c3c949a31be2:f973c7ff00000000] Sending CancelQueryFInstances rpc for query_id=6e44c3c949a31be2:f973c7ff00000000 backend=127.0.0.1:27000
I0207 18:25:48.155086 12737 control-service.cc:168] CancelQueryFInstances(): query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.155109 12737 query-exec-mgr.cc:97] QueryState: query_id=6e44c3c949a31be2:f973c7ff00000000 refcnt=4
I0207 18:25:48.155117 12737 query-state.cc:649] Cancel: query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.155129 12737 krpc-data-stream-mgr.cc:325] cancelling all streams for fragment_instance_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.155297 10418 coordinator.cc:687] 6e44c3c949a31be2:f973c7ff00000000] CancelBackends() query_id=6e44c3c949a31be2:f973c7ff00000000, tried to cancel 1 backends
I0207 18:25:48.155306 10418 coordinator.cc:859] 6e44c3c949a31be2:f973c7ff00000000] Release admission control resources for query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.170018 10571 krpc-data-stream-mgr.cc:294] 6e44c3c949a31be2:f973c7ff00000000] DeregisterRecvr(): fragment_instance_id=6e44c3c949a31be2:f973c7ff00000000, node=1
I0207 18:25:48.197767  5562 impala-beeswax-server.cc:239] close(): query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.197775  5562 impala-server.cc:1142] UnregisterQuery(): query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.197779  5562 impala-server.cc:1249] Cancel(): query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.225905 10529 query-state.cc:272] 6e44c3c949a31be2:f973c7ff00000000] UpdateBackendExecState(): last report for 6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.225889 10571 query-state.cc:632] 6e44c3c949a31be2:f973c7ff00000000] Instance completed. instance_id=6e44c3c949a31be2:f973c7ff00000000 #in-flight=4 status=CANCELLED: Cancelled
I0207 18:25:48.372977 12737 control-service.cc:125] ReportExecStatus(): Received report for unknown query ID (probably closed or cancelled): 6e44c3c949a31be2:f973c7ff00000000 remote host=127.0.0.1:50422
I0207 18:25:48.373118 10529 query-state.cc:431] 6e44c3c949a31be2:f973c7ff00000000] Cancelling fragment instances as directed by the coordinator. Returned status: ReportExecStatus(): Received report for unknown query ID (probably closed or cancelled): 6e44c3c949a31be2:f973c7ff00000000 remote host=127.0.0.1:50422
I0207 18:25:48.373138 10529 query-state.cc:649] 6e44c3c949a31be2:f973c7ff00000000] Cancel: query_id=6e44c3c949a31be2:f973c7ff00000000
I0207 18:25:48.429422  5562 query-exec-mgr.cc:184] ReleaseQueryState(): deleted query_id=6e44c3c949a31be2:f973c7ff00000000
{quote}

",2019-02-09T00:31:30.435+0000,2020-10-09T16:18:26.498+0000,Fixed,Blocker
ORC-432,openjdk 8 has a bug that prevents surefire from working,ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12547678'>],"It looks like the problem is https://bugs.openjdk.java.net/browse/JDK-8030046. It looks like:

{code:bash}
[ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /root/orc/java/shims && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xmx2048m -jar /root/orc/java/shims/target/surefire/surefirebooter4015168140687556977.jar /root/orc/java/shims/target/surefire 2018-11-02T12-32-24_319-jvmRun1 surefire8218006047690391850tmp surefire_04160000529152079754tmp
{code}

The surefire-reports/*.dumpstream looks like:
{code:bash}
Error: Could not find or load main class org.apache.maven.surefire.booter.ForkedBooter
{code}

 and we can work around the problem by changing the surefire configuration:

{code:bash}
+          <useSystemClassLoader>false</useSystemClassLoader>
{code}
",2018-11-02T19:35:12.243+0000,2019-01-02T19:12:44.793+0000,Fixed,Major
MPIR-380,Emails in developers section of pom are improperly handled,MPIR,Bug,Closed,[],3,"[<JIRA IssueLink: id='12574877'>, <JIRA IssueLink: id='12570063'>, <JIRA IssueLink: id='12556764'>]","Here is test case can be used: 
1) download this [1] pom file to the new folder
2) run `mvn clean site`
3) open target/site/team-list.html in browser
Result: email is converted to ""mailto:"" link

4) open this file in your favorite text editor
5) uncomment line 13 <!-- maven-project-info-reports-plugin.version>3.0.0</maven-project-info-reports-plugin.version-->
6) comment line 14 <maven-project-info-reports-plugin.version>2.9</maven-project-info-reports-plugin.version>
7) uncomment line 81 <!-- report>team</report-->
8) comment line 82 <report>project-team</report>
9) run `mvn clean site`
10) open target/site/team.html in browser
Result: email is displayed as plain link to nowhere

Initial investigation available here: https://github.com/devacfr/reflow-maven-skin/issues/40#issuecomment-445304386


[1] https://gist.github.com/solomax/2ce33116bb56cc5c5b1942f245f6d1b8
",2019-01-19T10:57:55.684+0000,2019-12-15T20:52:24.785+0000,Fixed,Major
CALCITE-909,Make ReduceExpressionsRule extensible,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12444626'>, <JIRA IssueLink: id='12444624'>]",,2015-10-03T13:31:11.887+0000,2015-11-10T08:02:42.886+0000,Fixed,Major
ORC-370,ORC column statistics should not use java.sql.Date,ORC,Bug,Closed,[],3,"[<JIRA IssueLink: id='12535388'>, <JIRA IssueLink: id='12597406'>, <JIRA IssueLink: id='12597407'>]","ORC PPD evaluation for Date type uses java.sql.Date for min/max comparison causing incorrect results.
Date.compareTo uses millis offset which can return incorrect results depending on the timezone. 
Running the testcase in HIVE-19726 passed in Los Angeles but failed in Paris as Date.compareTo return 0 for Los Angeles but returned -1 for Paris. 

Similar to bloomfilter, min/max evaluation should use DateWritable (as DateWritable.compareTo uses days offset). ",2018-05-30T19:24:40.787+0000,2021-01-12T03:16:18.466+0000,Duplicate,Blocker
HCATALOG-443,"Serde-reported schema support, enums as strings, misc fixes",HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12354116'>],"This issue is related to HIVE-2950.

When HCatalog queries the HiveMetaStore it gets back classes in the ""org.apache.hadoop.hive.metastore.api"" package. This represents exactly what is stored in the metastore database.

Hive has companion classes in ""org.apache.hadoop.hive.ql.metadata"" that provide some logic on top of what's stored in the actual database. For example:

* org.apache.hadoop.hive.metastore.api.Table.getCols shows columns explicitly stored in the database
* org.apache.hadoop.hive.ql.metadata.Table.getCols shows columns reported by the serde if there are any.

Except when serializing stuff into the job configuration HCatalog should use the ""metadata"" version of these classes so that the additional logic is called.",2012-07-01T00:17:42.920+0000,2012-09-05T00:28:14.271+0000,Won't Fix,Major
ACCUMULO-2714,Integration test classpath issues with Guava,ACCUMULO,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12396099'>, <JIRA IssueLink: id='12396536'>]","During fixing ACCUMULO-2665, I hadn't noticed any failures inside of the integration tests in the accumulo-test module.

In testing 1.6.1-RC3, I just had one test fail. I guess the classpath must not be fixed every time (or there is a specific path in HDFS that requires that class that not all tests hit) which cause the same error.",2014-04-22T16:23:00.769+0000,2014-09-11T00:29:26.569+0000,Fixed,Minor
CALCITE-2675,Type validation error as ReduceExpressionRule fails to preserve type nullability,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12548517'>, <JIRA IssueLink: id='12548269'>]","If a simplification could happen after some [ReduceExpression rewrite|https://github.com/apache/calcite/blob/fcc8bf7f44f92efb3c9a1e1f51ffc1a09cab27b9/core/src/main/java/org/apache/calcite/rel/rules/ReduceExpressionsRule.java#L794]; the simplification result may have a slightly different type in nullability. 
{code}
  @Test public <T> void testReduceCaseNullabilityChange() throws Exception {
    HepProgram program = new HepProgramBuilder()
        .addRuleInstance(ReduceExpressionsRule.FILTER_INSTANCE)
        .addRuleInstance(ReduceExpressionsRule.PROJECT_INSTANCE)
        .build();

    try (Hook.Closeable a = Hook.REL_BUILDER_SIMPLIFY.add(Hook.propertyJ(false))) {
      checkPlanning(program,
          ""select case when empno = 1 then 1 when 1 IS NOT NULL then 2 else null end as qx ""
              + ""from emp"");
    }
{code}

Exposed by CALCITE-1413 changes; I'm not sure if there is any other variations for which the same could happen.",2018-11-15T16:18:36.181+0000,2018-12-23T00:12:26.024+0000,Fixed,Major
CALCITE-2179,General improvements for materialized view rewriting rule,CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12527730'>],"This issue is for extending {{AbstractMaterializedViewRule}} rule:
- Support for rolling up date nodes. For instance, rewrite in the following case:
{code}
Materialization:
select ""empid"", floor(cast('1997-01-20' as timestamp) to month), count(*) + 1 as c, sum(""empid"") as s
from ""emps"" group by ""empid"", floor(cast('1997-01-20' as timestamp) to month);
Query:
select floor(cast('1997-01-20' as timestamp) to year), sum(""empid"") as s
from ""emps"" group by floor(cast('1997-01-20' as timestamp) to year);
{code}
- Add flag to enable/disable fast bail out for joins. By default it is true, and thus, we were only creating the rewriting in the minimal subtree of plan operators. For instance:
{code}
View: (A JOIN B) JOIN C
Query: (((A JOIN B) JOIN D) JOIN C) JOIN E
{code}
We produce it at:
{code}
((A JOIN B) JOIN D) JOIN C
{code}
But not at:
{code}
(((A JOIN B) JOIN D) JOIN C) JOIN E
{code}
This is important when the rule is used with the Volcano planner together with other rules, e.g. join reordering, as it prevents that the search space grows unnecessarily. However, if we use the rewriting rule in isolation, fast bail out can lead to missing rewriting opportunities (e.g. for bushy join trees).
- Possibility to provide a HepProgram to optimize query branch in union rewritings. Note that when we produce a partial rewriting with a Union, the branch that will execute the (partial) query can be fully rewritten so we can add the compensation predicate. (We cannot do the same for views because the expression might not be computable if the needed subexpressions are not available in the view output). If we use Volcano with a determined set of rules, this might not be needed, hence providing this program is optional.
- Multiple small fixes discovered while testing.",2018-02-14T23:04:33.981+0000,2018-03-17T17:43:19.279+0000,Fixed,Major
SPARK-10555,Add INotifyDStream to Spark Streaming,SPARK,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12451465'>],"Currently, spark streaming has support for fileStreams, and while this is super useful in general, it has its limitations - such as only being able to process new files under each folder. 

There are certain use cases (such as monitoring a root folder for incoming data, and registering the files into HIVE & performing file level replication across HDFS clusters) where taking actions based on multi level nested uploads is useful. 

We have a POC version of INotifyDStream that we are currently using in Staging environment at Uber. Would love to contribute that back, if it makes sense for Spark. ",2015-09-10T23:22:53.032+0000,2019-05-21T04:37:36.678+0000,Incomplete,Major
KYLIN-916,HiveColumnCardinalityJob fail to run,KYLIN,Bug,Closed,[],1,[<JIRA IssueLink: id='12432562'>],"
{code}
usage: HiveColumnCardinalityJob
 -output <path>        Output path
 -table <table name>   The hive table name
[pool-7-thread-1]:[2015-07-29 05:34:56,026][ERROR][org.apache.kylin.job.common.HadoopShellExecutable.doWork(HadoopShellExecutable.java:65)] - error execute HadoopShellExecutable{id=4a63f5c4-3cad-401c-b988-4da913542d52-00, name=null, state=RUNNING}
java.io.IOException: java.lang.NullPointerException
	at org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:97)
	at org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:51)
	at org.apache.kylin.job.hadoop.cardinality.HiveColumnCardinalityJob.run(HiveColumnCardinalityJob.java:81)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
	at org.apache.kylin.job.common.HadoopShellExecutable.doWork(HadoopShellExecutable.java:63)
	at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:106)
	at org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:50)
	at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:106)
	at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:134)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.<init>(FosterStorageHandler.java:59)
	at org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(HCatUtil.java:415)
	at org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(HCatUtil.java:378)
	at org.apache.hive.hcatalog.mapreduce.InitializeInput.extractPartInfo(InitializeInput.java:158)
	at org.apache.hive.hcatalog.mapreduce.InitializeInput.getInputJobInfo(InitializeInput.java:137)
	at org.apache.hive.hcatalog.mapreduce.InitializeInput.setInput(InitializeInput.java:86)
	at org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:95)
	... 12 more
[pool-7-thread-1]:[2015-07-29 05:34:56,038][DEBUG][org.apache.kylin.co

{code}",2015-07-29T12:37:48.004+0000,2015-09-29T06:44:38.062+0000,Won't Fix,Major
INFRA-15685,Raise UserTasksMax in systemd,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12526404'>],systemd limits the maximum number of processes (and native Java threads) a user may consume on a box via UserTasksMax.  This should probably be raised in the case of Jenkins build hosts.,2017-12-18T18:53:01.357+0000,2018-08-25T06:54:40.646+0000,Fixed,Critical
SQOOP-821,Hadoop has changed logic for job id in LocalJobRunner that breaks Lob* tests,SQOOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12362883'>],"I've noticed that MAPREDUCE-4278 got committed. This JIRA changed the logic of generating job id in LocalJobRunner from job_local_000 to job_local[randid]_000. As we're directly depending on the job ID name in one of our third party tests, we need to address this change.",2013-01-09T09:33:26.125+0000,2013-01-09T20:44:01.372+0000,Fixed,Blocker
SPARK-15348,Hive ACID,SPARK,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12527494'>, <JIRA IssueLink: id='12527495'>]","Spark does not support any feature of hive's transnational tables,
you cannot use spark to delete/update a table and it also has problems reading the aggregated data when no compaction was done.
Also it seems that compaction is not supported - alter table ... partition .... COMPACT 'major'",2016-05-16T14:57:48.329+0000,2021-05-25T01:54:21.927+0000,Incomplete,Major
ORC-375,v1.5.1 install from source fails under GCC 7.3.0,ORC,Bug,Closed,[],3,"[<JIRA IssueLink: id='12536028'>, <JIRA IssueLink: id='12543346'>, <JIRA IssueLink: id='12535918'>]","Build fails when trying to build a dependent package:

orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/common/async_stream.h:39:16: error: ‘std::function’ has not been declared

    std::function<void (const asio::error_code & error,

 ",2018-06-05T21:29:41.686+0000,2018-09-25T21:21:48.399+0000,Fixed,Major
OOZIE-3643,Oozie should pass mapreduce.job.tags to Beeline in Hive2 action,OOZIE,Task,Open,[],1,[<JIRA IssueLink: id='12627744'>],"Currently if we start a Hive2 action which starts a Tez child job and that runs for long and then we try to kill the Oozie workflow which started that Hive2 action we experience that the started Tez child job remains running.

To solve this problem Hive on Tez configurations should be set: *hive.server2.tez.initialize.default.sessions=false* and Oozie should pass the launcher job's tag to beeline to be able to find the Tez child job. In Oozie we could do something like:
{code:java}
arguments.add(""--hiveconf"");
arguments.add(""hive.query.tag="" + actionConf.get(MAPREDUCE_JOB_TAGS)); {code}
in Hive2Main class.

As I experienced in my test environment when I did killed such a workflow it killed the Tez child job too but unfortunately a new instance of my workflow wasn't able to start the Hive query, the launcher job stuck.",2021-12-02T09:54:26.430+0000,2021-12-02T09:56:18.243+0000,,Major
AMBARI-3049,Define spnego configs in yarn-site.xml for secure cluster,AMBARI,Task,Resolved,[],1,[<JIRA IssueLink: id='12374459'>],,2013-08-28T18:37:33.093+0000,2013-08-29T17:17:54.613+0000,Fixed,Major
TEZ-3837,Parallel sorting with inline sampling,TEZ,New Feature,Patch Available,"[<JIRA Issue: key='TEZ-3838', id='13103073'>]",1,[<JIRA IssueLink: id='12520102'>],,2017-09-18T20:44:05.646+0000,2017-11-15T00:03:04.759+0000,,Major
PHOENIX-1433,Add means of updating server config dynamically,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12401206'>],"Primarily for testing, but in some other situations as well (such as to disable tracing), we should provide a means of updating the config of a region server dynamically through a client action. Perhaps an endpoint coprocessor could be used to pass through key/value map where the keys are property names and the values are the property values.",2014-11-10T23:43:52.889+0000,2014-12-05T03:19:41.487+0000,,Major
CASSANDRA-4208,ColumnFamilyOutputFormat should support writing to multiple column families,CASSANDRA,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12351374'>],"It is not currently possible to output records to more than one column family in a single reducer.  Considering that writing values to Cassandra often involves multiple column families (i.e. updating your index when you insert a new value), this seems overly restrictive.  I am submitting a patch that moves the specification of column family from the job configuration to the write() call in ColumnFamilyRecordWriter.",2012-05-01T16:58:34.353+0000,2019-04-16T09:32:34.767+0000,Fixed,Normal
KNOX-1530,Improve Gzip Compression Handling Performance,KNOX,Improvement,Closed,"[<JIRA Issue: key='KNOX-1534', id='13192885'>, <JIRA Issue: key='KNOX-1532', id='13192878'>, <JIRA Issue: key='KNOX-1533', id='13192879'>, <JIRA Issue: key='KNOX-1535', id='13192904'>, <JIRA Issue: key='KNOX-1531', id='13192877'>]",11,"[<JIRA IssueLink: id='12546140'>, <JIRA IssueLink: id='12546139'>, <JIRA IssueLink: id='12546144'>, <JIRA IssueLink: id='12546151'>, <JIRA IssueLink: id='12546142'>, <JIRA IssueLink: id='12546150'>, <JIRA IssueLink: id='12546146'>, <JIRA IssueLink: id='12546149'>, <JIRA IssueLink: id='12546145'>, <JIRA IssueLink: id='12546143'>, <JIRA IssueLink: id='12546229'>]","While looking at KNOX-1524, I found that requesting compressed results can cause performance impacts. Knox currently does the following:
 * Apache HttpClient transparently decompresses each request
 ** [Apache HttpClient 4.1 added support for this|https://stackoverflow.com/questions/2777076/does-apache-commons-httpclient-support-gzip] - HTTPCLIENT-834

This lead to recompressing some streams (-KNOX-732,- -KNOX-855-, KNOX-856) based on MimeTypes. Even if we disableContentCompression, KNOX-565 added the following which should only come into play with the above HttpClient transparent decompression disabled (or multipart Gzip files - KNOX-1518):
 * Try to decompress the stream
 ** Currently uses try/catch
 * Run any rewrite filter rules
 * If decompressed, recompress the stream

For many use cases, there is no reason to decompress and recompress the same stream. This is because there are no rewrite rules that apply. One example of this is Hive where beeline requests compression and HiveServer2 added support for returning compressed results with HIVE-17194. Another is with WebHDFS where we don't want to change the content going back to the client.

I am planning to address this in a few pieces:
 * Determine if any rewrite rules apply before decompressing
 ** If rewrite rules apply, then decompress and recompress as before
 ** If rewrite rules do not apply, then copy stream as is
 * Remove gzip filter added by KNOX-732
 ** Figure out if there is another code path where decompress/recompress should happen
 ** We should not have to rely on Jetty to recompress content
 * Disable httpclient content compression
 ** Need to make sure we handle decompress/recompress where necessary

With all 3 improvements in place we should end up with:
 * One place where gzip decompress/recompress happens
 * Only decompress/recompress if rewrite rules match
 * Performance increases due to skipping unnecessary decompress/recompress",2018-10-19T15:22:15.740+0000,2019-03-28T13:57:05.690+0000,Fixed,Critical
SENTRY-1707,Add unit test to check response from HiveMetaStoreClient.getNextNotification when there is no new notification at Hive meta store,SENTRY,Test,Resolved,[],1,[<JIRA IssueLink: id='12501158'>],"Hive meta store returns null response when there is no new notification. Thrift throws exception with this null response, and calling HiveMetaStoreClient.getNextNotification gets this exception. 

The desired behavior is that Hive meta store returns normal response with set event. And the events set is empty. 

This bug is tracked in https://issues.apache.org/jira/browse/HIVE-15761

We need to add a unit test to expose this bug in Hive meta store. And Hive team can easily verify the fix with this unit test.",2017-04-19T03:55:06.019+0000,2017-07-24T14:44:42.713+0000,Won't Fix,Major
INFRA-20025,Jenkins report archival fails without explanation,INFRA,Planned Work,Closed,[],2,"[<JIRA IssueLink: id='12583938'>, <JIRA IssueLink: id='12620625'>]","We saw the htmlPublisher fail on [0]. The only explanation I can find is provided by the Blue Ocean view [1], which says ""script returned exit code 1"". Is it possible for someone to investigate the logs on worker ""H2"", seeking an explanation? the failure occurred at 2020-03-25T01:00:11.470Z. Thanks.

[0]: https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2565
[1]: https://builds.apache.org/blue/organizations/jenkins/HBase%20Nightly/detail/branch-2/2565/pipeline",2020-03-25T17:07:56.137+0000,2021-10-08T21:00:12.800+0000,Duplicate,Minor
HCATALOG-609,HCatalog's AddPartitionMessage must specify all partitions from Hive's AddPartitionEvent.,HCATALOG,Bug,Open,[],1,[<JIRA IssueLink: id='12368579'>],"This has to do with HIVE-3938. Hive-metastore currently sends 1 AddPartitionEvent per partition specified to add_partitions(). It does this in spite of adding all partitions atomically.

Once Hive's AddPartitionEvent supports multiple partitions, HCatalog must take that into account and send a single event for all partitions in an atomically added partition-set.",2013-01-25T00:18:53.937+0000,2013-05-06T02:06:22.660+0000,,Major
PARQUET-26,Parquet doesn't recognize the nested Array type in MAP as ArrayWritable.,PARQUET,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12401961'>, <JIRA IssueLink: id='12401959'>]","When trying to insert hive data of type of MAP<string, array<int>> into Parquet, it throws the following error 

Caused by: parquet.io.ParquetEncodingException: This should be an ArrayWritable or MapWritable: org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable@c644ef1c 
at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeData(DataWritableWriter.java:86) 

Problem is reproducible with following steps:
Relevant test data is attached.

1. 
CREATE TABLE test_hive (
node string,
stime string,
stimeutc string,
swver string,
moid MAP <string,string>,
pdfs MAP <string,array<int>>,
utcdate string,
motype string)
ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '|'
    COLLECTION ITEMS TERMINATED BY ','
    MAP KEYS TERMINATED BY '=';


2.
LOAD DATA LOCAL INPATH '/root/38388/test.dat' INTO TABLE test_hive; 

3.

CREATE TABLE test_parquet(
pdfs MAP <string,array<int>>
)
STORED AS PARQUET ;

4.

INSERT INTO TABLE test_parquet SELECT pdfs FROM test_hive;",2014-07-23T16:58:24.385+0000,2015-03-10T22:26:46.838+0000,Fixed,Major
IMPALA-8836,Support COMPUTE STATS on insert only ACID tables,IMPALA,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12604597'>, <JIRA IssueLink: id='12567834'>]",,2019-08-06T15:37:25.543+0000,2020-12-10T08:22:36.667+0000,Done,Critical
SPARK-20327,"Add CLI support for YARN custom resources, like GPUs",SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12526568'>, <JIRA IssueLink: id='12500729'>]","YARN-3926 adds the ability for administrators to configure custom resources, like GPUs.  This JIRA is to add support to Spark for requesting resources other than CPU virtual cores and memory.  See YARN-3926.",2017-04-13T16:41:14.216+0000,2018-10-17T15:42:35.837+0000,Fixed,Major
CALCITE-3923,Refactor how planner rules are parameterized,CALCITE,Bug,Closed,[],9,"[<JIRA IssueLink: id='12593098'>, <JIRA IssueLink: id='12598302'>, <JIRA IssueLink: id='12597277'>, <JIRA IssueLink: id='12622373'>, <JIRA IssueLink: id='12592859'>, <JIRA IssueLink: id='12593927'>, <JIRA IssueLink: id='12590137'>, <JIRA IssueLink: id='12592727'>, <JIRA IssueLink: id='12608326'>]","People often want different variants of planner rules. An example is {{FilterJoinRule}}, which has a 'boolean smart’ parameter, a predicate (which returns whether to pull up filter conditions), operands (which determine the precise sub-classes of {{RelNode}} that the rule should match) and a {{RelBuilderFactory}} (which controls the type of {{RelNode}} created by this rule).

Suppose you have an instance of {{FilterJoinRule}} and you want to change {{smart}} from true to false. The {{smart}} parameter is immutable (good!) but you can’t easily create a clone of the rule because you don’t know the values of the other parameters. Your instance might even be (unbeknownst to you) a sub-class with extra parameters and a private constructor.

So, my proposal is to put all of the config information of a {{RelOptRule}} into a single {{config}} parameter that contains all relevant properties. Each sub-class of {{RelOptRule}} would have one constructor with just a ‘config’ parameter. Each config knows which sub-class of {{RelOptRule}} to create. Therefore it is easy to copy a config, change one or more properties, and create a new rule instance.

Adding a property to a rule’s config does not require us to add or deprecate any constructors.

The operands are part of the config, so if you have a rule that matches a {{EnumerableFilter}} on an {{EnumerableJoin}} and you want to make it match an {{EnumerableFilter}} on an {{EnumerableNestedLoopJoin}}, you can easily create one with one changed operand.

The config is immutable and self-describing, so we can use it to automatically generate a unique description for each rule instance.

(See the email thread [[DISCUSS] Refactor how planner rules are parameterized|https://lists.apache.org/thread.html/rfdf6f9b7821988bdd92b0377e3d293443a6376f4773c4c658c891cf9%40%3Cdev.calcite.apache.org%3E].)",2020-04-14T21:01:27.873+0000,2021-09-03T00:52:45.537+0000,Fixed,Major
SENTRY-2313,alter database set owner command can be executed only by user with proper privilege,SENTRY,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12538752'>, <JIRA IssueLink: id='12541244'>, <JIRA IssueLink: id='12540686'>]","Need to set sentry privilege mapping to make sure ""ALTER DATABASE SET OWNER"" can be done only by user with ""all with grant option"" at parent obj

",2018-07-16T19:05:41.270+0000,2020-01-02T17:07:55.389+0000,Fixed,Major
TEZ-3191,NM container diagnostics for excess resource usage can be lost if task fails while being killed,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12462303'>],"This is the Tez version of MAPREDUCE-4955.  I saw a misconfigured Tez job report a task attempt as failed due to a filesystem closed error because the NM killed the container due to excess memory usage.  Unfortunately the SIGTERM sent by the NM caused the filesystem shutdown hook to close the filesystems, and that triggered a failure in the main thread.  If the failure is reported to the AM via the umbilical before the NM container status is received via the RM then the useful container diagnostics from the NM are lost in the job history.",2016-03-30T16:43:11.655+0000,2016-03-30T16:44:27.665+0000,,Major
PHOENIX-2477,ClassCastException in IndexedWALEditCodec after HBASE-14501 (possible dataloss),PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12450617'>, <JIRA IssueLink: id='12450628'>]","HBASE-14501 fixed the semantics in using the InputStream.available() and the interface between the actual Decoder and BaseDecoder. 


Running Phoenix with IndexedWALEditCodec on top of an HBase version containing HBASE-14501 now causes silent data loss since the Decoder throws 
{code}
java.lang.ClassCastException: org.apache.hadoop.hbase.codec.BaseDecoder$PBIS cannot be cast to java.io.DataInput
{code}
which gets silently ignored from ProtobufLogReader. ",2015-12-02T00:36:33.619+0000,2015-12-05T18:36:09.069+0000,Fixed,Critical
INFRA-19963,"Builds failing running apt-get install with ""Failed to fetch ... 404  Not Found [IP: 91.189.88.174 80]""",INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12583011'>],"Builds making use of docker, where cached layers are not available, are failing due to 404 communicating with an Ubuntu repository server. Building the docker image locally (with {{docker build . --no-cache}}) succeeds.",2020-03-12T21:42:44.817+0000,2020-03-13T22:23:23.660+0000,Invalid,Major
INFRA-18721,hbase-connectors builds fail because there is no JDK,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12565032'>],"Github PR checks fail, for example this one:
https://github.com/apache/hbase-connectors/pull/34

According to the logs, maven fails because:

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project hbase-kafka-model: Compilation failure
[ERROR] No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK?
{code}",2019-07-09T08:53:44.045+0000,2019-07-10T13:33:42.778+0000,Information Provided,Major
CALCITE-5158,count(1) with subquery count(distinct) gives wrong results with hive.optimize.distinct.rewrite=true and cbo on,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12640221'>],"{code:java}
create table count_distinct(a int, b int);
insert into table count_distinct values (1,2),(2,3);
set hive.execution.engine=tez;
set hive.cbo.enable=true;
set hive.optimize.distinct.rewrite=true;
select count(1) from ( 
      select count(distinct a) from count_distinct
) tmp; {code}
it give wrong result when hive.optimize.distinct.rewrite is true, By default, it's true for all 3.x versions. The test result is 2, and the expected result is 1.

Before CBO optimization，RelNode tree as this，
{code:java}
HiveProject(_o__c0=[$0])
  HiveAggregate(group=[{}], agg#0=[count($0)])
    HiveProject($f0=[1])
      HiveProject(_o__c0=[$0])
        HiveAggregate(group=[{}], agg#0=[count(DISTINCT $0)])
          HiveProject($f0=[$0])
            HiveTableScan(table=[[default.count_distinct]], table:alias=[count_distinct]) {code}
Optimized by HiveExpandDistinctAggregatesRule, RelNode tree as this，
{code:java}
HiveProject(_o__c0=[$0])
  HiveAggregate(group=[{}], agg#0=[count($0)])
    HiveProject($f0=[1])
      HiveProject(_o__c0=[$0])
        HiveAggregate(group=[{}], agg#0=[count($0)])
          HiveAggregate(group=[{0}])
            HiveProject($f0=[$0])
              HiveProject($f0=[$0])
                HiveTableScan(table=[[default.count_distinct]], table:alias=[count_distinct]) {code}
count(distinct xx) converte to count (xx) from (select xx from table_name group by xx) 

Optimized by Projection Pruning, RelNode tree as this, 
{code:java}
HiveAggregate(group=[{}], agg#0=[count()])
  HiveProject(DUMMY=[0])
    HiveAggregate(group=[{}])
      HiveAggregate(group=[{0}])
        HiveProject(a=[$0])
          HiveTableScan(table=[[default.count_distinct]], table:alias=[count_distinct]) {code}
In this case, an error occurs in the execution plan.",2022-05-18T02:52:49.205+0000,2022-09-10T08:42:45.727+0000,Invalid,Major
IMPALA-3976,Handle partition-key values with multiple synonymous string representations created in Hive.,IMPALA,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12596724'>, <JIRA IssueLink: id='12596725'>]","For several SQL statements that can create new partitions, Hive seems to generate partition-key values and the corresponding HDFS directory based on the user's string input rather than the corresponding literal value of the appropriate column type. This leads to a situation where a single logical partition-key value can map to multiple HDFS directories and Hive partitions.

Example in Hive:
{code}
CREATE TABLE t (i INT) PARTITIONED BY (p INT);
ALTER TABLE t ADD PARTITION (p=0);
ALTER TABLE t ADD PARTITION (p=00);
ALTER TABLE t ADD PARTITION (p=000);
SHOW PARTITIONS t;
p=0
p=00
p=000
{code}

The above statements will result in three different HDFS directories, one for each of the ""distinct"" partitions.

The same result can be achieved with static partition inserts from Hive, instead of ALTER TABLE ADD PARTITION.

Note that Impala will a canonical representation for any partition-key value based on the underlying LiteralExpr, so a similarly strange metadata state cannot be created from Impala, even if given the same input as in the example above.

A special case of this issue was reported in HIVE-6590 and IMPALA-3963, but the underlying problem is more general.

*Issues in Impala*
Impala has difficulties dealing with such ambiguous partitions due to the internal assumption that a single assignment of values to partition keys maps to a single Hive partition with a one corresponding HDFS directory.

As long as the cached partition metadata in Impala is correct, queries will return correct results even with partition filters. Impala effectively coalesces the different partition variants, for example, SELECT * FROM t WHERE p=0 will scan all three directories from the example above.

The following statements are known have problems in Impala if such ambiguous partitions exist:
* REFRESH <table> and REFRESH <partition>. After such a statement Impala may duplicate and/or missing partitions, leading to incorrect query results.
* ALTER TABLE RECOVER PARTITIONS, same as REFRESH above.
* ALTER TABLE <table> DROP PARTITIONS. Impala will only be able to drop the one partition with the the canonical value representation. Other variants of the same partition cannot be dropped.
* Any other ALTER TABLE ... PARTITION(). Impala will only modify the one partition with the canonical value representation (if any).
* It is safest to assume that all other metadata statements that operate on a single partition are likewise not functioning as intended.

*Workarounds*
* Ensure that partitions created via Hive do not exhibit ambiguity. Stick to a single partition-key value representation, e.g., use p=0 consistently and avoid variants like p=000.
* Avoid those statements in Hive that can create the bad metadata. Always use fully dynamic partition inserts and avoid adding partitions via static partition inserts or ALTER TABLE.
* Running INVALIDATE METADATA <table> will bring Impala's metadata back into a consistent state (including all partition variants). Queries will return correct results, but some DDL operations may still not fully work (like DROP PARTITION).",2016-08-15T03:54:35.000+0000,2020-08-13T21:40:29.315+0000,Won't Fix,Major
GIRAPH-346,Top Level POM,GIRAPH,Improvement,Resolved,[],5,"[<JIRA IssueLink: id='12358212'>, <JIRA IssueLink: id='12359233'>, <JIRA IssueLink: id='12359057'>, <JIRA IssueLink: id='12358893'>, <JIRA IssueLink: id='12358259'>]","Most Maven projects that have multiple modules (as we do with main / formats) have a top level pom.xml. This allows you avoid redundancy and also to build everything from top level all at once. I'm using a lot of the formats stuff, especially playing with hive / hcatalog and it's a pain having to constantly compile everything twice.

So I propose we move all the main code under a subdir, say giraph/ or main/. Then add a pom.xml that both the giraph/ and the formats/ subdirs can point to.

Note we will still build completely separate jars. If people want to just build giraph-main without formats they can do so directly from the subdirectory.

Let me know if you guys +1 the idea and I'll whip it up.",2012-09-26T04:37:48.111+0000,2012-10-20T17:24:23.776+0000,Fixed,Minor
SENTRY-1895,Sentry should handle the case of multiple notifications with the same ID,SENTRY,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12512637'>, <JIRA IssueLink: id='12512728'>, <JIRA IssueLink: id='12512638'>, <JIRA IssueLink: id='12512640'>]","As shown in HIVE-16886, notification IDs generated by Hive may be non-unique and there may be cases with different evnts sharing the same ID. This creates various problems for Sentry/Hive interaction and we should fine some short -term solution until it is fixed in Hive.

The issue was addressed in SENTRY-1803 by removing a primary-key constraint on the notification Id which allows for multiple keys. But this creates other problems:

1. We are using the primary key constraint to prevent multiple instances of Sentry from processing the same notifications multiple times.
2. We are using max(notificationId) to find the last processed event. When the field is a primary key, this operation is an index scan, but when it isn't, it is a full table scan which is more expensive.

We also have a few other problems caused by duplicate IDs which are not related and not addressed by SENTRY-1803:

1. There is a  synchronization mechanism between HMS and Sentry which ensures that a given event is processed. This doesn't work in the presence of duplicate IDs.
2. Some events may be missed due to the way they are processed.",2017-08-22T15:05:31.069+0000,2017-08-29T16:15:22.750+0000,Fixed,Major
TEZ-1041,Use VertexLocationHint consistently everywhere in the API,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12393323'>],VertexLocationHint is used internally and not by end users.,2014-04-10T21:49:06.868+0000,2014-09-06T01:35:45.410+0000,Fixed,Blocker
TEZ-1382,Change ObjectRegistry API to allow for future extensions,TEZ,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12393536'>, <JIRA IssueLink: id='12393602'>]","Per comments on https://issues.apache.org/jira/browse/TEZ-1153
",2014-08-06T02:45:13.003+0000,2014-09-06T01:35:27.095+0000,Fixed,Blocker
REEF-58,Add support for YARN SCM,REEF,Improvement,Open,[],1,[<JIRA IssueLink: id='12402628'>],"YARN-1492 is being worked on to introduce SCM, which speeds up job launch time by caching shared jars.

We can incorporate SCM Client logic into reef-runtime-yarn, speeding up REEF job launch time for those who use SCM.",2014-12-03T05:29:25.616+0000,2014-12-04T01:42:06.198+0000,,Minor
THRIFT-591,Make the C++ runtime library be compatible with Windows and Visual Studio,THRIFT,New Feature,Closed,[],4,"[<JIRA IssueLink: id='12355312'>, <JIRA IssueLink: id='12339198'>, <JIRA IssueLink: id='12339196'>, <JIRA IssueLink: id='12359537'>]","Modify the C++ runtime library to be compatible with Windows and able to be built by Visual Studio.

The work has been done and a patch is available. I will attach it soon.

Note that this issue and the attached patch supercedes the patches that I wrongly attached to JIRA 311. That issue is about making the C++ library support async client/server interaction.",2009-09-24T16:42:37.904+0000,2013-06-08T03:16:29.728+0000,Fixed,Major
TEZ-1928,Tez local mode hang in Pig tez local mode,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12405007'>],"Pig tez local mode tests hang under some scenario. I attached several stack trace of hanging tests.

By setting ""tez.am.inline.task.execution.max-tasks"", the test does not hang. However, we cannot make it general since Pig backend code is not designed to be multithread-safe. ",2015-01-08T06:20:52.515+0000,2015-06-30T04:52:47.133+0000,Fixed,Major
IMPALA-5766,Add date/timestamp format options to external tables,IMPALA,New Feature,Open,[],2,"[<JIRA IssueLink: id='12511270'>, <JIRA IssueLink: id='12516451'>]","Could we support more than one date separator while automatically parsing string timestamp columns. If possible, could we also enhance to specify a common timestamp format for a table?
In detail:
------------
  External table:
  CREATE EXTERNAL TABLE videowatchactivity(
    id int,
    user string,
    activitydttm timestamp,
    videoid string,
    activity string) 
      ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ',' 
      LOCATION '/user/videologger/activity';

If  HDFS file: ""/user/videologger/activity/activity_aug_4_2017.csv"" contains --> 
Venu,2017-08-04 11:23:00,video_id_0,start
Venu,2017-08-04 11:25:00,video_id_0,stop
Then,
""select activitydttm from videowatchactivity;""  outputs 2 valid rows.

If HDFS file: /user/videologger/activity/activity_aug_4_2017.csv contains -->
Venu,2017/08/04 11:23:00,video_id_0,start
Venu,2017/08/04 11:25:00,video_id_0,stop
Then,
""select activitydttm from videowatchactivity;""  outputs 2 NULL rows.

Could there be a provision to specify the date separator in the create table statement that could be used in the logic to automatically parse strings to timestamp columns?
E.g.,
CREATE EXTERNAL TABLE videowatchactivity(
    id int,
    user string,
    activitydttm timestamp,
    videoid string,
    activity string) 
      ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ',' 
      LOCATION '/user/videologger/activity'
      DATESEPARATOR '/';
Please note the new ""DATESEPARATOR '/'"" that could help us define any separator for the date field. 

If possible to extend, we could also use a new DATEFMT property of the table to specify a format like==> MM-dd-YYYY HH:mm:ss instead of the default format too.

Thanks,


",2017-08-04T19:17:37.930+0000,2017-10-03T23:49:08.999+0000,,Minor
THRIFT-876,Add SASL support,THRIFT,New Feature,Closed,[],5,"[<JIRA IssueLink: id='12333690'>, <JIRA IssueLink: id='12333651'>, <JIRA IssueLink: id='12333694'>, <JIRA IssueLink: id='12563525'>, <JIRA IssueLink: id='12333649'>]","It'd be nice if there were some way of securing Thrift communication in a pluggable fashion. SASL is the implementation chosen by Hadoop for this. Seems like a good option for Thrift, too.

I'll start with a Java implementation, then move on to support the other language bindings.",2010-08-29T04:27:49.571+0000,2019-06-18T09:56:39.250+0000,Fixed,Major
RATIS-271,"Ratis-backed distributed log: ""LogService"" ",RATIS,New Feature,Open,"[<JIRA Issue: key='RATIS-272', id='13174904'>, <JIRA Issue: key='RATIS-273', id='13174905'>, <JIRA Issue: key='RATIS-274', id='13174906'>, <JIRA Issue: key='RATIS-275', id='13174907'>, <JIRA Issue: key='RATIS-276', id='13174910'>, <JIRA Issue: key='RATIS-277', id='13174912'>, <JIRA Issue: key='RATIS-278', id='13174914'>, <JIRA Issue: key='RATIS-279', id='13174915'>, <JIRA Issue: key='RATIS-280', id='13174916'>, <JIRA Issue: key='RATIS-282', id='13174963'>, <JIRA Issue: key='RATIS-317', id='13184777'>, <JIRA Issue: key='RATIS-320', id='13185179'>, <JIRA Issue: key='RATIS-334', id='13187721'>, <JIRA Issue: key='RATIS-342', id='13189834'>, <JIRA Issue: key='RATIS-369', id='13193700'>, <JIRA Issue: key='RATIS-374', id='13194149'>, <JIRA Issue: key='RATIS-375', id='13194263'>, <JIRA Issue: key='RATIS-376', id='13194265'>, <JIRA Issue: key='RATIS-385', id='13195503'>, <JIRA Issue: key='RATIS-387', id='13195680'>, <JIRA Issue: key='RATIS-400', id='13197132'>, <JIRA Issue: key='RATIS-422', id='13198744'>, <JIRA Issue: key='RATIS-520', id='13227069'>]",2,"[<JIRA IssueLink: id='12539724'>, <JIRA IssueLink: id='12539712'>]","Umbrella issue for building a distributed log using Ratis:

Doc: [https://docs.google.com/document/d/1Su5py_T5Ytfh9RoTTX2s20KbSJwBHVxbO7ge5ORqbCk/edit#|https://docs.google.com/document/d/1Su5py_T5Ytfh9RoTTX2s20KbSJwBHVxbO7ge5ORqbCk/edit]

Discuss: https://lists.apache.org/thread.html/f80dc3900f6d9f4ee4d9f9e0898cee9a232e3b1ca9a4d9a53fea1d71@%3Cdev.ratis.apache.org%3E",2018-07-26T16:13:41.182+0000,2018-10-31T21:42:30.311+0000,,Major
SLIDER-77,use a window for tracking container failures,SLIDER,Sub-task,Resolved,[],3,"[<JIRA IssueLink: id='12388385'>, <JIRA IssueLink: id='12393904'>, <JIRA IssueLink: id='12393905'>]","Use sliding windows and/or weighted moving averages to track container failures over time, and only react if many are failing in a short period.

What we do want to do here is react fast to a sudden series of failures, as well as look at average failure rates over time. I think separating startup failures from operational failures could help here. We don't want 5 failures in 5 minutes to be ignored just because everything worked well for the previous month",2014-05-20T11:11:35.253+0000,2014-08-13T17:18:58.604+0000,Fixed,Minor
KUDU-2641,TestHiveMetastoreIntegration fails with JDK9+,KUDU,Bug,Resolved,[],1,[<JIRA IssueLink: id='12559644'>],"This test consistently fails for me on Ubuntu 18, where my Java version is openjdk 10.0.2. I suspect it's a known issue; HIVE-17632 suggests that Hive doesn't yet build with JDK9.

Anyway, here's the full test output:
{noformat}
16:11:19.169 [INFO - Test worker] (RandomUtils.java:46) Using random seed: 1545005479168
16:11:19.216 [INFO - Test worker] (KuduTestHarness.java:137) Creating a new MiniKuduCluster...
16:11:19.260 [INFO - Test worker] (KuduBinaryLocator.java:52) Using Kudu binary directory specified by system property 'kuduBinDir': /home/adar/Source/kudu/java/../build/latest/bin
16:11:19.279 [INFO - Test worker] (MiniKuduCluster.java:197) Starting process: [/home/adar/Source/kudu/java/../build/latest/bin/kudu, test, mini_cluster, --serialization=pb]
16:11:19.812 [DEBUG - Test worker] (MiniKuduCluster.java:165) Request: create_cluster {
  num_masters: 3
  num_tservers: 3
  enable_kerberos: false
  cluster_root: ""/tmp/mini-kudu-cluster2170731076069178574""
  hms_mode: ENABLE_METASTORE_INTEGRATION
  mini_kdc_options {
  }
}

16:11:19.829 [DEBUG - Test worker] (MiniKuduCluster.java:175) Response: 
16:11:19.852 [DEBUG - Test worker] (MiniKuduCluster.java:165) Request: start_cluster {
}

16:11:23.240 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 2018-12-16 16:11:23: Starting Hive Metastore Server
16:11:23.499 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack=true -Dderby.locks.deadlockTimeout=1
16:11:25.004 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) SLF4J: Class path contains multiple SLF4J bindings.
16:11:25.005 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) SLF4J: Found binding in [jar:file:/home/adar/Source/kudu/thirdparty/src/hive-498021fa15186aee8b282d3c032fbd2cede6bec4/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16:11:25.007 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) SLF4J: Found binding in [jar:file:/home/adar/Source/kudu/thirdparty/src/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16:11:25.008 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
16:11:25.018 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
16:11:31.428 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Starting hive metastore on port 0
16:11:35.091 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: An illegal reflective access operation has occurred
16:11:35.092 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Illegal reflective access by com.google.common.base.internal.Finalizer (file:/home/adar/Source/kudu/thirdparty/src/hive-498021fa15186aee8b282d3c032fbd2cede6bec4/lib/guava-14.0.1.jar) to field java.lang.Thread.inheritableThreadLocals
16:11:35.093 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Please consider reporting this to the maintainers of com.google.common.base.internal.Finalizer
16:11:35.093 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
16:11:35.100 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: All illegal access operations will be denied in a future release
16:11:45.710 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.709558 17544 external_mini_cluster.cc:845] Running /home/adar/Source/kudu/build/debug/bin/kudu-master
16:11:45.710 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) /home/adar/Source/kudu/build/debug/bin/kudu-master
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/master-0/wal
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/master-0/data
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --block_manager=log
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=localhost
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_ca_key_size=1024
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tsk_num_rsa_bits=512
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.62:40137
16:11:45.711 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.62
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:45.712 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/master-0/logs
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/master-0/data/info.pb
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags
16:11:45.713 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags
16:11:45.714 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_reuseport=true
16:11:45.714 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --master_addresses=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:45.714 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --hive_metastore_uris=thrift://127.0.0.1:32857 with env {}
16:11:45.887 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Logging before InitGoogleLogging() is written to STDERR
16:11:45.895 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.887481 17858 flags.cc:406] Enabled unsafe flag: --rpc_server_allow_ephemeral_ports=true
16:11:45.896 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.896389 17858 flags.cc:406] Enabled unsafe flag: --never_fsync=true
16:11:45.897 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.897676 17858 flags.cc:406] Enabled experimental flag: --hive_metastore_uris=thrift://127.0.0.1:32857
16:11:45.898 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.898212 17858 flags.cc:406] Enabled experimental flag: --ipki_ca_key_size=1024
16:11:45.898 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.898836 17858 flags.cc:406] Enabled experimental flag: --ipki_server_key_size=1024
16:11:45.899 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.899631 17858 flags.cc:406] Enabled experimental flag: --tsk_num_rsa_bits=512
16:11:45.900 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:45.900113 17858 flags.cc:406] Enabled experimental flag: --rpc_reuseport=true
16:11:45.902 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.902629 17858 master_main.cc:73] Master server non-default flags:
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/master-0/data
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/master-0/wal
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --hive_metastore_uris=thrift://127.0.0.1:32857
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_ca_key_size=1024
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --master_addresses=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:45.903 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tsk_num_rsa_bits=512
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.62:40137
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_reuseport=true
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports=true
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/master-0/data/info.pb
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.62
16:11:45.904 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync=true
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --heap_profile_path=/tmp/kudu-master.17858
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags=true
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags=true
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/master-0/logs
16:11:45.905 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:45.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr=true
16:11:45.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Master server version:
16:11:45.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) kudu 1.9.0-SNAPSHOT
16:11:45.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) revision ff8971c6e6e74c6173c216dfb02788a46dbafe90
16:11:45.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) build type DEBUG
16:11:45.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) built by adar at 16 Dec 2018 16:09:24 PST on adar-Precision-5520
16:11:45.914 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.914093 17858 master_main.cc:80] Initializing master server...
16:11:45.918 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.918095 17858 system_ntp.cc:199] NTP initialized. Skew: 500ppm Current error: 523500us
16:11:45.923 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.923055 17858 fs_manager.cc:263] Metadata directory not provided
16:11:45.924 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.924187 17858 fs_manager.cc:269] Using write-ahead log directory (fs_wal_dir) as metadata directory
16:11:45.925 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.925401 17858 server_base.cc:432] Could not load existing FS layout: Not found: could not find a healthy instance file
16:11:45.925 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.925886 17858 server_base.cc:433] Attempting to create new FS layout instead
16:11:45.940 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.940524 17858 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/master-0/data/instance:
16:11:45.940 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""9ef99883945f4dc99d641de3e016b608""
16:11:45.941 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:45 on adar-Precision-5520""
16:11:45.942 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.942494 17858 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/master-0/wal/instance:
16:11:45.942 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""9ef99883945f4dc99d641de3e016b608""
16:11:45.943 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:45 on adar-Precision-5520""
16:11:45.979 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.976631 17858 fs_manager.cc:503] Time spent creating directory manager: real 0.018s	user 0.004s	sys 0.000s
16:11:45.979 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.977140 17858 env_posix.cc:1676] Not raising this process' open files per process limit of 4096; it is already as high as it can go
16:11:45.979 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.977332 17858 file_cache.cc:466] Constructed file cache lbm with capacity 1638
16:11:45.980 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.980329 17858 fs_manager.cc:419] Time spent opening block manager: real 0.002s	user 0.001s	sys 0.000s
16:11:45.980 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.980753 17858 fs_manager.cc:436] Opened local filesystem: /tmp/mini-kudu-cluster2170731076069178574/master-0/data,/tmp/mini-kudu-cluster2170731076069178574/master-0/wal
16:11:45.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""9ef99883945f4dc99d641de3e016b608""
16:11:45.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:45 on adar-Precision-5520""
16:11:45.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:45.981703 17858 fs_report.cc:352] FS layout report
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --------------------
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) wal directory: /tmp/mini-kudu-cluster2170731076069178574/master-0/wal
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) metadata directory: /tmp/mini-kudu-cluster2170731076069178574/master-0/wal
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 1 data directories: /tmp/mini-kudu-cluster2170731076069178574/master-0/data/data
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live blocks: 0
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes: 0
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes (after alignment): 0
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total number of LBM containers: 0 (0 full)
16:11:45.982 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for missing blocks
16:11:45.983 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for orphaned blocks
16:11:45.983 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM containers with extra space: 0 (0 repaired)
16:11:45.983 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM container extra space in bytes: 0 (0 repaired)
16:11:45.983 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total incomplete LBM containers: 0 (0 repaired)
16:11:45.983 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total LBM partial records: 0 (0 repaired)
16:11:46.035 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.035470 17858 env_posix.cc:1676] Not raising this process' running threads per effective uid limit of 127647; it is already as high as it can go
16:11:46.041 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.041275 17858 master_main.cc:83] Starting Master server...
16:11:46.044 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.044827 17858 rpc_server.cc:205] RPC server started. Bound to: 127.17.34.62:40137
16:11:46.046 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.046063 17858 webserver.cc:175] Starting webserver on 127.17.34.62:0
16:11:46.047 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.046996 17858 webserver.cc:186] Document root disabled
16:11:46.048 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.048323 17858 webserver.cc:313] Webserver started. Bound to: http://127.17.34.62:39567/
16:11:46.049 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.049417 17858 server_base.cc:607] Dumped server information to /tmp/mini-kudu-cluster2170731076069178574/master-0/data/info.pb
16:11:46.052 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.051201 17544 external_mini_cluster.cc:907] Started /home/adar/Source/kudu/build/debug/bin/kudu-master as pid 17858
16:11:46.052 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.051470 17544 external_mini_cluster.cc:845] Running /home/adar/Source/kudu/build/debug/bin/kudu-master
16:11:46.052 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) /home/adar/Source/kudu/build/debug/bin/kudu-master
16:11:46.052 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/master-1/wal
16:11:46.052 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/master-1/data
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --block_manager=log
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=localhost
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_ca_key_size=1024
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tsk_num_rsa_bits=512
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.61:33927
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.61
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync
16:11:46.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/master-1/logs
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/master-1/data/info.pb
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports
16:11:46.054 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags
16:11:46.055 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags
16:11:46.055 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_reuseport=true
16:11:46.058 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --master_addresses=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:46.058 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --hive_metastore_uris=thrift://127.0.0.1:32857 with env {}
16:11:46.095 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.095399 17925 data_dirs.cc:938] Could only allocate 1 dirs of requested 3 for tablet 00000000000000000000000000000000. 1 dirs total, 0 dirs full, 0 dirs failed
16:11:46.107 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.106858 17925 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.136 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.136013 17925 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.136 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.136615 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.61:33927: Network error: Client connection negotiation failed: client connection to 127.17.34.61:33927: connect: Connection refused (error 111)
16:11:46.155 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.155727 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } attempt: 1
16:11:46.156 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.156206 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.61:33927: Network error: Client connection negotiation failed: client connection to 127.17.34.61:33927: connect: Connection refused (error 111)
16:11:46.165 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Logging before InitGoogleLogging() is written to STDERR
16:11:46.179 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.164886 17919 flags.cc:406] Enabled unsafe flag: --rpc_server_allow_ephemeral_ports=true
16:11:46.180 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.180217 17919 flags.cc:406] Enabled unsafe flag: --never_fsync=true
16:11:46.181 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.181309 17919 flags.cc:406] Enabled experimental flag: --hive_metastore_uris=thrift://127.0.0.1:32857
16:11:46.181 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.181630 17919 flags.cc:406] Enabled experimental flag: --ipki_ca_key_size=1024
16:11:46.181 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.181826 17919 flags.cc:406] Enabled experimental flag: --ipki_server_key_size=1024
16:11:46.182 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.181998 17919 flags.cc:406] Enabled experimental flag: --tsk_num_rsa_bits=512
16:11:46.182 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.182193 17919 flags.cc:406] Enabled experimental flag: --rpc_reuseport=true
16:11:46.190 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.190290 17919 master_main.cc:73] Master server non-default flags:
16:11:46.190 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/master-1/data
16:11:46.190 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/master-1/wal
16:11:46.190 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --hive_metastore_uris=thrift://127.0.0.1:32857
16:11:46.191 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_ca_key_size=1024
16:11:46.191 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --master_addresses=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:46.191 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:46.191 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tsk_num_rsa_bits=512
16:11:46.191 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.61:33927
16:11:46.191 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_reuseport=true
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports=true
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/master-1/data/info.pb
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.61
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync=true
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --heap_profile_path=/tmp/kudu-master.17919
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags=true
16:11:46.192 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags=true
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/master-1/logs
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr=true
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Master server version:
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) kudu 1.9.0-SNAPSHOT
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) revision ff8971c6e6e74c6173c216dfb02788a46dbafe90
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) build type DEBUG
16:11:46.193 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) built by adar at 16 Dec 2018 16:09:24 PST on adar-Precision-5520
16:11:46.199 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.199141 17919 master_main.cc:80] Initializing master server...
16:11:46.204 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.204295 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } attempt: 2
16:11:46.204 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.204819 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.61:33927: Network error: Client connection negotiation failed: client connection to 127.17.34.61:33927: connect: Connection refused (error 111)
16:11:46.207 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.207751 17919 system_ntp.cc:199] NTP initialized. Skew: 500ppm Current error: 524000us
16:11:46.208 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.208410 17919 fs_manager.cc:263] Metadata directory not provided
16:11:46.208 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.208446 17919 fs_manager.cc:269] Using write-ahead log directory (fs_wal_dir) as metadata directory
16:11:46.208 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.208591 17919 server_base.cc:432] Could not load existing FS layout: Not found: could not find a healthy instance file
16:11:46.208 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.208612 17919 server_base.cc:433] Attempting to create new FS layout instead
16:11:46.220 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.209933 17919 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/master-1/data/instance:
16:11:46.220 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""76a82bb9282243f7bb6dcbdddde7159e""
16:11:46.220 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.220 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.220454 17919 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/master-1/wal/instance:
16:11:46.220 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""76a82bb9282243f7bb6dcbdddde7159e""
16:11:46.221 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.240 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.240337 17919 fs_manager.cc:503] Time spent creating directory manager: real 0.020s	user 0.005s	sys 0.000s
16:11:46.241 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.241276 17919 env_posix.cc:1676] Not raising this process' open files per process limit of 4096; it is already as high as it can go
16:11:46.241 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.241853 17919 file_cache.cc:466] Constructed file cache lbm with capacity 1638
16:11:46.251 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.251375 17919 fs_manager.cc:419] Time spent opening block manager: real 0.008s	user 0.001s	sys 0.000s
16:11:46.251 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.251737 17919 fs_manager.cc:436] Opened local filesystem: /tmp/mini-kudu-cluster2170731076069178574/master-1/data,/tmp/mini-kudu-cluster2170731076069178574/master-1/wal
16:11:46.251 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""76a82bb9282243f7bb6dcbdddde7159e""
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.252398 17919 fs_report.cc:352] FS layout report
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --------------------
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) wal directory: /tmp/mini-kudu-cluster2170731076069178574/master-1/wal
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) metadata directory: /tmp/mini-kudu-cluster2170731076069178574/master-1/wal
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 1 data directories: /tmp/mini-kudu-cluster2170731076069178574/master-1/data/data
16:11:46.252 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live blocks: 0
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes: 0
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes (after alignment): 0
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total number of LBM containers: 0 (0 full)
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for missing blocks
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for orphaned blocks
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM containers with extra space: 0 (0 repaired)
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM container extra space in bytes: 0 (0 repaired)
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total incomplete LBM containers: 0 (0 repaired)
16:11:46.253 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total LBM partial records: 0 (0 repaired)
16:11:46.316 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.315912 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } attempt: 3
16:11:46.317 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.317008 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.61:33927: Network error: Client connection negotiation failed: client connection to 127.17.34.61:33927: connect: Connection refused (error 111)
16:11:46.349 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.349449 17919 env_posix.cc:1676] Not raising this process' running threads per effective uid limit of 127647; it is already as high as it can go
16:11:46.349 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.349885 17919 master_main.cc:83] Starting Master server...
16:11:46.374 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.374361 17919 rpc_server.cc:205] RPC server started. Bound to: 127.17.34.61:33927
16:11:46.375 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.375018 17919 webserver.cc:175] Starting webserver on 127.17.34.61:0
16:11:46.375 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.375324 17919 webserver.cc:186] Document root disabled
16:11:46.375 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.375864 17919 webserver.cc:313] Webserver started. Bound to: http://127.17.34.61:39561/
16:11:46.376 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.376477 17919 server_base.cc:607] Dumped server information to /tmp/mini-kudu-cluster2170731076069178574/master-1/data/info.pb
16:11:46.382 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.381718 17544 external_mini_cluster.cc:907] Started /home/adar/Source/kudu/build/debug/bin/kudu-master as pid 17919
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.381935 17544 external_mini_cluster.cc:845] Running /home/adar/Source/kudu/build/debug/bin/kudu-master
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) /home/adar/Source/kudu/build/debug/bin/kudu-master
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/master-2/wal
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/master-2/data
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --block_manager=log
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=localhost
16:11:46.383 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_ca_key_size=1024
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tsk_num_rsa_bits=512
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.60:36165
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.60
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:46.384 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/master-2/logs
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/master-2/data/info.pb
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_reuseport=true
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --master_addresses=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:46.385 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --hive_metastore_uris=thrift://127.0.0.1:32857 with env {}
16:11:46.389 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.389258 18011 data_dirs.cc:938] Could only allocate 1 dirs of requested 3 for tablet 00000000000000000000000000000000. 1 dirs total, 0 dirs full, 0 dirs failed
16:11:46.395 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.395607 18011 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.438 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.438135 18011 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.463 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.463333 18011 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.464 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.464193 18011 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.483 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.483307 18011 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 1
16:11:46.483 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.483917 18011 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.495 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.493103 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } attempt: 4
16:11:46.497 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.497071 17925 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.497 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.497736 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.532 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.532011 18011 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 2
16:11:46.533 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.533232 18011 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.560 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.560834 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 1
16:11:46.561 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.561372 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.578 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Logging before InitGoogleLogging() is written to STDERR
16:11:46.579 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.578761 18012 flags.cc:406] Enabled unsafe flag: --rpc_server_allow_ephemeral_ports=true
16:11:46.579 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.578884 18012 flags.cc:406] Enabled unsafe flag: --never_fsync=true
16:11:46.579 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.579638 18012 flags.cc:406] Enabled experimental flag: --hive_metastore_uris=thrift://127.0.0.1:32857
16:11:46.590 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.590023 18012 flags.cc:406] Enabled experimental flag: --ipki_ca_key_size=1024
16:11:46.590 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.590520 18012 flags.cc:406] Enabled experimental flag: --ipki_server_key_size=1024
16:11:46.590 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.590729 18012 flags.cc:406] Enabled experimental flag: --tsk_num_rsa_bits=512
16:11:46.590 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.590910 18012 flags.cc:406] Enabled experimental flag: --rpc_reuseport=true
16:11:46.592 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.592492 18012 master_main.cc:73] Master server non-default flags:
16:11:46.592 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/master-2/data
16:11:46.592 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/master-2/wal
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --hive_metastore_uris=thrift://127.0.0.1:32857
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_ca_key_size=1024
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --master_addresses=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tsk_num_rsa_bits=512
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.60:36165
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_reuseport=true
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports=true
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/master-2/data/info.pb
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.60
16:11:46.593 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync=true
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --heap_profile_path=/tmp/kudu-master.18012
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags=true
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags=true
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/master-2/logs
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr=true
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Master server version:
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) kudu 1.9.0-SNAPSHOT
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) revision ff8971c6e6e74c6173c216dfb02788a46dbafe90
16:11:46.594 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) build type DEBUG
16:11:46.595 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) built by adar at 16 Dec 2018 16:09:24 PST on adar-Precision-5520
16:11:46.605 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.605733 18012 master_main.cc:80] Initializing master server...
16:11:46.607 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.607746 18012 system_ntp.cc:199] NTP initialized. Skew: 500ppm Current error: 524000us
16:11:46.610 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.610766 18012 fs_manager.cc:263] Metadata directory not provided
16:11:46.613 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.613124 18012 fs_manager.cc:269] Using write-ahead log directory (fs_wal_dir) as metadata directory
16:11:46.613 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.613867 18012 server_base.cc:432] Could not load existing FS layout: Not found: could not find a healthy instance file
16:11:46.614 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.614331 18012 server_base.cc:433] Attempting to create new FS layout instead
16:11:46.615 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.615465 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 2
16:11:46.616 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.616019 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.621 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.621098 18012 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/master-2/data/instance:
16:11:46.621 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""751f8d595718468d925a0a03d96a620f""
16:11:46.621 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.623 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.623530 18012 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/master-2/wal/instance:
16:11:46.623 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""751f8d595718468d925a0a03d96a620f""
16:11:46.623 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.644 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.644335 18011 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 3
16:11:46.644 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.644834 18011 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.649 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.649624 18012 fs_manager.cc:503] Time spent creating directory manager: real 0.024s	user 0.004s	sys 0.000s
16:11:46.650 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.650774 18012 env_posix.cc:1676] Not raising this process' open files per process limit of 4096; it is already as high as it can go
16:11:46.651 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.651476 18012 file_cache.cc:466] Constructed file cache lbm with capacity 1638
16:11:46.654 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.654687 18012 fs_manager.cc:419] Time spent opening block manager: real 0.001s	user 0.001s	sys 0.000s
16:11:46.655 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.655238 18012 fs_manager.cc:436] Opened local filesystem: /tmp/mini-kudu-cluster2170731076069178574/master-2/data,/tmp/mini-kudu-cluster2170731076069178574/master-2/wal
16:11:46.655 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""751f8d595718468d925a0a03d96a620f""
16:11:46.655 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.655982 18012 fs_report.cc:352] FS layout report
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --------------------
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) wal directory: /tmp/mini-kudu-cluster2170731076069178574/master-2/wal
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) metadata directory: /tmp/mini-kudu-cluster2170731076069178574/master-2/wal
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 1 data directories: /tmp/mini-kudu-cluster2170731076069178574/master-2/data/data
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live blocks: 0
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes: 0
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes (after alignment): 0
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total number of LBM containers: 0 (0 full)
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for missing blocks
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for orphaned blocks
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM containers with extra space: 0 (0 repaired)
16:11:46.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM container extra space in bytes: 0 (0 repaired)
16:11:46.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total incomplete LBM containers: 0 (0 repaired)
16:11:46.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total LBM partial records: 0 (0 repaired)
16:11:46.699 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.696111 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 3
16:11:46.699 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.696632 17925 consensus_peers.cc:599] Error getting permanent uuid from config peer 127.17.34.60:36165: Network error: Client connection negotiation failed: client connection to 127.17.34.60:36165: connect: Connection refused (error 111)
16:11:46.714 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.714015 18012 env_posix.cc:1676] Not raising this process' running threads per effective uid limit of 127647; it is already as high as it can go
16:11:46.717 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.717267 18012 master_main.cc:83] Starting Master server...
16:11:46.734 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.734128 18012 rpc_server.cc:205] RPC server started. Bound to: 127.17.34.60:36165
16:11:46.735 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.734979 18012 webserver.cc:175] Starting webserver on 127.17.34.60:0
16:11:46.735 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.735493 18012 webserver.cc:186] Document root disabled
16:11:46.736 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.736132 18012 webserver.cc:313] Webserver started. Bound to: http://127.17.34.60:35355/
16:11:46.736 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.736886 18012 server_base.cc:607] Dumped server information to /tmp/mini-kudu-cluster2170731076069178574/master-2/data/info.pb
16:11:46.740 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.740659 17544 external_mini_cluster.cc:907] Started /home/adar/Source/kudu/build/debug/bin/kudu-master as pid 18012
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.741008 17544 external_mini_cluster.cc:845] Running /home/adar/Source/kudu/build/debug/bin/kudu-tserver
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) /home/adar/Source/kudu/build/debug/bin/kudu-tserver
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-0/wal
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/ts-0/data
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --block_manager=log
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.1:0
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --local_ip_for_outbound_sockets=127.17.34.1
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.1
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tserver_master_addrs=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:46.741 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-0/logs
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/ts-0/data/info.pb
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags
16:11:46.742 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags with env {}
16:11:46.753 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.753222 18074 data_dirs.cc:938] Could only allocate 1 dirs of requested 3 for tablet 00000000000000000000000000000000. 1 dirs total, 0 dirs full, 0 dirs failed
16:11:46.756 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.756230 18074 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.789 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.789252 18074 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.792 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.792701 18074 sys_catalog.cc:304] member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } has no permanent_uuid. Determining permanent_uuid...
16:11:46.821 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.820925 18011 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 4
16:11:46.835 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.834233 18074 tablet_bootstrap.cc:438] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: Bootstrap starting.
16:11:46.835 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.835042 18011 tablet_bootstrap.cc:438] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: Bootstrap starting.
16:11:46.836 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.835710 18074 tablet_bootstrap.cc:592] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: No blocks or log segments found. Creating new log.
16:11:46.836 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.836256 18074 log.cc:525] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: Log is configured to *not* fsync() on all Append() calls
16:11:46.836 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.836388 18011 tablet_bootstrap.cc:592] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: No blocks or log segments found. Creating new log.
16:11:46.837 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.836899 18011 log.cc:525] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: Log is configured to *not* fsync() on all Append() calls
16:11:46.851 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.851080 18011 tablet_bootstrap.cc:438] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: No bootstrap required, opened a new log
16:11:46.854 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.853977 18011 raft_consensus.cc:340] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 0 FOLLOWER]: Replica starting. Triggering 0 pending transactions. Active config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.854 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.854110 18011 raft_consensus.cc:366] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 0 FOLLOWER]: Consensus starting up: Expiring failure detector timer to make a prompt election more likely
16:11:46.854 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.854133 18011 raft_consensus.cc:705] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 0 FOLLOWER]: Becoming Follower/Learner. State: Replica: 76a82bb9282243f7bb6dcbdddde7159e, State: Initialized, Role: FOLLOWER
16:11:46.854 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.854435 18011 consensus_queue.cc:229] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 0, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.854 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.854697 18011 sys_catalog.cc:337] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: SysCatalogTable state changed. Reason: Started TabletReplica. Latest consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.854 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.854748 18011 sys_catalog.cc:340] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: This master's current role is: FOLLOWER
16:11:46.855 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.855437 18011 sys_catalog.cc:424] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: configured and running, proceeding with master startup.
16:11:46.860 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.860723 17925 consensus_peers.cc:609] Retrying to get permanent uuid for remote peer: member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } attempt: 4
16:11:46.863 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.861629 18074 tablet_bootstrap.cc:438] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: No bootstrap required, opened a new log
16:11:46.863 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.863453 17919 master_main.cc:86] Master server successfully started.
16:11:46.863 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.863515 18074 raft_consensus.cc:340] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 0 FOLLOWER]: Replica starting. Triggering 0 pending transactions. Active config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.863 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.863646 18074 raft_consensus.cc:366] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 0 FOLLOWER]: Consensus starting up: Expiring failure detector timer to make a prompt election more likely
16:11:46.863 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.863668 18074 raft_consensus.cc:705] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 0 FOLLOWER]: Becoming Follower/Learner. State: Replica: 751f8d595718468d925a0a03d96a620f, State: Initialized, Role: FOLLOWER
16:11:46.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.863960 18074 consensus_queue.cc:229] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 0, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.864212 18074 sys_catalog.cc:337] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: SysCatalogTable state changed. Reason: Started TabletReplica. Latest consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.864265 18074 sys_catalog.cc:340] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: This master's current role is: FOLLOWER
16:11:46.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.864954 18074 sys_catalog.cc:424] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: configured and running, proceeding with master startup.
16:11:46.866 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.866345 18081 sys_catalog.cc:337] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: SysCatalogTable state changed. Reason: RaftConsensus started. Latest consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.866 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.866423 18081 sys_catalog.cc:340] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: This master's current role is: FOLLOWER
16:11:46.866 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.866569 17925 tablet_bootstrap.cc:438] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Bootstrap starting.
16:11:46.868 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.868149 17925 tablet_bootstrap.cc:592] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: No blocks or log segments found. Creating new log.
16:11:46.868 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.868716 17925 log.cc:525] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Log is configured to *not* fsync() on all Append() calls
16:11:46.869 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.869005 18012 master_main.cc:86] Master server successfully started.
16:11:46.875 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.875459 18083 hms_notification_log_listener.cc:226] Skipping Hive Metastore notification log poll: Illegal state: Not the leader. Local UUID: 76a82bb9282243f7bb6dcbdddde7159e, Raft Consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.879 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.879766 18084 catalog_manager.cc:1094] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: acquiring CA information for follower catalog manager: Not found: root CA entry not found
16:11:46.884 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.884449 18084 catalog_manager.cc:584] Not found: root CA entry not found: failed to prepare follower catalog manager, will retry
16:11:46.885 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.885080 18085 sys_catalog.cc:337] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: SysCatalogTable state changed. Reason: RaftConsensus started. Latest consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.885 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.885164 18085 sys_catalog.cc:340] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: This master's current role is: FOLLOWER
16:11:46.886 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.886503 17967 tablet.cc:1766] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: Can't schedule compaction. Clean time has not been advanced past its initial value.
16:11:46.889 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.889802 18087 hms_notification_log_listener.cc:226] Skipping Hive Metastore notification log poll: Illegal state: Not the leader. Local UUID: 751f8d595718468d925a0a03d96a620f, Raft Consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.890 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.890591 17925 tablet_bootstrap.cc:438] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: No bootstrap required, opened a new log
16:11:46.892 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.892524 17925 raft_consensus.cc:340] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Replica starting. Triggering 0 pending transactions. Active config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.892 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.892645 17925 raft_consensus.cc:366] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Consensus starting up: Expiring failure detector timer to make a prompt election more likely
16:11:46.892 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.892668 17925 raft_consensus.cc:705] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Becoming Follower/Learner. State: Replica: 9ef99883945f4dc99d641de3e016b608, State: Initialized, Role: FOLLOWER
16:11:46.893 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.892932 17925 consensus_queue.cc:229] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 0, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.893 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.893190 17925 sys_catalog.cc:337] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: SysCatalogTable state changed. Reason: Started TabletReplica. Latest consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.893 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.893247 17925 sys_catalog.cc:340] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: This master's current role is: FOLLOWER
16:11:46.893 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.893276 18088 catalog_manager.cc:1094] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: acquiring CA information for follower catalog manager: Not found: root CA entry not found
16:11:46.893 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.893313 18088 catalog_manager.cc:584] Not found: root CA entry not found: failed to prepare follower catalog manager, will retry
16:11:46.894 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.893936 17925 sys_catalog.cc:424] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: configured and running, proceeding with master startup.
16:11:46.894 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.894752 17858 master_main.cc:86] Master server successfully started.
16:11:46.906 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.906688 18095 catalog_manager.cc:1094] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: acquiring CA information for follower catalog manager: Not found: root CA entry not found
16:11:46.909 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Logging before InitGoogleLogging() is written to STDERR
16:11:46.910 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.894660 18075 flags.cc:406] Enabled unsafe flag: --rpc_server_allow_ephemeral_ports=true
16:11:46.910 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.909943 18075 flags.cc:406] Enabled unsafe flag: --never_fsync=true
16:11:46.910 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.910648 18075 flags.cc:406] Enabled experimental flag: --ipki_server_key_size=1024
16:11:46.910 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.910707 18075 flags.cc:406] Enabled experimental flag: --local_ip_for_outbound_sockets=127.17.34.1
16:11:46.911 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.911254 18095 catalog_manager.cc:584] Not found: root CA entry not found: failed to prepare follower catalog manager, will retry
16:11:46.911 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.911842 18075 tablet_server_main.cc:78] Tablet server non-default flags:
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/ts-0/data
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-0/wal
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.1:0
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports=true
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/ts-0/data/info.pb
16:11:46.912 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.1
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tserver_master_addrs=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync=true
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --heap_profile_path=/tmp/kudu-tserver.18075
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags=true
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags=true
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --local_ip_for_outbound_sockets=127.17.34.1
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-0/logs
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr=true
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Tablet server version:
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) kudu 1.9.0-SNAPSHOT
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) revision ff8971c6e6e74c6173c216dfb02788a46dbafe90
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) build type DEBUG
16:11:46.921 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) built by adar at 16 Dec 2018 16:09:24 PST on adar-Precision-5520
16:11:46.922 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.911415 18092 sys_catalog.cc:337] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: SysCatalogTable state changed. Reason: RaftConsensus started. Latest consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.922 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.919023 18092 sys_catalog.cc:340] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: This master's current role is: FOLLOWER
16:11:46.922 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.919152 18094 hms_notification_log_listener.cc:226] Skipping Hive Metastore notification log poll: Illegal state: Not the leader. Local UUID: 9ef99883945f4dc99d641de3e016b608, Raft Consensus state: current_term: 0 committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.926 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.926090 18092 raft_consensus.cc:472] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Starting pre-election (no leader contacted us within the election timeout)
16:11:46.926 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.926157 18092 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Snoozing failure detection for 2.241s (starting election)
16:11:46.926 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.926203 18092 raft_consensus.cc:494] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Starting pre-election with config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.926 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.926625 18092 leader_election.cc:251] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 pre-election: Requesting pre-vote from peer 76a82bb9282243f7bb6dcbdddde7159e (127.17.34.61:33927)
16:11:46.928 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.927423 17986 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""00000000000000000000000000000000"" candidate_uuid: ""9ef99883945f4dc99d641de3e016b608"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" is_pre_election: true
16:11:46.928 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.927558 17986 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: Snoozing failure detection for 1.869s (vote granted)
16:11:46.928 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.927601 17986 raft_consensus.cc:2346] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 0 FOLLOWER]: Leader pre-election vote request: Granting yes vote for candidate 9ef99883945f4dc99d641de3e016b608 in term 0.
16:11:46.928 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.927933 17867 leader_election.cc:387] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 pre-election: Vote granted by peer 76a82bb9282243f7bb6dcbdddde7159e (127.17.34.61:33927)
16:11:46.928 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.927971 17867 leader_election.cc:278] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 pre-election: Election decided. Result: candidate won.
16:11:46.928 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.928759 18092 leader_election.cc:251] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 pre-election: Requesting pre-vote from peer 751f8d595718468d925a0a03d96a620f (127.17.34.60:36165)
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929172 18092 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Snoozing failure detection for 2.085s (election complete)
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929389 18092 raft_consensus.cc:2640] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Leader pre-election won for term 1
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929394 18050 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""00000000000000000000000000000000"" candidate_uuid: ""9ef99883945f4dc99d641de3e016b608"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""751f8d595718468d925a0a03d96a620f"" is_pre_election: true
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929534 18050 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: Snoozing failure detection for 1.512s (vote granted)
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929579 18050 raft_consensus.cc:2346] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 0 FOLLOWER]: Leader pre-election vote request: Granting yes vote for candidate 9ef99883945f4dc99d641de3e016b608 in term 0.
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929780 18092 raft_consensus.cc:472] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Starting leader election (no leader contacted us within the election timeout)
16:11:46.929 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929939 18092 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Snoozing failure detection for 1.597s (starting election)
16:11:46.930 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.929944 17869 leader_election.cc:387] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 pre-election: Vote granted by peer 751f8d595718468d925a0a03d96a620f (127.17.34.60:36165)
16:11:46.930 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.930086 18092 raft_consensus.cc:2886] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 0 FOLLOWER]: Advancing to term 1
16:11:46.930 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.930619 18092 raft_consensus.cc:494] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 1 FOLLOWER]: Starting leader election with config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.930 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.930783 18092 leader_election.cc:251] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 election: Requesting vote from peer 76a82bb9282243f7bb6dcbdddde7159e (127.17.34.61:33927)
16:11:46.931 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.931162 17986 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""00000000000000000000000000000000"" candidate_uuid: ""9ef99883945f4dc99d641de3e016b608"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""76a82bb9282243f7bb6dcbdddde7159e""
16:11:46.931 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.931227 17986 raft_consensus.cc:2886] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 0 FOLLOWER]: Advancing to term 1
16:11:46.931 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.931268 17986 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: Snoozing failure detection for 1.964s (vote granted)
16:11:46.931 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.931780 17986 raft_consensus.cc:2346] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 1 FOLLOWER]: Leader election vote request: Granting yes vote for candidate 9ef99883945f4dc99d641de3e016b608 in term 1.
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932066 17867 leader_election.cc:387] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 election: Vote granted by peer 76a82bb9282243f7bb6dcbdddde7159e (127.17.34.61:33927)
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932097 17867 leader_election.cc:278] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 election: Election decided. Result: candidate won.
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932176 18092 leader_election.cc:251] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 election: Requesting vote from peer 751f8d595718468d925a0a03d96a620f (127.17.34.60:36165)
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932269 18092 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Snoozing failure detection for 1.947s (election complete)
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932296 18092 raft_consensus.cc:2640] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 1 FOLLOWER]: Leader election won for term 1
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932330 18092 raft_consensus.cc:667] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [term 1 LEADER]: Becoming Leader. State: Replica: 9ef99883945f4dc99d641de3e016b608, State: Running, Role: LEADER
16:11:46.932 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.932495 18092 consensus_queue.cc:206] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [LEADER]: Queue going to LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 1, Majority size: 2, State: 0, Mode: LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } }
16:11:46.934 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.933176 18050 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""00000000000000000000000000000000"" candidate_uuid: ""9ef99883945f4dc99d641de3e016b608"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""751f8d595718468d925a0a03d96a620f""
16:11:46.934 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.933238 18050 raft_consensus.cc:2886] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 0 FOLLOWER]: Advancing to term 1
16:11:46.934 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.933276 18050 raft_consensus.cc:2834] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: Snoozing failure detection for 1.686s (vote granted)
16:11:46.934 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.933642 18092 sys_catalog.cc:337] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: SysCatalogTable state changed. Reason: New leader 9ef99883945f4dc99d641de3e016b608. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:46.934 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.933697 18092 sys_catalog.cc:340] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: This master's current role is: LEADER
16:11:46.934 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.933775 18050 raft_consensus.cc:2346] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 1 FOLLOWER]: Leader election vote request: Granting yes vote for candidate 9ef99883945f4dc99d641de3e016b608 in term 1.
16:11:46.935 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.934047 17869 leader_election.cc:387] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [CANDIDATE]: Term 1 election: Vote granted by peer 751f8d595718468d925a0a03d96a620f (127.17.34.60:36165)
16:11:46.936 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.936576 18075 tablet_server_main.cc:85] Initializing tablet server...
16:11:46.937 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.937196 18075 system_ntp.cc:199] NTP initialized. Skew: 500ppm Current error: 524000us
16:11:46.937 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.937599 18075 fs_manager.cc:263] Metadata directory not provided
16:11:46.937 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.937633 18075 fs_manager.cc:269] Using write-ahead log directory (fs_wal_dir) as metadata directory
16:11:46.937 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.937764 18075 server_base.cc:432] Could not load existing FS layout: Not found: could not find a healthy instance file
16:11:46.938 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.937784 18075 server_base.cc:433] Attempting to create new FS layout instead
16:11:46.939 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.939069 18075 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/ts-0/data/instance:
16:11:46.939 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""a023316415e24eaa8cf8c304ef236819""
16:11:46.939 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.939 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.939334 18075 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/ts-0/wal/instance:
16:11:46.939 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""a023316415e24eaa8cf8c304ef236819""
16:11:46.939 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.943 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.943495 18103 catalog_manager.cc:1036] Loading table and tablet metadata into memory...
16:11:46.944 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.944275 18103 catalog_manager.cc:1047] Initializing Kudu internal certificate authority...
16:11:46.971 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.971673 18075 fs_manager.cc:503] Time spent creating directory manager: real 0.032s	user 0.003s	sys 0.000s
16:11:46.972 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.972208 18075 env_posix.cc:1676] Not raising this process' open files per process limit of 4096; it is already as high as it can go
16:11:46.972 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.972401 18075 file_cache.cc:466] Constructed file cache lbm with capacity 1638
16:11:46.980 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.980680 18075 fs_manager.cc:419] Time spent opening block manager: real 0.007s	user 0.001s	sys 0.000s
16:11:46.980 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.980713 18075 fs_manager.cc:436] Opened local filesystem: /tmp/mini-kudu-cluster2170731076069178574/ts-0/data,/tmp/mini-kudu-cluster2170731076069178574/ts-0/wal
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""a023316415e24eaa8cf8c304ef236819""
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:46 on adar-Precision-5520""
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:46.980785 18075 fs_report.cc:352] FS layout report
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --------------------
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) wal directory: /tmp/mini-kudu-cluster2170731076069178574/ts-0/wal
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) metadata directory: /tmp/mini-kudu-cluster2170731076069178574/ts-0/wal
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 1 data directories: /tmp/mini-kudu-cluster2170731076069178574/ts-0/data/data
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live blocks: 0
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes: 0
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes (after alignment): 0
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total number of LBM containers: 0 (0 full)
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for missing blocks
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for orphaned blocks
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM containers with extra space: 0 (0 repaired)
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM container extra space in bytes: 0 (0 repaired)
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total incomplete LBM containers: 0 (0 repaired)
16:11:46.981 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total LBM partial records: 0 (0 repaired)
16:11:46.985 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:46.985620 18030 tablet.cc:1766] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: Can't schedule compaction. Clean time has not been advanced past its initial value.
16:11:47.049 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.049155 18075 env_posix.cc:1676] Not raising this process' running threads per effective uid limit of 127647; it is already as high as it can go
16:11:47.049 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.049506 17986 raft_consensus.cc:1181] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [term 1 FOLLOWER]: Refusing update from remote peer 9ef99883945f4dc99d641de3e016b608: Log matching property violated. Preceding OpId in replica: term: 0 index: 0. Preceding OpId from leader: term: 1 index: 2. (index mismatch)
16:11:47.050 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.049506 18050 raft_consensus.cc:1181] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [term 1 FOLLOWER]: Refusing update from remote peer 9ef99883945f4dc99d641de3e016b608: Log matching property violated. Preceding OpId in replica: term: 0 index: 0. Preceding OpId from leader: term: 1 index: 2. (index mismatch)
16:11:47.050 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.050066 18092 consensus_queue.cc:976] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [LEADER]: Connected to new peer: Peer: permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 }, Status: LMP_MISMATCH, Last received: 0.0, Next index: 1, Last known committed idx: 0, Time since last communication: 0.000s
16:11:47.050 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.050164 18101 consensus_queue.cc:976] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [LEADER]: Connected to new peer: Peer: permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 }, Status: LMP_MISMATCH, Last received: 0.0, Next index: 1, Last known committed idx: 0, Time since last communication: 0.000s
16:11:47.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.053088 18081 sys_catalog.cc:337] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: SysCatalogTable state changed. Reason: New leader 9ef99883945f4dc99d641de3e016b608. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:47.053 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.053192 18081 sys_catalog.cc:340] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: This master's current role is: FOLLOWER
16:11:47.056 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.055966 17874 tablet.cc:1766] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Can't schedule compaction. Clean time has not been advanced past its initial value.
16:11:47.057 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.057185 18075 ts_tablet_manager.cc:356] Loaded tablet metadata (0 live tablets)
16:11:47.057 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.057691 18075 tablet_server_main.cc:90] Starting tablet server...
16:11:47.062 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.062166 18075 rpc_server.cc:205] RPC server started. Bound to: 127.17.34.1:43175
16:11:47.062 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.062925 18075 webserver.cc:175] Starting webserver on 127.17.34.1:0
16:11:47.063 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.063299 18075 webserver.cc:186] Document root disabled
16:11:47.064 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.063966 18075 webserver.cc:313] Webserver started. Bound to: http://127.17.34.1:37223/
16:11:47.064 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.064802 18075 server_base.cc:607] Dumped server information to /tmp/mini-kudu-cluster2170731076069178574/ts-0/data/info.pb
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.071401 17544 external_mini_cluster.cc:907] Started /home/adar/Source/kudu/build/debug/bin/kudu-tserver as pid 18075
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.071673 17544 external_mini_cluster.cc:845] Running /home/adar/Source/kudu/build/debug/bin/kudu-tserver
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) /home/adar/Source/kudu/build/debug/bin/kudu-tserver
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-1/wal
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/ts-1/data
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --block_manager=log
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.2:0
16:11:47.072 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --local_ip_for_outbound_sockets=127.17.34.2
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.2
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tserver_master_addrs=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-1/logs
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/ts-1/data/info.pb
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags with env {}
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.072309 18123 mvcc.cc:203] Tried to move safe_time back from 6328342556869881856 to 6328342556397236224. Current Snapshot: MvccSnapshot[committed={T|T < 6328342556869881856}]
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.072623 18092 sys_catalog.cc:337] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: SysCatalogTable state changed. Reason: Peer health change. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:47.073 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.072685 18092 sys_catalog.cc:340] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: This master's current role is: LEADER
16:11:47.079 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.073889 18081 sys_catalog.cc:337] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: SysCatalogTable state changed. Reason: Replicated consensus-only round. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:47.079 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.073958 18081 sys_catalog.cc:340] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e [sys.catalog]: This master's current role is: FOLLOWER
16:11:47.080 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.080407 18075 tablet_server_main.cc:93] Tablet server successfully started.
16:11:47.084 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.084470 18085 sys_catalog.cc:337] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: SysCatalogTable state changed. Reason: New leader 9ef99883945f4dc99d641de3e016b608. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:47.084 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.084555 18085 sys_catalog.cc:340] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: This master's current role is: FOLLOWER
16:11:47.091 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.091536 18128 mvcc.cc:203] Tried to move safe_time back from 6328342556869881856 to 6328342556397236224. Current Snapshot: MvccSnapshot[committed={T|T < 6328342556869881856}]
16:11:47.094 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.094913 18103 catalog_manager.cc:931] Generated new certificate authority record
16:11:47.095 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.095638 18103 catalog_manager.cc:1056] Loading token signing keys...
16:11:47.097 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.097574 18236 heartbeater.cc:346] Connected to a master server at 127.17.34.61:33927
16:11:47.098 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.098677 18236 heartbeater.cc:426] Registering TS with master...
16:11:47.099 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.099460 18236 heartbeater.cc:475] Master 127.17.34.61:33927 requested a full tablet report, sending...
16:11:47.100 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.100879 17972 ts_manager.cc:98] Registered new tserver with Master: a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:47.101 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.101059 18101 sys_catalog.cc:337] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: SysCatalogTable state changed. Reason: Peer health change. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:47.101 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.101112 18101 sys_catalog.cc:340] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608 [sys.catalog]: This master's current role is: LEADER
16:11:47.102 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.102035 18085 sys_catalog.cc:337] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: SysCatalogTable state changed. Reason: Replicated consensus-only round. Latest consensus state: current_term: 1 leader_uuid: ""9ef99883945f4dc99d641de3e016b608"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""9ef99883945f4dc99d641de3e016b608"" member_type: VOTER last_known_addr { host: ""127.17.34.62"" port: 40137 } } peers { permanent_uuid: ""76a82bb9282243f7bb6dcbdddde7159e"" member_type: VOTER last_known_addr { host: ""127.17.34.61"" port: 33927 } } peers { permanent_uuid: ""751f8d595718468d925a0a03d96a620f"" member_type: VOTER last_known_addr { host: ""127.17.34.60"" port: 36165 } } }
16:11:47.102 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.102108 18085 sys_catalog.cc:340] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f [sys.catalog]: This master's current role is: FOLLOWER
16:11:47.102 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.102638 18244 mvcc.cc:203] Tried to move safe_time back from 6328342556869881856 to 6328342556397236224. Current Snapshot: MvccSnapshot[committed={T|T < 6328342556869881856}]
16:11:47.104 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.104013 18103 catalog_manager.cc:4085] T 00000000000000000000000000000000 P 9ef99883945f4dc99d641de3e016b608: Generated new TSK 0
16:11:47.104 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.104717 18103 catalog_manager.cc:1067] Loading latest processed Hive Metastore notification log event ID...
16:11:47.112 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.112020 18237 heartbeater.cc:346] Connected to a master server at 127.17.34.60:36165
16:11:47.112 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.112062 18237 heartbeater.cc:426] Registering TS with master...
16:11:47.112 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.112195 18237 heartbeater.cc:475] Master 127.17.34.60:36165 requested a full tablet report, sending...
16:11:47.113 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.113296 18235 heartbeater.cc:346] Connected to a master server at 127.17.34.62:40137
16:11:47.113 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.113334 18235 heartbeater.cc:426] Registering TS with master...
16:11:47.113 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.113458 18235 heartbeater.cc:475] Master 127.17.34.62:40137 requested a full tablet report, sending...
16:11:47.113 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.113660 18032 ts_manager.cc:98] Registered new tserver with Master: a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:47.114 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.114117 17878 ts_manager.cc:98] Registered new tserver with Master: a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:47.115 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.115156 17878 master_service.cc:253] Signed X509 certificate for tserver {username='adar'} at 127.17.34.1:42523
16:11:47.223 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Logging before InitGoogleLogging() is written to STDERR
16:11:47.223 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.223338 18240 flags.cc:406] Enabled unsafe flag: --rpc_server_allow_ephemeral_ports=true
16:11:47.223 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.223453 18240 flags.cc:406] Enabled unsafe flag: --never_fsync=true
16:11:47.224 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.224123 18240 flags.cc:406] Enabled experimental flag: --ipki_server_key_size=1024
16:11:47.224 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.224182 18240 flags.cc:406] Enabled experimental flag: --local_ip_for_outbound_sockets=127.17.34.2
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.225267 18240 tablet_server_main.cc:78] Tablet server non-default flags:
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/ts-1/data
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-1/wal
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.2:0
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports=true
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/ts-1/data/info.pb
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.2
16:11:47.225 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tserver_master_addrs=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync=true
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --heap_profile_path=/tmp/kudu-tserver.18240
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags=true
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags=true
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --local_ip_for_outbound_sockets=127.17.34.2
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-1/logs
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr=true
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Tablet server version:
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) kudu 1.9.0-SNAPSHOT
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) revision ff8971c6e6e74c6173c216dfb02788a46dbafe90
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) build type DEBUG
16:11:47.226 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) built by adar at 16 Dec 2018 16:09:24 PST on adar-Precision-5520
16:11:47.254 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.254020 18240 tablet_server_main.cc:85] Initializing tablet server...
16:11:47.254 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.254657 18240 system_ntp.cc:199] NTP initialized. Skew: 500ppm Current error: 524500us
16:11:47.255 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.255087 18240 fs_manager.cc:263] Metadata directory not provided
16:11:47.255 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.255120 18240 fs_manager.cc:269] Using write-ahead log directory (fs_wal_dir) as metadata directory
16:11:47.255 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.255257 18240 server_base.cc:432] Could not load existing FS layout: Not found: could not find a healthy instance file
16:11:47.255 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.255288 18240 server_base.cc:433] Attempting to create new FS layout instead
16:11:47.256 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.256556 18240 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/ts-1/data/instance:
16:11:47.256 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""73407c178d394581974abe148305180e""
16:11:47.256 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:47 on adar-Precision-5520""
16:11:47.256 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.256798 18240 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/ts-1/wal/instance:
16:11:47.257 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""73407c178d394581974abe148305180e""
16:11:47.257 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:47 on adar-Precision-5520""
16:11:47.310 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.310138 18240 fs_manager.cc:503] Time spent creating directory manager: real 0.053s	user 0.001s	sys 0.003s
16:11:47.310 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.310659 18240 env_posix.cc:1676] Not raising this process' open files per process limit of 4096; it is already as high as it can go
16:11:47.311 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.310863 18240 file_cache.cc:466] Constructed file cache lbm with capacity 1638
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.313467 18240 fs_manager.cc:419] Time spent opening block manager: real 0.001s	user 0.001s	sys 0.000s
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.313503 18240 fs_manager.cc:436] Opened local filesystem: /tmp/mini-kudu-cluster2170731076069178574/ts-1/data,/tmp/mini-kudu-cluster2170731076069178574/ts-1/wal
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""73407c178d394581974abe148305180e""
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:47 on adar-Precision-5520""
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.313583 18240 fs_report.cc:352] FS layout report
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --------------------
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) wal directory: /tmp/mini-kudu-cluster2170731076069178574/ts-1/wal
16:11:47.313 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) metadata directory: /tmp/mini-kudu-cluster2170731076069178574/ts-1/wal
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 1 data directories: /tmp/mini-kudu-cluster2170731076069178574/ts-1/data/data
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live blocks: 0
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes: 0
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes (after alignment): 0
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total number of LBM containers: 0 (0 full)
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for missing blocks
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for orphaned blocks
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM containers with extra space: 0 (0 repaired)
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM container extra space in bytes: 0 (0 repaired)
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total incomplete LBM containers: 0 (0 repaired)
16:11:47.314 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total LBM partial records: 0 (0 repaired)
16:11:47.341 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.341162 18240 env_posix.cc:1676] Not raising this process' running threads per effective uid limit of 127647; it is already as high as it can go
16:11:47.341 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.341806 18240 ts_tablet_manager.cc:356] Loaded tablet metadata (0 live tablets)
16:11:47.341 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.341886 18240 tablet_server_main.cc:90] Starting tablet server...
16:11:47.345 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.345870 18240 rpc_server.cc:205] RPC server started. Bound to: 127.17.34.2:46251
16:11:47.346 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.346146 18240 webserver.cc:175] Starting webserver on 127.17.34.2:0
16:11:47.346 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.346159 18240 webserver.cc:186] Document root disabled
16:11:47.346 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.346478 18240 webserver.cc:313] Webserver started. Bound to: http://127.17.34.2:37761/
16:11:47.346 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.346803 18240 server_base.cc:607] Dumped server information to /tmp/mini-kudu-cluster2170731076069178574/ts-1/data/info.pb
16:11:47.353 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.353855 17544 external_mini_cluster.cc:907] Started /home/adar/Source/kudu/build/debug/bin/kudu-tserver as pid 18240
16:11:47.354 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.354099 17544 external_mini_cluster.cc:845] Running /home/adar/Source/kudu/build/debug/bin/kudu-tserver
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) /home/adar/Source/kudu/build/debug/bin/kudu-tserver
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-2/wal
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/ts-2/data
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --block_manager=log
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.3:0
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --local_ip_for_outbound_sockets=127.17.34.3
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.3
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tserver_master_addrs=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-2/logs
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/ts-2/data/info.pb
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags
16:11:47.355 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags with env {}
16:11:47.373 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.373430 18240 tablet_server_main.cc:93] Tablet server successfully started.
16:11:47.391 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.391856 18371 heartbeater.cc:346] Connected to a master server at 127.17.34.61:33927
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.391959 18371 heartbeater.cc:426] Registering TS with master...
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392321 18371 heartbeater.cc:475] Master 127.17.34.61:33927 requested a full tablet report, sending...
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392550 18370 heartbeater.cc:346] Connected to a master server at 127.17.34.62:40137
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392578 18370 heartbeater.cc:426] Registering TS with master...
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392592 18372 heartbeater.cc:346] Connected to a master server at 127.17.34.60:36165
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392618 18372 heartbeater.cc:426] Registering TS with master...
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392681 18370 heartbeater.cc:475] Master 127.17.34.62:40137 requested a full tablet report, sending...
16:11:47.392 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392716 18372 heartbeater.cc:475] Master 127.17.34.60:36165 requested a full tablet report, sending...
16:11:47.393 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.392997 17972 ts_manager.cc:98] Registered new tserver with Master: 73407c178d394581974abe148305180e (127.17.34.2:46251)
16:11:47.393 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.393115 17878 ts_manager.cc:98] Registered new tserver with Master: 73407c178d394581974abe148305180e (127.17.34.2:46251)
16:11:47.393 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.393580 17878 master_service.cc:253] Signed X509 certificate for tserver {username='adar'} at 127.17.34.2:57893
16:11:47.394 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.394172 18032 ts_manager.cc:98] Registered new tserver with Master: 73407c178d394581974abe148305180e (127.17.34.2:46251)
16:11:47.478 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) WARNING: Logging before InitGoogleLogging() is written to STDERR
16:11:47.478 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.478139 18374 flags.cc:406] Enabled unsafe flag: --rpc_server_allow_ephemeral_ports=true
16:11:47.478 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.478253 18374 flags.cc:406] Enabled unsafe flag: --never_fsync=true
16:11:47.478 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.478880 18374 flags.cc:406] Enabled experimental flag: --ipki_server_key_size=1024
16:11:47.485 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:47.485548 18374 flags.cc:406] Enabled experimental flag: --local_ip_for_outbound_sockets=127.17.34.3
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.486960 18374 tablet_server_main.cc:78] Tablet server non-default flags:
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_data_dirs=/tmp/mini-kudu-cluster2170731076069178574/ts-2/data
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --fs_wal_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-2/wal
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --ipki_server_key_size=1024
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_bind_addresses=127.17.34.3:0
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --rpc_server_allow_ephemeral_ports=true
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --metrics_log_interval_ms=1000
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_format=pb
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --server_dump_info_path=/tmp/mini-kudu-cluster2170731076069178574/ts-2/data/info.pb
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_interface=127.17.34.3
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --webserver_port=0
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --tserver_master_addrs=127.17.34.62:40137,127.17.34.61:33927,127.17.34.60:36165
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --never_fsync=true
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --heap_profile_path=/tmp/kudu-tserver.18374
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --redact=none
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_experimental_flags=true
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --unlock_unsafe_flags=true
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --enable_minidumps=false
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --local_ip_for_outbound_sockets=127.17.34.3
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --log_dir=/tmp/mini-kudu-cluster2170731076069178574/ts-2/logs
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logbuflevel=-1
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --logtostderr=true
16:11:47.487 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Tablet server version:
16:11:47.488 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) kudu 1.9.0-SNAPSHOT
16:11:47.488 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) revision ff8971c6e6e74c6173c216dfb02788a46dbafe90
16:11:47.488 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) build type DEBUG
16:11:47.488 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) built by adar at 16 Dec 2018 16:09:24 PST on adar-Precision-5520
16:11:47.506 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.506403 18374 tablet_server_main.cc:85] Initializing tablet server...
16:11:47.513 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.513797 18374 system_ntp.cc:199] NTP initialized. Skew: 500ppm Current error: 524500us
16:11:47.514 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.514616 18374 fs_manager.cc:263] Metadata directory not provided
16:11:47.514 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.514914 18374 fs_manager.cc:269] Using write-ahead log directory (fs_wal_dir) as metadata directory
16:11:47.515 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.515235 18374 server_base.cc:432] Could not load existing FS layout: Not found: could not find a healthy instance file
16:11:47.515 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.515465 18374 server_base.cc:433] Attempting to create new FS layout instead
16:11:47.517 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.516952 18374 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/ts-2/data/instance:
16:11:47.517 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""790bf4a387214fc1a49bb862543a1ded""
16:11:47.517 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:47 on adar-Precision-5520""
16:11:47.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.523464 18374 fs_manager.cc:602] Generated new instance metadata in path /tmp/mini-kudu-cluster2170731076069178574/ts-2/wal/instance:
16:11:47.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""790bf4a387214fc1a49bb862543a1ded""
16:11:47.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:47 on adar-Precision-5520""
16:11:47.550 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.548074 18374 fs_manager.cc:503] Time spent creating directory manager: real 0.024s	user 0.001s	sys 0.002s
16:11:47.550 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.548563 18374 env_posix.cc:1676] Not raising this process' open files per process limit of 4096; it is already as high as it can go
16:11:47.550 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.548753 18374 file_cache.cc:466] Constructed file cache lbm with capacity 1638
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.551622 18374 fs_manager.cc:419] Time spent opening block manager: real 0.001s	user 0.000s	sys 0.000s
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.551656 18374 fs_manager.cc:436] Opened local filesystem: /tmp/mini-kudu-cluster2170731076069178574/ts-2/data,/tmp/mini-kudu-cluster2170731076069178574/ts-2/wal
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) uuid: ""790bf4a387214fc1a49bb862543a1ded""
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) format_stamp: ""Formatted at 2018-12-17 00:11:47 on adar-Precision-5520""
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.551735 18374 fs_report.cc:352] FS layout report
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) --------------------
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) wal directory: /tmp/mini-kudu-cluster2170731076069178574/ts-2/wal
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) metadata directory: /tmp/mini-kudu-cluster2170731076069178574/ts-2/wal
16:11:47.556 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) 1 data directories: /tmp/mini-kudu-cluster2170731076069178574/ts-2/data/data
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live blocks: 0
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes: 0
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total live bytes (after alignment): 0
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total number of LBM containers: 0 (0 full)
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for missing blocks
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Did not check for orphaned blocks
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM containers with extra space: 0 (0 repaired)
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total full LBM container extra space in bytes: 0 (0 repaired)
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total incomplete LBM containers: 0 (0 repaired)
16:11:47.557 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Total LBM partial records: 0 (0 repaired)
16:11:47.616 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.616888 18374 env_posix.cc:1676] Not raising this process' running threads per effective uid limit of 127647; it is already as high as it can go
16:11:47.623 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.623586 18374 ts_tablet_manager.cc:356] Loaded tablet metadata (0 live tablets)
16:11:47.624 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.624123 18374 tablet_server_main.cc:90] Starting tablet server...
16:11:47.628 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.628824 18374 rpc_server.cc:205] RPC server started. Bound to: 127.17.34.3:42215
16:11:47.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.629520 18374 webserver.cc:175] Starting webserver on 127.17.34.3:0
16:11:47.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.629849 18374 webserver.cc:186] Document root disabled
16:11:47.630 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.630403 18374 webserver.cc:313] Webserver started. Bound to: http://127.17.34.3:45719/
16:11:47.631 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.631026 18374 server_base.cc:607] Dumped server information to /tmp/mini-kudu-cluster2170731076069178574/ts-2/data/info.pb
16:11:47.639 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.637472 17544 external_mini_cluster.cc:907] Started /home/adar/Source/kudu/build/debug/bin/kudu-tserver as pid 18374
16:11:47.649 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.643985 18386 reactor.cc:635] StartConnect: connect in progress for 127.17.34.61:33927
16:11:47.649 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.649168 18374 tablet_server_main.cc:93] Tablet server successfully started.
16:11:47.666 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.666087 18498 heartbeater.cc:346] Connected to a master server at 127.17.34.62:40137
16:11:47.666 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.666209 18498 heartbeater.cc:426] Registering TS with master...
16:11:47.666 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.666613 18498 heartbeater.cc:475] Master 127.17.34.62:40137 requested a full tablet report, sending...
16:11:47.667 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.667273 17878 ts_manager.cc:98] Registered new tserver with Master: 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:47.667 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.667754 17878 master_service.cc:253] Signed X509 certificate for tserver {username='adar'} at 127.17.34.3:46831
16:11:47.669 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.669657 18499 heartbeater.cc:346] Connected to a master server at 127.17.34.61:33927
16:11:47.670 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.669695 18499 heartbeater.cc:426] Registering TS with master...
16:11:47.670 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.669785 18499 heartbeater.cc:475] Master 127.17.34.61:33927 requested a full tablet report, sending...
16:11:47.670 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.670225 17972 ts_manager.cc:98] Registered new tserver with Master: 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:47.670 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.670442 18500 heartbeater.cc:346] Connected to a master server at 127.17.34.60:36165
16:11:47.670 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.670472 18500 heartbeater.cc:426] Registering TS with master...
16:11:47.670 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.670532 18500 heartbeater.cc:475] Master 127.17.34.60:36165 requested a full tablet report, sending...
16:11:47.671 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.671067 18032 ts_manager.cc:98] Registered new tserver with Master: 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:47.702 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.702765 17544 external_mini_cluster.cc:535] 3 TS(s) registered with all masters
16:11:47.702 [DEBUG - Test worker] (MiniKuduCluster.java:175) Response: 
16:11:47.704 [DEBUG - Test worker] (MiniKuduCluster.java:165) Request: get_masters {
}

16:11:47.715 [DEBUG - Test worker] (MiniKuduCluster.java:175) Response: get_masters {
  masters {
    id {
      type: MASTER
      index: 0
    }
    bound_rpc_address {
      host: ""127.17.34.62""
      port: 40137
    }
  }
  masters {
    id {
      type: MASTER
      index: 1
    }
    bound_rpc_address {
      host: ""127.17.34.61""
      port: 33927
    }
  }
  masters {
    id {
      type: MASTER
      index: 2
    }
    bound_rpc_address {
      host: ""127.17.34.60""
      port: 36165
    }
  }
}

16:11:47.719 [DEBUG - Test worker] (MiniKuduCluster.java:165) Request: get_tservers {
}

16:11:47.726 [DEBUG - Test worker] (MiniKuduCluster.java:175) Response: get_tservers {
  tservers {
    id {
      type: TSERVER
      index: 0
    }
    bound_rpc_address {
      host: ""127.17.34.1""
      port: 43175
    }
  }
  tservers {
    id {
      type: TSERVER
      index: 1
    }
    bound_rpc_address {
      host: ""127.17.34.2""
      port: 46251
    }
  }
  tservers {
    id {
      type: TSERVER
      index: 2
    }
    bound_rpc_address {
      host: ""127.17.34.3""
      port: 42215
    }
  }
}

16:11:47.727 [INFO - Test worker] (KuduTestHarness.java:139) Creating a new Kudu client...
16:11:47.887 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.887452 18084 catalog_manager.cc:1092] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: acquiring CA information for follower catalog manager: success
16:11:47.888 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.888465 18084 catalog_manager.cc:1120] T 00000000000000000000000000000000 P 76a82bb9282243f7bb6dcbdddde7159e: importing token verification keys for follower catalog manager: success; most recent TSK sequence number 0
16:11:47.895 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.895563 18088 catalog_manager.cc:1092] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: acquiring CA information for follower catalog manager: success
16:11:47.896 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:47.896570 18088 catalog_manager.cc:1120] T 00000000000000000000000000000000 P 751f8d595718468d925a0a03d96a620f: importing token verification keys for follower catalog manager: success; most recent TSK sequence number 0
16:11:48.116 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:48.116657 18235 heartbeater.cc:467] Master 127.17.34.62:40137 was elected leader, sending a full tablet report...
16:11:48.395 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:48.395656 18370 heartbeater.cc:467] Master 127.17.34.62:40137 was elected leader, sending a full tablet report...
16:11:48.585 [DEBUG - Test worker] (SecurityUtil.java:123) Could not login via JAAS. Using no credentials: Unable to obtain Principal Name for authentication 
16:11:48.669 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:48.669067 18498 heartbeater.cc:467] Master 127.17.34.62:40137 was elected leader, sending a full tablet report...
16:11:49.306 [DEBUG - New I/O worker #1] (Negotiator.java:460) SASL mechanism PLAIN chosen for peer 127.17.34.62
16:11:49.344 [DEBUG - New I/O worker #2] (Negotiator.java:460) SASL mechanism PLAIN chosen for peer 127.17.34.61
16:11:49.344 [DEBUG - New I/O worker #3] (Negotiator.java:460) SASL mechanism PLAIN chosen for peer 127.17.34.60
16:11:49.529 [DEBUG - New I/O worker #2] (Negotiator.java:764) Authenticated connection [id: 0x3f9dc486, /127.0.0.1:34550 => /127.17.34.61:33927] using SASL/PLAIN
16:11:49.531 [DEBUG - New I/O worker #3] (Negotiator.java:764) Authenticated connection [id: 0xf83a9acd, /127.0.0.1:35460 => /127.17.34.60:36165] using SASL/PLAIN
16:11:49.537 [DEBUG - New I/O worker #1] (Negotiator.java:764) Authenticated connection [id: 0xe0cfdf03, /127.0.0.1:39780 => /127.17.34.62:40137] using SASL/PLAIN
16:11:49.595 [DEBUG - New I/O worker #1] (AsyncKuduClient.java:2085) Learned about tablet Kudu Master for table 'Kudu Master' with partition [<start>, <end>)
16:11:49.596 [DEBUG - New I/O worker #1] (AsyncKuduClient.java:1249) Retrying sending RPC KuduRpc(method=CreateTable, tablet=null, attempt=1, DeadlineTracker(timeout=50000, elapsed=583), Traces: [0ms] querying master, [156ms] Sub rpc: ConnectToMaster sending RPC to server master-127.17.34.62:40137, [196ms] Sub rpc: ConnectToMaster sending RPC to server master-127.17.34.61:33927, [196ms] Sub rpc: ConnectToMaster sending RPC to server master-127.17.34.60:36165, [518ms] Sub rpc: ConnectToMaster received from server master-127.17.34.60:36165 response OK, [519ms] Sub rpc: ConnectToMaster received from server master-127.17.34.62:40137 response OK, [519ms] Sub rpc: ConnectToMaster received from server master-127.17.34.61:33927 response OK, null) after lookup
16:11:49.612 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=retry RPC@2126995845 returned Deferred@2135376009(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after retry RPC@2126995845), errback=(continuation of Deferred@1721323338 after retry RPC@2126995845)), so the following Deferred is getting paused: Deferred@1721323338(state=PAUSED, result=Deferred@2135376009, callback=org.apache.kudu.client.AsyncKuduClient$1@c8cb35b -> org.apache.kudu.client.AsyncKuduClient$2@49023006 -> wakeup thread Time-limited test, errback=passthrough -> passthrough -> wakeup thread Time-limited test)
16:11:50.023 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:50.021348 17878 catalog_manager.cc:1376] Servicing CreateTable request from {username='adar'} at 127.0.0.1:39780:
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) name: ""default.testOverrideTableOwner""
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) schema {
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   columns {
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     name: ""key""
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     type: INT32
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_key: true
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_nullable: false
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     cfile_block_size: 0
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   }
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   columns {
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     name: ""column1_i""
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     type: INT32
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_key: false
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_nullable: false
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     cfile_block_size: 0
16:11:50.024 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   }
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   columns {
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     name: ""column2_i""
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     type: INT32
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_key: false
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_nullable: false
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     cfile_block_size: 0
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   }
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   columns {
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     name: ""column3_s""
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     type: STRING
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_key: false
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_nullable: true
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     encoding: DICT_ENCODING
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     compression: LZ4
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     cfile_block_size: 4096
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   }
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   columns {
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     name: ""column4_b""
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     type: BOOL
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_key: false
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     is_nullable: false
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     cfile_block_size: 0
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   }
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) }
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) partition_schema {
16:11:50.025 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   range_schema {
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     columns {
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)       name: ""key""
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)     }
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535)   }
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) }
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) owner: ""alice""
16:11:50.026 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:50.022984 17878 catalog_manager.cc:1527] The number of live tablet servers is not enough to re-replicate a tablet replica of the newly created table default.testoverridetableowner in case of a server failure: 4 tablet servers would be needed, 3 are available. Consider bringing up more tablet servers.
16:11:51.437 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:51.437813 18093 hms_client.cc:245] Time spent create HMS table: real 1.414s	user 0.000s	sys 0.000s
16:11:51.470 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.470484 17878 rpcz_store.cc:269] Call kudu.master.MasterService.CreateTable from 127.0.0.1:39780 (request call id 1) took 1858ms. Request Metrics: {""Hive Metastore.queue_time_us"":29,""Hive Metastore.run_cpu_time_us"":525,""Hive Metastore.run_wall_time_us"":1414134,""child_traces"":[[""txn"",{""apply.queue_time_us"":3107,""num_ops"":2,""prepare.queue_time_us"":17078,""prepare.run_cpu_time_us"":587,""prepare.run_wall_time_us"":2648,""raft.queue_time_us"":134,""raft.run_cpu_time_us"":706,""raft.run_wall_time_us"":757,""replication_time_us"":10988,""spinlock_wait_cycles"":604416,""thread_start_us"":199,""threads_started"":4,""wal-append.queue_time_us"":10613}]]}
16:11:51.482 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$1@c8cb35b@210547547 returned Deferred@428400259(state=PENDING, result=null, callback=org.apache.kudu.client.AsyncKuduClient$6@3f7f7f1f -> (continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$1@c8cb35b@210547547), errback=passthrough -> (continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$1@c8cb35b@210547547)), so the following Deferred is getting paused: Deferred@1721323338(state=PAUSED, result=Deferred@428400259, callback=org.apache.kudu.client.AsyncKuduClient$2@49023006 -> wakeup thread Time-limited test, errback=passthrough -> wakeup thread Time-limited test)
16:11:51.491 [DEBUG - New I/O worker #1] (AsyncKuduClient.java:737) Opened table 77ac3e6b265e40cab76acd13ab838321
16:11:51.504 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134)), so the following Deferred is getting paused: Deferred@1721323338(state=PAUSED, result=Deferred@96872086, callback=wakeup thread Time-limited test, errback=wakeup thread Time-limited test)
16:11:51.508 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.504006 18296 tablet_service.cc:772] Processing CreateTablet for tablet e7e66e6f324b4e918f7b0e09f01b7dd5 (table=default.testoverridetableowner [id=77ac3e6b265e40cab76acd13ab838321]), partition=RANGE (key) PARTITION UNBOUNDED
16:11:51.508 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.504524 18296 data_dirs.cc:938] Could only allocate 1 dirs of requested 3 for tablet e7e66e6f324b4e918f7b0e09f01b7dd5. 1 dirs total, 0 dirs full, 0 dirs failed
16:11:51.508 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.507890 18296 ts_tablet_manager.cc:1173] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Registered tablet (data state: TABLET_DATA_READY)
16:11:51.512 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.511884 18160 tablet_service.cc:772] Processing CreateTablet for tablet e7e66e6f324b4e918f7b0e09f01b7dd5 (table=default.testoverridetableowner [id=77ac3e6b265e40cab76acd13ab838321]), partition=RANGE (key) PARTITION UNBOUNDED
16:11:51.512 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.512349 18160 data_dirs.cc:938] Could only allocate 1 dirs of requested 3 for tablet e7e66e6f324b4e918f7b0e09f01b7dd5. 1 dirs total, 0 dirs full, 0 dirs failed
16:11:51.513 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.513453 18416 tablet_service.cc:772] Processing CreateTablet for tablet e7e66e6f324b4e918f7b0e09f01b7dd5 (table=default.testoverridetableowner [id=77ac3e6b265e40cab76acd13ab838321]), partition=RANGE (key) PARTITION UNBOUNDED
16:11:51.513 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.513929 18416 data_dirs.cc:938] Could only allocate 1 dirs of requested 3 for tablet e7e66e6f324b4e918f7b0e09f01b7dd5. 1 dirs total, 0 dirs full, 0 dirs failed
16:11:51.516 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.515489 18160 ts_tablet_manager.cc:1173] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Registered tablet (data state: TABLET_DATA_READY)
16:11:51.517 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.517103 18416 ts_tablet_manager.cc:1173] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Registered tablet (data state: TABLET_DATA_READY)
16:11:51.520 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626)), so the following Deferred is getting paused: Deferred@1989261758(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.523097 18590 ts_tablet_manager.cc:1028] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Bootstrapping tablet
16:11:51.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.523372 18589 ts_tablet_manager.cc:1028] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Bootstrapping tablet
16:11:51.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.523566 18590 tablet_bootstrap.cc:438] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Bootstrap starting.
16:11:51.523 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.523679 18589 tablet_bootstrap.cc:438] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Bootstrap starting.
16:11:51.524 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.524941 18589 tablet_bootstrap.cc:592] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: No blocks or log segments found. Creating new log.
16:11:51.525 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.524941 18590 tablet_bootstrap.cc:592] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: No blocks or log segments found. Creating new log.
16:11:51.525 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.525707 18589 log.cc:525] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Log is configured to *not* fsync() on all Append() calls
16:11:51.526 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.526268 18590 log.cc:525] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Log is configured to *not* fsync() on all Append() calls
16:11:51.529 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.529350 18588 ts_tablet_manager.cc:1028] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Bootstrapping tablet
16:11:51.529 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638)), so the following Deferred is getting paused: Deferred@1475931837(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.529 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.529844 18588 tablet_bootstrap.cc:438] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Bootstrap starting.
16:11:51.531 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.531255 18588 tablet_bootstrap.cc:592] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: No blocks or log segments found. Creating new log.
16:11:51.531 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.531797 18588 log.cc:525] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Log is configured to *not* fsync() on all Append() calls
16:11:51.535 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.535006 18589 tablet_bootstrap.cc:438] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: No bootstrap required, opened a new log
16:11:51.535 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.535158 18589 ts_tablet_manager.cc:1045] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Time spent bootstrapping tablet: real 0.012s	user 0.002s	sys 0.000s
16:11:51.537 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.537147 18589 raft_consensus.cc:340] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 0 FOLLOWER]: Replica starting. Triggering 0 pending transactions. Active config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.537 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.537571 18589 raft_consensus.cc:366] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 0 FOLLOWER]: Consensus starting up: Expiring failure detector timer to make a prompt election more likely
16:11:51.537 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.537721 18589 raft_consensus.cc:705] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 0 FOLLOWER]: Becoming Follower/Learner. State: Replica: a023316415e24eaa8cf8c304ef236819, State: Initialized, Role: FOLLOWER
16:11:51.538 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.538197 18589 consensus_queue.cc:229] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 0, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.545054 18590 tablet_bootstrap.cc:438] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: No bootstrap required, opened a new log
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.545204 18590 ts_tablet_manager.cc:1045] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Time spent bootstrapping tablet: real 0.022s	user 0.003s	sys 0.000s
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.547062 18590 raft_consensus.cc:340] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Replica starting. Triggering 0 pending transactions. Active config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.547200 18590 raft_consensus.cc:366] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Consensus starting up: Expiring failure detector timer to make a prompt election more likely
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.547222 18590 raft_consensus.cc:705] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Becoming Follower/Learner. State: Replica: 790bf4a387214fc1a49bb862543a1ded, State: Initialized, Role: FOLLOWER
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.547523 18590 consensus_queue.cc:229] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 0, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.547799 18589 ts_tablet_manager.cc:1073] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Time spent starting tablet: real 0.013s	user 0.003s	sys 0.000s
16:11:51.548 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.547925 18590 ts_tablet_manager.cc:1073] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Time spent starting tablet: real 0.003s	user 0.003s	sys 0.000s
16:11:51.551 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587)), so the following Deferred is getting paused: Deferred@647570931(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.559 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.557986 18588 tablet_bootstrap.cc:438] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: No bootstrap required, opened a new log
16:11:51.559 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.558135 18588 ts_tablet_manager.cc:1045] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Time spent bootstrapping tablet: real 0.028s	user 0.003s	sys 0.000s
16:11:51.560 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.560806 18588 raft_consensus.cc:340] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Replica starting. Triggering 0 pending transactions. Active config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.561 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.561250 18588 raft_consensus.cc:366] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Consensus starting up: Expiring failure detector timer to make a prompt election more likely
16:11:51.561 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.561445 18588 raft_consensus.cc:705] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Becoming Follower/Learner. State: Replica: 73407c178d394581974abe148305180e, State: Initialized, Role: FOLLOWER
16:11:51.562 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.561928 18588 consensus_queue.cc:229] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [NON_LEADER]: Queue going to NON_LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 0, Majority size: -1, State: 0, Mode: NON_LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.562 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.562686 18588 ts_tablet_manager.cc:1073] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Time spent starting tablet: real 0.005s	user 0.003s	sys 0.000s
16:11:51.570 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766)), so the following Deferred is getting paused: Deferred@794594445(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.579 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:51.578789 18238 tablet.cc:1766] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Can't schedule compaction. Clean time has not been advanced past its initial value.
16:11:51.589 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766) -> (continuation of Deferred@1825198794 after org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766) -> (continuation of Deferred@1825198794 after org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750)), so the following Deferred is getting paused: Deferred@1825198794(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.603 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:51.603348 18373 tablet.cc:1766] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Can't schedule compaction. Clean time has not been advanced past its initial value.
16:11:51.613 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@5bba686d@1538943085 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766) -> (continuation of Deferred@1825198794 after org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750) -> (continuation of Deferred@273268595 after org.apache.kudu.client.AsyncKuduClient$15@5bba686d@1538943085), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766) -> (continuation of Deferred@1825198794 after org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750) -> (continuation of Deferred@273268595 after org.apache.kudu.client.AsyncKuduClient$15@5bba686d@1538943085)), so the following Deferred is getting paused: Deferred@273268595(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.628681 18596 raft_consensus.cc:472] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Starting pre-election (no leader contacted us within the election timeout)
16:11:51.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.628747 18596 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Snoozing failure detection for 2.100s (starting election)
16:11:51.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.628792 18596 raft_consensus.cc:494] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Starting pre-election with config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.629302 18596 leader_election.cc:251] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 pre-election: Requesting pre-vote from peer 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:51.629 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.629473 18596 leader_election.cc:251] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 pre-election: Requesting pre-vote from peer a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:51.648 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:51.648211 18501 tablet.cc:1766] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Can't schedule compaction. Clean time has not been advanced past its initial value.
16:11:51.650 [DEBUG - New I/O worker #1] (Deferred.java:1330) callback=org.apache.kudu.client.AsyncKuduClient$15@90c88e5@151816421 returned Deferred@96872086(state=PENDING, result=null, callback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766) -> (continuation of Deferred@1825198794 after org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750) -> (continuation of Deferred@273268595 after org.apache.kudu.client.AsyncKuduClient$15@5bba686d@1538943085) -> (continuation of Deferred@938395404 after org.apache.kudu.client.AsyncKuduClient$15@90c88e5@151816421), errback=(continuation of Deferred@1721323338 after org.apache.kudu.client.AsyncKuduClient$2@49023006@1224880134) -> (continuation of Deferred@1989261758 after org.apache.kudu.client.AsyncKuduClient$15@16d383f2@382960626) -> (continuation of Deferred@1475931837 after org.apache.kudu.client.AsyncKuduClient$15@1141101e@289476638) -> (continuation of Deferred@647570931 after org.apache.kudu.client.AsyncKuduClient$15@5d3e3ba3@1564359587) -> (continuation of Deferred@794594445 after org.apache.kudu.client.AsyncKuduClient$15@3f4110de@1061228766) -> (continuation of Deferred@1825198794 after org.apache.kudu.client.AsyncKuduClient$15@31349e6@51595750) -> (continuation of Deferred@273268595 after org.apache.kudu.client.AsyncKuduClient$15@5bba686d@1538943085) -> (continuation of Deferred@938395404 after org.apache.kudu.client.AsyncKuduClient$15@90c88e5@151816421)), so the following Deferred is getting paused: Deferred@938395404(state=PAUSED, result=Deferred@96872086, callback=<none>, errback=<none>)
16:11:51.651 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.650887 18595 raft_consensus.cc:472] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Starting pre-election (no leader contacted us within the election timeout)
16:11:51.652 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.650954 18595 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Snoozing failure detection for 1.742s (starting election)
16:11:51.652 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.650997 18595 raft_consensus.cc:494] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Starting pre-election with config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.652 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.651530 18595 leader_election.cc:251] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [CANDIDATE]: Term 1 pre-election: Requesting pre-vote from peer 73407c178d394581974abe148305180e (127.17.34.2:46251)
16:11:51.652 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.651713 18595 leader_election.cc:251] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [CANDIDATE]: Term 1 pre-election: Requesting pre-vote from peer a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.653741 18187 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""e7e66e6f324b4e918f7b0e09f01b7dd5"" candidate_uuid: ""73407c178d394581974abe148305180e"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""a023316415e24eaa8cf8c304ef236819"" is_pre_election: true
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.653880 18187 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Snoozing failure detection for 1.784s (vote granted)
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.653923 18187 raft_consensus.cc:2346] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 0 FOLLOWER]: Leader pre-election vote request: Granting yes vote for candidate 73407c178d394581974abe148305180e in term 0.
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654280 18261 leader_election.cc:387] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 pre-election: Vote granted by peer a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654326 18261 leader_election.cc:278] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 pre-election: Election decided. Result: candidate won.
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654533 18596 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Snoozing failure detection for 2.244s (election complete)
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654559 18596 raft_consensus.cc:2640] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Leader pre-election won for term 1
16:11:51.656 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654578 18596 raft_consensus.cc:472] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Starting leader election (no leader contacted us within the election timeout)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654551 18452 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""e7e66e6f324b4e918f7b0e09f01b7dd5"" candidate_uuid: ""73407c178d394581974abe148305180e"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""790bf4a387214fc1a49bb862543a1ded"" is_pre_election: true
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654588 18596 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Snoozing failure detection for 2.122s (starting election)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654601 18596 raft_consensus.cc:2886] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 0 FOLLOWER]: Advancing to term 1
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654633 18452 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Snoozing failure detection for 2.173s (vote granted)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654657 18452 raft_consensus.cc:2346] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Leader pre-election vote request: Granting yes vote for candidate 73407c178d394581974abe148305180e in term 0.
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.654924 18259 leader_election.cc:387] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 pre-election: Vote granted by peer 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655103 18596 raft_consensus.cc:494] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 1 FOLLOWER]: Starting leader election with config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655241 18596 leader_election.cc:251] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 election: Requesting vote from peer 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655311 18596 leader_election.cc:251] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 election: Requesting vote from peer a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655577 18452 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""e7e66e6f324b4e918f7b0e09f01b7dd5"" candidate_uuid: ""73407c178d394581974abe148305180e"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""790bf4a387214fc1a49bb862543a1ded""
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655623 18187 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""e7e66e6f324b4e918f7b0e09f01b7dd5"" candidate_uuid: ""73407c178d394581974abe148305180e"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""a023316415e24eaa8cf8c304ef236819""
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655639 18452 raft_consensus.cc:2886] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 0 FOLLOWER]: Advancing to term 1
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655681 18452 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Snoozing failure detection for 1.753s (vote granted)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655679 18187 raft_consensus.cc:2886] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 0 FOLLOWER]: Advancing to term 1
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.655720 18187 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819: Snoozing failure detection for 1.641s (vote granted)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.656152 18187 raft_consensus.cc:2346] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 1 FOLLOWER]: Leader election vote request: Granting yes vote for candidate 73407c178d394581974abe148305180e in term 1.
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.656152 18452 raft_consensus.cc:2346] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 1 FOLLOWER]: Leader election vote request: Granting yes vote for candidate 73407c178d394581974abe148305180e in term 1.
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.656406 18259 leader_election.cc:387] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 election: Vote granted by peer 790bf4a387214fc1a49bb862543a1ded (127.17.34.3:42215)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.656440 18261 leader_election.cc:387] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 election: Vote granted by peer a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175)
16:11:51.657 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.656460 18259 leader_election.cc:278] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [CANDIDATE]: Term 1 election: Election decided. Result: candidate won.
16:11:51.660 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.658514 18596 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e: Snoozing failure detection for 1.821s (election complete)
16:11:51.660 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.658561 18596 raft_consensus.cc:2640] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 1 FOLLOWER]: Leader election won for term 1
16:11:51.661 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.658702 18596 raft_consensus.cc:667] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 1 LEADER]: Becoming Leader. State: Replica: 73407c178d394581974abe148305180e, State: Running, Role: LEADER
16:11:51.661 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.658926 18596 consensus_queue.cc:206] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [LEADER]: Queue going to LEADER mode. State: All replicated index: 0, Majority replicated index: 0, Committed index: 0, Last appended: 0.0, Last appended by leader: 0, Current term: 1, Majority size: 2, State: 0, Mode: LEADER, active raft config: opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } }
16:11:51.661 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.661171 17879 catalog_manager.cc:3781] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e reported cstate change: term changed from 0 to 1, leader changed from <none> to 73407c178d394581974abe148305180e (127.17.34.2). New cstate: current_term: 1 leader_uuid: ""73407c178d394581974abe148305180e"" committed_config { opid_index: -1 OBSOLETE_local: false peers { permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 } health_report { overall_health: UNKNOWN } } peers { permanent_uuid: ""73407c178d394581974abe148305180e"" member_type: VOTER last_known_addr { host: ""127.17.34.2"" port: 46251 } health_report { overall_health: HEALTHY } } peers { permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 } health_report { overall_health: UNKNOWN } } }
16:11:51.679 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.679214 18310 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""e7e66e6f324b4e918f7b0e09f01b7dd5"" candidate_uuid: ""790bf4a387214fc1a49bb862543a1ded"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""73407c178d394581974abe148305180e"" is_pre_election: true
16:11:51.679 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.679360 18310 raft_consensus.cc:2307] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 1 LEADER]: Leader pre-election vote request: Denying vote to candidate 790bf4a387214fc1a49bb862543a1ded for term 1 because replica is either leader or believes a valid leader to be alive.
16:11:51.679 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.679819 18386 leader_election.cc:401] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [CANDIDATE]: Term 1 pre-election: Vote denied by peer 73407c178d394581974abe148305180e (127.17.34.2:46251). Message: Invalid argument: T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [term 1 LEADER]: Leader pre-election vote request: Denying vote to candidate 790bf4a387214fc1a49bb862543a1ded for term 1 because replica is either leader or believes a valid leader to be alive.
16:11:51.691 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.691555 18187 tablet_service.cc:1008] Received RequestConsensusVote() RPC: tablet_id: ""e7e66e6f324b4e918f7b0e09f01b7dd5"" candidate_uuid: ""790bf4a387214fc1a49bb862543a1ded"" candidate_term: 1 candidate_status { last_received { term: 0 index: 0 } } ignore_live_leader: false dest_uuid: ""a023316415e24eaa8cf8c304ef236819"" is_pre_election: true
16:11:51.692 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.691673 18187 raft_consensus.cc:2276] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 1 FOLLOWER]: Leader pre-election vote request: Denying vote to candidate 790bf4a387214fc1a49bb862543a1ded in current term 1: Already voted for candidate 73407c178d394581974abe148305180e in this term.
16:11:51.692 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.692502 18388 leader_election.cc:401] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [CANDIDATE]: Term 1 pre-election: Vote denied by peer a023316415e24eaa8cf8c304ef236819 (127.17.34.1:43175). Message: Invalid argument: T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 1 FOLLOWER]: Leader pre-election vote request: Denying vote to candidate 790bf4a387214fc1a49bb862543a1ded in current term 1: Already voted for candidate 73407c178d394581974abe148305180e in this term.
16:11:51.693 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.692562 18388 leader_election.cc:278] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [CANDIDATE]: Term 1 pre-election: Election decided. Result: candidate lost.
16:11:51.693 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.692773 18595 raft_consensus.cc:2834] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded: Snoozing failure detection for 2.051s (election complete)
16:11:51.693 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:51.692807 18595 raft_consensus.cc:2588] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 1 FOLLOWER]: Leader pre-election lost for term 1. Reason: could not achieve majority
16:11:51.744 [INFO - Time-limited test] (HiveConf.java:181) Found configuration file null
16:11:52.200 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:52.200296 18452 raft_consensus.cc:1181] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 790bf4a387214fc1a49bb862543a1ded [term 1 FOLLOWER]: Refusing update from remote peer 73407c178d394581974abe148305180e: Log matching property violated. Preceding OpId in replica: term: 0 index: 0. Preceding OpId from leader: term: 1 index: 1. (index mismatch)
16:11:52.200 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:52.200354 18187 raft_consensus.cc:1181] T e7e66e6f324b4e918f7b0e09f01b7dd5 P a023316415e24eaa8cf8c304ef236819 [term 1 FOLLOWER]: Refusing update from remote peer 73407c178d394581974abe148305180e: Log matching property violated. Preceding OpId in replica: term: 0 index: 0. Preceding OpId from leader: term: 1 index: 1. (index mismatch)
16:11:52.200 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:52.200794 18608 consensus_queue.cc:976] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [LEADER]: Connected to new peer: Peer: permanent_uuid: ""a023316415e24eaa8cf8c304ef236819"" member_type: VOTER last_known_addr { host: ""127.17.34.1"" port: 43175 }, Status: LMP_MISMATCH, Last received: 0.0, Next index: 1, Last known committed idx: 0, Time since last communication: 0.000s
16:11:52.202 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:52.202618 18608 consensus_queue.cc:976] T e7e66e6f324b4e918f7b0e09f01b7dd5 P 73407c178d394581974abe148305180e [LEADER]: Connected to new peer: Peer: permanent_uuid: ""790bf4a387214fc1a49bb862543a1ded"" member_type: VOTER last_known_addr { host: ""127.17.34.3"" port: 42215 }, Status: LMP_MISMATCH, Last received: 0.0, Next index: 1, Last known committed idx: 0, Time since last communication: 0.000s
16:11:52.774 [DEBUG - Time-limited test] (Configuration.java:629) Handling deprecation for all properties in config...
16:11:52.774 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.reducers.bytes.per.reducer
16:11:52.774 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.client.capability.check
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.storeManagerType
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.aux.jars.path
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.stagingdir
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggregate.stats.false.positive.probability
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.rack
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.default.partition.name
16:11:52.775 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.event.expiry.duration
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.mode.local.auto.input.files.max
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.broker.address.default
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.key.prefix
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.orc.time.counters
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.splits.ms.footer.cache.ppd.enabled
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.task.scale.memory.reserve-fraction.min
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.skewjoin.compiletime
16:11:52.776 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.smbjoin.cache.rows
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.mapjoin.overflow.repeated.threshold
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.event.message.factory
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.metrics.enabled
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.post.hooks
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.hs2.user.access
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for mapreduce.input.fileinputformat.split.minsize
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.quorum
16:11:52.777 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for stream.stderr.reporter.prefix
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.storage.storageDirectory
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.command.whitelist
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.zk.sm.connectionString
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.join.emit.interval
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.am.liveness.connection.timeout.ms
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.dynamic.semijoin.reduction.threshold
16:11:52.778 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.client.connect.retry.limit
16:11:52.780 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.xmx.headroom
16:11:52.780 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.dynamic.semijoin.reduction
16:11:52.780 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.prewarm.enabled
16:11:52.780 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.allocator.direct
16:11:52.780 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.io.rcfile.record.buffer.size
16:11:52.780 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.default.rcfile.serde
16:11:52.781 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.management.acl.blocked
16:11:52.781 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.schema.validateConstraints
16:11:52.781 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.createtable.owner.grants
16:11:52.781 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.multi.insert.move.tasks.share.dependencies
16:11:52.781 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.users.in.admin.role
16:11:52.784 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.max.partition.factor
16:11:52.785 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.enforce.stats
16:11:52.785 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.log.explain.output
16:11:52.785 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.skewjoin
16:11:52.786 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.default.fileformat
16:11:52.786 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.client.consistent.splits
16:11:52.786 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.optimized.hashtable.wbsize
16:11:52.788 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.session.lifetime
16:11:52.788 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.metastore.authorization.auth.reads
16:11:52.788 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.remove.identity.project
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.timedout.txn.reaper.start
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.cache.ttl
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.worker.threads
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.infer.bucket.sort.num.buckets.power.two
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.management.acl
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.future.timeout
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.max.idle.time
16:11:52.789 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.worker.keepalive.time
16:11:52.790 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.cookie.auth.enabled
16:11:52.790 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.delegation.token.lifetime
16:11:52.790 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.archive.intermediate.archived
16:11:52.790 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.ldap.guidKey
16:11:52.791 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.input.format
16:11:52.791 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.strict.checks.large.query
16:11:52.792 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.ats.hook.queue.capacity
16:11:52.803 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.bigtable.minsize.semijoin.reduction
16:11:52.803 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.localize.resource.num.wait.attempts
16:11:52.803 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.limit.optimize.enable
16:11:52.815 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.createtable.role.grants
16:11:52.815 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.decode.partition.name
16:11:52.815 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.exponential.backoff.slot.length
16:11:52.815 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.mapjoin.native.enabled
16:11:52.815 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compat
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.allocator.alloc.min
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.smallfiles.avgsize
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.client.user
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.metadata.password
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.alloc.size
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hbase.wal.enabled
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.logging.operation.enabled
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.lockmgr.zookeeper.default.partition.name
16:11:52.816 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.wait.queue.comparator.class.name
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.support.concurrency
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.output.service.port
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.cache.use.soft.references
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.file.max.footer
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.enabled
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.mode.prefix
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cli.print.header
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.task.scale.memory.reserve.fraction.max
16:11:52.817 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.tasklog.debug.timeout
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hashtable.loadfactor
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapred.local.mem
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.client.drop.partitions.using.expressions
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.communicator.listener.thread-count
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.auto.progress
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.dynamic.partition
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.container.max.java.heap.fraction
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.column.autogather
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.reducededuplication
16:11:52.818 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.am.liveness.heartbeat.interval.ms
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.index.filter.compact.minsize
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.decoding.metrics.percentiles.intervals
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.copyfile.maxsize
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.enabled
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.manager
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.groupby.position.alias
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.in.test
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.clean.until
16:11:52.819 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.txn.store.impl
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.hybridgrace.hashtable
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.reliable
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.use.groupby.shuffle
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.object.cache.enabled
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.map.groupby.sorted
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hashtable.initialCapacity
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.idle.operation.timeout
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.hdfs.read
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.server.connect.timeout
16:11:52.820 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.parallel.ops.in.session
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.transport.mode
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.path
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.groupby.limit.extrastep
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.mode.nosamplelist
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.use.ssl
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.sasl.qop
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.delta.num.threshold
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.log4j.file
16:11:52.821 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.map.aggr.hash.percentmemory
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.job.debug.capture.stacktraces
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.server.max.message.size
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cluster.delegation.token.store.zookeeper.acl
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.service.metrics.file.location
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.client.retry.delay.seconds
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.sample.seednumber
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.materializedview.fileformat
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapred.reduce.tasks.speculative.execution
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.groupby.flush.percent
16:11:52.822 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.limit.optimize.fetch.max
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.num.file.cleaner.threads
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.parallel
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.fail.compaction
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.submitviachild
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.blobstore.use.blobstore.as.scratchdir
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.service.metrics.class
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.udtf.auto.progress
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.archive.enabled
16:11:52.823 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.convert.join.bucket.mapjoin.tez
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.execution.engine
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.basePersistDirectory
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.allocator.mmap.path
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.container.size
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.download.permanent.fns
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.max.historic.queries
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.use.SSL
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.reducesink.new.enabled
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.max.num.delta
16:11:52.824 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.null.scan
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.history.retention.attempted
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.smalltable.filesize
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.query.string
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.port
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.join.use.nonstaged
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.keytab.file
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.initiator.failed.compacts.threshold
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.idle.session.check.operation
16:11:52.825 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.hybridgrace.minnumpartitions
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.keystore.path
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.server.tcp.keepalive
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.service.metrics.reporter
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.rpc.threads
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.io.rcfile.column.number.conf
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.map.aggr.hash.min.reduction
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.cpu
16:11:52.826 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.clean.extra.nodes
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.metadataonly
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.output.service.max.pending.writes
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.added.archives.path
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.insert.into.multilevel.dirs
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hmshandler.retry.attempts
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.execution.mode
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.query.lifetime.hooks
16:11:52.827 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.enable.grace.join.in.llap
16:11:52.828 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.conf.restricted.list
16:11:52.828 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.sortmerge.join.to.mapjoin
16:11:52.828 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.fetch.task.aggr
16:11:52.828 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.limittranspose
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.warehouse.subdir.inherit.perms
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.memory.mode
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.progress.timeout
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.returnpath.hiveop
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.select.threshold
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.threadpool.size
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.scratchdir
16:11:52.830 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.scratchdir.lock
16:11:52.839 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.keystore.password
16:11:52.839 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.debug.localtask
16:11:52.839 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.use.spnego
16:11:52.839 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.createtable.user.grants
16:11:52.851 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server.tcp.keepalive
16:11:52.851 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.service.metrics.file.frequency
16:11:52.851 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.ppd
16:11:52.851 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.script.maxerrsize
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.session.id
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.connect.timeout
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.join.noconditionaltask
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.input.format
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.fetch.column.stats
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.cleaner.run.interval
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.skewjoin.mapjoin.map.tasks
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.hs2.coordinator.enabled
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.schema.verification.record.version
16:11:52.852 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for mapreduce.job.reduces
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.scheduler.timeout.seconds
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.support.quoted.identifiers
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.filter.stats.reduction
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.initiator.on
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.fs.handler.class
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.task.factory
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.typecheck.on.insert
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.distinct.rewrite
16:11:52.853 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.authorization.storage.checks
16:11:52.854 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.orc.base.delta.ratio
16:11:52.854 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.fastpath
16:11:52.854 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.clear.dangling.scratchdir
16:11:52.854 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.builtin.udf.blacklist
16:11:52.854 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.fail.heartbeater
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.schema.validateTables
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.file.cleanup.delay.seconds
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.ppd.storage
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.management.rpc.port
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.local.fs.read
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.hybridgrace.bloomfilter
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.max.full
16:11:52.855 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.correlation
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.enabled
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.user.install.directory
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.insert.into.external.tables
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.groupby.checkinterval
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cli.print.current.db
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.retryable.sqlex.regex
16:11:52.856 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.io.exception.handlers
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.jobname.length
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.enforce.tree
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.stats.ndv.tuner
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.direct.sql.max.query.length
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.added.jars.path
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.bind.host
16:11:52.857 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.history.retention.failed
16:11:52.858 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.initialize.default.sessions
16:11:52.858 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.max.open.batch
16:11:52.859 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.check.interval
16:11:52.862 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.close.session.on.disconnect
16:11:52.862 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.ppd.windowing
16:11:52.862 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.query.id
16:11:52.862 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.transactional.table.scan
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.current.database
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.max.variable.length
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.start.cleanup.scratchdir
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.rcfile.use.explicit.header
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.initial.metadata.count.enabled
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.orc.split.strategy
16:11:52.863 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.async.exec.keepalive.time
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.default.serde
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.listbucketing
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.connection.basesleeptime
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.host
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.ds.connection.url.hook
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.partition.name.whitelist.pattern
16:11:52.864 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.query.result.fileformat
16:11:52.865 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.constant.propagation
16:11:52.865 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.transform.escape.input
16:11:52.865 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.splits.ms.footer.cache.enabled
16:11:52.865 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.point.lookup.min
16:11:52.865 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.pre.hooks
16:11:52.865 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.conf.validation
16:11:52.872 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.operator.id.env.var
16:11:52.872 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.added.files.path
16:11:52.872 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.Multithreaded
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.file.metadata.threads
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.rework.mapredwork
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.client.connect.retry.delay
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.reducers.max
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.service.refresh.interval.sec
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.fetch.task.conversion.threshold
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.limit.row.max.size
16:11:52.873 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.thrift.compact.protocol.enabled
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.max.output.size
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.rpc.server.address
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.join.noconditionaltask.size
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.operator.truncate.env
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.join.cache.size
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.driver.parallel.compilation
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.skewjoin.key
16:11:52.874 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.rdbms.initializeColumnInfo
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.remote.token.requires.signing
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.reloadable.aux.jars.path
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.bucket.pruning
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.cache.allow.synthetic.fileid
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hash.table.inflation.factor
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hmshandler.retry.interval
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggr.stats.hbase.ttl
16:11:52.875 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.local.scratchdir
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.max.message.size
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.job.credential.provider.path
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.gather.num.threads
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.archive.intermediate.original
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.mode.local.auto.inputbytes.max
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.enforce.vectorized
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.localtask.max.memory.usage
16:11:52.876 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.writeset.reaper.interval
16:11:52.877 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.use.vector.serde.deserialize
16:11:52.877 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.order.columnalignment
16:11:52.877 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hbase.snapshot.restoredir
16:11:52.877 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.output.service.send.buffer.size
16:11:52.877 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.worker.timeout
16:11:52.877 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.keystore.password
16:11:52.884 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.max.partitions
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compute.splits.in.am
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.new.job.grouping.set.cardinality
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.sessions.restricted.configs
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.kerberos.principal
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.schema.evolution
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.enforce.sortmergebucketmapjoin
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.direct.sql.max.elements.values.clause
16:11:52.885 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.smb.number.waves
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.max.writer.wait
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.allow.uber
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.llap.concurrent.queries
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.keystore.path
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for stream.stderr.reporter.enabled
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.indexer.partition.size.max
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.auth
16:11:52.886 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.int.timestamp.conversion.in.seconds
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.repl.task.factory
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.auto.reducer.parallelism
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.rawstore.impl
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.metastore.authorization.manager
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.splits.include.fileid
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.jar.path
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orderby.position.alias
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.communicator.num.threads
16:11:52.887 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggregate.stats.max.partitions
16:11:52.888 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.communicator.connection.sleep.between.retries.ms
16:11:52.888 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.mapjoin.native.multikey.only.enabled
16:11:52.888 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.limit.query.max.table.partition
16:11:52.888 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.service.metrics.hadoop2.component
16:11:52.888 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.yarn.shuffle.port
16:11:52.888 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.logging.operation.level
16:11:52.890 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.cache.level2.type
16:11:52.891 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.direct.sql.batch.size
16:11:52.891 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.direct.sql.max.elements.in.clause
16:11:52.891 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.stats.ndv.densityfunction
16:11:52.891 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.passiveWaitTimeMs
16:11:52.891 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.load.dynamic.partitions.thread
16:11:52.891 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.exec.print.summary
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.compress.intermediate
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.expression.proxy
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.recordreader
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.autogather
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.sort.dynamic.partition
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.thrift.framed.transport.enabled
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.reduce.groupby.enabled
16:11:52.892 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.dbclass
16:11:52.893 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.indexer.segments.granularity
16:11:52.893 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.response.header.size
16:11:52.893 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.conf.internal.variable.list
16:11:52.893 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.concatenate.check.index
16:11:52.893 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.rpc.server.port
16:11:52.897 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.connectionPoolingType
16:11:52.897 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.rcfile.use.sync.cache
16:11:52.897 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.cache.pinobjtypes
16:11:52.897 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.limittranspose.reductionpercentage
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.fileformat.check
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.default.aggregator
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.repl.cm.enabled
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.keystore.path
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.client.retry.limit
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.resultset.serialize.in.tasks
16:11:52.898 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cluster.delegation.token.store.zookeeper.connectString
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.infer.bucket.sort
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.ttl
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.submit.local.task.via.child
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.index.compact.file
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cluster.delegation.token.store.zookeeper.znode
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.query.timeout.seconds
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.splits.directory.batch.ms
16:11:52.899 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.service.metrics.hadoop2.frequency
16:11:52.900 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.cache.max.reader.wait
16:11:52.900 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.followby.map.aggr.hash.percentmemory
16:11:52.900 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.lock.manager
16:11:52.911 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.exec.inplace.progress
16:11:52.911 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.variable.substitute.depth
16:11:52.911 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapper.cannot.span.multiple.partitions
16:11:52.911 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.ignore.mapjoin.hint
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.table.parameters.default
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.sampling.orderby.percent
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.lock.mapred.only.operation
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.min.partition.factor
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.kerberos.keytab.file
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.groupby.mapaggr.checkinterval
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.ConnectionUserName
16:11:52.912 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.scheduler.node.reenable.max.timeout.ms
16:11:52.913 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.max.open.txns
16:11:52.913 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.extended
16:11:52.914 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.sortmerge.join.reduce.side
16:11:52.914 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.operator.env.blacklist
16:11:52.914 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.webui.spnego.principal
16:11:52.914 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.in.tez.test
16:11:52.914 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.bucketmapjoin.sortedmerge
16:11:52.923 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.zookeeper.publish.configs
16:11:52.923 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.schema.autoCreateAll
16:11:52.923 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.index.groupby
16:11:52.923 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.sortmerge.join
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.join.hashtable.max.entries
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.truststore.path
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.sessions.init.threads
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.createtable.group.grants
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.authorization.storage.check.externaltable.drop
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.zk.registry.user
16:11:52.924 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.rcfile.block.level
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.execution.mode
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.cnf.maxnodes
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.adaptor.usage.mode
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.materializedview.rewriting
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.log.level
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.mapfiles
16:11:52.925 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.client.socket.lifetime
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.max.variance
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for fs.har.impl
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.ldap.groupMembershipKey
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.catalog.cache.size
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.hybridgrace.memcheckfrequency
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.index.filter.compact.maxsize
16:11:52.926 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.counters.pull.interval
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.end.function.listeners
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.show.warnings
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.downloaded.resources.dir
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.fshandler.threads
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compute.query.using.stats
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.lazysimple.extended_boolean_literal
16:11:52.927 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.splits.include.file.footer
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.error.on.empty.partition
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.max.bloom.filter.entries
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hadoop.bin.path
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.metadata.fraction
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.materializedview.serde
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.autogen.columnalias.prefix.includefuncname
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.port
16:11:52.928 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.task.scheduler.wait.queue.size
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggr.stats.cache.entries
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.max.created.files
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cli.prompt
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.deserialization.factor
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metadata.export.location
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.operational.properties
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggr.stats.memory.ttl
16:11:52.929 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.NonTransactionalRead
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.rpc.port
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.nonvector.wrapper.enabled
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exim.strict.repl.tables
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.use.vectorized.input.format
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.collect.tablekeys
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggregate.stats.cache.size
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.cte.materialize.threshold
16:11:52.930 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.display.partition.cols.separately
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.disallow.incompatible.col.type.changes
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.async.exec.shutdown.timeout
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.dummystats.aggregator
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.mode
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.querylog.enable.plan.progress
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.cache.clean.until
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.warehouse.dir
16:11:52.931 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.semijoin.conversion
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.collect.scancols
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.port
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.dummystats.publisher
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.dynamic.partition.pruning
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.uris
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.cookie.is.httponly
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.querylog.location
16:11:52.932 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.querylog.plan.progress.interval
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.job.debug.timeout
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.partition.inherit.table.properties
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cluster.delegation.token.store.class
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.metrics.enabled
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.repl.rootdir
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.client.port
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.cookie.max.age
16:11:52.933 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.alias
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.default.queues
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapred.partitioner
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.async.log.enabled
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.limit.partition.request
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.logger
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.entity.capture.transform
16:11:52.934 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.allow.udf.load.on.demand
16:11:52.935 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.index.blockfilter.file
16:11:52.935 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cli.tez.session.async
16:11:52.935 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.table.type.mapping
16:11:52.935 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.event.db.listener.timetolive
16:11:52.935 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.filter.hook
16:11:52.936 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.union.remove
16:11:52.936 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.global.init.file.location
16:11:52.936 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.bloom.filter.factor
16:11:52.947 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.am-reporter.max.threads
16:11:52.958 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.try.direct.sql
16:11:52.958 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.failure.retries
16:11:52.958 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.intermediate.compression.type
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.analyze.stmt.collect.partlevel.stats
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hbase.generatehfiles
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.join.factor
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.pre.event.listeners
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.map.fair.scheduler.queue
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.localize.resource.wait.interval
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.use.file.size.for.mapjoin
16:11:52.959 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.sasl.enabled
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.strict.checks.bucketing
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.rpc.query.plan
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.truststore.password
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.bucket.pruning.compat
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.spnego.principal
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.mapredfiles
16:11:52.960 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cache.expr.evaluation
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for yarn.bin.path
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.counters.group.name
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.transactionIsolation
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.spnego.keytab
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.groupby.skewindata
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.batch.retrieve.max
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.entity.separator
16:11:52.961 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.binary.record.max.length
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.max.dynamic.partitions
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.check.memory.rows
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.task.preemption.metrics.intervals
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.allocator.arena.count
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.shuffle.dir.watcher.enabled
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.use.SSL
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.communicator.connection.timeout.ms
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.execute.setugi
16:11:52.962 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.index.compact.query.max.entries
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.transpose.aggr.join
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for mapreduce.input.fileinputformat.split.maxsize
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.bucket.cache.size
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.drop.ignorenonexistent
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.maxTries
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.metadata.base
16:11:52.963 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.dynamic.partition.pruning.max.data.size
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggr.stats.invalidator.frequency
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.serdes.using.metastore.for.schema
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.allocator.mmap
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.use.lrfu
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.coordinator.address.default
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.resultset.max.fetch.size
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.sparkfiles
16:11:52.964 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exim.uri.scheme.whitelist
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.conf.hidden.list
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.query.redactor.hooks
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.log4j.file
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.io.sarg.cache.max.weight.mb
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.plan
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.clear.dangling.scratchdir.interval
16:11:52.965 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.serde
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.sleep.time
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.ddl.createtablelike.properties.whitelist
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.node
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.bucketmapjoin
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.principal
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.use.row.serde.deserialize
16:11:52.966 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.compile.lock.timeout
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.mapjoin.minmax.enabled
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.event.clean.freq
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.session.hook
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.sortmerge.join.bigtable.selection.policy
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stageid.rearrange
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.timedout.txn.reaper.interval
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.temporary.table.storage
16:11:52.967 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.optimized.hashtable
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.groupby.maxentries
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.webui.spnego.keytab
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.aggregate.stats.max.variance
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authenticator.manager
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.client.stats.publishers
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.io.rcfile.record.interval
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.lrfu.lambda
16:11:52.968 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.fetch.task.conversion
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.builtin.udf.whitelist
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.metadata.db.type
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.rpc.max.size
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.spnego.principal
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.authz.sstd.hs2.mode
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.async.exec.threads
16:11:52.969 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.output.stream.timeout
16:11:52.970 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.ConnectionPassword
16:11:52.970 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.transactional.events.mem
16:11:52.972 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.resultset.default.fetch.size
16:11:52.972 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.zk.sm.keytab.file
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.size
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.session.silent
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.repl.cm.retain
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.cardinality.check
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.min.worker.threads
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.metadata.uri
16:11:52.973 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.ldap.groupClassKey
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.sessions.per.default.queue
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.point.lookup
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.port
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.allow.permanent.fns
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.logging.operation.log.location
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.web.ssl
16:11:52.974 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.ConnectionURL
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.semantic.analyzer.hook
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hmshandler.force.reload.conf
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.job.queue
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.schema.validateColumns
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.prewarm.numcontainers
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.identifierFactory
16:11:52.975 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cli.errors.ignore
16:11:52.976 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.multigroupby.singlereducer
16:11:52.984 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.manager.dump.lock.state.on.acquire.timeout
16:11:52.985 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.history.retention.succeeded
16:11:52.985 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.sampling.orderby.number
16:11:52.985 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.timeout
16:11:52.985 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.fetch.partition.stats
16:11:52.985 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.use.fileid.path
16:11:52.985 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.server.max.threads
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.limit.optimize.limit.file
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.script.allow.partial.consumption
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.try.direct.sql.ddl
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.hybridgrace.minwbsize
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.namespace
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.long.polling.timeout
16:11:52.986 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.worker.keepalive.time
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.acl.blocked
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.enforce.bucketmapjoin
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.allow.user.substitution
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.index.autoupdate
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.ssl.protocol.blacklist
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.dynamic.partition.pruning
16:11:52.987 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.max.dynamic.partitions.pernode
16:11:52.988 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.abortedtxn.threshold
16:11:52.988 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.map.aggr
16:11:52.988 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.PersistenceManagerFactoryClass
16:11:52.990 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.lock.numretries
16:11:52.990 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.auto.convert.join
16:11:52.990 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.support.dynamic.service.discovery
16:11:52.990 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.metadata.username
16:11:52.990 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.slice.row.count
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.zk.sm.principal
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.optimized.hashtable.probe.percent
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.cache.level2
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.select.distribute
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.am.use.fqdn
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.kerberos.principal
16:11:52.991 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.rdbms.useLegacyNativeValueStrategy
16:11:52.992 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.scheduler.node.reenable.min.timeout.ms
16:11:52.992 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.support.special.characters.tablename
16:11:52.992 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.validate.acls
16:11:52.992 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mv.files.thread
16:11:53.000 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.skip.compile.udf.check
16:11:53.001 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.index.compact.binary.search
16:11:53.001 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.cookie.is.secure
16:11:53.001 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.orcfile.stripe.level
16:11:53.001 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.reorder.nway.joins
16:11:53.001 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.compress.output
16:11:53.001 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.list.num.entries
16:11:53.002 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.sqlstd.confwhitelist.append
16:11:53.002 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.vector.serde.enabled
16:11:53.002 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.explain.dependency.append.tasktype
16:11:53.002 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.bucketingsorting
16:11:53.002 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.login.timeout
16:11:53.002 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.scratch.dir.permission
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.repl.cm.interval
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.hashtable.key.count.adjustment
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.failure.hooks
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.integral.jdo.pushdown
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.keytab.file
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.sleep.interval.between.start.attempts
16:11:53.003 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.client.socket.timeout
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.DetachAllOnCommit
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.yarn.container.mb
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for javax.jdo.option.ConnectionDriverName
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.delta.pct.threshold
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.vectorized.execution.reduce.enabled
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.http.read.timeout
16:11:53.004 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.orm.retrieveMapNullsAsEmptyStrings
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.blobstore.optimizations.enabled
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.orc.gap.cache
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.tezfiles
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.index.filter
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.authorization.sqlstd.confwhitelist
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.dynamic.partition.hashjoin
16:11:53.005 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.copyfile.maxnumfiles
16:11:53.006 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.map.num.entries
16:11:53.006 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.dynamic.partition.pruning.max.event.size
16:11:53.006 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.enable
16:11:53.006 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.formats
16:11:53.006 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.mode.local.auto
16:11:53.006 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.reducededuplication.min.reducer
16:11:53.007 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.max.start.attempts
16:11:53.007 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.dynamic.partition.mode
16:11:53.007 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.max.worker.threads
16:11:53.007 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.network
16:11:53.007 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.fpp
16:11:53.007 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.driver.run.hooks
16:11:53.008 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.http.numConnection
16:11:53.008 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.session.history.enabled
16:11:53.008 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.unlock.numretries
16:11:53.008 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.num.executors
16:11:53.008 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.task.scheduler.enable.preemption
16:11:53.016 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.groupby
16:11:53.017 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.cache.max.full
16:11:53.017 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.connection.class
16:11:53.017 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.sessions.custom.queue.allowed
16:11:53.017 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.service.principal
16:11:53.017 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.check.crossproducts
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server.read.socket.timeout
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.perf.logger
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.plugin.pluginRegistryBundleCheck
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.slice.lrr
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.parallel.thread.number
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.dbaccess.ssl.properties
16:11:53.018 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.client.password
16:11:53.019 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.max.reader.wait
16:11:53.019 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.security.metastore.authenticator.manager
16:11:53.019 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.hbase.cache.max.writer.wait
16:11:53.019 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.default.fileformat.managed
16:11:53.019 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.index.compact.file.ignore.hdfs
16:11:53.019 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.thrift.http.request.header.size
16:11:53.020 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.webui.max.threads
16:11:53.020 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.limittranspose.reductiontuples
16:11:53.020 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.rollbacktxn
16:11:53.020 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.acl
16:11:53.020 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.scheduler.num.schedulable.tasks.per.node
16:11:53.020 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.memory.size
16:11:53.021 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.strict.checks.type.safety
16:11:53.021 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.async.exec.async.compile
16:11:53.021 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.auto.max.input.size
16:11:53.021 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.limit.pushdown.memory.usage
16:11:53.021 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.enable.memory.manager
16:11:53.021 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.batch.retrieve.table.partition.max
16:11:53.022 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.msck.repair.batch.size
16:11:53.026 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.blobstore.supported.schemes
16:11:53.026 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.dynamic.partition.pruning.max.data.size
16:11:53.026 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metadata.move.exported.metadata.to.trash
16:11:53.027 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cli.pretty.output.num.cols
16:11:53.027 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.splits.allow.synthetic.fileid
16:11:53.027 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.session.timeout
16:11:53.027 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.fetch.output.serde
16:11:53.027 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.log.trace.id
16:11:53.027 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.resultset.use.unique.column.names
16:11:53.028 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.skewjoin.mapjoin.min.split
16:11:53.028 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.zookeeper.connection.max.retries
16:11:53.028 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.session.check.interval
16:11:53.028 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for parquet.memory.pool.ratio
16:11:53.028 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.filter.in.factor
16:11:53.028 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.use.op.stats
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.client.stats.counters
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.ppd.recognizetransivity
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.spnego.keytab
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.io.rcfile.tolerate.corruptions
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.secret.bits
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.input.listing.max.threads
16:11:53.029 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.autogen.columnalias.prefix.label
16:11:53.030 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.event.listeners
16:11:53.030 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.tez.session.lifetime.jitter
16:11:53.030 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.compute.splits.num.threads
16:11:53.030 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.rowoffset
16:11:53.030 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.default.publisher
16:11:53.030 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.web.port
16:11:53.031 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.script.recordwriter
16:11:53.031 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.ppd.remove.duplicatefilters
16:11:53.031 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.keystore.password
16:11:53.031 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.strict.checks.cartesian.product
16:11:53.031 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.manager
16:11:53.031 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.variable.substitute
16:11:53.043 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.rpc.num.handlers
16:11:53.043 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.vcpus.per.instance
16:11:53.043 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.intermediate.compression.codec
16:11:53.044 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.server.min.threads
16:11:53.044 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.count.open.txns.interval
16:11:53.044 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.aggregate.stats.cache.enabled
16:11:53.044 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.min.bloom.filter.entries
16:11:53.044 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.partition.columns.separate
16:11:53.044 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.init.hooks
16:11:53.045 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.dml.events
16:11:53.045 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.log.every.n.records
16:11:53.047 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.orc.cache.stripe.details.mem.size
16:11:53.047 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.heartbeat.interval
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.index.compact.query.max.size
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.heartbeat.threadpool.size
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.scheduler.locality.delay
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.lock.sleep.between.retries
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.repl.cmrootdir
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.test.mode.samplefreq
16:11:53.048 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.task.scheduler.node.disable.backoff.factor
16:11:53.049 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication
16:11:53.049 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.map.aggr.hash.force.flush.memory.threshold
16:11:53.049 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.async.exec.wait.queue.size
16:11:53.049 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.explain.user
16:11:53.049 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.schema.verification
16:11:53.049 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.exec.inplace.progress
16:11:53.050 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.am.liveness.connection.sleep.between.retries.ms
16:11:53.050 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.connect.retries
16:11:53.050 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.working.directory
16:11:53.050 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.archive.intermediate.extracted
16:11:53.050 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.token.signature
16:11:53.051 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.memory.per.instance.mb
16:11:53.051 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.hdfs.write
16:11:53.051 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.authentication.kerberos.keytab
16:11:53.052 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.cpu.vcores
16:11:53.052 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.msck.path.validation
16:11:53.052 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.parquet.timestamp.skip.conversion
16:11:53.053 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.task.scale.memory.reserve.fraction
16:11:53.053 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.size.per.task
16:11:53.053 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for datanucleus.connectionPool.maxPoolSize
16:11:53.056 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.merge.nway.joins
16:11:53.056 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.txn.strict.locking.mode
16:11:53.056 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.compactor.history.reaper.interval
16:11:53.057 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.client.rpc.sasl.mechanisms
16:11:53.057 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.optimize.sampling.orderby
16:11:53.057 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.script.trust
16:11:53.057 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.mapjoin.followby.gby.localtask.max.memory.usage
16:11:53.057 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.spark.job.monitor.timeout
16:11:53.057 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.exec.show.job.failure.debug.info
16:11:53.058 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.groupby.orderby.position.alias
16:11:53.058 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.cbo.costmodel.local.fs.write
16:11:53.058 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.metastore.transactional.event.listeners
16:11:53.058 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.encode.vector.serde.async.enabled
16:11:53.058 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.tez.input.generate.consistent.splits
16:11:53.058 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.in.place.progress
16:11:53.059 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.ndv.error
16:11:53.059 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.stats.atomic
16:11:53.059 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.zookeeper.namespace
16:11:53.059 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.enable.doAs
16:11:53.059 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.druid.indexer.memory.rownum.max
16:11:53.059 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.daemon.work.dirs
16:11:53.060 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.idle.session.timeout
16:11:53.060 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.server2.xsrf.filter.enabled
16:11:53.060 [DEBUG - Time-limited test] (Configuration.java:634) Handling deprecation for hive.llap.io.allocator.alloc.max
16:11:53.809 [ERROR - Time-limited test] (MetaStoreUtils.java:1387) Got exception: java.lang.ClassCastException java.base/[Ljava.lang.Object; cannot be cast to java.base/[Ljava.net.URI;
java.lang.ClassCastException: java.base/[Ljava.lang.Object; cannot be cast to java.base/[Ljava.net.URI;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:200)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:129)
	at org.apache.kudu.client.TestHiveMetastoreIntegration.testOverrideTableOwner(TestHiveMetastoreIntegration.java:64)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:844)
16:11:53.828 [ERROR - Time-limited test] (MetaStoreUtils.java:1388) Converting exception to MetaException
16:11:53.838 [DEBUG - New I/O worker #1] (Connection.java:688) [peer master-127.17.34.62:40137(127.17.34.62:40137)] cleaning up while in state READY due to: connection disconnected
16:11:53.843 [DEBUG - New I/O worker #2] (Connection.java:688) [peer master-127.17.34.61:33927(127.17.34.61:33927)] cleaning up while in state READY due to: connection disconnected
16:11:53.850 [DEBUG - New I/O worker #3] (Connection.java:688) [peer master-127.17.34.60:36165(127.17.34.60:36165)] cleaning up while in state READY due to: connection disconnected
16:11:53.851 [DEBUG - Test worker] (AsyncKuduClient.java:2231) Releasing all remaining resources
16:11:53.858 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:53.858582 17544 external_mini_cluster.cc:1043] Killing /home/adar/Source/kudu/build/debug/bin/kudu-tserver with pid 18075
16:11:53.863 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.862948 18388 connection.cc:512] client connection to 127.17.34.1:43175 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.1:43175): Connection reset by peer (error 104)
16:11:53.864 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.862958 17869 connection.cc:512] client connection to 127.17.34.1:43175 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.1:43175): Connection reset by peer (error 104)
16:11:53.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.862970 17960 connection.cc:512] server connection from 127.17.34.1:44333 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.1:44333): Connection reset by peer (error 104)
16:11:53.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.863037 17866 connection.cc:512] server connection from 127.17.34.1:42523 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.1:42523): Connection reset by peer (error 104)
16:11:53.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.863173 18025 connection.cc:512] server connection from 127.17.34.1:39223 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.1:39223): Connection reset by peer (error 104)
16:11:53.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.863359 18261 connection.cc:512] client connection to 127.17.34.1:43175 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.1:43175): Connection reset by peer (error 104)
16:11:53.865 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:53.864557 17544 external_mini_cluster.cc:1043] Killing /home/adar/Source/kudu/build/debug/bin/kudu-tserver with pid 18240
16:11:53.868 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.868619 18386 connection.cc:512] client connection to 127.17.34.2:46251 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.2:46251): Connection reset by peer (error 104)
16:11:53.869 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.868625 17867 connection.cc:512] client connection to 127.17.34.2:46251 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.2:46251): Connection reset by peer (error 104)
16:11:53.869 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.868707 17869 connection.cc:512] server connection from 127.17.34.2:57893 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.2:57893): Connection reset by peer (error 104)
16:11:53.869 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.868806 18387 connection.cc:512] server connection from 127.17.34.2:46633 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.2:46633): Connection reset by peer (error 104)
16:11:53.869 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.868818 18023 connection.cc:512] server connection from 127.17.34.2:35465 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.2:35465): Connection reset by peer (error 104)
16:11:53.869 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.868973 17959 connection.cc:512] server connection from 127.17.34.2:35725 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.2:35725): Connection reset by peer (error 104)
16:11:53.871 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:53.871448 17544 external_mini_cluster.cc:1043] Killing /home/adar/Source/kudu/build/debug/bin/kudu-tserver with pid 18374
16:11:53.880 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.874941 17958 connection.cc:512] server connection from 127.17.34.3:32883 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.3:32883): Connection reset by peer (error 104)
16:11:53.880 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.874941 17867 connection.cc:512] server connection from 127.17.34.3:46831 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.3:46831): Connection reset by peer (error 104)
16:11:53.880 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:53.875028 17544 external_mini_cluster.cc:1043] Killing /home/adar/Source/kudu/build/debug/bin/kudu-master with pid 17858
16:11:53.880 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.875032 18022 connection.cc:512] server connection from 127.17.34.3:33967 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.3:33967): Connection reset by peer (error 104)
16:11:53.880 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.880028 17551 connection.cc:512] client connection to 127.17.34.62:40137 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.62:40137): Connection reset by peer (error 104)
16:11:53.882 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.882154 18025 connection.cc:512] server connection from 127.0.0.1:35446 recv error: Network error: failed to read from TLS socket (remote: 127.0.0.1:35446): Connection reset by peer (error 104)
16:11:53.882 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.882560 18024 connection.cc:512] client connection to 127.17.34.62:40137 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.62:40137): Connection reset by peer (error 104)
16:11:53.882 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.882561 17960 connection.cc:512] server connection from 127.0.0.1:34516 recv error: Network error: failed to read from TLS socket (remote: 127.0.0.1:34516): Connection reset by peer (error 104)
16:11:53.883 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.882959 17960 connection.cc:512] client connection to 127.17.34.62:40137 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.62:40137): Connection reset by peer (error 104)
16:11:53.890 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:53.890909 17544 external_mini_cluster.cc:1043] Killing /home/adar/Source/kudu/build/debug/bin/kudu-master with pid 17919
16:11:53.895 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.895766 17551 connection.cc:512] client connection to 127.17.34.61:33927 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.61:33927): Connection reset by peer (error 104)
16:11:53.896 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.895802 18023 connection.cc:512] client connection to 127.17.34.61:33927 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.61:33927): Connection reset by peer (error 104)
16:11:53.896 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) I1216 16:11:53.895889 17544 external_mini_cluster.cc:1043] Killing /home/adar/Source/kudu/build/debug/bin/kudu-master with pid 18012
16:11:53.900 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) W1216 16:11:53.900543 17551 connection.cc:512] client connection to 127.17.34.60:36165 recv error: Network error: failed to read from TLS socket (remote: 127.17.34.60:36165): Connection reset by peer (error 104)
16:11:53.983 [INFO - cluster stderr printer] (MiniKuduCluster.java:535) Shutting down hive metastore.
16:11:54.510 [ERROR - Test worker] (RetryRule.java:80) testOverrideTableOwner(org.apache.kudu.client.TestHiveMetastoreIntegration): failed attempt 1
MetaException(message:Got exception: java.lang.ClassCastException java.base/[Ljava.lang.Object; cannot be cast to java.base/[Ljava.net.URI;)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1389)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:204)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:129)
	at org.apache.kudu.client.TestHiveMetastoreIntegration.testOverrideTableOwner(TestHiveMetastoreIntegration.java:64)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:844)
16:11:54.511 [ERROR - Test worker] (RetryRule.java:83) testOverrideTableOwner(org.apache.kudu.client.TestHiveMetastoreIntegration): giving up after 1 attempts
{noformat}",2018-12-17T00:16:22.449+0000,2020-06-02T19:18:18.976+0000,Fixed,Major
OOZIE-1526,Oozie does not work with a secure HA JobTracker or ResourceManager,OOZIE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12375164'>, <JIRA IssueLink: id='12375783'>]","HadoopAccessorService#getMRTokenRenewerInternal handles getting the delegation token for the JT/RM.  OOZIE-1159 modified this slightly and it now tries to parse the JT/RM address to get the hostname.  However, if you try to use JT HA (which isn't in vanilla Hadoop), it uses a logical name (e.g. ""ha-jt-uri"") just like we do with HDFS HA.  As such, when HadoopAccessorService tries to do
{code:java}
String addr = NetUtils.createSocketAddr(target).getHostName();
{code}
it will get an exception:
{noformat}
Caused by: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: ha-jt-uri
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:211)
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:163)
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:152)
	at org.apache.oozie.service.HadoopAccessorService.getMRTokenRenewerInternal(HadoopAccessorService.java:484)
	at org.apache.oozie.service.HadoopAccessorService.getMRDelegationTokenRenewer(HadoopAccessorService.java:463)
	at org.apache.oozie.service.HadoopAccessorService.createJobClient(HadoopAccessorService.java:374)
	at org.apache.oozie.action.hadoop.JavaActionExecutor.createJobClient(JavaActionExecutor.java:991)
	at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:743)
	... 10 more
{noformat}
because there isn't a host or port in the logical name.  

Once RM HA is done (which will be in vanilla Hadoop), it will use a logical name just like JT HA, and will run into the same problem.  I think we can fix this by having it fall back to the old behavior when it gets the IllegalArgumentException.  ",2013-09-12T20:37:35.498+0000,2014-12-09T00:45:15.418+0000,Fixed,Major
ZOOKEEPER-2709,"Clarify documentation around ""auth"" ACL scheme",ZOOKEEPER,Task,Closed,[],1,[<JIRA IssueLink: id='12496189'>],"We recently found up in HBASE-17717 that we were incorrectly setting an ACL on our ""sensitive"" znodes after the output of {{getACL}} on these nodes didn't match what was expected.

In referencing the documentation about how the {{auth}} ACL scheme was supposed to work, it was unclear if it was a ZooKeeper bug or an HBase bug. After reading some ZooKeeper code, we found that it was an HBase bug, but it would be nice to clarify the docs around this ACL scheme.",2017-03-03T21:28:28.429+0000,2017-05-18T03:44:03.017+0000,Fixed,Minor
TEZ-4238,Check null mrReader in MRInput.close,TEZ,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12600972'>],"In HIVE-24207, an optimization is introduced on task side, where a task can exit quickly if it's needed according to an object cache value, which reflects the current records already pushed through the limit operator. 
The AM side optimization will more likely be TEZ-2103 for this scenario.
Early exit can cause uninitialized inputs, that's what this patch is meant to solve.",2020-10-05T15:03:13.678+0000,2022-01-12T12:49:09.171+0000,Fixed,Major
SLIDER-599,When application is created as user hdfs need to call destroy twice to delete the hdfs folder for the app,SLIDER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12400305'>],"It was also reported by another user. This is not a critical issue as it is not expected that application be created as user ""hdfs"".

Assigning to check if there is any other issue hiding behind this symptom.

{noformat}
[hdfs@c6403 bin]$ ./slider destroy cl1
2014-11-01 04:34:06,112 [main] INFO  impl.TimelineClientImpl - Timeline service address: http://c6403.ambari.apache.org:8188/ws/v1/timeline/
2014-11-01 04:34:07,161 [main] WARN  shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
2014-11-01 04:34:07,172 [main] INFO  client.RMProxy - Connecting to ResourceManager at c6403.ambari.apache.org/192.168.64.103:8050
2014-11-01 04:34:07,516 [main] INFO  zk.BlockingZKWatcher - waiting for ZK event
2014-11-01 04:34:07,568 [main-EventThread] INFO  zk.BlockingZKWatcher - ZK binding callback received
2014-11-01 04:34:07,572 [main] INFO  client.SliderClient - Deleting zookeeper path /services/slider/users/hdfs/cl1
2014-11-01 04:34:07,852 [main] INFO  imps.CuratorFrameworkImpl - Starting
2014-11-01 04:34:07,942 [main-EventThread] INFO  state.ConnectionStateManager - State change: CONNECTED
2014-11-01 04:34:07,943 [ConnectionStateManager-0] WARN  state.ConnectionStateManager - There are no ConnectionStateListeners registered.
2014-11-01 04:34:08,969 [main] INFO  client.SliderClient - Destroyed cluster cl1
2014-11-01 04:34:08,977 [main] INFO  util.ExitUtil - Exiting with status 0
{noformat}

{noformat}
[hdfs@c6403 bin]$ ./slider create cl1 --template /usr/work/hbase/appConfig.json --resources /usr/work/hbase/resources.json
2014-11-01 04:35:12,816 [main] INFO  impl.TimelineClientImpl - Timeline service address: http://c6403.ambari.apache.org:8188/ws/v1/timeline/
2014-11-01 04:35:13,561 [main] WARN  shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
2014-11-01 04:35:13,568 [main] INFO  client.RMProxy - Connecting to ResourceManager at c6403.ambari.apache.org/192.168.64.103:8050
2014-11-01 04:35:14,028 [main] INFO  zk.BlockingZKWatcher - waiting for ZK event
2014-11-01 04:35:14,052 [main-EventThread] INFO  zk.BlockingZKWatcher - ZK binding callback received
2014-11-01 04:35:14,063 [main] INFO  agent.AgentClientProvider - Validating app definition .slider/package/HBASE/slider-hbase-app-package-0.98.4.2.2.0.0-1623-hadoop2.zip
2014-11-01 04:35:14,064 [main] INFO  agent.AgentUtils - Reading metainfo at .slider/package/HBASE/slider-hbase-app-package-0.98.4.2.2.0.0-1623-hadoop2.zip
2014-11-01 04:35:14,299 [main] INFO  tools.SliderUtils - Reading metainfo.xml of size 6909
2014-11-01 04:35:14,447 [main] ERROR tools.CoreFileSystem - Dir hdfs://c6403.ambari.apache.org:8020/user/hdfs/.slider/cluster/cl1 exists: hdfs://c6403.ambari.apache.org:8020/user/hdfs/.slider/cluster/cl1/database	0

2014-11-01 04:35:14,448 [main] ERROR main.ServiceLauncher - Application Instance dir already exists: hdfs://c6403.ambari.apache.org:8020/user/hdfs/.slider/cluster/cl1
2014-11-01 04:35:14,450 [main] INFO  util.ExitUtil - Exiting with status 75
{noformat}

{noformat}
[hdfs@c6403 bin]$ hdfs dfs -ls /user/hdfs/.slider/cluster
Found 1 items
drwxr-xr-x   - hdfs hdfs          0 2014-11-01 04:34 /user/hdfs/.slider/cluster/cl1
{noformat}

{noformat}
[hdfs@c6403 bin]$ ./slider destroy cl1
2014-11-01 04:37:25,003 [main] INFO  impl.TimelineClientImpl - Timeline service address: http://c6403.ambari.apache.org:8188/ws/v1/timeline/
2014-11-01 04:37:25,682 [main] WARN  shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
2014-11-01 04:37:25,692 [main] INFO  client.RMProxy - Connecting to ResourceManager at c6403.ambari.apache.org/192.168.64.103:8050
2014-11-01 04:37:25,965 [main] INFO  zk.BlockingZKWatcher - waiting for ZK event
2014-11-01 04:37:25,989 [main-EventThread] INFO  zk.BlockingZKWatcher - ZK binding callback received
2014-11-01 04:37:25,993 [main] INFO  client.SliderClient - Deleting zookeeper path /services/slider/users/hdfs/cl1
2014-11-01 04:37:26,037 [main] INFO  imps.CuratorFrameworkImpl - Starting
2014-11-01 04:37:26,099 [main-EventThread] INFO  state.ConnectionStateManager - State change: CONNECTED
2014-11-01 04:37:26,100 [ConnectionStateManager-0] WARN  state.ConnectionStateManager - There are no ConnectionStateListeners registered.
2014-11-01 04:37:27,107 [main] INFO  client.SliderClient - Destroyed cluster cl1
2014-11-01 04:37:27,109 [main] INFO  util.ExitUtil - Exiting with status 0
{noformat}

{noformat}
[hdfs@c6403 bin]$ hdfs dfs -ls /user/hdfs/.slider/cluster
[hdfs@c6403 bin]$
{noformat}",2014-11-01T04:42:47.530+0000,2014-11-02T06:13:31.643+0000,Invalid,Major
PHOENIX-5993,HBase 2.2.5 public maven artifacts are incompatible with Hadoop 3,PHOENIX,Bug,Resolved,[],6,"[<JIRA IssueLink: id='12592872'>, <JIRA IssueLink: id='12593732'>, <JIRA IssueLink: id='12592873'>, <JIRA IssueLink: id='12593679'>, <JIRA IssueLink: id='12593750'>, <JIRA IssueLink: id='12594291'>]","The HBase artifacts downloadable from maven central are built with  Hadoop 2.x 

However, up to now, we could use these artifacts with Hadoop 3.0 and they worked. (Or we just didn't hit the incompatibilities in our test suite.)

This seems to have changed with 2.2.5, as the public maven artifact doesn't work with Hadoop 3.0.3 or 3.1.2 . 

This is a known issue in HBase, and documented, but this means that 
 * Any client JAR we'd build with Hbase 2.2.5 would have the same problem.
 * We cannot run our tests with Hbase 2.2.5

HBase's suggested solution is to rebuild HBase with the Hadoop version used in the cluster, and use that. This, however doesn't fit into our test or distribution process.

",2020-07-07T07:55:27.119+0000,2020-07-31T07:05:41.794+0000,Fixed,Major
THRIFT-2660,Validate the bytes received in TSaslTransport,THRIFT,Bug,Closed,[],2,"[<JIRA IssueLink: id='12395194'>, <JIRA IssueLink: id='12393980'>]","In TSaslTransport#receiveSaslMessage, we are doing two things incorrectly:

- Not validating the status byte code.
- Not validating the decoded payload size integer before allocating a whole array with it.

The latter especially is bad when a network security software sends a thrift server port some garbage data, causing it to receive failures like:

{code}
java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:181)
	at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)
{code}

Or even,

{code}
ERROR org.apache.thrift.server.TThreadPoolServer: Error occurred during processing of message.
java.lang.NegativeArraySizeException
        at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:181)
        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)
{code}",2014-08-12T20:08:44.532+0000,2014-11-05T04:48:58.108+0000,Fixed,Major
TEZ-1066,Generate events to integrate with YARN timeline server ,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12388094'>, <JIRA IssueLink: id='12388520'>]",,2014-04-17T18:37:34.766+0000,2014-09-06T01:35:24.056+0000,Fixed,Major
SPARK-3902,Stabilize AsyncRDDActions and expose its methods in Java API,SPARK,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12398893'>, <JIRA IssueLink: id='12398835'>, <JIRA IssueLink: id='12398750'>]","The AsyncRDDActions methods are currently the easiest way to determine Spark jobs' ids for use in progress-monitoring code (see SPARK-2636).  AsyncRDDActions is currently marked as {{@Experimental}}; for 1.2, I think that we should stabilize this API and expose it in Java, too.

One concern is whether there's a better async API design that we should prefer over this one as our stable API; I had some ideas for a more general API in SPARK-3626 (discussed in much greater detail on GitHub: https://github.com/apache/spark/pull/2482) but decided against the more general API due to its confusing cancellation semantics.  Given this, I'd be comfortable stabilizing our current API.",2014-10-10T21:38:40.456+0000,2014-10-20T03:02:53.668+0000,Fixed,Major
IMPALA-7204,"Add support for GROUP BY ROLLUP, CUBE and GROUPING SETS",IMPALA,New Feature,Open,"[<JIRA Issue: key='IMPALA-9897', id='13313638'>, <JIRA Issue: key='IMPALA-9898', id='13313639'>, <JIRA Issue: key='IMPALA-9914', id='13314361'>, <JIRA Issue: key='IMPALA-9917', id='13315002'>]",6,"[<JIRA IssueLink: id='12592132'>, <JIRA IssueLink: id='12537005'>, <JIRA IssueLink: id='12537002'>, <JIRA IssueLink: id='12537003'>, <JIRA IssueLink: id='12537004'>, <JIRA IssueLink: id='12537167'>]","Now suppose that we'd like to analyze our sales data, to study the amount of sales that is occurring for different products, in different states and regions. Using the ROLLUP feature of SQL 2003, we could issue the query:
{code:sql}
select region, state, product, sum(sales) total_sales
from sales_history 
group by rollup (region, state, product)
{code}
Semantically, the above query is equivalent to

 
{code:sql}
select region, state, product, sum(sales) total_sales
from sales_history 
group by region, state, product
union
select region, state, null, sum(sales) total_sales
from sales_history 
group by region, state
union
select region, null, null, sum(sales) total_sales
from sales_history 
group by region
union
select null, null, null, sum(sales) total_sales
from sales_history
 
{code}
The query might produce results that looked something like:
{noformat}
REGION STATE PRODUCT TOTAL_SALES
------ ----- ------- -----------
null null null 6200
EAST MA BOATS 100
EAST MA CARS 1500
EAST MA null 1600
EAST NY BOATS 150
EAST NY CARS 1000
EAST NY null 1150
EAST null null 2750
WEST CA BOATS 750
WEST CA CARS 500
WEST CA null 1250
WEST AZ BOATS 2000
WEST AZ CARS 200
WEST AZ null 2200
WEST null null 3450

{noformat}
We have a lot of production queries that work around this missing Impala functionality by having three UNION ALLs. Physical execution plan shows Impala actually reads full fact table three times. So it could be a three times improvement (or more, depending on number of columns that are being rolled up).

I can't find another SQL on Hadoop engine that doesn't support this feature. 
 *Checked Spark, Hive, PIG, Flink and some other engines - they all do support this basic SQL feature*.

Would be great to have a matching feature in Impala too.",2018-06-23T16:42:22.863+0000,2020-06-25T22:46:49.254+0000,,Major
PHOENIX-4459,Region assignments are failing for the test cases with extended clocks to support SCN,PHOENIX,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12525394'>, <JIRA IssueLink: id='12523440'>]","There are test cases using own clock are failing with TableNotFoundException during region assignment. The reason is the meta scan is not giving any results because of the past timestamps. Need to check in more details. Because of the region assignment failures during create table procedure hbase client wait for 30 mins. So not able to continue running the other tests as well.
{noformat}
2017-12-14 16:48:03,153 ERROR [ProcExecWrkr-9] org.apache.hadoop.hbase.master.TableStateManager(135): Unable to get table T000008 state
org.apache.hadoop.hbase.TableNotFoundException: T000008
	at org.apache.hadoop.hbase.master.TableStateManager.getTableState(TableStateManager.java:175)
	at org.apache.hadoop.hbase.master.TableStateManager.isTableState(TableStateManager.java:132)
	at org.apache.hadoop.hbase.master.assignment.AssignProcedure.startTransition(AssignProcedure.java:161)
	at org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.execute(RegionTransitionProcedure.java:294)
	at org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.execute(RegionTransitionProcedure.java:85)
	at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:845)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1452)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1221)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$800(ProcedureExecutor.java:77)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1731)
{noformat}

List of tests hanging because of this:-
ExplainPlanWithStatsEnabledIT#testBytesRowsForSelectOnTenantViews
ConcurrentMutationsIT
PartialIndexRebuilderIT
",2017-12-14T11:28:25.292+0000,2018-07-26T01:14:20.296+0000,Fixed,Major
HCATALOG-236,Exception during local-metastore shutdown on HCat-client.,HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12350577'>],"Here's an exception that we see when HiveMetastoreClient.close() is called:


org.apache.thrift.transport.TTransportException: SASL authentication not complete
        at org.apache.thrift.transport.TSaslTransport.write(TSaslTransport.java:443)
        at org.apache.thrift.transport.TSaslClientTransport.write(TSaslClientTransport.java:37)
        at org.apache.hadoop.hive.thrift.TFilterTransport.write(TFilterTransport.java:72)
        at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:163)
        at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:91)
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
        at com.facebook.fb303.FacebookService$Client.send_shutdown(FacebookService.java:421)
        at com.facebook.fb303.FacebookService$Client.shutdown(FacebookService.java:415)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:308)
        at org.apache.hcatalog.mapreduce.InitializeInput.setInput(InitializeInput.java:116)
        at org.apache.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:40)
        at org.apache.hcatalog.pig.HCatLoader.setLocation(HCatLoader.java:90)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:263)
        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:962)
        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:979)
        at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1082)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)


We see this both from Pig (0.9) Grunt-shell and in the Output-committer task-logs. Doesn't happen on HCat 0.2 (Hive0.8 + Thrift 0.5.0).",2012-01-26T23:17:24.565+0000,2012-05-02T23:26:54.982+0000,Fixed,Major
TEZ-4311,Bump hadoop dependency version to 3.3.x,TEZ,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12629575'>, <JIRA IssueLink: id='12632477'>]",This change is for bumping hadoop.version to 3.3 when every other needed subtask is done.,2021-05-21T10:10:46.745+0000,2022-01-31T14:56:59.853+0000,Fixed,Major
IMPALA-6813,Hedged reads metrics broken when scanning non-HDFS based table,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12531401'>],"When preads are enabled ADLS scans can fail updating the Hedged reads metrics

{code}
(gdb) bt
#0  0x0000003346c32625 in raise () from /lib64/libc.so.6
#1  0x0000003346c33e05 in abort () from /lib64/libc.so.6
#2  0x00007f185be140b5 in os::abort(bool) ()
   from /usr/java/jdk1.8.0_121-cloudera/jre/lib/amd64/server/libjvm.so
#3  0x00007f185bfb6443 in VMError::report_and_die() ()
   from /usr/java/jdk1.8.0_121-cloudera/jre/lib/amd64/server/libjvm.so
#4  0x00007f185be195bf in JVM_handle_linux_signal ()
   from /usr/java/jdk1.8.0_121-cloudera/jre/lib/amd64/server/libjvm.so
#5  0x00007f185be0fb03 in signalHandler(int, siginfo*, void*) ()
   from /usr/java/jdk1.8.0_121-cloudera/jre/lib/amd64/server/libjvm.so
#6  <signal handler called>
#7  0x00007f185bbc1a7b in jni_invoke_nonstatic(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*) ()
   from /usr/java/jdk1.8.0_121-cloudera/jre/lib/amd64/server/libjvm.so
#8  0x00007f185bbc7e81 in jni_CallObjectMethodV ()
   from /usr/java/jdk1.8.0_121-cloudera/jre/lib/amd64/server/libjvm.so
#9  0x000000000212e2b7 in invokeMethod ()
#10 0x0000000002131297 in hdfsGetHedgedReadMetrics ()
#11 0x00000000011601c0 in impala::io::ScanRange::Close() ()
#12 0x0000000001158a95 in impala::io::DiskIoMgr::HandleReadFinished(impala::io::DiskIoMgr::DiskQueue*, impala::io::RequestContext*, std::unique_ptr<impala::io::BufferDescriptor, std::default_delete<impala::io::BufferDescriptor> >) ()
#13 0x0000000001158e1c in impala::io::DiskIoMgr::ReadRange(impala::io::DiskIoMgr::DiskQueue*, impala:---Type <return> to continue, or q <return> to quit---
:io::RequestContext*, impala::io::ScanRange*) ()
#14 0x0000000001159052 in impala::io::DiskIoMgr::WorkLoop(impala::io::DiskIoMgr::DiskQueue*) ()
#15 0x0000000000d5fcaf in impala::Thread::SuperviseThread(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::ThreadDebugInfo const*, impala::Promise<long>*) ()
#16 0x0000000000d604aa in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::ThreadDebugInfo const*, impala::Promise<long>*), boost::_bi::list5<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::ThreadDebugInfo*>, boost::_bi::value<impala::Promise<long>*> > > >::run() ()
#17 0x00000000012d6dfa in ?? ()
#18 0x0000003347007aa1 in start_thread () from /lib64/libpthread.so.0
#19 0x0000003346ce893d in clone () from /lib64/libc.so.6
{code}

{code}
CREATE TABLE adls.lineitem (
  l_orderkey BIGINT,
  l_partkey BIGINT,
  l_suppkey BIGINT,
  l_linenumber BIGINT,
  l_quantity DOUBLE,
  l_extendedprice DOUBLE,
  l_discount DOUBLE,
  l_tax DOUBLE,
  l_returnflag STRING,
  l_linestatus STRING,
  l_commitdate STRING,
  l_receiptdate STRING,
  l_shipinstruct STRING,
  l_shipmode STRING,
  l_comment STRING,
  l_shipdate STRING
)
STORED AS PARQUET
LOCATION 'adl://foo.azuredatalakestore.net/adls-test.db/lineitem'
{code}

select * from adls.lineitem limit 10;",2018-04-05T20:31:55.442+0000,2018-05-29T19:14:10.477+0000,Fixed,Blocker
PARQUET-281,Statistic and Filter need a mechanism to get customized comparator from high layer user,PARQUET,Improvement,Open,[],1,[<JIRA IssueLink: id='12424598'>],"As discussed in HIVE-10254, we might need a customized comparator from high layer user for generating statistic when writing and applying filter when reading. 

The problem is that (use Decimal type in Hive as an example):
Decimal in Hive is mapped to Binary in Parquet. When using predicate and statistic to filter values, comparing Binary values in Parquet cannot reflect the correct relationship of Decimal values in Hive. This type mapping causes 2 problems:
1. When writing Decimal column, Binary.compareTo() is used to judge and set the column statistic (min, max). The generated statistic value is not correct from a Decimal perspective.
2. When reading with Predicate (also Filter), in which the expected Decimal value is converted to Binary type, Binary.compareTo() is used to compare the expected value and column statistic value. They are Binary perspective, and also the result is not right.

We could add an interface for customized comparator, and high level user like Hive provides the comparator to Parquet, since Hive knows how to decode the binary to Decimal and compare. Then Parquet could switch between customized and original comparison method.",2015-05-13T07:36:55.572+0000,2019-05-07T20:01:09.406+0000,,Major
PARQUET-114,Sample NanoTime class serializes and deserializes Timestamp incorrectly,PARQUET,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12398418'>, <JIRA IssueLink: id='12401807'>]","The class should [write timestamp|https://github.com/Parquet/parquet-mr/issues/218#issuecomment-30185966] in little endian with time of day nanos first and julian date second.
",2014-10-07T16:30:28.194+0000,2015-04-07T20:45:57.500+0000,Fixed,Major
SPARK-3655,Support sorting of values in addition to keys (i.e. secondary sort),SPARK,New Feature,Resolved,[],4,"[<JIRA IssueLink: id='12421889'>, <JIRA IssueLink: id='12468765'>, <JIRA IssueLink: id='12397462'>, <JIRA IssueLink: id='12436213'>]","Now that spark has a sort based shuffle, can we expect a secondary sort soon? There are some use cases where getting a sorted iterator of values per key is helpful.",2014-09-23T02:50:27.061+0000,2019-06-06T13:57:38.084+0000,Auto Closed,Major
ORC-341,Add ability to use timestamp in UTC rather than local timezone.,ORC,Improvement,Closed,[],5,"[<JIRA IssueLink: id='12550771'>, <JIRA IssueLink: id='12569148'>, <JIRA IssueLink: id='12537306'>, <JIRA IssueLink: id='12531918'>, <JIRA IssueLink: id='12534160'>]","Currently, time zone is hardcoded as the system default time zone and ORC applies displacement between timestamp values read/written based on time zone.

This issue aims at adding the option to pass the time zone as a parameter to the reader/writer.",2018-04-14T11:30:18.770+0000,2019-09-03T22:49:52.501+0000,Fixed,Major
BIGTOP-1403,Add YARN-2410 to Longevity tests.,BIGTOP,Bug,Open,[],1,[<JIRA IssueLink: id='12394024'>],"YARN-2410 highlights an interesting longevity bug, wherein it appears if there are large # of reducers ( > 6K )+ 40 map outputs per node, reducers all asking for mapper node outputs at a similar time can lead to scheduler crashing.  

Would be great to see a workload that simulates this in the longevity tests.",2014-08-13T18:05:05.272+0000,2014-08-14T03:10:19.141+0000,,Major
TEZ-388,Pig on Tez,TEZ,Bug,Open,"[<JIRA Issue: key='TEZ-389', id='12665289'>, <JIRA Issue: key='TEZ-390', id='12665290'>, <JIRA Issue: key='TEZ-391', id='12665291'>, <JIRA Issue: key='TEZ-392', id='12665292'>, <JIRA Issue: key='TEZ-393', id='12665294'>, <JIRA Issue: key='TEZ-394', id='12665300'>, <JIRA Issue: key='TEZ-601', id='12677907'>, <JIRA Issue: key='TEZ-764', id='12691118'>]",14,"[<JIRA IssueLink: id='12374591'>, <JIRA IssueLink: id='12388793'>, <JIRA IssueLink: id='12382476'>, <JIRA IssueLink: id='12379796'>, <JIRA IssueLink: id='12380807'>, <JIRA IssueLink: id='12390808'>, <JIRA IssueLink: id='12382342'>, <JIRA IssueLink: id='12381868'>, <JIRA IssueLink: id='12382508'>, <JIRA IssueLink: id='12385580'>, <JIRA IssueLink: id='12388792'>, <JIRA IssueLink: id='12381869'>, <JIRA IssueLink: id='12382329'>, <JIRA IssueLink: id='12384452'>]",  Umbrella jira to track Pig on Tez issues.,2013-08-23T18:41:21.433+0000,2014-07-02T23:41:45.751+0000,,Major
TEZ-3353,Tez should adjust processor memory on demand,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12475101'>],"Hive requests some amount of memory for its map join, which sometimes is not allocated as much as expected. Tez should make adjustment and satisfy that request as is.

This is related to HIVE-13934.",2016-07-15T22:29:37.186+0000,2016-07-18T23:14:26.115+0000,,Major
INFRA-15855,Upgrade PreCommit-HIVE-Build jenkins job to use java 8 ,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12524454'>],After the JIRA upgrade Hive precommit jobs were failing will SSLException as reported in HIVE-18461. We have fixed it with a workaround in the code but it would be good to start using Java 8 for Hive precommit job.,2018-01-17T06:27:14.437+0000,2018-08-25T06:55:03.426+0000,Fixed,Major
CALCITE-2205,JoinPushTransitivePredicatesRule should not create a Filter on top of an equivalent Filter,CALCITE,Bug,Closed,[],4,"[<JIRA IssueLink: id='12528684'>, <JIRA IssueLink: id='12528682'>, <JIRA IssueLink: id='12528767'>, <JIRA IssueLink: id='12632608'>]","CALCITE-2200 resolves some cases of infinite loop via stopping of recursion in HepPlanner#applyRules, when newVertex is the same as vertex.

In this jira one more case of infinite loop is described:
JoinPushTransitivePredicatesRule#onMatch generates new right or left inputs via using RelBuilder#filter method on top of LogicalFilter RelNode with the same condition. 
In this case a new RelNode shouldn't be created. Possible fix to change logic of RelBuilder#filter method.

TestCase for reproduce:
{code}
  @Test public void testJoinPushTransitivePredicatesRule2() {
    HepProgramBuilder builder = new HepProgramBuilder();
    builder.addRuleInstance(JoinPushTransitivePredicatesRule.INSTANCE);
    HepProgram build = builder.build();
    HepPlanner hepPlanner = new HepPlanner(build);

    final String sql = ""select n1.SAL from EMPNULLABLES_20 n1 where n1.SAL\n""
        + ""IN (select n2.SAL from EMPNULLABLES_20 n2 ""
        + ""where n1.SAL = n2.SAL or n1.SAL = 4)"";

    sql(sql)
        .withDecorrelation(true)
        .with(hepPlanner)
        .check();
  }
{code}",2018-03-06T12:42:41.281+0000,2022-02-02T10:50:44.914+0000,Fixed,Major
SPARK-33019,Use spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=1 by default,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12599515'>, <JIRA IssueLink: id='12600875'>]","By default, Spark should use a safe file output committer algorithm to avoid MAPREDUCE-7282.",2020-09-28T18:40:42.966+0000,2020-10-07T21:01:31.937+0000,Fixed,Blocker
KNOX-1585,YARN v2 UI - Application - Log link broken,KNOX,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12574829'>, <JIRA IssueLink: id='12566425'>]","[http://localhost:8443/gateway/test/yarnuiv2/redirect#/yarn-app/application_1541711200634_0002/attempts]

The application ""Log Link""
 * redirects to [http://ambari:8042/node/containerlogs/container_e01_1541711200634_0002_01_000001/ambari-qa]
 * Correct link should be: http://localhost:8443/gateway/test/yarnuiv2/redirect#/yarn-app/application_1541711200634_0002/logs",2018-11-09T13:45:56.189+0000,2019-11-22T15:25:30.802+0000,Implemented,Minor
TEZ-3253,Remove special handling for last app attempt (absence of ApplicationConstants.MAX_APP_ATTEMPTS_ENV in AM env),TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12492063'>],ApplicationConstants.MAX_APP_ATTEMPTS_ENV was removed in hadoop-3.x. This breaks max attempts handling and will cause the AM to unregister after only one attempt. ,2016-05-11T04:06:47.916+0000,2017-08-22T00:02:45.016+0000,Fixed,Major
SLIDER-744,close WS/ back door for REST resources,SLIDER,Sub-task,Open,[],1,[<JIRA IssueLink: id='12404959'>],"SLIDER-710 re-opened a back door for the ws/* REST operations, as the YARN proxy doesn't support it.
When YARN does, this back door must be blocked, otherwise unauthed users can manipulate slider clusters.
This is a simple matter of flipping a switch in {{org/apache/slider/slider.xml}}, along with test runs to verify that it is closed.

Time estimate is for functional test completion",2015-01-09T15:37:28.844+0000,2015-02-18T17:35:35.396+0000,,Major
TEZ-692,Unify job submission in either TezClient or TezSession,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12390917'>],Its confusing to have 2 ways to create and submit a tez job. The developer has to spend time thinking about and deciding which method to use.,2013-12-29T12:47:21.382+0000,2014-09-06T01:35:55.054+0000,Fixed,Major
TEZ-996,Support for fetching credentials for mapreduce.job.hdfs-servers,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12385658'>],mapreduce.job.hdfs-servers is a property that JobClient looks at and fetches delegation tokens before submitting the job in MR. This is used lot of cross cluster communication. Would be good to have support added in Tez by default for that instead of multiple projects like Pig and Hive repeating it. ,2014-03-28T19:35:33.310+0000,2017-02-24T10:58:20.046+0000,,Major
PHOENIX-5188,IndexedKeyValue should populate KeyValue fields,PHOENIX,Bug,Closed,[],1,[<JIRA IssueLink: id='12556519'>],"IndexedKeyValue subclasses the HBase KeyValue class, which has three primary fields: bytes, offset, and length. These fields aren't populated by IndexedKeyValue because it's concerned with index mutations, and has its own fields that its own methods use. 

However, KeyValue and its Cell interface have quite a few methods that assume these fields are populated, and the HBase-level factory methods generally ensure they're populated. Phoenix code should do the same, to maintain the polymorphic contract. This is important in cases like custom ReplicationEndpoints where HBase-level code may be iterating over WALEdits that contain both KeyValues and IndexKeyValues and may need to interrogate their contents. 

Since the index mutation has a row key, this is straightforward. ",2019-03-12T02:57:52.073+0000,2019-05-30T18:40:50.034+0000,Fixed,Major
ZOOKEEPER-3794,upgrade netty to address CVE-2020-11612,ZOOKEEPER,Task,Closed,[],1,[<JIRA IssueLink: id='12585934'>],"The owasp checker is failing with the following. I looked and seems like a DOS attack vector ""The ZlibDecoders in Netty 4.1.x before 4.1.46 allow for unbounded memory allocation while decoding a ZlibEncoded byte stream. An attacker could send a large ZlibEncoded byte stream to the Netty server, forcing the server to allocate all of its free memory to a single decoder.""

[ERROR] Failed to execute goal org.owasp:dependency-check-maven:5.3.0:check (default-cli) on project zookeeper:
[ERROR] 
[ERROR] One or more dependencies were identified with vulnerabilities that have a CVSS score greater than or equal to '0.0':
[ERROR] 
[ERROR] netty-handler-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-common-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-buffer-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-transport-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-resolver-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-codec-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-transport-native-epoll-4.1.45.Final.jar: CVE-2020-11612
[ERROR] netty-transport-native-unix-common-4.1.45.Final.jar: CVE-2020-11612
[ERROR] ",2020-04-13T21:55:56.476+0000,2020-05-11T15:41:15.626+0000,Fixed,Blocker
ZOOKEEPER-1066,Multi should have an async version,ZOOKEEPER,Bug,Open,[],2,"[<JIRA IssueLink: id='12376650'>, <JIRA IssueLink: id='12359341'>]","per the code review on ZOOKEEPER-965 it seems that multi should have an asynchronous version.

The semantics should be essentially identical.  The only difference is that the original caller shouldn't wait for the result.  Cloning existing multi-operations should be a decent implementation strategy.",2011-05-21T21:09:14.139+0000,2013-10-10T17:24:12.298+0000,,Major
ZOOKEEPER-2120,SSL feature on Netty,ZOOKEEPER,New Feature,Resolved,"[<JIRA Issue: key='ZOOKEEPER-2119', id='12775640'>, <JIRA Issue: key='ZOOKEEPER-2123', id='12776023'>, <JIRA Issue: key='ZOOKEEPER-2125', id='12776561'>, <JIRA Issue: key='ZOOKEEPER-2148', id='12784712'>, <JIRA Issue: key='ZOOKEEPER-2153', id='12786835'>]",3,"[<JIRA IssueLink: id='12424640'>, <JIRA IssueLink: id='12641805'>, <JIRA IssueLink: id='12412168'>]","As we discussed in ZOOKEEPER-2094, the SSL work would be divided into several subtask:

1. Provide implementation of X509 AuthenticationProvider
2. Modify ZooKeeper Netty server and client to support SSL
3. Modify ZooKeeperServerMain to support SSL

This is the umbrella task.",2015-02-17T20:20:45.877+0000,2022-08-16T11:01:19.991+0000,Fixed,Major
SPARK-23719,Use correct hostname in non-host networking mode in hadoop 3 docker support,SPARK,Improvement,Open,[],3,"[<JIRA IssueLink: id='12529736'>, <JIRA IssueLink: id='12529737'>, <JIRA IssueLink: id='12530105'>]","
Hostname (node-id's hostname field) specified by RM in allocated containers is the NM_HOST and not the hostname which will be used by the container when running in docker container executor : the actual container hostname is generated at runtime.

Due to this spark executor's are unable to launch in non-host networking mode when leveraging docker support in hadoop 3 - due to bind failures as hostname they are trying to bind to is of the host machine and not the container.

We can leverage YARN-7935 to fetch the container's hostname (when available) else fallback to existing mechanism - when running executors.",2018-03-17T02:03:13.235+0000,2020-03-16T22:55:32.505+0000,,Major
WHIRR-351,configure_hadoop should create+chown all data dirs listed in hadoop-hdfs.dfs.data.dir,WHIRR,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12341660'>, <JIRA IssueLink: id='12341659'>]","If the directories listed in dfs.data.dir do not exist, 
the datanodes will try to create them. This fails if the hadoop user
has sufficient permissions.
To avoid the problem, the directories should be created before starting the datanodes.",2011-08-02T15:20:41.290+0000,2012-08-23T20:56:49.437+0000,Fixed,Minor
TWILL-73,Unable to allocate small container after large container has been acquired,TWILL,Bug,Resolved,[],1,[<JIRA IssueLink: id='12392827'>],,2014-04-17T00:09:17.559+0000,2014-07-29T18:07:24.503+0000,Fixed,Major
CALCITE-1650,Difference in BOOLEAN cast between Hive and Calcite,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12494978'>, <JIRA IssueLink: id='12495585'>, <JIRA IssueLink: id='12495154'>]",Hive {{CAST( ... AS BOOLEAN)}} gives true for anything 'not zero' (eg. {{CAST(1 as BOOLEAN)}} is {{true}}) while Calcite uses strict Java {{Boolean.valueOf(...)}} which yields true only for the string 'true'.,2017-02-21T14:17:16.804+0000,2017-02-26T10:40:35.283+0000,Duplicate,Major
TEZ-2310,Deadlock caused by StateChangeNotifier sending notifications on thread holding locks,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12421590'>, <JIRA IssueLink: id='12421085'>]","See the following deadlock in testing:

Thread#1:
{code}
Daemon Thread [App Shared Pool - #3] (Suspended)	
	owns: VertexManager$VertexManagerPluginContextImpl  (id=327)	
	owns: ShuffleVertexManager  (id=328)	
	owns: VertexManager  (id=329)	
	waiting for: VertexManager$VertexManagerPluginContextImpl  (id=326)	
	VertexManager$VertexManagerPluginContextImpl.onStateUpdated(VertexStateUpdate) line: 344	
	StateChangeNotifier$ListenerContainer.sendStateUpdate(VertexStateUpdate) line: 138	
	StateChangeNotifier$ListenerContainer.access$100(StateChangeNotifier$ListenerContainer, VertexStateUpdate) line: 122	
	StateChangeNotifier.sendStateUpdate(TezVertexID, VertexStateUpdate) line: 116	
	StateChangeNotifier.stateChanged(TezVertexID, VertexStateUpdate) line: 106	
	VertexImpl.maybeSendConfiguredEvent() line: 3385	
	VertexImpl.doneReconfiguringVertex() line: 1634	
	VertexManager$VertexManagerPluginContextImpl.doneReconfiguringVertex() line: 339	
	ShuffleVertexManager.schedulePendingTasks(int) line: 561	
	ShuffleVertexManager.schedulePendingTasks() line: 620	
	ShuffleVertexManager.handleVertexStateUpdate(VertexStateUpdate) line: 731	
	ShuffleVertexManager.onVertexStateUpdated(VertexStateUpdate) line: 744	
	VertexManager$VertexManagerEventOnVertexStateUpdate.invoke() line: 527	
	VertexManager$VertexManagerEvent$1.run() line: 612	
	VertexManager$VertexManagerEvent$1.run() line: 607	
	AccessController.doPrivileged(PrivilegedExceptionAction<T>, AccessControlContext) line: not available [native method]	
	Subject.doAs(Subject, PrivilegedExceptionAction<T>) line: 415	
	UserGroupInformation.doAs(PrivilegedExceptionAction<T>) line: 1548	
	VertexManager$VertexManagerEventOnVertexStateUpdate(VertexManager$VertexManagerEvent).call() line: 607	
	VertexManager$VertexManagerEventOnVertexStateUpdate(VertexManager$VertexManagerEvent).call() line: 596	
	ListenableFutureTask<V>(FutureTask<V>).run() line: 262	
	ThreadPoolExecutor.runWorker(ThreadPoolExecutor$Worker) line: 1145	
	ThreadPoolExecutor$Worker.run() line: 615	
	Thread.run() line: 745	
{code}
Thread #2
{code}
Daemon Thread [App Shared Pool - #2] (Suspended)	
	owns: VertexManager$VertexManagerPluginContextImpl  (id=326)	
	owns: PigGraceShuffleVertexManager  (id=344)	
	owns: VertexManager  (id=345)	
	Unsafe.park(boolean, long) line: not available [native method]	
	LockSupport.park(Object) line: 186	
	ReentrantReadWriteLock$NonfairSync(AbstractQueuedSynchronizer).parkAndCheckInterrupt() line: 834	
	ReentrantReadWriteLock$NonfairSync(AbstractQueuedSynchronizer).doAcquireShared(int) line: 964	
	ReentrantReadWriteLock$NonfairSync(AbstractQueuedSynchronizer).acquireShared(int) line: 1282	
	ReentrantReadWriteLock$ReadLock.lock() line: 731	
	VertexImpl.getTotalTasks() line: 952	
	VertexManager$VertexManagerPluginContextImpl.getVertexNumTasks(String) line: 162	
	PigGraceShuffleVertexManager(ShuffleVertexManager).updateSourceTaskCount() line: 435	
	PigGraceShuffleVertexManager(ShuffleVertexManager).onVertexStarted(Map<String,List<Integer>>) line: 353	
	VertexManager$VertexManagerEventOnVertexStarted.invoke() line: 541	
	VertexManager$VertexManagerEvent$1.run() line: 612	
	VertexManager$VertexManagerEvent$1.run() line: 607	
	AccessController.doPrivileged(PrivilegedExceptionAction<T>, AccessControlContext) line: not available [native method]	
	Subject.doAs(Subject, PrivilegedExceptionAction<T>) line: 415	
	UserGroupInformation.doAs(PrivilegedExceptionAction<T>) line: 1548	
	VertexManager$VertexManagerEventOnVertexStarted(VertexManager$VertexManagerEvent).call() line: 607	
	VertexManager$VertexManagerEventOnVertexStarted(VertexManager$VertexManagerEvent).call() line: 596	
	ListenableFutureTask<V>(FutureTask<V>).run() line: 262	
	ThreadPoolExecutor.runWorker(ThreadPoolExecutor$Worker) line: 1145	
	ThreadPoolExecutor$Worker.run() line: 615	
	Thread.run() line: 745	
{code}
What happens is thread #1 holding a writeLock (VertexImpl:1628) and enter into a synchronized block (ShuffleVertexManager.onVertexStateUpdated), in the mean time, thread #2 already in the synchronized block (ShuffleVertexManager.onVertexStarted) and try to get a readLock(VertexImpl:952). Holding a lock and then enter a synchronized block might be dangerous. 

I attach a patch which avoiding that and then deadlock goes away. Not sure if that is the right fix or if any other patterns like this.",2015-04-11T22:02:36.176+0000,2015-06-30T04:53:07.534+0000,Fixed,Major
TEZ-4021,API incompatibility wro4j-maven-plugin,TEZ,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12548808'>],,2018-11-26T17:56:46.251+0000,2020-08-25T15:00:04.921+0000,Fixed,Major
BOOKKEEPER-243,BK JM : After restart bookkeeper continuously throwing error because BK JM create all Namenode related znode under '/ledgers' znode.,BOOKKEEPER,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12351556'>],"Issue :

Bookkeeper journal manager create all the Namenode related znode under '/ledgers' znode in zookeeper. When bookkeeper read all the ledgers from  '/ledgers' znode it consider this znode (version, lock, maxtxid)  as incorrect format ledger and log following error.

{noformat}
2012-04-20 11:52:25,611 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: ledgers
2012-04-20 11:52:25,611 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: lock
2012-04-20 11:52:25,612 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: maxtxid
2012-04-20 11:52:26,613 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: version
2012-04-20 11:52:26,613 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: ledgers
2012-04-20 11:52:26,613 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: lock
2012-04-20 11:52:26,613 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: maxtxid
2012-04-20 11:52:27,614 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: version
2012-04-20 11:52:27,614 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: ledgers
2012-04-20 11:52:27,614 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: lock
2012-04-20 11:52:27,615 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: maxtxid
2012-04-20 11:52:28,616 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: version
2012-04-20 11:52:28,616 - WARN  [main-EventThread:AbstractZkLedgerManager$2@123] - Error extracting ledgerId from ZK ledger node: ledgers


{noformat}

I think Namenode related znode should be create in separate znode in zookeeper.

",2012-05-08T06:10:17.435+0000,2017-10-09T09:51:51.704+0000,Won't Do,Major
CALCITE-591,Drop support for Java 1.6 (and JDBC 4.0),CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12519070'>, <JIRA IssueLink: id='12407646'>, <JIRA IssueLink: id='12407659'>]","Drop support for Java 1.6:
* The code would no longer compile under JDK 1.6
* Compiler would have source 1.7 target 1.7
* Class files would run on JDK 1.7 and higher
* Developers can use 1.7 syntax such as strings in switch statements and try-with-resources

We would continue to build and run under JDK 1.7 and 1.8.",2015-02-09T22:27:08.798+0000,2017-11-01T21:31:27.124+0000,Fixed,Major
SLIDER-787,App Upgrade/Reconfig support in Slider,SLIDER,New Feature,Resolved,"[<JIRA Issue: key='SLIDER-813', id='12781633'>, <JIRA Issue: key='SLIDER-833', id='12786003'>, <JIRA Issue: key='SLIDER-839', id='12787775'>, <JIRA Issue: key='SLIDER-841', id='12818875'>, <JIRA Issue: key='SLIDER-842', id='12818894'>, <JIRA Issue: key='SLIDER-854', id='12820552'>, <JIRA Issue: key='SLIDER-855', id='12820553'>]",3,"[<JIRA IssueLink: id='12422967'>, <JIRA IssueLink: id='12408385'>, <JIRA IssueLink: id='12452432'>]","*Terminologies:*
Here ""App"" means the application that is deployed by Slider e.g. HBase, Storm, Accumulo, Memcached, etc.

*Use-case:*
- YARN/HDFS/Zookeeper Upgrade 
- YARN/HDFS/Zookeeper reconfiguration
- Slider Upgrade
- App Upgrade
- App Reconfiguration

*YARN/HDFS Upgrade:*
Upgrade of YARN/HDFS underlying infra, libraries, etc.

*YARN/HDFS reconfiguration:*
Reconfiguration of infrastructure components which Slider relies on.

*Slider Upgrade:*
Today you can do it by stopping the application and use the new version of the Slider to start the application.

*Slider Upgrade translates to the upgrade of:*
- Slider AppMaster
- Slider Client
- Slider Agent

*App Upgrade*
- Support manual upgrade of Slider application
- Provide individual component specific upgrade [ Nice to Have ] 
- Need to allow rollback

*Steps to upgrade:*
- Stop app
- slider set-app-package app --package <new package> 
- Start app

*Rollback:*
- Stop app
- slider set-app-package app --package <old package> 
- Start app

New component instances should be tried at best to be created in same host and port [ strong preference ] including AM. This task should be possible through CLI and new REST API.

*Rolling upgrade:*
- Slider CLI will not provide component upgrade ordering and start/stop - needs to be orchestrated by external scripts

*App Reconfiguration*
- Allow updating application configurations (appConfig.json or resources.json)
- Allow rolling restart of all containers to have the new configuration take effect
 
*Slider needs to implement*
- Need specific container stop/start ability
- Need upgrade package functionality [ required for in-place upgrade too ]
- Need update capability for configuration
- Component and AM host port to be preserved in best-effort

*Backward Compatibility*
- New apps ""may"" have new configuration based on new packaging. So upgrade needs to take care of that.
- Reconfiguration will NOT require to support new style of configuration metadata.",2015-02-18T00:28:21.254+0000,2015-12-19T13:08:50.625+0000,Fixed,Major
SPARK-24607,Distribute by rand() can lead to data inconsistency,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12536769'>, <JIRA IssueLink: id='12536770'>]","Noticed the following queries can give different results:
{code:java}
select count(*) from tbl;
select count(*) from (select * from tbl distribute by rand()) a;{code}
this issue was first reported by someone using kylin for building cube with hiveSQL which include  distribute by rand, data inconsistency may happen during failure tolerance operations. Since spark has similar failure tolerance mechanism, I think it's also an hidden serious problem in sparksql.",2018-06-20T13:14:32.435+0000,2020-12-04T06:21:25.580+0000,Incomplete,Major
CALCITE-2802,"Druid adapter: Usage of range conditions like ""2010-01-01 < timestamp"" leads to incorrect results",CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12552523'>],"Timestamp range conditions when the timestamp is on left hand side work correctly; however when the literal is on the left hand side results are missing.

{code}
  @Test
  public void testRangeCalc() {
    final Fixture2 f = new Fixture2();
    checkDateRange(f,
        f.and(
            f.le(f.timestampLiteral(2011, Calendar.JANUARY, 1), f.t),
            f.le(f.t, f.timestampLiteral(2012, Calendar.FEBRUARY, 2))),
        is(""[2011-01-01T00:00:00.000Z/2012-02-02T00:00:00.001Z]""));
  }
{code}


Fail:
{code}
java.lang.AssertionError: 
Expected: is ""[2011-01-01T00:00:00.000Z/2012-02-02T00:00:00.001Z]""
     but: was ""[1900-01-01T00:00:00.000Z/2011-01-01T00:00:00.001Z]""
{code}
",2019-01-24T13:58:10.186+0000,2019-03-25T00:11:23.518+0000,Fixed,Major
INFRA-15024,Need to update commit message to add JIRA number,INFRA,Project,Closed,[],1,[<JIRA IssueLink: id='12513910'>],"I made a commit to Hadoop trunk (master) branch where I missed including the JIRA numbers in the commit. The commit for trunk is *7996eca7dcfaa1bdf970e32022274f2699bef8a1* and corresponding cherry-pick to branch-2 is *80516b3de79e9aac4dba1374638257b4611f199e*. I need to update the commit message to include the JIRA number, i.e. YARN-5328. FYI, we have explicitly disabled force push to these branches (INFRA-11236).",2017-09-05T20:11:24.472+0000,2017-10-19T11:27:15.890+0000,Not A Problem,Minor
THRIFT-377,TFileTransport port in Java,THRIFT,New Feature,Closed,[],1,[<JIRA IssueLink: id='12323801'>],"there are environments in which data is being logged in TFileTransport (presumably C++ land) and data is desired to be consumed/imported into Hadoop.

For this a Java version of TFileTransport is required. I am planning to post one that only reads the data - this code is already in use and tested and fulfils the current requirements.",2009-03-17T22:06:39.219+0000,2010-10-27T23:28:20.304+0000,Fixed,Major
SLIDER-82,Support ANTI_AFFINITY_REQUIRED option,SLIDER,Task,Resolved,"[<JIRA Issue: key='SLIDER-947', id='12904881'>, <JIRA Issue: key='SLIDER-963', id='12910785'>, <JIRA Issue: key='SLIDER-965', id='12911148'>, <JIRA Issue: key='SLIDER-966', id='12911153'>, <JIRA Issue: key='SLIDER-967', id='12911289'>, <JIRA Issue: key='SLIDER-969', id='12911291'>, <JIRA Issue: key='SLIDER-970', id='12911294'>, <JIRA Issue: key='SLIDER-978', id='12912225'>, <JIRA Issue: key='SLIDER-979', id='12912245'>, <JIRA Issue: key='SLIDER-985', id='12912912'>, <JIRA Issue: key='SLIDER-988', id='12913756'>, <JIRA Issue: key='SLIDER-989', id='12913850'>, <JIRA Issue: key='SLIDER-994', id='12914155'>, <JIRA Issue: key='SLIDER-998', id='12914899'>, <JIRA Issue: key='SLIDER-1026', id='12921953'>, <JIRA Issue: key='SLIDER-1029', id='12922660'>, <JIRA Issue: key='SLIDER-1047', id='12928102'>, <JIRA Issue: key='SLIDER-1048', id='12928103'>]",8,"[<JIRA IssueLink: id='12412235'>, <JIRA IssueLink: id='12448048'>, <JIRA IssueLink: id='12486845'>, <JIRA IssueLink: id='12424465'>, <JIRA IssueLink: id='12452279'>, <JIRA IssueLink: id='12444875'>, <JIRA IssueLink: id='12449112'>, <JIRA IssueLink: id='12412236'>]","slider has an anti-affinity flag in roles (visible in resources.json?), which is ignored.

YARN-1042 promises this for YARN, slider will need
# flag in resources.json
# use in container requests

we may also want two policies: anti-affinity-desired, and -required. Then if required nodes get >1 container for the same component type on the same node, it'd have to request a new one and return the old one (Risk: getting the same one back). ",2014-05-20T11:30:20.154+0000,2016-11-17T21:36:24.498+0000,Fixed,Major
LEGAL-222,Can HBase redistribute JRuby?,LEGAL,Question,Closed,[],1,[<JIRA IssueLink: id='12432750'>],"HBase currently relies on an older version of JRuby (1.6.8) that we redistribute via the jruby-complete jar made by that project.

While cleaning up our license notifications (HBASE-14085) I've been verifying that we have any supplemental licensing info from included dependencies (that is, stuff beyond the license listed in a pom).

The overall licensing for JRuby is CPL 1.0 and it mentions bundling some other works under a combination of licenses that are all categorized as various degrees of fine for inclusion by an ASF project ([ref jruby 1.6.8 COPYING information|https://github.com/jruby/jruby/blob/1.6.8/COPYING]).

It also includes the ruby language files from MRI ruby 1.8 and 1.9 under ""The Ruby License"" ([ref jruby 1.6.8 LICENSE.RUBY details|https://github.com/jruby/jruby/blob/1.6.8/LICENSE.RUBY] and referenced no-op [LEGAL|https://github.com/jruby/jruby/blob/1.6.8/LEGAL]). These files are needed for jruby to function, so I can't e.g. excise them from the version we redistribute.

The legal FAQ mentions that it's okay for ruby-implemented projects to have dependencies that are under the Ruby License, but it doesn't categorize the license generally or state if it's okay to redistribute.

Can we redistribute this jar?",2015-07-30T15:52:48.689+0000,2016-02-23T03:21:51.960+0000,Fixed,Major
SPARK-4985,Parquet support for date type,SPARK,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12407574'>, <JIRA IssueLink: id='12404274'>]",Parquet serde support for DATE type,2014-12-29T06:35:50.442+0000,2015-03-23T03:49:26.199+0000,Fixed,Major
TEZ-2873,Add an option to disable relocalization,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12446121'>],"Re-localization does not handle all scenarios - such as patterns, setting permissions etc. An option to turn off relocalization would be useful to handle such situations without writing new patches.",2015-10-08T20:52:28.624+0000,2015-12-15T08:00:55.578+0000,,Major
YETUS-441,Add a precommit check for known CVEs from dependencies,YETUS,New Feature,Patch Available,[],3,"[<JIRA IssueLink: id='12540095'>, <JIRA IssueLink: id='12540094'>, <JIRA IssueLink: id='12534101'>]","Add in a precommit test that makes use of [The OWASP Dependency Check|https://www.owasp.org/index.php/OWASP_Dependency_Check] to look for known bad dependencies.

there's a maven plugin, ant task, and command line tool. So we should be able to build similar support to what we have for RAT.",2016-08-15T21:36:35.105+0000,2018-12-31T05:30:31.352+0000,,Major
LEGAL-227,UnboundID LDAP SDK for *TESTING* Apache Hive's HiveServer2,LEGAL,Question,Closed,[],1,[<JIRA IssueLink: id='12445805'>],"LEGAL-160 states that UnboundID's license is not compatible with ALv2. (Has this changed since then? ) I understand that it is not appropriate to re-distribute UnboundID binaries along with an Apache project.

My question is around using these libraries to STRICTLY test a feature of an Apache project in its automated unit test suite. The binaries will NOT be re-distributed with the Apache Hive binaries. The core project source code (not the unit test code that is also part of the git-hub project) does not rely, use or contain references to the UnboundID binaries. 

Only the unit test code uses it. 
Thank you",2015-10-12T18:36:04.896+0000,2016-02-18T09:57:26.053+0000,Won't Fix,Major
PHOENIX-4460,"High GC / RS shutdown when we use select query with ""IN"" clause using 4.10 phoenix client on 4.13 phoenix server",PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12522421'>, <JIRA IssueLink: id='12522327'>]","We were able to reproduce the High GC / RS shutdown / phoenix KeyRange query high object count issue on cluster today. 

Main observation is that this is reproducible when firing lots of query    select from xyz where abc in (?, ?, ...)  of this type with 4.10 phoenix client hitting 4.13 phoenix on HBase server side
 (4.10 client/4.10 server works fine, 4.13 client with 4.13 server works fine)

We wrote a loader client (attached) with the below table/query , upserted ~100 million rows and fired the query in parallel using 4-5 loader clients with 16 threads each

{code}
TABLE:  = ""CREATE TABLE "" + TABLE_NAME_TEMPLATE 
     + "" (\n"" + "" TestKey varchar(255) PRIMARY KEY, TestVal1 varchar(200), TestVal2 varchar(200), ""  + ""TestValue varchar(10000))"";

QUERY: = ""SELECT * FROM "" +  TABLE_NAME_TEMPLATE + "" WHERE TestKey IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""
{code}

After running this client immediately within a min or two we see the phoenix.query.KeyRange object count immediately going up to several lakhs and keeps on increasing continuously. This count doesn't seem to come down even after shutting down the clients 

{code}
-bash-4.1$ ~/current/bigdata-util/tools/Linux/jdk/jdk1.8.0_102_x64/bin/jmap -histo:live 90725 | grep KeyRange
  47:        274852        6596448  org.apache.phoenix.query.KeyRange
1851:             2             48  org.apache.phoenix.query.KeyRange$Bound
2434:             1             24  [Lorg.apache.phoenix.query.KeyRange$Bound;
3411:             1             16  org.apache.phoenix.query.KeyRange$1
3412:             1             16  org.apache.phoenix.query.KeyRange$2
{code}
After some time we also started seeing High GC issues and RegionServers crashing

Experiment Summary:
- 4.13 client/4.13 Server --- Issue not reproducible (we do see KeyRange count increasing upto few 100's)
- 4.10 client/4.10 Server --- Issue not reproducible (we do see KeyRange count increasing upto few 100's)
- 4.10 client/4.13 Server --- Issue reproducible as described above


",2017-12-14T17:41:53.772+0000,2018-03-29T09:49:30.292+0000,Fixed,Blocker
SPARK-28620,Double type returned for float type in Beeline/JDBC,SPARK,Bug,Open,[],3,"[<JIRA IssueLink: id='12567267'>, <JIRA IssueLink: id='12567052'>, <JIRA IssueLink: id='12567051'>]","{code:sql}
  test(""SPARK-28620"") {
    withJdbcStatement {
      statement =>
        val rs = statement.executeQuery(
          ""SELECT CAST('1.2345678901234e+20' AS FLOAT), CAST('4.56' AS FLOAT)"")
        rs.next()
        assert(rs.getObject(1).isInstanceOf[java.lang.Double])
        assert(rs.getObject(2).isInstanceOf[java.lang.Double])
    }
  }
{code}

This inconsistent with spark-shell and spark-sql:

{code:sql}
0: jdbc:hive2://localhost:10000> SELECT CAST('1.2345678901234e+20' AS FLOAT), CAST('4.56' AS FLOAT);
+-------------------------------------+----------------------+
| CAST(1.2345678901234e+20 AS FLOAT)  | CAST(4.56 AS FLOAT)  |
+-------------------------------------+----------------------+
| 1.2345679000000001E20               | 4.56                 |
+-------------------------------------+----------------------+
1 row selected (0.268 seconds)
{code}

{code:sql}
spark-sql> SELECT CAST('1.2345678901234e+20' AS FLOAT), CAST('4.56' AS FLOAT);
1.2345679E20	4.56

scala> spark.sql(""SELECT CAST('1.2345678901234e+20' AS FLOAT), CAST('4.56' AS FLOAT)"").show
+----------------------------------+-------------------+
|CAST(1.2345678901234e+20 AS FLOAT)|CAST(4.56 AS FLOAT)|
+----------------------------------+-------------------+
|                      1.2345679E20|               4.56|
+----------------------------------+-------------------+
{code}
",2019-08-05T07:51:18.839+0000,2021-09-11T20:53:06.210+0000,,Major
ORC-475,ORC reader should lazily get filesystem,ORC,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12574148'>, <JIRA IssueLink: id='12574147'>]","Get filesystem can be expensive for blob storage or when fs cache is not used. When orc tail is specified via reader options, orc reader doesn't have to eagerly get filesystem as it won't use it anyways. ",2019-03-06T22:52:54.459+0000,2019-11-15T06:56:08.736+0000,Fixed,Minor
HCATALOG-407,Fresh checkout of Hcatalog doesn't build; needs hive-serde in ivy,HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12351772'>],"A fresh git or svn checkout doesn't compile. The compiler can't find the types in Hive's serde package (TypeInfo, Serde, etc.).",2012-05-10T21:52:30.673+0000,2012-07-05T22:08:31.719+0000,Cannot Reproduce,Major
SENTRY-810,CTAS without location is not verified properly ,SENTRY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12431944'>],"HIVE-11319 puts us in an awkward situation where you can leverage your current permissions to overwrite existing directories. I recommend we edit the CTAS permissions map to reflect that of a true insert overwrite directory. This will be annoying for users trying to do legit CTAS operations within the warehouse but as it stands it leave a gaping hole in the security model

",2015-07-22T23:20:19.849+0000,2015-08-05T07:47:18.727+0000,Fixed,Major
SPARK-35286,Replace SessionState.start with SessionState.setCurrentSessionState,SPARK,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12614982'>],"To avoid SessionState.createSessionDirs creating too many directories:

https://user-images.githubusercontent.com/5399861/116766834-28ea7080-aa5f-11eb-85ff-07bcaee444e5.png",2021-05-01T14:11:50.383+0000,2021-05-16T10:40:26.182+0000,Fixed,Major
CALCITE-739,Extend RexUtil.pullFactors to recognize additional common factors,CALCITE,Bug,Open,[],1,[<JIRA IssueLink: id='12425321'>],"RexUtil.pullFactors canonizes at the term level (i.e. ""a or b or a"" becomes ""a or b"" but does not attempt to recognize terms that are equivalent). Further, it does not exploit the symmetry of '=' (i.e. a = b iff b = a).

- A first extension would be to normalize comparisons between field references and literals so that the lower field reference is always on the left. So, ""$6 = $3"" becomes ""$3 = $6""; ""$6 > $3"" becomes ""$3< $6"". And ""literal <= $5"" becomes ""$5 >= literal"". This would not damage performance, and would improve a few plans.

- Another possible extension. Given the predicate ""(a or b) and ((x and a) or (y and b))"", the first factor can be removed so the expression consists only of ""(x and a) or (y and b)"".
One possible way to recognize such cases is to transform the second factor to CNF i.e. ""(x or y) and (x or b) and (a or y) and (a or b)"", and as it contains ""(a or b)"", we would know that we can discard it. Then we could just use the original expression i.e. ""(x and a) or (y and b)"" in the predicate, once we have done the check.",2015-05-20T13:14:29.886+0000,2015-05-21T21:36:14.098+0000,,Major
ACCUMULO-3957,Consider moving off getContentSummary in the monitor,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12434096'>],"Recently heard about an issue where a large Hadoop installation which had Accumulo running was experiencing long pauses in the Namenode. Inspecting NN audit logs, it was found that the user running Accumulo issues a {{getContentSummary(""/"")}} call just before the NN pauses were experienced.

In {{DefaultServlet.java}}, we use this call to compute the total HDFS disk usage and present a ratio of space that Accumulo uses relative to the total available space.

It's still unclear why this was causing issues in this case (as this operation should only be acquiring a read-lock in the namenode), it was recommended to me that Accumulo use the JMX metrics for the NN instead of making this call.",2015-08-11T00:52:38.054+0000,2015-08-29T21:51:12.275+0000,Fixed,Critical
SENTRY-1971,Hive integration for auth-2 should handle creating function properly,SENTRY,Bug,Open,[],2,"[<JIRA IssueLink: id='12516026'>, <JIRA IssueLink: id='12516081'>]","Sergio found hive does not include UDF class name for creating function command as input to sentry. That will break the function authorization.

Once HIVE-17544 is fixed, sentry should change code accordingly to make authorization for creating function work.",2017-09-27T21:49:04.966+0000,2019-06-14T19:19:05.366+0000,,Critical
YETUS-485,Yetus run is failing on branch after rebase/force push,YETUS,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12493106'>, <JIRA IssueLink: id='12493105'>]",I rebased YARN-2915 with trunk and did a force push following which all Yetus runs fails as the local enlistment in the jenkins slave is outdated: https://builds.apache.org/job/PreCommit-YARN-Build/14775/console. Looked at it with [~chris.douglas] and [~asuresh]'s help and we decided to directly use the commit hash based on https://github.com/apache/yetus/blob/master/precommit/test-patch.sh#L1356-L1368 but even that didn't work out as yetus does a pull after the checkout: https://builds.apache.org/job/PreCommit-YARN-Build/14800/console.,2017-01-31T21:02:36.160+0000,2017-06-26T22:56:05.510+0000,Fixed,Critical
TEZ-3250,TezTaskRunner2 should accept ExecutorService,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12466167'>],TezTaskRunner2 currently accepts ListeningExecutorService. Also it is not making use of the callback listener feature of ListeningExecutorService. It should instead accept the base ExecutorService.,2016-05-10T20:21:43.235+0000,2016-07-09T19:16:06.868+0000,Fixed,Major
THRIFT-1468,Memory leak in TSaslServerTransport,THRIFT,Bug,Closed,[],2,"[<JIRA IssueLink: id='12346355'>, <JIRA IssueLink: id='12347034'>]","I'm working on the HCatalog project. HCatalog uses a (slightly dated) version of Hive that in turn depends on libthrift-0.5.0. The HCatalog-server is a continuously running process that serves (meta)data over thrift. (The bug I describe is related to HCATALOG-183.)

We observed that on running the HCatalog-server with continuous client-requests, the memory footprint of the server grows steadily, until we see an OutOfMemoryError exception. I took a memory snapshot of the running process, to check for leaks. I noticed that the majority of the memory (over 1.3GB) was being consumed by the org.apache.thrift.transport.TSaslServerTransport$Factory::transportMap. There were over 52000 instances of WeakHashMap$Entry, consuming 3MB of shallow-heap, and 1.3GB of retained heap.

I suspect that entries in the WeakHashMap (transportMap) are not being collected during GC, as is expected in code. That would only be so if there are outstanding hard-references to the key in the map (TTransport).

From the code in TSaslTransport and TSaslServerTransport, it appears that there is an inadvertent cyclic reference that the runtime is unable to detect:
1. TSaslTransport has a (hard) back-reference to the ""underlyingTransport"", i.e. TTransport.
2. TSaslServerTransport::Factory::transportMap is a WeakHashMap< TTransport, TSaslServerTransport >. Here, the ""underlyingTransport"" is mapped back to the decorating TSaslServerTransport.

From #2, an entry can only be GCed if there's no outstanding hard-reference to the TTransport. But from #1, the hard-reference comes from the value-part of the hashmap entry. The runtime can't deduce that there's a cycle, presumably because it's not explicit.

(I'll be attaching a sample program to better illustrate the WeakHashMap behaviour, in case I've botched the explanation above.)

The simple solution would be to change the back-reference in #1 into a WeakReference. I'll attach a patch here that might be suitable.",2011-12-16T08:10:27.046+0000,2012-01-12T19:42:50.606+0000,Fixed,Major
BIGTOP-2077,Bump HBase version to 1.1,BIGTOP,Improvement,Closed,[],8,"[<JIRA IssueLink: id='12448380'>, <JIRA IssueLink: id='12448379'>, <JIRA IssueLink: id='12448378'>, <JIRA IssueLink: id='12457113'>, <JIRA IssueLink: id='12457115'>, <JIRA IssueLink: id='12457357'>, <JIRA IssueLink: id='12465386'>, <JIRA IssueLink: id='12448238'>]",Time to upgrade to HBase 1.1.x,2015-09-28T13:28:53.327+0000,2017-03-29T18:46:03.897+0000,Fixed,Major
BIGTOP-3575,Hive rpm build warning due to missing dependency,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12620871'>],"A warning occurs when building rpm packages for HIVE:
{code:java}
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/directory/client/ldap/ldap-client-api/0.1-SNAPSHOT/maven-metadata.xml
Downloading from apache.snapshots: https://repository.apache.org/snapshots/org/apache/directory/client/ldap/ldap-client-api/0.1-SNAPSHOT/maven-metadata.xml
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/directory/client/ldap/ldap-client-api/0.1-SNAPSHOT/ldap-client-api-0.1-SNAPSHOT.pom
Downloading from apache.snapshots: https://repository.apache.org/snapshots/org/apache/directory/client/ldap/ldap-client-api/0.1-SNAPSHOT/ldap-client-api-0.1-SNAPSHOT.pom
[WARNING] The POM for org.apache.directory.client.ldap:ldap-client-api:jar:0.1-SNAPSHOT is missing, no dependency information available
... 
[INFO] --- maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ hive-service ---
[WARNING] Missing POM for org.apache.directory.client.ldap:ldap-client-api:jar:0.1-SNAPSHOT
... {code}
The problem is solved in HIVE-21777.
 It is enough to apply the patch from this task.",2021-08-10T18:49:02.382+0000,2021-08-11T22:31:45.322+0000,Fixed,Major
SQOOP-1064,Move HCatalog tests to Unit tests after Hcatalog hadoop 2.0 artifacts are released,SQOOP,Bug,Open,[],3,"[<JIRA IssueLink: id='12369775'>, <JIRA IssueLink: id='12390922'>, <JIRA IssueLink: id='12369777'>]","With the integration of HCatalog with Sqoop, we wanted to run all HCatalog functional tests as part of the build given the criticality of the feature.  But unfortunately, Hive/HCatalog teams have not released the HCatalog artifacts for Hadoop 2.x which hampers that

To make progress, we have decided to make the HCatalog tests as an integration test but ideally we would like to run it as unit tests.

When HIVE-4460 is fixed, we will revert the HCatalog tests as unit tests.   This JIRA issue is used to track the Hive issue and do the necessary changes once it is fixed",2013-06-02T20:28:42.744+0000,2014-07-05T01:51:38.502+0000,,Major
HCATALOG-442,Documentation needs update for using HCatalog with pig,HCATALOG,Bug,Closed,[],1,[<JIRA IssueLink: id='12357134'>],"By virtue of https://issues.apache.org/jira/browse/PIG-2766, users can now work with hcatalog via pig by just including a -useHCatalog flag with pig as 'pig -useHCatalog'. This brings in the appropriate jars for pig to work with hcatalog. This needs to be documented.",2012-06-29T22:16:26.401+0000,2013-02-15T21:32:49.122+0000,Fixed,Major
BIGTOP-3304,Fix build failure of Flume against hive-2.3.6,BIGTOP,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12580344'>, <JIRA IssueLink: id='12580270'>, <JIRA IssueLink: id='12580433'>]",Compilation of flume-hive-sink fails when building with {{-Dhive.version=2.3.6}}.,2020-02-12T12:28:43.780+0000,2020-02-15T10:43:40.621+0000,Fixed,Major
DRILL-1569,Support Hive SQL,DRILL,Task,Open,[],2,"[<JIRA IssueLink: id='12399528'>, <JIRA IssueLink: id='12399526'>]","Hive 0.14 added cost based optimization and optiq operator tree is generated for most select queries. If drill is able to use those optiq operator trees and convert optiq operator tree to drill logical plan, then running hive sql on drill become possible. 

This task is to implement the hive optiq operator tree -> drill logical plan conversion and enable hive sql support in Drill.  ",2014-10-22T17:31:08.675+0000,2014-12-28T23:31:18.994+0000,,Major
TEZ-2910,Set caller context for tracing ( integrate with HDFS-9184 ) ,TEZ,Bug,Closed,[],4,"[<JIRA IssueLink: id='12471193'>, <JIRA IssueLink: id='12452061'>, <JIRA IssueLink: id='12452060'>, <JIRA IssueLink: id='12455301'>]",,2015-10-23T22:39:05.174+0000,2016-06-22T01:04:35.140+0000,Fixed,Major
ACCUMULO-1083,add concurrency to HDFS write-ahead log,ACCUMULO,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12365546'>, <JIRA IssueLink: id='12376513'>, <JIRA IssueLink: id='12365635'>, <JIRA IssueLink: id='12399438'>]","When running tablet servers on beefy nodes (lots of disks), the write-ahead log can be a serious bottleneck. Today we ran a continuous ingest test of 1.5-SNAPSHOT on an 8-node (plus a master node) cluster in which the nodes had 32 cores and 15 drives each. Running with write-ahead log off resulted in a >4x performance improvement sustained over a long period.

I believe the culprit is that the WAL is only using one file at a time per tablet server, which means HDFS is only appending to one drive (plus replicas). If we increase the number of concurrent WAL files supported on a tablet server we could probably drastically improve the performance on systems with many disks. As it stands, I believe Accumulo is significantly more optimized for a larger number of smaller nodes (3-4 drives).",2013-02-22T02:53:32.515+0000,2015-04-04T02:28:55.175+0000,Won't Fix,Major
SPARK-34562,Leverage parquet bloom filters,SPARK,Improvement,Resolved,[],7,"[<JIRA IssueLink: id='12609647'>, <JIRA IssueLink: id='12609644'>, <JIRA IssueLink: id='12609649'>, <JIRA IssueLink: id='12609650'>, <JIRA IssueLink: id='12609648'>, <JIRA IssueLink: id='12609645'>, <JIRA IssueLink: id='12609646'>]","The currently in-progress SPARK-34542 brings in parquet 1.12, which contains PARQUET-41.

From searching the issues, it seems there is no current tracker for this, though I found a [comment|https://issues.apache.org/jira/browse/SPARK-20901?focusedCommentId=17052473&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17052473] from [~dongjoon] that points out the missing parquet support up until now.",2021-02-27T18:00:47.027+0000,2021-04-12T14:08:03.053+0000,Fixed,Major
TEZ-4359,ShuffleHandler: Make sure of properly releasing netty reference counted objects,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12628582'>],"Most probably, the very same fix should be applied to the tez shufflehandler as HIVE-25706 as they have a common codebase and both of them were upgraded to netty4 in a similar way.",2021-12-14T10:01:26.113+0000,2022-03-10T19:55:01.642+0000,Fixed,Major
SUBMARINE-1,Move code base of submarine from yarn-applications to top directory,SUBMARINE,Task,Resolved,[],1,[<JIRA IssueLink: id='12554543'>],We need to move submarine project from yarn-applications to top directory of hadoop.,2019-02-11T23:06:25.587+0000,2019-02-28T19:36:18.306+0000,Fixed,Major
ORC-309,WriterImpl.close() calls WriterCallback methods out of order,ORC,Bug,Open,[],1,[<JIRA IssueLink: id='12528269'>],"}}org.apache.orc.impl.WriterImpl}}
{noformat}
  @Override
  public void close() throws IOException {
    if (callback != null) {
      callback.preFooterWrite(callbackContext);
    }
    // remove us from the memory manager so that we don't get any callbacks
    memoryManager.removeWriter(path);
    // actually close the file
    flushStripe();
    lastFlushOffset = writeFooter();
    physicalWriter.close();
  }
 {noformat}
so this causes preFooterWrite() to be called before {{preStripeWrite()}} from {{flushStripe()}}.

HIVE-18817 is one place where this shows up",2018-02-28T22:32:45.042+0000,2018-03-02T19:02:04.336+0000,,Major
ORC-477,BloomFilter for ACID table does not get created,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12556535'>, <JIRA IssueLink: id='12558733'>]",see HIVE-21397,2019-03-08T13:14:46.234+0000,2019-06-28T03:56:42.491+0000,Fixed,Major
SPARK-27733,Upgrade to Avro 1.10.1,SPARK,Sub-task,Resolved,[],7,"[<JIRA IssueLink: id='12593867'>, <JIRA IssueLink: id='12561017'>, <JIRA IssueLink: id='12601122'>, <JIRA IssueLink: id='12601121'>, <JIRA IssueLink: id='12609622'>, <JIRA IssueLink: id='12610945'>, <JIRA IssueLink: id='12580812'>]","Avro 1.9.2 was released with many nice features including reduced size (1MB less), and removed dependencies, no paranamer, no shaded guava, security updates, so probably a worth upgrade.

Avro 1.10.0 was released and this is still not done.

There is at the moment (2020/08) still a blocker because of Hive related transitive dependencies bringing older versions of Avro, so we could say that this is somehow still blocked until HIVE-21737 is solved.",2019-05-15T21:00:10.306+0000,2021-03-17T17:26:47.662+0000,Fixed,Major
TEZ-1045,TezMiniCluster tests can fail intermittently,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12393947'>],,2014-04-11T16:31:37.778+0000,2014-09-06T01:36:06.440+0000,Fixed,Critical
SPARK-33416,Avoid Hive metastore stack overflow when InSet predicate have many values,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12603095'>],"How to reproduce this issue:
{code:sql}
CREATE TABLE test_partition (id bigint, cal_date string) USING PARQUET PARTITIONED BY (cal_date);

SELECT count(*) FROM test_partition where cal_date in(
 '2004-12-19',
 '2009-12-31',
 '1995-01-22',
 '1999-03-13',
 '2015-01-31',
 '2017-12-17',
 '2007-09-23',
 '1995-01-28',
 '2012-06-09',
 '2015-07-08',
 '1991-04-03',
 '1999-09-23',
 '2008-11-30',
 '2010-05-26',
 '2017-09-12',
 '2012-12-11',
 '2013-09-29',
 '1993-12-12',
 '2018-06-17',
 '1995-10-14',
 '1998-02-05',
 '2005-12-06',
 '2013-12-10',
 '1995-12-02',
 '2005-12-02',
 '1995-03-06',
 '1993-06-11',
 '2009-05-23',
 '2019-04-30',
 '2013-11-22',
 '1998-07-22',
 '2011-07-08',
 '2014-03-26',
 '1993-11-02',
 '2014-09-30',
 '1995-09-27',
 '2001-02-12',
 '1990-12-05',
 '1995-11-06',
 '1996-10-11',
 '1992-02-17',
 '2012-10-17',
 '2012-07-07',
 '2003-02-26',
 '2012-10-25',
 '2020-06-17',
 '1995-02-24',
 '2018-10-13',
 '2007-08-21',
 '2007-09-02',
 '2010-11-01',
 '2013-02-14',
 '2001-06-10',
 '1992-08-17',
 '1991-11-16',
 '2015-02-26',
 '2006-04-07',
 '1990-03-16',
 '1990-07-20',
 '1998-03-09',
 '1990-11-10',
 '1995-01-05',
 '2007-07-08',
 '2016-10-17',
 '2012-11-12',
 '2016-04-28',
 '1990-12-12',
 '2006-06-18',
 '2016-07-15',
 '2017-08-25',
 '2001-04-15',
 '1997-01-27',
 '1993-02-06',
 '1991-01-17',
 '1998-12-12',
 '1990-01-22',
 '2015-04-25',
 '2015-01-16',
 '2002-05-18',
 '1991-09-11',
 '1996-09-16',
 '2008-08-18',
 '1990-03-22',
 '2004-08-03',
 '1997-05-15',
 '2009-07-27',
 '2004-10-23',
 '2020-03-18',
 '2011-12-22',
 '2014-05-14',
 '2016-03-09',
 '2015-01-25',
 '2020-06-11',
 '1990-07-01',
 '2017-06-16',
 '2009-12-05',
 '1992-02-29',
 '2012-12-08',
 '1997-12-24',
 '2002-05-22',
 '2019-02-19',
 '1993-07-24',
 '2014-02-05',
 '2011-11-28',
 '2000-08-02',
 '2003-04-28',
 '2000-12-04',
 '1994-01-26',
 '2011-05-30',
 '1992-05-19',
 '2015-12-04',
 '2008-12-08',
 '1998-03-23',
 '2003-05-23',
 '2015-03-25',
 '1999-03-18',
 '2003-05-13',
 '1994-07-27',
 '2014-11-14',
 '2020-03-15',
 '2013-03-28',
 '2003-06-30',
 '1997-04-04',
 '2007-10-02',
 '2007-02-14',
 '2013-07-07',
 '2007-01-04',
 '2019-01-13',
 '2008-09-17',
 '2019-01-19',
 '1995-07-30',
 '1997-05-27',
 '2014-05-01',
 '2015-08-10',
 '2017-11-08',
 '2012-08-01',
 '1999-08-19',
 '1995-01-15',
 '2000-08-18',
 '2014-11-21',
 '1992-08-19',
 '2012-01-13',
 '2008-05-20',
 '1995-07-04',
 '2000-10-16',
 '1999-08-22',
 '2005-01-12',
 '2019-06-08',
 '2000-07-06',
 '1994-11-18',
 '1995-12-29',
 '2016-12-09',
 '2004-05-01',
 '2013-04-07',
 '1993-10-04',
 '2017-07-21',
 '2006-10-03',
 '1991-09-13',
 '2017-03-18',
 '1992-07-22',
 '2017-09-02',
 '2016-11-22',
 '1991-07-05',
 '2008-05-18',
 '1998-08-23',
 '2008-06-09',
 '1993-04-22',
 '1998-09-28',
 '1997-10-07',
 '2002-06-16',
 '2005-03-27',
 '2003-11-15',
 '2006-05-07',
 '1999-09-26',
 '2000-09-19',
 '1992-05-22',
 '2016-05-31',
 '1994-07-23',
 '2000-06-10',
 '1999-04-19',
 '1991-12-02',
 '2013-09-08',
 '2005-04-15',
 '1990-10-12',
 '2011-07-22',
 '2009-09-29',
 '2005-01-15',
 '1998-05-12',
 '2000-08-29',
 '1992-06-10',
 '2009-07-04',
 '1999-04-02',
 '2016-08-05',
 '2009-07-16',
 '1999-07-27',
 '1990-01-12',
 '2004-11-16',
 '2006-02-19',
 '1991-06-27',
 '1998-05-04',
 '2003-09-26',
 '2016-07-24',
 '2007-05-31',
 '1993-10-21',
 '1999-02-13',
 '2007-12-23',
 '2017-10-08',
 '2014-01-31',
 '2006-10-18',
 '1994-05-23',
 '2011-08-06',
 '1993-05-19',
 '2008-05-14',
 '2010-12-05',
 '1995-06-06',
 '1995-12-13',
 '2000-01-05',
 '2000-06-19',
 '2006-06-07',
 '2004-11-14',
 '2002-05-19',
 '2004-12-28',
 '2008-08-09',
 '2011-05-04',
 '1995-06-02',
 '2002-03-27',
 '1997-05-02',
 '2014-08-28',
 '1997-05-06',
 '2020-09-03',
 '1996-09-13',
 '2003-12-20',
 '2003-07-17',
 '1992-05-03',
 '2014-08-30',
 '2015-02-06',
 '2009-04-13',
 '2012-01-07',
 '1994-03-31',
 '2010-08-15',
 '2016-03-29',
 '2020-02-22',
 '1999-12-11',
 '2005-08-17',
 '1993-02-14',
 '1996-08-14',
 '2016-12-22',
 '2011-01-10',
 '2020-08-16',
 '2007-12-27',
 '2006-01-13',
 '2007-02-20',
 '2000-12-31',
 '2015-11-26',
 '2016-08-11',
 '2012-12-15',
 '1991-02-27',
 '2014-08-10',
 '2001-11-24',
 '2014-06-01',
 '2009-01-17',
 '2005-06-03',
 '2015-12-23',
 '1990-09-04',
 '2015-07-21',
 '2011-03-25',
 '2006-12-28',
 '2007-02-24',
 '1993-07-09',
 '2020-03-22',
 '2006-09-09',
 '2000-08-03',
 '1994-06-30',
 '2001-09-01',
 '2016-04-26',
 '2001-09-27',
 '2015-10-24',
 '2020-02-11',
 '2005-05-07',
 '2005-06-26',
 '2013-05-04',
 '1994-08-04',
 '1994-03-16',
 '2005-05-31',
 '2003-11-30',
 '2020-07-01',
 '1993-06-10',
 '2019-04-08',
 '2000-06-27',
 '2009-07-11',
 '2020-03-26',
 '2001-12-26',
 '2013-01-17',
 '1998-03-20',
 '2019-03-23',
 '1996-07-15',
 '1991-04-05',
 '1993-12-28',
 '2001-09-19',
 '1997-04-23',
 '2009-03-01',
 '2012-12-26',
 '1994-09-01',
 '2009-08-18',
 '2018-06-15',
 '1991-06-29',
 '2008-08-30',
 '2013-12-21',
 '2013-05-17',
 '2012-08-10',
 '2001-03-02',
 '2017-07-25',
 '2008-10-11',
 '1994-01-04',
 '2018-11-28',
 '2018-08-29',
 '2018-03-09',
 '2006-06-23',
 '2013-03-30',
 '2020-09-16',
 '2004-10-01',
 '2005-11-05',
 '2006-01-26',
 '1991-04-08',
 '2003-01-02',
 '1992-10-09',
 '1991-04-10',
 '2012-11-01',
 '1992-12-04',
 '2018-05-24',
 '2001-04-22',
 '2004-04-21',
 '1993-06-28',
 '2002-08-04',
 '2005-03-25',
 '2014-11-19',
 '2003-10-23',
 '2004-12-26',
 '2002-12-30',
 '2011-08-20',
 '2015-11-04',
 '1990-04-28',
 '2000-04-16',
 '2013-11-11',
 '2001-10-05',
 '2011-07-13',
 '2005-02-15',
 '2015-02-21',
 '2020-02-03',
 '2001-05-30',
 '1992-01-13',
 '2011-07-19',
 '1994-09-24',
 '2018-08-20',
 '1991-03-03',
 '2016-05-13',
 '1996-08-28',
 '2010-06-13',
 '2002-08-03',
 '1996-03-18',
 '1995-04-12',
 '2016-01-10',
 '1992-06-07',
 '2015-09-10',
 '1999-03-24',
 '1993-06-16',
 '2005-05-30',
 '1996-09-25',
 '1995-06-04',
 '2012-05-20',
 '1990-09-18',
 '2004-04-10',
 '1998-09-19',
 '2007-06-03',
 '2004-11-26',
 '2018-05-07',
 '2003-10-02',
 '2013-04-10',
 '2011-03-19',
 '2000-09-30',
 '1996-06-26',
 '2016-11-29',
 '2010-05-07',
 '2002-04-08',
 '2017-05-21',
 '1998-11-10',
 '2018-02-12',
 '2011-12-12',
 '2000-01-18',
 '1994-01-20',
 '2011-05-08',
 '2015-09-30',
 '1995-10-07',
 '2016-10-10',
 '1992-08-11',
 '1999-01-03',
 '2010-12-08',
 '2020-09-12',
 '2009-01-08',
 '2007-02-18',
 '1991-11-10',
 '2005-01-29',
 '1993-09-15',
 '2002-01-03',
 '2003-09-22',
 '2009-07-28',
 '1994-11-03',
 '2014-08-16',
 '2009-10-11',
 '1991-04-11',
 '2009-04-21',
 '2018-11-09',
 '2018-07-09',
 '1994-10-10',
 '1994-12-06',
 '2020-06-01',
 '2002-08-21',
 '1994-07-20',
 '1997-09-15',
 '2009-09-04',
 '2000-03-24',
 '2017-09-06',
 '2010-11-06',
 '2004-08-08',
 '2003-01-19',
 '2004-09-13',
 '2000-12-10',
 '2004-10-05',
 '2007-09-13',
 '2016-01-11',
 '1992-08-21',
 '2004-06-05',
 '1992-06-08',
 '1991-05-27',
 '2016-05-30',
 '1996-04-04',
 '2003-11-04',
 '2017-04-24',
 '2010-12-12',
 '2002-04-14',
 '2014-06-29',
 '2014-02-02',
 '2010-11-07',
 '2010-01-02',
 '2010-04-20',
 '2009-05-10',
 '2013-11-27',
 '1994-11-13',
 '2011-01-21',
 '2012-09-15',
 '1993-07-15',
 '2003-11-23',
 '2018-01-17',
 '2007-10-12',
 '2018-01-09',
 '2013-03-25',
 '2019-09-29',
 '2017-10-01',
 '2004-01-25',
 '2005-05-09',
 '2005-01-10',
 '2016-10-14',
 '2008-07-12',
 '2018-05-30',
 '2011-08-29',
 '2016-03-28',
 '2019-11-17',
 '1990-07-09',
 '1994-08-02',
 '1992-09-03',
 '2020-06-07',
 '1990-10-31',
 '2010-08-27',
 '2010-11-15',
 '2009-09-25',
 '2009-01-27',
 '2007-10-01',
 '1991-05-13',
 '2001-11-15',
 '2011-12-27',
 '2004-08-16',
 '2012-01-16',
 '2001-09-08',
 '2008-08-15',
 '1998-04-02',
 '2019-02-16',
 '2011-12-26',
 '2014-01-07',
 '2013-10-29',
 '1996-01-23',
 '2004-12-24',
 '2007-09-06',
 '2019-11-14',
 '2008-09-07',
 '2002-09-21',
 '1998-06-06',
 '1990-05-20',
 '2004-07-03',
 '1998-08-12',
 '2002-07-16',
 '1998-11-12',
 '2003-09-18',
 '2007-02-07',
 '2013-05-05',
 '2019-11-20',
 '1992-09-19',
 '2001-06-22',
 '1995-06-29',
 '2018-07-01',
 '2006-07-09',
 '2006-12-07',
 '1992-06-14',
 '2014-08-09',
 '1990-12-14',
 '2015-09-09',
 '1997-02-07',
 '2017-08-24',
 '2004-04-05',
 '2009-09-02',
 '1996-01-12',
 '1998-07-14',
 '1993-03-24',
 '2001-09-14',
 '1990-04-29',
 '1991-01-27',
 '1992-04-13',
 '2001-01-02',
 '1996-02-06',
 '1995-05-29',
 '2007-08-28',
 '2014-01-04',
 '2012-02-01',
 '2007-04-25',
 '2006-10-07',
 '1993-10-26',
 '2019-02-04',
 '2017-05-13',
 '2009-12-19',
 '2017-11-17',
 '2003-01-10',
 '2013-01-12',
 '2000-12-28',
 '2017-08-21',
 '2003-09-08',
 '2008-11-11',
 '2003-03-19',
 '2013-01-23',
 '2004-01-20',
 '1996-08-25',
 '1995-10-29',
 '1992-03-02',
 '2000-03-19',
 '1990-11-22',
 '1999-05-22',
 '1998-06-29',
 '2020-09-30',
 '2009-07-22',
 '2001-02-07',
 '2008-05-11',
 '2006-11-11',
 '2009-04-09',
 '2011-07-21',
 '2008-10-18',
 '2016-06-01',
 '2004-03-15',
 '1992-08-18',
 '2005-05-15',
 '2011-04-28',
 '2010-08-29',
 '1991-08-23',
 '1999-11-28',
 '2009-02-19',
 '2002-04-04',
 '2006-10-20',
 '2006-12-17',
 '1997-01-16',
 '2016-01-08',
 '1999-11-08',
 '2000-09-09',
 '1995-01-03',
 '2013-09-27',
 '2005-08-06',
 '2007-04-22',
 '1999-08-10',
 '2007-07-06',
 '1999-04-15',
 '2016-09-16',
 '2007-06-20',
 '2010-04-18',
 '1992-06-21',
 '1991-12-07',
 '2006-04-27',
 '2012-09-25',
 '1998-04-20',
 '2011-10-29',
 '2005-06-15',
 '2008-12-01',
 '2012-12-02',
 '2012-04-23',
 '2010-12-04',
 '1991-10-25',
 '2004-05-15',
 '2011-03-17',
 '2000-06-18',
 '2019-11-28',
 '2013-01-21',
 '2011-09-23',
 '2016-04-20',
 '2017-08-04',
 '2015-03-05',
 '1991-06-23',
 '2000-10-22',
 '2013-12-02',
 '1999-02-19',
 '2017-12-01',
 '2018-12-17',
 '2008-08-04',
 '2003-05-16',
 '2013-01-27',
 '2018-06-19',
 '2013-05-06',
 '2007-04-24',
 '2009-04-27',
 '2016-05-11',
 '2017-09-05',
 '2018-07-12',
 '2007-11-16',
 '2008-10-19',
 '2007-09-27',
 '1990-04-04',
 '2001-06-24',
 '1999-01-13',
 '2003-12-11',
 '1991-07-25',
 '2005-04-11',
 '1993-04-18',
 '2015-03-28',
 '2014-09-24',
 '1998-07-03',
 '2010-07-13',
 '1991-03-13',
 '2006-07-13',
 '2000-11-28',
 '2008-10-05',
 '2012-11-09',
 '2002-07-01',
 '1992-10-29',
 '1991-05-01',
 '1990-12-10',
 '2001-02-17',
 '1996-10-13',
 '2013-08-27',
 '2014-05-13',
 '2004-12-13',
 '2011-01-22',
 '2001-10-25',
 '2001-06-19',
 '2004-01-15',
 '2003-04-01',
 '2005-11-08',
 '1994-04-24',
 '2015-01-05',
 '1997-01-09',
 '1997-12-13',
 '2003-12-12',
 '1995-08-21',
 '2007-11-25',
 '2019-03-22',
 '2015-02-28',
 '2012-02-02',
 '2017-10-10',
 '2000-01-20',
 '2013-06-05',
 '1993-07-03',
 '1992-06-23',
 '2016-05-04',
 '2013-05-22',
 '1990-05-13',
 '2007-02-08',
 '1995-11-13',
 '2013-01-29',
 '2019-07-09',
 '2019-07-02',
 '2017-06-08',
 '2000-05-24',
 '2012-12-27',
 '2002-06-29',
 '2016-02-12',
 '1997-10-11',
 '2013-01-18',
 '1998-10-16',
 '2005-09-16',
 '1999-03-14',
 '2005-10-28',
 '1998-09-25',
 '2003-07-01',
 '1991-06-22',
 '2010-06-22',
 '2011-04-20',
 '2012-06-15',
 '2011-06-30',
 '1991-08-31',
 '2001-09-30',
 '2002-05-27',
 '2016-04-17',
 '2013-08-11',
 '2007-08-04',
 '2005-09-18',
 '2004-10-12',
 '2005-05-01',
 '2007-07-07',
 '1996-08-23',
 '2011-01-31',
 '2015-11-02',
 '2016-02-11',
 '1996-11-13',
 '2019-12-13',
 '2000-02-16',
 '2000-04-24',
 '1995-10-30',
 '2017-08-18',
 '2010-04-06',
 '1993-09-21',
 '1990-03-14',
 '2015-11-11',
 '2004-05-24',
 '2000-08-25',
 '2004-12-05',
 '2000-10-25',
 '1997-04-20',
 '2002-07-14',
 '1990-08-27',
 '2009-02-22',
 '1996-01-05',
 '2011-12-02',
 '2014-01-17',
 '1993-11-05',
 '1996-01-15',
 '1996-04-09',
 '1999-01-16',
 '2015-10-07',
 '1992-10-23',
 '2013-02-24',
 '2013-06-15',
 '2013-02-10',
 '1992-12-13',
 '2018-09-10',
 '2007-12-10',
 '2018-11-24',
 '2016-11-24',
 '1994-07-09',
 '1998-09-05',
 '2004-01-14',
 '2019-08-15',
 '2015-11-15',
 '2002-05-13',
 '2019-10-05',
 '2016-01-21',
 '1997-01-06',
 '2015-12-31',
 '2000-04-01',
 '1994-09-10',
 '2001-09-29',
 '2020-07-10',
 '2013-08-30',
 '1991-05-30',
 '2004-11-05',
 '1997-10-09',
 '2017-05-20',
 '2015-11-27',
 '2005-07-17',
 '1995-12-06',
 '1991-02-28',
 '2016-10-06',
 '1999-07-23',
 '1998-07-04',
 '2000-03-22',
 '2004-07-30',
 '1998-10-09',
 '2007-04-02',
 '2000-09-02',
 '1994-10-04',
 '2004-05-21',
 '2000-03-08',
 '2014-01-25',
 '2015-06-16',
 '2005-07-23',
 '1997-10-06',
 '2019-06-23',
 '1992-03-24',
 '2012-04-15',
 '1990-06-01',
 '2004-09-17',
 '2010-11-02',
 '2008-02-20',
 '2010-07-20',
 '2015-08-29',
 '1990-09-13',
 '2018-10-18',
 '2018-01-12',
 '1998-08-05',
 '1994-02-20',
 '1992-04-29',
 '2009-01-24',
 '2003-04-15',
 '1997-11-23',
 '2012-01-09',
 '2008-04-13',
 '1991-01-28',
 '1995-10-09',
 '2002-08-24',
 '1993-10-27',
 '2004-08-27',
 '2000-08-06',
 '1997-03-05',
 '1998-11-28',
 '2008-05-09',
 '2012-03-09',
 '2018-07-10',
 '2011-07-02',
 '1997-04-14',
 '2012-12-14',
 '2003-06-06',
 '2009-11-08',
 '1998-02-11',
 '1998-05-28',
 '2005-10-27',
 '2003-10-19',
 '2008-04-18',
 '1991-09-12',
 '1993-12-10',
 '1997-05-19',
 '2000-10-14',
 '2019-01-09',
 '1992-04-20',
 '1998-01-05',
 '2014-03-06',
 '2019-01-28',
 '2009-10-22',
 '2016-04-19',
 '1997-09-27',
 '2007-03-19',
 '2018-01-31',
 '2013-10-31',
 '2009-04-26',
 '2011-03-26',
 '2009-11-02',
 '1998-12-21',
 '2007-07-22',
 '2002-12-23',
 '1995-04-13',
 '2007-07-25',
 '2017-07-09',
 '1999-02-07',
 '2015-07-01',
 '2004-02-03',
 '1999-04-30',
 '2012-10-16',
 '1995-03-10',
 '2003-05-08',
 '1993-03-07',
 '2004-01-07',
 '2009-09-17',
 '2006-10-02',
 '2004-02-10',
 '1998-11-19',
 '2007-11-29',
 '2001-04-01',
 '2006-12-08',
 '1992-08-30',
 '1992-01-17',
 '1994-03-21',
 '2017-12-15',
 '2003-03-18',
 '2008-05-07',
 '1992-07-24',
 '2015-02-16',
 '1994-11-24',
 '2003-07-05',
 '2013-02-18',
 '2016-09-12',
 '2016-05-26',
 '1992-04-22',
 '2020-08-17',
 '2012-12-01',
 '2002-09-26',
 '2010-02-09',
 '1998-02-10',
 '2016-06-02',
 '1993-08-15',
 '1994-03-23',
 '2017-03-05',
 '2003-03-17',
 '2017-06-03',
 '1990-09-14',
 '2016-07-30',
 '2002-05-07',
 '2007-06-09',
 '1995-05-08',
 '1997-08-31',
 '2013-11-05',
 '2010-05-08',
 '1995-07-18',
 '1995-05-23',
 '1999-11-04',
 '1990-11-11',
 '2000-09-27',
 '2008-07-20',
 '2001-10-16',
 '2017-10-19',
 '2002-06-21',
 '1992-01-21',
 '1991-04-14',
 '2013-04-02',
 '2019-01-03',
 '1995-12-19',
 '1998-11-02',
 '2013-09-18',
 '1991-04-04',
 '1991-12-27',
 '1996-08-12',
 '2015-04-16',
 '2010-06-11',
 '2017-12-02',
 '2010-03-04',
 '2016-06-15',
 '2016-08-28',
 '1994-08-26',
 '1992-09-18',
 '2008-08-19',
 '1991-02-16',
 '1990-11-27',
 '1991-04-07',
 '1996-04-13',
 '2001-03-01',
 '2003-12-13',
 '2005-07-04',
 '1990-06-17',
 '1996-10-26',
 '1991-02-25',
 '1995-09-08',
 '2013-11-25',
 '2014-11-23',
 '1991-02-23',
 '2014-08-14',
 '2001-02-24',
 '1990-09-16',
 '1998-01-18',
 '1999-01-22',
 '2001-03-26',
 '2002-01-02',
 '2002-03-12',
 '2015-10-03',
 '2001-06-26',
 '1998-05-02',
 '2001-10-30',
 '1995-01-07',
 '2012-05-05',
 '2001-09-20',
 '2005-07-19',
 '2011-10-10',
 '1996-09-19',
 '2017-01-07',
 '2019-06-02',
 '1999-02-28',
 '2016-11-05',
 '1997-11-04',
 '1995-03-27',
 '2013-11-30',
 '1993-06-25',
 '1995-02-22',
 '2015-02-02',
 '1997-12-06',
 '2002-10-04',
 '2017-02-05',
 '1997-03-07',
 '2017-05-15',
 '2009-12-22',
 '1994-12-30',
 '2007-11-14',
 '1998-08-28',
 '2012-10-21',
 '1999-07-30',
 '1992-02-26',
 '2017-08-14',
 '2009-10-15',
 '2017-07-01',
 '2007-05-17',
 '1994-07-29',
 '2003-01-07',
 '2003-03-07',
 '1993-07-02',
 '1993-02-22',
 '2001-09-15',
 '2000-05-25',
 '1990-12-08',
 '1990-04-17',
 '1992-04-21',
 '2007-04-11',
 '2017-07-15',
 '1999-10-01',
 '2007-11-04',
 '2000-10-15',
 '2009-06-08',
 '1994-03-17',
 '2003-12-03',
 '2002-07-26',
 '2004-11-01',
 '2003-05-05',
 '1995-04-30',
 '1994-12-26',
 '1997-11-27',
 '2001-09-25',
 '1997-04-19',
 '1997-06-11',
 '1991-04-09',
 '2001-12-10',
 '1997-05-31',
 '1998-05-14',
 '1999-08-15',
 '2013-08-24',
 '2017-09-22',
 '2012-02-10',
 '2005-12-12',
 '2008-03-02',
 '1997-10-27',
 '2012-12-29',
 '1996-02-05',
 '2014-07-07',
 '2015-05-07',
 '2003-12-22',
 '1990-02-28',
 '2003-07-12',
 '1990-03-27',
 '2007-04-09',
 '2014-03-22',
 '1994-01-31',
 '1993-07-29',
 '2014-03-12',
 '2016-09-11',
 '2011-06-11',
 '2005-01-01',
 '2013-03-13',
 '2009-10-09',
 '1993-10-22',
 '2009-01-26',
 '2017-08-30',
 '2004-01-31',
 '2013-11-04',
 '2012-09-26',
 '2011-11-26',
 '2011-07-06',
 '2018-02-14',
 '1996-12-01',
 '2011-09-30',
 '2010-07-30',
 '1999-12-22',
 '2010-11-05',
 '2010-05-31',
 '2019-02-11',
 '2016-02-04',
 '1997-11-18',
 '2008-05-05',
 '1992-07-15',
 '2006-03-25',
 '2008-08-29',
 '1994-10-25',
 '1998-08-13',
 '2019-05-29',
 '1995-04-28',
 '1994-06-03',
 '2012-08-27',
 '2007-12-31',
 '1998-10-15',
 '1990-06-04',
 '2002-11-30',
 '1999-07-20',
 '2002-07-10',
 '2001-08-18',
 '1992-07-19',
 '1994-06-02',
 '2012-10-08',
 '2016-07-18',
 '2010-08-20',
 '1997-04-18',
 '2003-08-17',
 '2016-05-06',
 '2001-12-08',
 '1999-09-14',
 '2000-09-16',
 '2019-08-07',
 '1993-12-18',
 '1990-05-05',
 '2014-03-24',
 '1990-02-20',
 '1991-07-08',
 '2017-01-13',
 '1990-01-24',
 '2019-09-25',
 '2015-07-06',
 '2018-03-11',
 '1997-06-13',
 '2003-06-05',
 '1995-11-25',
 '2013-02-28',
 '2003-05-18',
 '2013-04-29',
 '2018-11-20',
 '2016-06-07',
 '2005-04-03',
 '2014-05-10',
 '1993-02-05',
 '1991-12-22',
 '2018-10-30',
 '2013-06-01',
 '2005-03-16',
 '2000-04-13',
 '2000-08-11',
 '1990-02-01',
 '2010-12-31',
 '2000-07-29',
 '2017-10-18',
 '2012-10-07',
 '2010-09-21',
 '1992-02-10',
 '2003-07-11',
 '2020-09-11',
 '2015-12-24',
 '2018-12-01',
 '2000-03-07',
 '2018-05-08',
 '2009-06-22',
 '1993-10-10',
 '2014-10-16',
 '2003-12-28',
 '2000-08-07',
 '2009-12-15',
 '2002-08-18',
 '2003-07-26',
 '2020-05-13',
 '1999-09-19',
 '2019-10-11',
 '1992-07-30',
 '1996-07-20',
 '1991-08-06',
 '2003-06-12',
 '2001-01-07',
 '2005-10-02',
 '2009-02-20',
 '2003-01-27',
 '2001-01-18',
 '2015-11-14',
 '2001-02-18',
 '1994-05-22',
 '2009-04-16',
 '2000-09-23',
 '1990-05-19',
 '2009-05-02',
 '2001-02-06',
 '2007-08-18',
 '1999-06-01',
 '1995-04-04',
 '2013-01-20',
 '2003-03-04',
 '2019-11-05',
 '1999-03-16',
 '2015-11-10',
 '2020-02-23',
 '2002-11-05',
 '1999-09-02',
 '2003-08-16',
 '2007-03-24',
 '1991-02-20',
 '2004-09-10',
 '2007-12-01',
 '2004-12-04',
 '2017-03-13',
 '2020-08-15',
 '2000-06-02',
 '1994-06-27',
 '2001-02-13',
 '1992-01-04',
 '2004-07-13',
 '2019-11-15',
 '2012-09-04',
 '1998-05-30',
 '1996-12-21',
 '2002-04-03',
 '2009-12-27',
 '2003-05-14',
 '2002-12-20',
 '2005-10-15',
 '2006-04-08',
 '2013-12-07',
 '1991-03-19',
 '2011-04-05',
 '2019-07-01',
 '2018-12-09',
 '2012-03-31',
 '1995-05-18',
 '2015-01-04',
 '2000-07-12',
 '2008-11-22',
 '2017-11-18',
 '2009-07-20',
 '2012-07-08',
 '2009-03-28',
 '2004-12-16',
 '1993-07-06',
 '1992-03-25',
 '1999-02-20',
 '2013-02-25',
 '2012-07-23',
 '2004-05-25',
 '1993-12-22',
 '1991-09-19',
 '1993-02-16',
 '2011-01-24',
 '1991-03-10',
 '2006-01-07',
 '2017-04-05',
 '1997-01-23',
 '1993-08-25',
 '2015-01-23',
 '1991-03-01',
 '2010-12-16',
 '1993-07-22',
 '2004-11-17',
 '1991-12-23',
 '2016-03-18',
 '2014-12-19',
 '2006-10-14',
 '2002-08-25',
 '2006-07-06',
 '2018-04-21',
 '1992-09-05',
 '2015-03-14',
 '1992-08-13',
 '2000-12-17',
 '2004-09-25',
 '2009-11-11',
 '2009-03-14',
 '1996-06-17',
 '2008-08-11',
 '2014-03-02',
 '2004-07-20',
 '2006-05-30',
 '1994-04-05',
 '2020-04-02',
 '2015-08-31',
 '1991-02-18',
 '1990-07-18',
 '1994-01-22',
 '1996-11-14',
 '2000-10-30',
 '2014-03-29',
 '2006-02-27',
 '1996-03-28',
 '2013-12-09',
 '1993-11-22',
 '2010-10-27',
 '2006-07-27',
 '2017-12-27',
 '2016-07-31',
 '1999-12-08',
 '2012-02-08',
 '2014-07-09',
 '2013-02-03',
 '2017-06-15',
 '2000-04-18',
 '1993-05-09',
 '2009-04-10',
 '1995-12-07',
 '1993-09-01',
 '2003-10-29',
 '2015-09-13',
 '2000-01-10',
 '2013-05-26',
 '2016-08-03',
 '2013-04-25',
 '2001-02-10',
 '1999-04-09',
 '2000-02-01',
 '2016-12-10',
 '1999-11-06',
 '2013-09-06',
 '2002-04-17',
 '2004-05-13',
 '2012-10-14',
 '1990-01-18',
 '2012-07-02',
 '2003-01-11',
 '1997-11-26',
 '1992-05-28',
 '2007-10-04',
 '1991-09-15',
 '1996-08-27',
 '2006-11-28',
 '2007-04-10',
 '1995-03-19',
 '1995-03-11',
 '1991-07-03',
 '2009-01-10',
 '2000-05-07',
 '2013-10-01',
 '2003-07-09',
 '1990-02-23',
 '2020-09-17',
 '2014-10-29',
 '1995-11-15',
 '1990-07-28',
 '2008-10-21',
 '2012-06-01',
 '2016-12-07',
 '1994-08-05',
 '2010-07-12',
 '1997-08-19',
 '1997-05-29',
 '2003-09-07',
 '2018-01-05',
 '1993-12-25',
 '1997-05-14',
 '1999-07-28',
 '2009-08-06',
 '1994-01-18',
 '2016-08-30',
 '1999-01-18',
 '2001-02-16',
 '2001-04-23',
 '2010-01-20',
 '1999-08-13',
 '1996-12-10',
 '1990-09-09',
 '2003-08-29',
 '1992-10-18',
 '2019-11-11',
 '2004-07-29',
 '2012-05-09',
 '2010-02-23',
 '2018-12-18',
 '2000-10-29',
 '2008-01-12',
 '2014-09-28',
 '2011-08-02',
 '1992-09-24',
 '2009-05-21',
 '2018-04-14',
 '2008-08-12',
 '2011-10-14',
 '2000-04-26',
 '1995-06-19',
 '1990-03-29',
 '2019-07-29',
 '2020-07-18',
 '1996-02-23',
 '1996-03-12',
 '1992-03-21',
 '2014-11-03',
 '2005-12-19',
 '2002-04-21',
 '1997-09-19',
 '2018-05-19',
 '2008-08-14',
 '1998-10-29',
 '2012-09-13',
 '2005-03-31',
 '1999-08-28',
 '1991-12-19',
 '1994-05-14',
 '2000-02-26',
 '2018-01-13',
 '2014-11-29',
 '2007-02-15',
 '2012-11-10',
 '2013-06-12',
 '2020-07-30',
 '2014-10-31',
 '2005-12-09',
 '1993-02-20',
 '2003-06-03',
 '2006-07-07',
 '1991-01-06',
 '2019-03-04',
 '2000-05-18',
 '2013-10-26',
 '2008-03-25',
 '1994-03-06',
 '2006-02-24',
 '1992-10-11',
 '2009-03-18',
 '2015-08-21',
 '2003-10-05',
 '1998-03-11',
 '2003-02-18',
 '2006-04-13',
 '2012-03-04',
 '2018-05-05',
 '2006-05-16',
 '2008-11-12',
 '2007-12-18',
 '2016-02-19',
 '2014-11-06',
 '2006-04-18',
 '1993-09-09',
 '2001-09-06',
 '1995-09-11',
 '2000-09-17',
 '2006-07-04',
 '2018-02-15',
 '2019-09-11',
 '1997-03-19',
 '2007-04-03',
 '2014-11-28',
 '1994-03-03',
 '1990-10-22',
 '2005-10-08',
 '2002-06-06',
 '2007-04-26',
 '2015-09-26',
 '1993-03-19',
 '2012-04-28',
 '2017-09-18',
 '2016-04-07',
 '2005-06-30',
 '1994-07-30',
 '2001-08-24',
 '1991-12-17',
 '1995-11-16',
 '1995-08-15',
 '2005-02-16',
 '2011-09-03',
 '2000-08-16',
 '2006-11-06',
 '2005-09-19',
 '2016-02-21',
 '2015-01-22',
 '1993-01-24',
 '2018-03-26',
 '2010-07-24',
 '1990-01-30',
 '1995-09-24',
 '1996-08-16',
 '2018-05-25',
 '2001-11-21',
 '2011-02-13',
 '2015-11-22',
 '2018-12-26',
 '2017-10-15',
 '2006-02-25',
 '1992-03-22',
 '2016-12-12',
 '1997-01-11',
 '2016-09-08',
 '1996-08-06',
 '2000-01-23',
 '2002-05-31',
 '2004-11-29',
 '2008-12-12',
 '1998-05-26',
 '2007-09-19',
 '2005-05-17',
 '2015-05-26',
 '2020-03-25',
 '1998-03-31',
 '1993-08-12',
 '2006-02-08',
 '2018-12-25',
 '1991-09-23',
 '2004-06-08',
 '2018-12-14',
 '2016-02-08',
 '2000-08-26',
 '2014-02-23',
 '2019-01-23',
 '1998-10-04',
 '1993-03-06',
 '2005-06-27',
 '2014-09-01',
 '2005-12-31',
 '2003-01-08',
 '2017-12-18',
 '2000-07-01',
 '1992-02-06',
 '1998-12-16',
 '2009-11-28',
 '2019-01-31',
 '2008-11-17',
 '2008-10-27',
 '2019-08-06',
 '2001-07-21',
 '1990-08-06',
 '2014-09-20',
 '2015-12-01',
 '2008-05-23',
 '2012-04-18',
 '1990-07-08',
 '2005-11-20',
 '1995-11-07',
 '1991-06-16',
 '1995-03-18',
 '1995-09-23',
 '2010-02-02',
 '1998-01-28',
 '2016-11-28',
 '2014-11-16',
 '1994-11-06',
 '1993-04-24',
 '2000-01-21',
 '1998-07-07',
 '2001-06-30',
 '2010-05-04',
 '2011-09-17',
 '1993-12-24',
 '2008-06-26',
 '2019-02-28',
 '1997-12-02',
 '1997-02-11',
 '1990-01-21',
 '2003-07-24',
 '2018-07-29',
 '2006-07-26',
 '2012-10-22',
 '1990-11-05',
 '2012-07-01',
 '1990-08-10',
 '2010-06-26',
 '2000-07-17',
 '2010-12-09',
 '1994-03-11',
 '1993-11-25',
 '2002-03-25',
 '2008-01-24',
 '2010-11-10',
 '2006-10-22',
 '2016-07-13',
 '2020-09-01',
 '1990-10-01',
 '1997-08-02',
 '2011-01-11',
 '1990-08-12',
 '2008-10-13',
 '2006-05-19',
 '1995-12-17',
 '2001-12-20',
 '2017-01-11',
 '2000-10-07',
 '2008-10-08',
 '2007-08-17',
 '2017-04-16',
 '1998-01-08',
 '2001-10-01',
 '2004-06-29',
 '2000-11-14',
 '2005-11-25',
 '1997-05-30',
 '2008-12-22',
 '1996-08-10',
 '2002-11-25',
 '2010-06-16',
 '2016-03-13',
 '1993-05-10',
 '1992-12-29',
 '2005-12-23',
 '2009-08-26',
 '2001-05-04',
 '1999-01-21',
 '2020-01-17',
 '2018-06-25',
 '1998-10-03',
 '2016-03-26',
 '2004-08-28',
 '2016-03-20',
 '2011-01-18',
 '2001-05-29',
 '2018-02-07',
 '1994-08-22',
 '1997-03-06',
 '1993-02-24',
 '2000-05-26',
 '2007-12-14',
 '1992-01-15',
 '2018-11-11',
 '2013-09-13',
 '2015-10-18',
 '1992-01-10',
 '1994-03-29',
 '2011-09-14',
 '2010-10-11',
 '2008-08-26',
 '2010-03-19',
 '1994-10-19',
 '2011-07-05',
 '1991-01-03',
 '1998-10-22',
 '2006-02-16',
 '1998-04-23',
 '2013-06-06',
 '1990-06-05',
 '2007-07-10',
 '1992-11-03',
 '2011-05-02',
 '2009-05-03',
 '1990-11-23',
 '1998-07-12',
 '1991-05-29',
 '1998-04-29',
 '1998-09-17',
 '1991-05-11',
 '2006-11-01',
 '1993-05-27',
 '1999-03-01',
 '2018-05-13',
 '1995-07-03',
 '2014-10-23',
 '1995-07-02',
 '2009-02-08',
 '2011-05-21',
 '2000-03-15',
 '2012-06-12',
 '1998-11-24',
 '2002-02-03',
 '1998-03-16',
 '2006-01-03',
 '1993-02-03',
 '2016-10-07',
 '2005-07-11',
 '2008-01-18',
 '2006-10-28',
 '1999-01-11',
 '2011-09-01',
 '1994-08-16',
 '2019-12-21',
 '2001-10-06',
 '2012-10-03',
 '2002-12-09',
 '1996-10-12',
 '1996-02-10',
 '2007-06-05',
 '2019-12-17',
 '2015-01-20',
 '2001-08-14',
 '1996-06-01',
 '2014-07-14',
 '2005-01-25',
 '2019-02-27',
 '1991-05-12',
 '2001-08-20',
 '2000-05-31',
 '2001-06-16',
 '2009-08-07',
 '2007-06-01',
 '1998-07-24',
 '2001-05-25',
 '2003-09-24',
 '2008-09-21',
 '2014-12-12',
 '1998-03-10',
 '1992-12-02',
 '2019-05-24',
 '2003-01-03',
 '2013-11-24',
 '2011-05-18',
 '2009-05-12',
 '2017-11-19',
 '1997-06-04',
 '1995-03-31',
 '2002-02-07',
 '2001-07-28',
 '2019-04-01',
 '2001-04-25',
 '2000-04-02',
 '2008-04-30',
 '2010-09-18',
 '1991-07-23',
 '1997-05-11',
 '2013-06-27',
 '2010-09-17',
 '2013-12-25',
 '2009-06-27',
 '2018-08-25',
 '1997-07-11',
 '1995-02-07',
 '1999-01-15',
 '2019-09-28',
 '2005-03-20',
 '2014-03-16',
 '1995-03-04',
 '2013-05-10',
 '2020-04-01',
 '1992-03-16',
 '2013-02-06',
 '1992-09-02',
 '2019-10-22',
 '2007-10-13',
 '2016-01-16',
 '2014-04-19',
 '2020-01-22',
 '2018-03-27',
 '1995-02-06',
 '2008-08-08',
 '2018-05-02',
 '2019-05-04',
 '1996-12-02',
 '2004-03-21',
 '2008-04-06',
 '2019-12-15',
 '1994-10-01',
 '2009-05-09',
 '2005-11-11',
 '2017-11-21',
 '2016-08-06',
 '2012-04-22',
 '2007-05-06',
 '2013-12-03',
 '2013-03-20',
 '1994-01-15',
 '1996-12-31',
 '1995-08-22',
 '1997-01-02',
 '2012-02-17',
 '1994-09-09',
 '2008-06-13',
 '2011-02-04',
 '2014-05-29',
 '1991-06-15',
 '2012-05-14',
 '1993-04-14',
 '2006-03-12',
 '2003-06-24',
 '2010-08-14',
 '2010-12-07',
 '1997-10-31',
 '2000-10-08',
 '1990-03-05',
 '2015-04-12',
 '2005-06-24',
 '2004-03-13',
 '1991-08-05',
 '2001-06-03',
 '2002-04-05',
 '1990-05-24',
 '1994-10-21',
 '1998-03-06',
 '2013-11-15',
 '2002-02-02',
 '2011-08-13',
 '1999-02-26',
 '2019-07-24',
 '2019-06-21',
 '1996-04-23',
 '2010-05-14',
 '2018-09-22',
 '2014-04-21',
 '1997-01-28',
 '2008-10-02',
 '1997-07-18',
 '1997-05-09',
 '2006-09-01',
 '1990-05-22',
 '2005-03-18',
 '1995-10-10',
 '2011-04-11',
 '2003-06-21',
 '1999-03-25',
 '1990-12-03',
 '2018-02-23',
 '1997-08-26',
 '2013-03-04',
 '2016-11-11',
 '1991-12-21',
 '2012-01-14',
 '1997-06-14',
 '2000-05-22',
 '2003-05-24',
 '1998-07-26',
 '2005-07-01',
 '1994-01-21',
 '1996-12-26',
 '1997-02-24',
 '1995-10-20',
 '1990-08-18',
 '2001-08-21',
 '1997-05-25',
 '2006-08-09',
 '2010-05-12',
 '1992-05-31',
 '2008-06-20',
 '1992-08-29',
 '2015-05-06',
 '2008-04-11',
 '2016-07-22',
 '2015-03-17',
 '2005-11-29',
 '2014-10-17',
 '2018-08-07',
 '2000-01-14',
 '2006-12-02',
 '2005-04-02',
 '1993-05-22',
 '2006-07-08',
 '2006-05-01',
 '2007-04-06',
 '1993-07-14',
 '1998-07-25',
 '2001-06-20',
 '2001-12-31',
 '2007-08-02',
 '2007-05-28',
 '2008-02-23',
 '2014-12-15',
 '1995-01-16',
 '1992-11-05',
 '1996-06-20',
 '1996-03-16',
 '2010-05-22',
 '2010-10-13',
 '2012-11-29',
 '1999-07-07',
 '2013-05-14',
 '2010-01-16',
 '2017-09-15',
 '1996-05-11',
 '2002-03-23',
 '1996-01-27',
 '2010-11-17',
 '2010-10-25',
 '1999-07-06',
 '2018-04-01',
 '2019-06-25',
 '2014-11-11',
 '1992-02-11',
 '1996-04-12',
 '2015-05-09',
 '2004-07-15',
 '2002-09-18',
 '2018-07-18',
 '1991-06-07',
 '2010-04-22',
 '2008-03-11',
 '2009-11-30',
 '2006-10-06',
 '2005-10-03',
 '1999-01-07',
 '2018-03-02',
 '1994-11-09',
 '1999-10-19',
 '2008-04-19',
 '1991-07-13',
 '1992-11-30',
 '1996-09-11',
 '2015-08-28',
 '1999-07-24',
 '2019-07-21',
 '2000-01-03',
 '2017-03-10',
 '1997-09-23',
 '2005-12-13',
 '2014-01-26',
 '2003-02-12',
 '2000-03-11',
 '2010-02-16',
 '2019-07-07',
 '1997-07-28',
 '1990-11-13',
 '2018-12-20',
 '2019-01-12',
 '2008-10-30',
 '1995-04-27',
 '2008-11-27',
 '2015-01-14',
 '2017-07-03',
 '2003-11-08',
 '2018-07-27',
 '2010-08-06',
 '1999-02-06',
 '1992-05-23',
 '2014-09-10',
 '2009-04-25',
 '2000-06-15',
 '2010-01-23',
 '2012-08-25',
 '2001-02-05',
 '2003-01-13',
 '2005-02-18',
 '2018-01-15',
 '1999-04-20',
 '2006-04-21',
 '1994-05-06',
 '2011-02-18',
 '1991-05-24',
 '2005-08-22',
 '1992-02-18',
 '1995-07-21',
 '2012-03-10',
 '2000-06-09',
 '1990-06-30',
 '1993-04-21',
 '1996-07-08',
 '1994-12-16',
 '2002-09-09',
 '2015-02-07',
 '2002-01-18',
 '2017-10-27',
 '1996-06-27',
 '1998-02-21',
 '2017-07-14',
 '2000-06-23',
 '1998-05-09',
 '1998-02-12',
 '1991-10-04',
 '2016-08-20',
 '1997-11-14',
 '2000-12-16',
 '1997-08-14',
 '2006-05-05',
 '2002-11-29',
 '1997-09-21',
 '1995-03-25',
 '1998-06-10',
 '2012-01-29',
 '1990-05-28',
 '1991-12-15',
 '1992-12-30',
 '1997-04-03',
 '2017-12-22',
 '1994-09-26',
 '1997-02-19',
 '2011-12-15',
 '2016-02-16',
 '1997-12-30',
 '1997-03-04',
 '2020-01-07',
 '1994-05-05',
 '2004-04-27',
 '1999-07-04',
 '1997-03-30',
 '2006-09-28',
 '2019-04-18',
 '2009-09-20',
 '2020-02-29',
 '2001-06-18',
 '2004-11-24',
 '1991-02-06',
 '1991-11-06',
 '1999-06-05',
 '2020-04-10',
 '1994-07-08',
 '2018-07-19',
 '1997-09-05',
 '1999-08-20',
 '2016-12-30',
 '1990-01-01',
 '2015-10-09',
 '2002-08-17',
 '2018-01-24',
 '1990-08-25',
 '1990-11-18',
 '2012-09-29',
 '2008-08-07',
 '2013-08-08',
 '2012-03-17',
 '2007-04-15',
 '2013-06-09',
 '2006-06-04',
 '1992-05-11',
 '1998-05-19',
 '2000-12-27',
 '2014-07-10',
 '2012-12-07',
 '1999-11-13',
 '1999-02-09',
 '1994-10-30',
 '2004-08-18',
 '1993-04-06',
 '2014-03-07',
 '2005-03-15',
 '2016-10-09',
 '2008-09-25',
 '2012-02-27',
 '2011-10-11',
 '1995-07-31',
 '2009-09-18',
 '1991-06-25',
 '1993-02-25',
 '2019-05-05',
 '1999-08-11',
 '2016-01-24',
 '1999-12-19',
 '1992-06-26',
 '2009-02-25',
 '2009-01-13',
 '2017-03-08',
 '2012-07-13',
 '2017-06-04',
 '2020-05-17',
 '2009-03-11',
 '1999-04-13',
 '2002-06-15',
 '2016-10-20',
 '2001-10-19',
 '2003-04-19',
 '2011-02-24',
 '1993-06-18',
 '2003-03-24',
 '2005-11-16',
 '1993-01-18',
 '2006-02-21',
 '2015-06-14',
 '1996-06-06',
 '2012-01-25',
 '1992-07-04',
 '2008-01-08',
 '1994-01-01',
 '2016-06-12',
 '1998-10-23',
 '1994-05-10',
 '1993-01-13',
 '2017-05-03',
 '1992-09-17',
 '1999-06-02',
 '2019-05-18',
 '2004-06-11',
 '1991-06-09',
 '2003-10-15',
 '2014-05-16',
 '2002-08-28',
 '2016-02-27',
 '1999-07-21',
 '2013-11-10',
 '2008-10-03',
 '2000-06-28',
 '2017-10-06',
 '2009-01-18',
 '1996-11-29',
 '2001-07-10',
 '2011-03-01',
 '2004-05-18',
 '1993-11-29',
 '1994-04-04',
 '2011-10-05',
 '2019-09-06',
 '2001-02-23',
 '2017-01-23',
 '1990-09-21',
 '2013-08-03',
 '2014-01-23',
 '2007-07-17',
 '2005-02-01',
 '1991-02-08',
 '2016-07-05',
 '2018-09-30',
 '1990-05-10',
 '1993-10-13',
 '2014-08-06',
 '1993-09-05',
 '2004-07-31',
 '1990-04-07',
 '1992-03-19',
 '2016-09-27',
 '1994-03-20',
 '1995-04-29',
 '2016-04-12',
 '1994-06-07',
 '2005-11-03',
 '1997-08-10',
 '2000-07-11',
 '2003-02-20',
 '1991-07-09',
 '2007-05-08',
 '1997-11-08',
 '2001-07-25',
 '2009-03-17',
 '2016-08-27',
 '2014-08-12',
 '1995-06-30',
 '2000-11-12',
 '2018-12-21',
 '2004-02-14',
 '2001-05-03',
 '2004-04-11',
 '2013-05-29',
 '2008-11-02',
 '2008-08-24',
 '1993-04-08',
 '2017-12-19',
 '2012-06-24',
 '2012-10-27',
 '1996-01-01',
 '1996-03-08',
 '2011-07-04',
 '2012-11-13',
 '1997-08-25',
 '1995-08-13',
 '1990-03-30',
 '2015-01-09',
 '1992-04-25',
 '2002-09-08',
 '2014-02-01',
 '2007-07-14',
 '2001-12-09',
 '2005-02-04',
 '2010-10-06',
 '1992-01-01',
 '1999-03-31',
 '2006-08-26',
 '2020-01-04',
 '1999-10-31',
 '2011-05-16',
 '1994-09-19',
 '1994-05-09',
 '2000-12-13',
 '2001-05-18',
 '2020-06-23',
 '2002-02-15',
 '2001-03-17',
 '1996-11-22',
 '1992-12-01',
 '2013-03-22',
 '2007-02-02',
 '1990-05-26',
 '1999-05-02',
 '2005-08-01',
 '1990-12-29',
 '2006-03-11',
 '2010-07-18',
 '2004-08-29',
 '2009-10-18',
 '2012-07-18',
 '1995-05-19',
 '2012-12-09',
 '2012-02-14',
 '2003-02-07',
 '2002-07-23',
 '2005-04-25',
 '2002-01-15',
 '2016-03-22',
 '2008-06-12',
 '2006-05-11',
 '2004-06-14',
 '2014-12-04',
 '1991-03-30',
 '2002-11-03',
 '2003-07-23',
 '2010-04-29',
 '2014-06-08',
 '2020-02-08',
 '2006-09-15',
 '2002-10-12',
 '2010-08-12',
 '2008-06-10',
 '2012-09-09',
 '2006-03-22',
 '1996-10-15',
 '1999-06-22',
 '2014-07-16',
 '2003-04-12',
 '1990-12-13',
 '2007-07-09',
 '1992-05-01',
 '2003-02-25',
 '1990-05-01',
 '1999-04-28',
 '1999-06-12',
 '2005-05-22',
 '2008-03-05',
 '1991-09-17',
 '1993-05-04',
 '1998-08-22',
 '2012-01-24',
 '2004-09-18',
 '1991-08-18',
 '2002-03-31',
 '2014-12-27',
 '1996-08-19',
 '2003-11-05',
 '2007-07-29',
 '2009-03-30',
 '2014-10-18',
 '1996-01-13',
 '1994-06-22',
 '2001-11-09',
 '1995-05-25',
 '1991-05-09',
 '1991-01-25',
 '2010-02-11',
 '1992-08-01',
 '2011-02-16',
 '2010-11-19',
 '2002-02-22',
 '2019-12-07',
 '1997-07-14',
 '2011-03-07',
 '2008-08-05',
 '2004-02-08',
 '1998-04-25',
 '2000-06-01',
 '2000-11-22',
 '2001-05-24',
 '2013-03-27',
 '2005-08-13',
 '2019-11-07',
 '1995-05-20',
 '2001-12-21',
 '2007-02-17',
 '1995-02-04',
 '2019-06-26',
 '1998-12-05',
 '1994-02-01',
 '2009-01-05',
 '2006-03-14',
 '1997-07-21',
 '2009-09-16',
 '2020-05-12',
 '1992-12-31',
 '2019-05-17',
 '2020-07-09',
 '2014-12-09',
 '2018-07-21',
 '1995-09-04',
 '2003-05-06',
 '1993-02-07',
 '2003-10-06',
 '2015-05-15',
 '1998-12-13',
 '1992-01-19',
 '2001-10-23',
 '1999-05-27',
 '2009-03-16',
 '1998-02-25',
 '2004-09-01',
 '2012-03-06',
 '2018-10-28',
 '2012-01-06',
 '2003-12-18',
 '2004-03-27',
 '2000-04-05',
 '2004-06-16',
 '2006-06-19',
 '2006-06-16',
 '1998-07-02',
 '2002-10-26',
 '2019-03-08',
 '2010-10-29',
 '1996-06-05',
 '2006-06-06',
 '2005-05-12',
 '2020-05-10',
 '1996-02-24',
 '2019-10-31',
 '2017-03-01',
 '2011-04-19',
 '2009-02-12',
 '1993-10-23',
 '2009-06-04',
 '2013-02-12',
 '2001-08-13',
 '2016-12-31',
 '1999-10-17',
 '2004-11-09',
 '1996-02-11',
 '2015-05-02',
 '2001-04-14',
 '2000-10-28',
 '2009-05-27',
 '2016-07-27',
 '1994-02-26',
 '2001-09-12',
 '2002-08-08',
 '2014-04-09',
 '1994-09-04',
 '2010-06-02',
 '1993-06-07',
 '1993-08-14',
 '2011-12-20',
 '2009-07-06',
 '2019-08-02',
 '2010-04-25',
 '1994-03-02',
 '2016-11-21',
 '1994-08-20',
 '1997-07-31',
 '1997-11-05',
 '1996-09-01',
 '1998-07-10',
 '2005-07-25',
 '2008-03-09',
 '2020-07-13',
 '1991-08-14',
 '2015-07-20',
 '2012-05-12',
 '1998-10-13',
 '2020-09-23',
 '2008-05-25',
 '1993-01-26',
 '2007-07-05',
 '1997-12-15',
 '1999-03-12',
 '2012-08-11',
 '2020-06-21',
 '2006-12-30',
 '2011-09-26',
 '2008-02-26',
 '2005-06-17',
 '2008-05-01',
 '2016-10-31',
 '1999-03-08',
 '1991-06-21',
 '1998-07-05',
 '2002-02-28',
 '2006-01-15',
 '1995-11-08',
 '2004-07-17',
 '2008-07-14',
 '1996-04-29',
 '2019-03-02',
 '2001-10-31',
 '2002-10-21',
 '2015-12-14',
 '1994-05-16',
 '2009-05-11',
 '2012-02-04',
 '2017-09-26',
 '2018-09-17',
 '1996-11-10',
 '2001-01-22',
 '1997-09-11',
 '2018-09-03',
 '1998-02-20',
 '2020-07-31',
 '2004-01-02',
 '2000-06-03',
 '1999-05-21',
 '2012-04-10',
 '2005-12-10',
 '1998-08-27',
 '1997-12-27',
 '2009-09-24',
 '2007-08-05',
 '2009-04-03',
 '2009-11-03',
 '2003-09-03',
 '1997-08-30',
 '2019-05-31',
 '2003-10-12',
 '2012-10-10',
 '2015-10-05',
 '2018-09-19',
 '2016-02-13',
 '2013-07-28',
 '2015-04-29',
 '2020-08-14',
 '2017-03-25',
 '2019-02-01',
 '2013-03-08',
 '1997-09-25',
 '2016-02-20',
 '1995-02-10',
 '1991-11-29',
 '2004-07-04',
 '1994-05-03',
 '2019-04-09',
 '2005-04-28',
 '1993-09-17',
 '2012-03-21',
 '2018-07-11',
 '1993-02-21',
 '1995-12-10',
 '2005-10-13',
 '1995-03-24',
 '2009-07-03',
 '1994-05-25',
 '2017-02-07',
 '2020-01-11',
 '2017-04-26',
 '2005-05-29',
 '2003-11-12',
 '2019-01-27',
 '2018-04-11',
 '1997-07-06',
 '2018-01-16',
 '1996-02-15',
 '2002-11-09',
 '2004-10-15',
 '2005-02-11',
 '2005-04-16',
 '2012-07-22',
 '1990-02-21',
 '1993-03-30',
 '2000-07-25',
 '2002-07-07',
 '2015-05-25',
 '2018-11-06',
 '2005-12-05',
 '2010-08-17',
 '1997-08-09',
 '2017-02-09',
 '1995-04-06',
 '2006-01-10',
 '2013-11-09',
 '2012-06-11',
 '1996-06-10',
 '1991-03-14',
 '1999-12-15',
 '1996-08-26',
 '2015-06-28',
 '1990-02-13',
 '2020-06-16',
 '2010-10-22',
 '2008-10-09',
 '2012-11-08',
 '1991-01-09',
 '2008-07-31',
 '2010-11-30',
 '1995-03-14',
 '2003-05-28',
 '1996-01-16',
 '1996-07-24',
 '1992-08-23',
 '2011-03-04',
 '1992-07-26',
 '2003-09-06',
 '2015-08-25',
 '1990-12-30',
 '1994-03-01',
 '1997-12-09',
 '2019-04-15',
 '1992-05-26',
 '2018-04-29',
 '1998-10-24',
 '2013-12-30',
 '2003-07-10',
 '2008-04-02',
 '2011-09-15',
 '2016-10-04',
 '2011-02-23',
 '2009-04-17',
 '2019-11-29',
 '2009-02-28',
 '2020-04-19',
 '1995-04-08',
 '1996-01-31',
 '1998-01-01',
 '2016-07-10',
 '2012-04-08',
 '2002-03-16',
 '1998-07-08',
 '2003-06-23',
 '2010-12-02',
 '2010-12-22',
 '2014-12-13',
 '2017-01-09',
 '2008-02-29',
 '2006-05-29',
 '2013-08-05',
 '2011-02-28',
 '2001-03-11',
 '2015-05-27',
 '1993-07-12',
 '2020-03-01',
 '2007-09-14',
 '2015-09-17',
 '2020-04-24',
 '1995-04-16',
 '2009-02-24',
 '2020-06-30',
 '2018-10-26',
 '2002-06-10',
 '1994-03-30',
 '2007-06-04',
 '1998-12-02',
 '2008-02-19',
 '2013-09-24',
 '2020-03-23',
 '1998-08-10',
 '2001-01-10',
 '2017-08-13',
 '2010-06-07',
 '2016-10-02',
 '2017-11-04',
 '2000-02-12',
 '2003-06-18',
 '2011-12-30',
 '1999-11-02',
 '1997-06-15',
 '2013-08-16',
 '1991-02-10',
 '1992-03-03',
 '1994-02-10',
 '2005-09-04',
 '2008-07-01',
 '1990-07-11',
 '1998-12-30',
 '2008-03-28',
 '1995-11-29',
 '2003-12-09',
 '2003-02-09',
 '2014-03-19',
 '2010-03-05',
 '2015-07-18',
 '2002-02-19',
 '2020-01-13',
 '1996-01-09',
 '1998-01-24',
 '2004-02-29',
 '2015-03-31',
 '2008-10-31',
 '2009-10-14',
 '2011-03-20',
 '2014-12-17',
 '1993-07-10',
 '2001-05-19',
 '2016-07-16',
 '2009-01-15',
 '2007-12-16',
 '1991-10-09',
 '2018-11-10',
 '2017-08-02',
 '1998-11-07',
 '2001-07-27',
 '2011-07-24',
 '2001-12-07',
 '2013-06-23',
 '2006-12-10',
 '2013-10-07',
 '1991-02-19',
 '2015-01-03',
 '2010-02-03',
 '2015-11-17',
 '2003-09-14',
 '2011-10-12',
 '2004-04-02',
 '1999-06-23',
 '2006-10-17',
 '1999-10-03',
 '2000-12-01',
 '2020-09-20',
 '2017-09-10',
 '2008-01-02',
 '2011-09-21',
 '2002-05-08',
 '2018-02-03',
 '2002-04-01',
 '2003-03-30',
 '2009-11-01',
 '2018-03-10',
 '2008-05-16',
 '2013-07-22',
 '2007-05-26',
 '2006-12-06',
 '1990-03-11',
 '2020-08-25',
 '2012-08-18',
 '2016-02-18',
 '2013-09-07',
 '2001-10-11',
 '2011-01-12',
 '1999-09-24',
 '2013-09-02',
 '2008-04-03',
 '2011-12-28',
 '2000-03-06',
 '1990-09-30',
 '1997-08-22',
 '2010-09-02',
 '2018-11-14',
 '2020-01-19',
 '2004-10-17',
 '2004-01-29',
 '1998-08-04',
 '2004-04-13',
 '1998-03-13',
 '2014-11-20',
 '2015-08-05',
 '1995-08-28',
 '2006-06-03',
 '2003-04-05',
 '1999-03-26',
 '2007-02-19',
 '2012-08-24',
 '2004-07-21',
 '2011-06-26',
 '1998-06-26',
 '1994-04-18',
 '2002-09-17',
 '2008-02-17',
 '1995-12-03',
 '2004-11-08',
 '1996-04-21',
 '2014-07-03',
 '2011-04-14',
 '1990-06-23',
 '2010-09-27',
 '2020-07-07',
 '2018-09-04',
 '1990-03-31',
 '1993-02-15',
 '2017-04-21',
 '2007-02-10',
 '1999-09-01',
 '2014-09-21',
 '2000-03-14',
 '2019-10-17',
 '2000-07-20',
 '1990-06-21',
 '2014-05-05',
 '2017-12-26',
 '1994-06-26',
 '2009-10-26',
 '2018-05-09',
 '2001-01-16',
 '2010-08-03',
 '1991-04-23',
 '2003-02-06',
 '1997-05-18',
 '1994-01-16',
 '2011-05-07',
 '1992-07-28',
 '2008-03-26',
 '2020-06-05',
 '2003-05-11',
 '2000-08-15',
 '2001-10-13',
 '2015-12-02',
 '2014-05-06',
 '2010-08-18',
 '1993-06-29',
 '1991-04-19',
 '2019-05-12',
 '2002-07-27',
 '2004-05-16',
 '2000-05-11',
 '2009-07-13',
 '2002-04-29',
 '1992-06-18',
 '2010-09-25',
 '2002-05-10',
 '2002-04-26',
 '2012-11-07',
 '2008-06-22',
 '2007-11-08',
 '1996-09-17',
 '1996-02-28',
 '2010-03-18',
 '2020-02-16',
 '2019-04-25',
 '2007-06-10',
 '1997-10-05',
 '1990-03-25',
 '1991-07-24',
 '1991-12-06',
 '1995-04-10',
 '2019-12-05',
 '2008-02-04',
 '1990-09-25',
 '2018-01-10',
 '2016-11-16',
 '2007-01-24',
 '2000-12-15',
 '1995-07-08',
 '2015-04-22',
 '1993-11-28',
 '2014-01-21',
 '2005-08-23',
 '1990-10-19',
 '1996-03-01',
 '1995-07-01',
 '2005-07-12',
 '2019-03-21',
 '2010-05-11',
 '1999-05-14',
 '2010-02-27',
 '2012-07-26',
 '2003-08-21',
 '2014-12-02',
 '1996-10-21',
 '2000-01-16',
 '2005-10-24',
 '2011-04-21',
 '2016-06-25',
 '1991-07-31',
 '2004-08-05',
 '2009-09-26',
 '2005-12-17',
 '1990-03-20',
 '2001-06-04',
 '1993-01-19',
 '2019-08-21',
 '2006-05-31',
 '2006-04-02',
 '2005-05-13',
 '1993-10-30',
 '2017-12-21',
 '1999-03-11',
 '2018-07-16',
 '1994-01-13',
 '1990-11-09',
 '2004-07-09',
 '1991-05-04',
 '2014-06-17',
 '2015-06-27',
 '1990-02-09',
 '2011-06-24',
 '2002-11-26',
 '1993-02-17',
 '1999-08-30',
 '1994-10-29',
 '2006-03-23',
 '2008-09-28',
 '2005-07-27',
 '2018-01-06',
 '2017-05-24',
 '1990-08-31',
 '2014-05-08',
 '2019-05-28',
 '1998-09-01',
 '2011-07-16',
 '2004-05-19',
 '2001-04-30',
 '1997-01-31',
 '2000-07-26',
 '2014-12-24',
 '2008-02-24',
 '2013-07-27',
 '2000-05-17',
 '2005-12-24',
 '2003-02-23',
 '2011-01-09',
 '1995-08-04',
 '1992-10-02',
 '2018-11-30',
 '1991-02-24',
 '2013-01-13',
 '2008-05-04',
 '2020-05-18',
 '2012-07-31',
 '2020-05-19',
 '2017-04-14',
 '2017-03-12',
 '2004-01-01',
 '1996-01-29',
 '2004-04-09',
 '1991-07-21',
 '2003-06-25',
 '2004-07-23',
 '2010-12-10',
 '2013-04-17',
 '2000-07-04',
 '2015-06-10',
 '2019-10-09',
 '2003-04-18',
 '2020-03-27',
 '2018-01-07',
 '1998-06-24',
 '2002-05-23',
 '2004-01-26',
 '2000-02-13',
 '2005-01-03',
 '1992-06-13',
 '2002-11-04',
 '2011-06-20',
 '1999-12-12',
 '2004-06-17',
 '2020-04-23',
 '2017-01-26',
 '1992-04-24',
 '2016-04-11',
 '2004-01-27',
 '1997-09-30',
 '2014-04-22',
 '1995-10-28',
 '2009-11-04',
 '2004-11-21',
 '2005-03-04',
 '1999-05-13',
 '2013-06-04',
 '2001-12-04',
 '1992-01-20',
 '2018-05-06',
 '1992-10-22',
 '2002-12-12',
 '2017-11-27',
 '2011-11-16',
 '2007-12-07',
 '1997-02-09',
 '2016-05-02',
 '2019-06-09',
 '2016-04-23',
 '2002-02-24',
 '2009-04-18',
 '2017-10-12',
 '1990-10-26',
 '2014-04-05',
 '1995-08-11',
 '2011-07-29',
 '2005-11-13',
 '2004-04-24',
 '1990-06-11',
 '1993-04-23',
 '2000-01-02',
 '2015-05-17',
 '2016-04-30',
 '2001-07-04',
 '2017-03-20',
 '2000-10-31',
 '2003-05-27',
 '1995-09-19',
 '2014-10-27',
 '2008-04-21',
 '1993-06-24',
 '2018-10-20',
 '1990-07-31',
 '2020-04-21',
 '2018-03-24',
 '1996-07-22',
 '2019-10-03',
 '1996-11-23',
 '2001-06-25',
 '2009-07-31',
 '1993-11-24',
 '1997-08-21',
 '1991-08-10',
 '1992-03-01',
 '2005-11-01',
 '2012-02-21',
 '2019-02-24',
 '1999-02-27',
 '1997-01-12',
 '2012-02-11',
 '2000-12-24',
 '1993-11-16',
 '2015-08-09',
 '2020-09-27',
 '2005-04-17',
 '2011-06-16',
 '1992-12-24',
 '2005-02-25',
 '2001-10-09',
 '2007-09-17',
 '1995-09-05',
 '1993-04-02',
 '2009-05-25',
 '1994-04-03',
 '2012-09-03',
 '2010-05-15',
 '2016-09-14',
 '2020-03-28',
 '2001-08-27',
 '1990-04-01',
 '2006-11-24',
 '2004-12-17',
 '2000-09-08',
 '1996-10-22',
 '2020-05-07',
 '1991-11-26',
 '2010-09-04',
 '2017-02-17',
 '2020-05-01',
 '2009-06-19',
 '1991-02-21',
 '1993-05-02',
 '2001-08-11',
 '2018-08-19',
 '1995-01-09',
 '1997-05-12',
 '2013-07-23',
 '2020-03-24',
 '2012-08-12',
 '1994-11-27',
 '2018-07-14',
 '2015-10-19',
 '2011-12-21',
 '2015-04-17',
 '2001-07-26',
 '2006-12-09',
 '2013-08-09',
 '2006-06-29',
 '2013-01-09',
 '2014-01-02',
 '1995-01-19',
 '1995-03-16',
 '2001-02-14',
 '2010-05-20',
 '2017-04-03',
 '1991-04-29',
 '2004-10-18',
 '2011-05-01',
 '2004-09-28',
 '2017-07-08',
 '2014-02-13',
 '2014-11-09',
 '2001-11-03',
 '2017-09-04',
 '2009-02-06',
 '2005-10-30',
 '2001-01-04',
 '1998-03-24',
 '1996-06-12',
 '2000-09-03',
 '2007-04-23',
 '2010-06-29',
 '2001-11-26',
 '1995-08-14',
 '1998-04-05',
 '2011-10-25',
 '2015-10-10',
 '2001-05-01',
 '2011-03-21',
 '2000-10-10',
 '2002-08-15',
 '2003-05-04',
 '2015-12-12',
 '2019-12-04',
 '2008-05-21',
 '2007-10-27',
 '1994-06-29',
 '2012-11-04',
 '2014-10-14',
 '1991-07-04',
 '1991-04-12',
 '2000-02-19',
 '2020-08-07',
 '2012-12-12',
 '1990-04-25',
 '1999-07-17',
 '2000-05-21',
 '2015-03-03',
 '1999-06-29',
 '2011-08-22',
 '2019-12-24',
 '2013-01-08',
 '1997-12-05',
 '1994-05-15',
 '2015-09-25',
 '1993-11-23',
 '2015-11-23',
 '2015-06-20',
 '1995-10-15',
 '2011-08-11',
 '2006-07-31',
 '1999-06-21',
 '2009-11-17',
 '2015-09-20',
 '1999-06-13',
 '2006-06-05',
 '2019-09-19',
 '2001-04-04',
 '1990-11-21',
 '2013-07-09',
 '2007-07-13',
 '2013-07-19',
 '2006-08-01',
 '1998-09-29',
 '1996-05-23',
 '2006-10-01',
 '2007-03-15',
 '2014-11-13',
 '2010-01-14',
 '1993-11-08',
 '2011-11-20',
 '2006-08-18',
 '2001-07-15',
 '2009-10-12',
 '1994-12-27',
 '1999-12-25',
 '1992-06-09',
 '2012-10-30',
 '2017-08-31',
 '1995-08-01',
 '1990-10-08',
 '2016-02-14',
 '2002-08-09',
 '2008-12-30',
 '2013-08-13',
 '1991-01-15',
 '1998-08-17',
 '2003-10-01',
 '2000-04-06',
 '2006-10-05',
 '1999-05-25',
 '2000-01-13',
 '1993-10-01',
 '2003-08-18',
 '2010-07-23',
 '2003-04-06',
 '2002-01-14',
 '1996-08-07',
 '2008-11-10',
 '2001-01-03',
 '1995-04-23',
 '2000-07-24',
 '2011-06-22',
 '2004-03-31',
 '2013-06-19',
 '2012-10-23',
 '1994-02-17',
 '1990-09-02',
 '2020-03-21',
 '2003-09-23',
 '2016-11-03',
 '2017-04-11',
 '2015-08-12',
 '2011-01-28',
 '2010-04-03',
 '2019-10-23',
 '2006-03-28',
 '1999-01-10',
 '2020-04-14',
 '2002-11-18',
 '2003-05-17',
 '2006-04-11',
 '1997-03-29',
 '2004-11-19',
 '1999-10-05',
 '2016-12-20',
 '2017-08-26',
 '2012-09-12',
 '2004-02-26',
 '2014-02-14',
 '2003-09-19',
 '2017-03-26',
 '2007-01-05',
 '2019-12-01',
 '2010-05-03',
 '1993-01-20',
 '2002-10-05',
 '2019-12-09',
 '2014-03-09',
 '2016-11-12',
 '2017-06-14',
 '1990-10-05',
 '2003-01-15',
 '2008-09-06',
 '2009-08-30',
 '2002-07-09',
 '2014-04-27',
 '2001-12-01',
 '1994-09-08',
 '1993-06-17',
 '1998-01-26',
 '2013-08-21',
 '2005-04-22',
 '2011-11-04',
 '2005-09-15',
 '2016-05-29',
 '2012-04-26',
 '2007-03-05',
 '1995-06-05',
 '2007-03-12',
 '2018-04-19',
 '1993-02-02',
 '2011-03-23',
 '2005-11-19',
 '1992-06-28',
 '2013-10-16',
 '2016-06-08',
 '2016-04-27',
 '2017-06-05',
 '2010-05-18',
 '2002-12-21',
 '2020-09-19',
 '2010-06-09',
 '2013-08-28',
 '1995-05-07',
 '2002-03-21',
 '1993-04-28',
 '2003-03-14',
 '1990-01-31',
 '2015-01-07',
 '2012-10-15',
 '2017-12-12',
 '1999-10-07',
 '2013-07-26',
 '1998-08-25',
 '2005-01-30',
 '1990-02-08',
 '2015-02-19',
 '2013-10-28',
 '2007-10-18',
 '1997-04-24',
 '1991-12-25',
 '1994-01-05',
 '2019-04-10',
 '1992-08-12',
 '1998-06-22',
 '2019-11-27',
 '2006-04-15',
 '2005-09-10',
 '2002-12-28',
 '2007-12-03',
 '2010-03-25',
 '2016-07-25',
 '2003-01-04',
 '1994-02-08',
 '2008-05-29',
 '2004-10-13',
 '2002-05-04',
 '1995-11-23',
 '2010-02-01',
 '1995-08-25',
 '2020-04-27',
 '2006-12-05',
 '1994-05-12',
 '1997-10-12',
 '2012-12-31',
 '2009-01-11',
 '2012-10-02',
 '2016-06-05',
 '2009-12-04',
 '2014-05-15',
 '2019-11-25',
 '2018-06-08',
 '1994-12-19',
 '1994-12-18',
 '2020-02-09',
 '2004-06-04',
 '2016-12-05',
 '2012-12-25',
 '2003-04-25',
 '1999-06-20',
 '2014-08-08',
 '1995-09-26',
 '2014-01-01',
 '2017-03-22',
 '2018-03-03',
 '2003-03-12',
 '1995-07-12',
 '2005-02-14',
 '1994-04-23',
 '2005-07-18',
 '2013-10-02',
 '2002-03-24',
 '2013-02-15',
 '2012-05-18',
 '1999-04-05',
 '2002-03-14',
 '2004-10-10',
 '2012-02-07',
 '2015-01-30',
 '2017-11-15',
 '2012-11-27',
 '2018-02-02',
 '2001-03-23',
 '1993-05-25',
 '2015-04-26',
 '1993-06-01',
 '1993-04-12',
 '2012-08-28',
 '1999-02-15',
 '1996-06-09',
 '1992-01-29',
 '1990-06-13',
 '2005-07-02',
 '2005-08-12',
 '1997-07-02',
 '1992-12-25',
 '2020-05-11',
 '1996-02-19',
 '2019-08-22',
 '2012-12-30',
 '2013-01-19',
 '2008-01-09',
 '1997-09-09',
 '2003-01-22',
 '2007-08-10',
 '2018-11-07',
 '1991-11-17',
 '2003-08-13',
 '2018-06-22',
 '2014-10-19',
 '2017-04-12',
 '2011-10-21',
 '2001-10-27',
 '1999-01-12',
 '1995-11-03',
 '1991-10-30',
 '2003-11-03',
 '2008-07-18',
 '1997-10-29',
 '1994-02-21',
 '2006-09-06',
 '1990-07-22',
 '1992-09-11',
 '2005-02-20',
 '2007-06-08',
 '2001-03-03',
 '2009-04-15',
 '2013-02-21',
 '1997-07-17',
 '1990-05-27',
 '2006-02-13',
 '2008-11-01',
 '1998-01-16',
 '2008-09-22',
 '1991-01-19',
 '1998-09-16',
 '2005-03-11',
 '2020-01-06',
 '2019-06-14',
 '1996-05-08',
 '1993-01-28',
 '2018-09-20',
 '1999-08-24',
 '1992-04-03',
 '2003-01-06',
 '1995-09-21',
 '2011-12-09',
 '2013-01-15',
 '2012-04-19',
 '1991-10-11',
 '1993-12-14',
 '2018-03-01',
 '1991-05-19',
 '2020-02-20',
 '2002-10-31',
 '1999-03-02',
 '2002-05-20',
 '1992-05-21',
 '2008-12-10',
 '2014-03-11',
 '2014-02-25',
 '2012-06-29',
 '1998-02-18',
 '2011-11-12',
 '2004-10-07',
 '1994-04-16',
 '1990-01-23',
 '2016-11-19',
 '1990-09-05',
 '2010-02-28',
 '2016-12-21',
 '2010-08-08',
 '2006-06-21',
 '2015-02-20',
 '1996-05-09',
 '2012-04-16',
 '2004-02-06',
 '2015-05-30',
 '2004-08-01',
 '2018-08-01',
 '2006-01-20',
 '2011-08-01',
 '2003-11-20',
 '2005-07-30',
 '1991-12-04',
 '2003-01-30',
 '2019-09-17',
 '2015-06-24',
 '2012-09-19',
 '2000-07-31',
 '2019-08-10',
 '1999-10-29',
 '2020-06-24',
 '1993-09-25',
 '2019-04-12',
 '1997-11-24',
 '2005-02-05',
 '1997-01-03',
 '2007-09-21',
 '2014-05-07',
 '2012-10-18',
 '2016-09-13',
 '2009-02-11',
 '2000-04-20',
 '2018-12-27',
 '2016-04-04',
 '2003-01-31',
 '1995-10-04',
 '2002-04-30',
 '2020-07-21',
 '2001-12-29',
 '1993-11-27',
 '1999-11-05',
 '2015-02-14',
 '1992-06-04',
 '1994-11-19',
 '2010-05-24',
 '2017-05-07',
 '1990-12-07',
 '2016-06-14',
 '2009-03-07',
 '2005-01-08',
 '2010-11-08',
 '2017-06-24',
 '1994-09-06',
 '2011-06-01',
 '2000-12-30',
 '2014-04-20',
 '2011-09-20',
 '2001-03-10',
 '1991-04-01',
 '2007-06-25',
 '1998-01-20',
 '2012-11-15',
 '2008-07-05',
 '2001-02-25',
 '1995-11-19',
 '2018-09-14',
 '2017-04-01',
 '2008-10-22',
 '1990-09-23',
 '2006-09-13',
 '1994-05-19',
 '2000-08-27',
 '1996-04-19',
 '2011-04-08',
 '2014-03-23',
 '1998-11-15',
 '2014-07-28',
 '2014-03-15',
 '2012-07-16',
 '1993-10-03',
 '1994-11-20',
 '1992-01-26',
 '1992-06-22',
 '2017-05-17',
 '2003-12-10',
 '2008-11-08',
 '2000-11-10',
 '1999-03-28',
 '2018-05-17',
 '2010-08-26',
 '2018-03-06',
 '1993-06-04',
 '1992-08-31',
 '2019-10-19',
 '1996-05-10',
 '2018-05-31',
 '2016-11-09',
 '2003-06-09',
 '2005-02-08',
 '2010-11-09',
 '1991-04-22',
 '2010-05-21',
 '2010-07-25',
 '1995-04-14',
 '1996-03-06',
 '2003-01-01',
 '1992-05-20',
 '1991-05-17',
 '2011-08-09',
 '1993-12-30',
 '2012-04-03',
 '2006-05-08',
 '2011-04-12',
 '2015-04-02',
 '2018-05-21',
 '2010-10-16',
 '2017-01-29',
 '2016-07-09',
 '1996-02-18',
 '2010-10-15',
 '1996-05-29',
 '2007-08-11',
 '2011-04-13',
 '2008-03-06',
 '2005-05-28',
 '2020-03-20',
 '1999-09-18',
 '1990-03-07',
 '1997-12-17',
 '2019-05-09',
 '2000-05-19',
 '1998-06-30',
 '1993-05-24',
 '2012-05-23',
 '2011-01-17',
 '2014-10-22',
 '2005-03-02',
 '2014-07-12',
 '1993-02-08',
 '2000-04-03',
 '1992-06-19',
 '1994-03-07',
 '1996-10-30',
 '2003-06-11',
 '1996-02-17',
 '2012-07-05',
 '2008-07-19',
 '1996-05-22',
 '2006-06-12',
 '2012-02-29',
 '2010-01-08',
 '2020-09-25',
 '1993-03-08',
 '2007-11-05',
 '2004-02-28',
 '2013-11-13',
 '1993-04-04',
 '2016-09-10',
 '2013-04-13',
 '1990-11-01',
 '1999-04-06',
 '2007-05-09',
 '1992-06-05',
 '1991-11-21',
 '1994-12-10',
 '2000-02-10',
 '2006-11-18',
 '2020-09-05',
 '1990-12-15',
 '2016-06-13',
 '1991-09-05',
 '2008-11-07',
 '2009-03-22',
 '1994-11-07',
 '2000-11-06',
 '2014-10-24',
 '1990-12-02',
 '2018-12-05',
 '2017-02-25',
 '2012-07-30',
 '1996-09-24',
 '1993-02-12',
 '2020-10-08',
 '2017-09-24',
 '2005-10-07',
 '2003-01-17',
 '2003-03-03',
 '2015-08-14',
 '2018-06-01',
 '2002-04-06',
 '2011-11-30',
 '1991-07-07',
 '2001-09-02',
 '1997-12-23',
 '2016-01-30',
 '2010-02-06',
 '1990-06-20',
 '1999-12-16',
 '1997-06-07',
 '2010-03-22',
 '2000-06-14',
 '2015-10-06',
 '2002-01-12',
 '2017-07-27',
 '2018-03-16',
 '1995-07-17',
 '2007-02-28',
 '2002-08-11',
 '1993-06-23',
 '2013-09-28',
 '2003-12-02',
 '2004-07-22',
 '1996-11-15',
 '1993-01-14',
 '1993-05-23',
 '2007-05-02',
 '2014-12-31',
 '2015-08-15',
 '2005-01-17',
 '2011-10-27',
 '2008-06-29',
 '2013-03-07',
 '2004-04-26',
 '1999-07-25',
 '1995-04-22',
 '1994-06-10',
 '2005-05-19',
 '1993-01-22',
 '2014-01-22',
 '2013-02-09',
 '2020-09-22',
 '2007-12-04',
 '2020-04-15',
 '2006-03-19',
 '2019-02-06',
 '1991-05-22',
 '2003-11-19',
 '2005-01-06',
 '2020-06-29',
 '2007-02-27',
 '2007-12-11',
 '1999-02-03',
 '1991-05-23',
 '2003-09-29',
 '2001-03-28',
 '1997-07-22',
 '2012-09-27',
 '1999-11-20',
 '2010-06-27',
 '2010-04-13',
 '1994-12-20',
 '2007-06-29',
 '1993-01-02',
 '2004-11-12',
 '2004-09-08',
 '2017-12-13',
 '2012-09-08',
 '2019-04-20',
 '2002-11-19',
 '1998-04-13',
 '2013-06-25',
 '1994-11-25',
 '1998-09-07',
 '1999-01-19',
 '2001-11-20',
 '1997-05-10',
 '2008-07-08',
 '1995-11-26',
 '2013-08-31',
 '2018-12-16',
 '2009-10-25',
 '2005-07-07',
 '2016-08-19',
 '2015-05-14',
 '2010-09-29',
 '2012-06-05',
 '2002-06-22',
 '2003-09-13',
 '2000-09-07',
 '2009-11-25',
 '2002-10-09',
 '2006-11-17',
 '2015-08-02',
 '1994-11-15',
 '1995-03-26',
 '2020-05-25',
 '1990-11-07',
 '1994-01-03',
 '1994-11-04',
 '1997-02-15',
 '1996-11-09',
 '1992-08-09',
 '1990-02-10',
 '2000-01-29',
 '2000-09-05',
 '1996-04-18',
 '1993-08-24',
 '1995-10-05',
 '1992-01-07',
 '2001-11-06',
 '2014-08-02',
 '2001-09-23',
 '2013-03-29',
 '1991-05-14',
 '2011-01-15',
 '1995-07-06',
 '2007-01-13',
 '2015-11-05',
 '2012-08-22',
 '2017-02-01',
 '1996-08-11',
 '2004-10-28',
 '2012-01-17',
 '2008-06-01',
 '2004-06-07',
 '1996-11-12',
 '2017-06-20',
 '2006-09-29',
 '2016-04-24',
 '2019-02-13',
 '2002-09-22',
 '1997-02-25',
 '2020-10-07',
 '1998-01-11',
 '1992-09-04',
 '2006-11-29',
 '1993-10-14',
 '2000-09-24',
 '2014-02-12',
 '2007-11-24',
 '1997-03-22',
 '1997-07-03',
 '2005-12-03',
 '1995-07-26',
 '2014-03-03',
 '2003-02-27',
 '2008-08-13',
 '2010-02-25',
 '2000-01-17',
 '1998-06-07',
 '2007-02-26',
 '2004-03-16',
 '2017-10-25',
 '2001-10-02',
 '2004-07-06',
 '2003-03-01',
 '2009-02-02',
 '2002-02-09',
 '1991-07-15',
 '1995-10-02',
 '2016-09-30',
 '1991-11-30',
 '2005-03-09',
 '1993-01-23',
 '1997-08-15',
 '1993-02-27',
 '1990-06-26',
 '2020-08-27',
 '2005-08-11',
 '2000-04-12',
 '2019-03-20',
 '2019-12-03',
 '2015-02-18',
 '1998-12-15',
 '2010-11-16',
 '2007-06-24',
 '2009-12-01',
 '2001-12-13',
 '2007-07-27',
 '2018-08-17',
 '2010-10-18',
 '2012-08-05',
 '2005-01-18',
 '2011-08-24',
 '2015-12-29',
 '2009-02-05',
 '2019-10-18',
 '2003-12-19',
 '1995-01-18',
 '1990-07-12',
 '2006-02-26',
 '2012-10-26',
 '2018-07-04',
 '2019-08-19',
 '2012-01-22',
 '2009-12-25',
 '1991-01-20',
 '2008-09-01',
 '2002-07-03',
 '2015-12-11',
 '1993-07-17',
 '2012-01-12',
 '2007-06-22',
 '1996-01-25',
 '2004-12-12',
 '2007-10-15',
 '2008-04-15',
 '1991-12-28',
 '2018-03-15',
 '1994-10-17',
 '1990-11-16',
 '1993-05-17',
 '2017-09-25',
 '1999-11-23',
 '1997-03-24',
 '2004-09-21',
 '2006-05-04',
 '2003-01-26',
 '2008-10-29',
 '2013-09-14',
 '2007-07-28',
 '2007-05-03',
 '2006-07-16',
 '2016-01-29',
 '1998-07-21',
 '2007-08-26',
 '2006-12-24',
 '2007-07-26',
 '2009-03-02',
 '1994-07-07',
 '1999-10-27',
 '2010-08-19',
 '1998-03-21',
 '2016-11-18',
 '2019-09-15',
 '2020-06-06',
 '2000-03-28',
 '2002-12-22',
 '2001-01-17',
 '2006-09-17',
 '1993-09-16',
 '2006-09-20',
 '1990-10-04',
 '1991-09-30',
 '2017-03-24',
 '2004-09-24',
 '2015-06-13',
 '2005-07-05',
 '1991-11-01',
 '1996-06-24',
 '1996-07-18',
 '1994-03-14',
 '2016-03-15',
 '1992-01-09',
 '2007-06-26',
 '2005-07-09',
 '2003-11-24',
 '2008-12-11',
 '2009-09-08',
 '1997-09-06',
 '1997-07-27',
 '2007-12-30',
 '2011-03-11',
 '2010-09-11',
 '2013-06-22',
 '1992-04-19',
 '2003-02-14',
 '1998-12-01',
 '2013-01-10',
 '2014-08-23',
 '1991-05-08',
 '1996-04-16',
 '1993-12-20',
 '2006-07-19',
 '2011-10-19',
 '2010-01-25',
 '1990-04-27',
 '2016-04-06',
 '1997-02-04',
 '2001-01-05',
 '2016-01-01',
 '2004-05-12',
 '1990-02-24',
 '1994-01-07',
 '2008-05-27',
 '1999-11-03',
 '2006-10-13',
 '2018-12-31',
 '2010-11-20',
 '2011-09-12',
 '1993-01-16',
 '2002-07-28',
 '2011-10-04',
 '1993-07-30',
 '2012-05-10',
 '1998-06-01',
 '2012-05-16',
 '2020-10-03',
 '2006-08-16',
 '1994-08-15',
 '1995-05-15',
 '2015-10-30',
 '2010-05-02',
 '2013-03-01',
 '1992-10-24',
 '2009-02-13',
 '1993-12-21',
 '2017-08-10',
 '2011-04-29',
 '2017-01-18',
 '2008-07-04',
 '2002-08-12',
 '2015-12-13',
 '2008-09-08',
 '2001-04-24',
 '2000-07-14',
 '2004-01-05',
 '2015-08-13',
 '2002-02-23',
 '2008-05-30',
 '2003-06-02',
 '2003-12-21',
 '1998-04-10',
 '2014-11-25',
 '2013-05-27',
 '1994-02-05',
 '1995-03-05',
 '1997-05-13',
 '2009-11-20',
 '1998-10-12',
 '1997-01-24',
 '1994-01-25',
 '1997-01-01',
 '1994-08-25',
 '1995-04-21',
 '1998-04-15',
 '2004-05-03',
 '2001-06-06',
 '1990-04-09',
 '1995-12-28',
 '1991-11-23',
 '1996-09-03',
 '2017-03-04',
 '2013-07-15',
 '2000-06-13',
 '2007-10-29',
 '2005-06-09',
 '1994-11-28',
 '1994-06-19',
 '2014-12-28',
 '2011-05-26',
 '1994-08-23',
 '2005-12-07',
 '1998-03-14',
 '1994-07-24',
 '2015-06-26',
 '2009-12-11',
 '2014-11-04',
 '1993-04-03',
 '2011-04-04',
 '1993-02-01',
 '2003-07-04',
 '2004-08-12',
 '2009-01-23',
 '2007-02-06',
 '2018-04-08',
 '2000-08-13',
 '2019-05-27',
 '1995-10-03',
 '2000-06-30',
 '1998-08-30',
 '1995-01-25',
 '1993-11-04',
 '2017-07-07',
 '2007-01-19',
 '2001-11-04',
 '2009-03-12',
 '1997-03-20',
 '2012-01-02',
 '2008-02-07',
 '2000-07-08',
 '1996-03-02',
 '1994-05-26',
 '2006-12-26',
 '2004-07-07',
 '2014-08-11',
 '1999-04-11',
 '2009-08-15',
 '2003-01-12',
 '1990-11-12',
 '2010-04-28',
 '1992-05-04',
 '2004-05-04',
 '2016-04-22',
 '2002-02-06',
 '1998-07-06',
 '2008-05-03',
 '1991-01-26',
 '2012-06-07',
 '2000-05-09',
 '2014-04-13',
 '1996-04-28',
 '2008-02-05',
 '1997-10-28',
 '1997-06-23',
 '1997-10-14',
 '2017-09-13',
 '2018-02-19',
 '2007-05-13',
 '2006-09-16',
 '2015-09-23',
 '1991-03-11',
 '2020-09-13',
 '2017-04-30',
 '1996-07-19',
 '1998-04-01',
 '1990-08-26',
 '1999-06-03',
 '2014-09-23',
 '2020-03-03',
 '2020-10-04',
 '1994-11-02',
 '1991-08-13',
 '2017-06-25',
 '2006-05-26',
 '2019-11-30',
 '1996-07-21',
 '2004-10-27',
 '1994-11-17',
 '2005-03-21',
 '1995-07-28',
 '2019-01-24',
 '2020-08-23',
 '2014-06-03',
 '2010-01-28',
 '2000-09-22',
 '1992-04-26',
 '1992-07-09',
 '1993-06-27',
 '2017-01-27',
 '1999-06-26',
 '2012-03-14',
 '1992-11-10',
 '1994-11-22',
 '2001-09-10',
 '2008-04-25',
 '2014-03-01',
 '1997-06-29',
 '2001-07-29',
 '1995-08-19',
 '1991-10-29',
 '2002-02-17',
 '2012-09-21',
 '2007-03-14',
 '2011-06-08',
 '2006-04-30',
 '1992-07-01',
 '2006-11-13',
 '2010-12-15',
 '2003-02-17',
 '2013-09-22',
 '2009-08-28',
 '2011-09-24',
 '2015-01-19',
 '2018-10-03',
 '1997-12-16',
 '1992-04-28',
 '2000-04-08',
 '2019-06-22',
 '2008-02-22',
 '2006-02-14',
 '2008-01-20',
 '1999-09-20',
 '2006-02-09',
 '1993-09-07',
 '1993-05-21',
 '2016-03-24',
 '2017-08-29',
 '2018-12-15',
 '1998-12-17',
 '1998-09-08',
 '1991-05-02',
 '2017-02-26',
 '2010-02-14',
 '2017-06-13',
 '2007-11-09',
 '1997-03-26',
 '2018-05-04',
 '2006-08-25',
 '1999-12-20',
 '1998-04-16',
 '2003-11-10',
 '2001-08-23',
 '2012-01-28',
 '2018-06-30',
 '2020-01-10',
 '2006-04-28',
 '2002-07-02',
 '2004-06-09',
 '1992-03-15',
 '1998-01-23',
 '1998-01-13',
 '2006-12-21',
 '1990-12-06',
 '2002-09-29',
 '2018-11-12',
 '1990-08-14',
 '1993-03-04',
 '2017-11-07',
 '1999-01-17',
 '1998-06-04',
 '2001-10-08',
 '2010-07-19',
 '2007-03-02',
 '2004-06-26',
 '2008-06-18',
 '2008-03-23',
 '2014-01-30',
 '1995-10-08',
 '2005-09-23',
 '2000-02-24',
 '2011-05-20',
 '2003-08-28',
 '2017-09-19',
 '2010-01-03',
 '2005-10-17',
 '2018-03-07',
 '2013-08-17',
 '2007-12-08',
 '1996-12-18',
 '2015-04-11',
 '1993-09-24',
 '1995-02-08',
 '2006-08-24',
 '2002-01-26',
 '2009-04-05',
 '1994-01-14',
 '2015-11-28',
 '2019-06-16',
 '1997-09-29',
 '2007-09-12',
 '1994-03-13',
 '2018-02-21',
 '2003-04-13',
 '2010-05-29',
 '2006-01-08',
 '2010-01-26',
 '2018-09-21',
 '2000-09-20',
 '1992-10-01',
 '2010-07-04',
 '1997-05-28',
 '1996-11-06',
 '2012-08-23',
 '2009-03-09',
 '2016-04-05',
 '2008-01-21',
 '1990-12-21',
 '1996-06-13',
 '2014-02-19',
 '2020-02-14',
 '1996-12-24',
 '2000-01-07',
 '2008-01-30',
 '2019-12-26',
 '2000-02-17',
 '2006-05-18',
 '2004-01-19',
 '2015-11-07',
 '2004-09-15',
 '1998-03-18',
 '2019-08-04',
 '2003-02-02',
 '2000-12-06',
 '1996-03-29',
 '2000-03-26',
 '2010-11-28',
 '2015-08-19',
 '1992-03-12',
 '2008-09-23',
 '2017-04-22',
 '1998-08-08',
 '2012-10-12',
 '1992-10-31',
 '1997-04-30',
 '1993-01-05',
 '2002-12-10',
 '1994-04-11',
 '2015-11-13',
 '2005-05-14',
 '1998-08-26',
 '2002-02-08',
 '1993-12-09',
 '2017-01-20',
 '2017-09-28',
 '2007-08-22',
 '1994-06-15',
 '1991-08-29',
 '2014-06-26',
 '2007-10-10',
 '1992-10-06',
 '2002-02-04',
 '2009-01-31',
 '2016-06-27',
 '2006-12-29',
 '2012-02-12',
 '2020-09-28',
 '1992-09-13',
 '2008-10-24',
 '1992-03-14',
 '2011-07-14',
 '2017-09-09',
 '2013-04-01',
 '2019-10-06',
 '2019-04-28',
 '2003-12-15',
 '2011-03-06',
 '2001-03-20',
 '2015-10-26',
 '1997-06-10',
 '1991-11-19',
 '2014-01-19',
 '2017-07-18',
 '1992-02-20',
 '1993-11-07',
 '2011-07-09',
 '1993-07-13',
 '2017-11-25',
 '1993-03-13',
 '2020-10-09',
 '2009-10-07',
 '2018-08-23',
 '2016-08-08',
 '2006-09-30',
 '1997-12-26',
 '1993-05-28',
 '2012-07-24',
 '2008-11-28',
 '2002-05-29',
 '2013-09-20',
 '2014-06-13',
 '2001-12-06',
 '1994-12-22',
 '2017-07-26',
 '2019-03-17',
 '1995-04-25',
 '2002-08-14',
 '1995-05-22',
 '2008-11-25',
 '1998-11-20',
 '2019-03-14',
 '1992-07-29',
 '1991-09-09',
 '2017-08-07',
 '2003-03-02',
 '2004-04-15',
 '1996-12-04',
 '1996-01-10',
 '2011-04-27',
 '2010-09-16',
 '2014-10-12',
 '1998-01-19',
 '2007-10-17',
 '2006-03-07',
 '1999-06-14',
 '2015-10-25',
 '1994-07-11',
 '1991-05-28',
 '2009-07-12',
 '1992-11-18',
 '2005-11-07',
 '2009-06-10',
 '2017-03-09',
 '1991-10-14',
 '2020-04-07',
 '2004-08-20',
 '1991-11-12',
 '1990-02-22',
 '1998-12-19',
 '2018-01-03',
 '2007-10-07',
 '2017-06-07',
 '2006-06-10',
 '2008-01-04',
 '1990-12-31',
 '2003-09-20',
 '2019-07-30',
 '2015-04-06',
 '2003-01-24',
 '2018-04-23',
 '1999-09-04',
 '2017-04-09',
 '1995-05-09',
 '2007-09-30',
 '1991-01-08',
 '2011-04-30',
 '2002-01-31',
 '2020-02-12',
 '2009-10-05',
 '2019-01-30',
 '1994-04-19',
 '2004-11-07',
 '1990-11-14',
 '1992-01-08',
 '2015-05-01',
 '1992-06-06',
 '2018-08-31',
 '2008-03-04',
 '2012-01-05',
 '2012-07-15',
 '2002-03-01',
 '1995-01-29',
 '2020-07-15',
 '2007-07-16',
 '2013-06-16',
 '2009-03-10',
 '2004-11-15',
 '1997-11-30',
 '2005-01-21',
 '2007-10-05',
 '1995-08-05',
 '2013-07-18',
 '1992-09-07',
 '2007-08-12',
 '2016-02-09',
 '2017-07-16',
 '2020-06-27',
 '2006-11-14',
 '2007-01-21',
 '2000-11-13',
 '1995-12-24',
 '2016-03-04',
 '2010-06-18',
 '1998-04-06',
 '2002-10-19',
 '2003-07-19',
 '2012-05-06',
 '2008-04-07',
 '2003-08-12',
 '1995-07-13',
 '2001-05-26',
 '2009-10-16',
 '2014-09-04',
 '1996-09-04',
 '1998-07-28',
 '1994-09-27',
 '2005-08-27',
 '1995-03-09',
 '2006-05-03',
 '1996-04-05',
 '1993-02-23',
 '2008-09-04',
 '2001-07-22',
 '2004-03-24',
 '1995-05-21',
 '1998-02-03',
 '2016-01-31',
 '2006-03-08',
 '1992-02-16',
 '1999-11-17',
 '1999-03-06',
 '2015-08-01',
 '2017-06-11',
 '2007-01-15',
 '2002-11-21',
 '2014-07-25',
 '2020-01-18',
 '1999-01-30',
 '1995-06-07',
 '2012-10-28',
 '2006-03-02',
 '1998-10-08',
 '2016-08-17',
 '2019-01-25',
 '1994-09-25',
 '1993-08-16',
 '2006-04-25',
 '2012-03-23',
 '2008-06-28',
 '2019-10-07',
 '2016-03-17',
 '1994-02-11',
 '2011-10-23',
 '1992-04-11',
 '1997-02-06',
 '2008-09-30',
 '2004-01-10',
 '2012-04-14',
 '1993-03-20',
 '2004-10-30',
 '1994-04-06',
 '2017-01-16',
 '2004-05-29',
 '2015-07-16',
 '2005-11-28',
 '2015-05-13',
 '2018-03-04',
 '2017-12-25',
 '2017-01-02',
 '2001-11-11',
 '2011-08-03',
 '2003-11-18',
 '2013-09-25',
 '2019-10-15',
 '1998-03-08',
 '2014-05-27',
 '1993-05-12',
 '2019-04-21',
 '1995-06-03',
 '2005-02-07',
 '2004-07-24',
 '2017-01-28',
 '2001-03-24',
 '1993-12-08',
 '2009-12-23',
 '2017-06-10',
 '1991-11-07',
 '2015-09-05',
 '1997-12-03',
 '1997-07-08',
 '2000-11-30',
 '2016-04-14',
 '1997-06-21',
 '1999-03-23',
 '2019-12-19',
 '2017-03-21',
 '1995-09-22',
 '2005-02-28',
 '1990-06-09',
 '1994-12-23',
 '2006-01-27',
 '1998-03-22',
 '2010-06-17',
 '1991-11-27',
 '1996-04-30',
 '2004-11-27',
 '2020-08-09',
 '2008-07-28',
 '2015-09-24',
 '2005-09-02',
 '2019-11-12',
 '2016-09-29',
 '2010-03-24',
 '1997-10-18',
 '1990-04-16',
 '2006-07-05',
 '2008-04-24',
 '2001-09-18',
 '1995-04-11',
 '2004-02-19',
 '1998-03-01',
 '2014-11-15',
 '2016-02-28',
 '2012-06-27',
 '2013-07-12',
 '2010-07-01',
 '2000-05-28',
 '1996-07-17',
 '2007-03-03',
 '2007-11-28',
 '2013-01-14',
 '1997-11-09',
 '2011-11-13',
 '2007-02-21',
 '1991-06-01',
 '1995-10-06',
 '2005-01-22',
 '2009-08-10',
 '2019-10-25',
 '2019-09-23',
 '2017-03-14',
 '2009-10-21',
 '1991-11-02',
 '1996-03-13',
 '2006-02-15',
 '2003-05-21',
 '2014-03-20',
 '2017-02-06',
 '2012-12-19',
 '1991-03-17',
 '2019-10-04',
 '2010-06-28',
 '2010-11-25',
 '2002-02-05',
 '1996-05-26',
 '1995-05-16',
 '2011-02-14',
 '2017-05-09',
 '2003-10-14',
 '2012-03-19',
 '2007-01-18',
 '2015-03-19',
 '1995-02-15',
 '1999-05-26',
 '1994-02-22',
 '1990-10-18',
 '2013-09-19',
 '2020-01-15',
 '2004-10-24',
 '2000-11-19',
 '2018-03-17',
 '1998-02-04',
 '2013-06-20',
 '2009-08-04',
 '2010-01-17',
 '2013-12-16',
 '1995-01-08',
 '1999-09-08',
 '2002-12-07',
 '1994-01-30',
 '1992-10-25',
 '2006-12-04',
 '1993-04-20',
 '2020-06-12',
 '2020-07-20',
 '1990-05-04',
 '2002-01-17',
 '1991-12-20',
 '2001-08-02',
 '2009-10-17',
 '2020-01-03',
 '2006-03-29',
 '2006-06-09',
 '2008-01-31',
 '2000-12-20',
 '1998-01-07',
 '2014-09-22',
 '2016-12-02',
 '2017-04-06',
 '2013-07-08',
 '1993-12-31',
 '2008-06-03',
 '1991-09-10',
 '1991-02-04',
 '2003-08-04',
 '1992-03-08',
 '1996-12-17',
 '1993-08-21',
 '2005-09-08',
 '2005-01-26',
 '1996-03-11',
 '2016-06-09',
 '2008-04-05',
 '2017-01-21',
 '2005-11-21',
 '2008-01-17',
 '2004-04-16',
 '2003-12-25',
 '1996-07-11',
 '1999-10-22',
 '1997-03-31',
 '1993-07-04',
 '2015-08-18',
 '2018-03-29',
 '2004-08-19',
 '2020-01-23',
 '2004-05-20',
 '2008-01-01',
 '2016-07-28',
 '1995-12-16',
 '2013-03-10',
 '1999-05-20',
 '1990-10-02',
 '2010-08-07',
 '2015-03-29',
 '2013-07-25',
 '2019-07-14',
 '2011-06-02',
 '2015-02-11',
 '2019-07-23',
 '2010-12-21',
 '2007-03-10',
 '2010-01-24',
 '1995-01-21',
 '2015-12-07',
 '1997-04-16',
 '2019-08-23',
 '2014-10-10',
 '1992-09-29',
 '2015-04-13',
 '1993-12-07',
 '1999-09-29',
 '1999-05-18',
 '2020-08-05',
 '2003-02-28',
 '2014-01-11',
 '2019-05-30',
 '1991-11-11',
 '1999-08-02',
 '2006-05-02',
 '1996-12-03',
 '1999-01-26',
 '2016-01-03',
 '2013-07-17',
 '2002-01-22',
 '1993-08-04',
 '2009-06-28',
 '2001-07-06',
 '1992-02-14',
 '2011-10-07',
 '2004-09-02',
 '2014-05-12',
 '2002-05-21',
 '1999-01-24',
 '2004-05-11',
 '2013-03-12',
 '2017-06-19',
 '2004-12-20',
 '2007-06-11',
 '1994-05-31',
 '2011-04-10',
 '2010-07-15',
 '2017-01-06',
 '2004-10-09',
 '2017-05-25',
 '1996-03-27',
 '1998-09-13',
 '1990-09-17',
 '2005-06-16',
 '1992-12-07',
 '2002-01-23',
 '2007-04-19',
 '2012-04-12',
 '2020-09-18',
 '2011-02-05',
 '2011-03-09',
 '2004-07-12',
 '1998-08-24',
 '2003-01-21',
 '1996-12-20',
 '1999-01-01',
 '1995-09-02',
 '2009-03-27',
 '2011-01-01',
 '1991-10-22',
 '1991-09-26',
 '1996-10-31',
 '2011-02-08',
 '2014-06-16',
 '2017-03-29',
 '2010-01-27',
 '2002-07-06',
 '2009-10-02',
 '2007-03-28',
 '2003-03-22',
 '1992-06-03',
 '2011-02-09',
 '2020-06-02',
 '1999-05-10',
 '2013-08-25',
 '1996-09-20',
 '1995-04-02',
 '1992-01-03',
 '1990-09-06',
 '2005-10-21',
 '2017-11-29',
 '2017-08-17',
 '2014-01-15',
 '1992-12-19',
 '1999-10-26',
 '2013-09-21',
 '2004-10-29',
 '2007-03-04',
 '2010-05-01',
 '2006-08-14',
 '2019-06-12',
 '2016-01-12',
 '2018-06-05',
 '1998-04-12',
 '2015-11-20',
 '2018-10-16',
 '2005-08-25',
 '2000-07-21',
 '2004-09-05',
 '2016-11-14',
 '2000-07-10',
 '2008-12-17',
 '2008-06-25',
 '2001-09-09',
 '2002-05-30',
 '2016-09-20',
 '2012-09-18',
 '1998-09-26',
 '2012-02-19',
 '2012-02-18',
 '1994-05-28',
 '1994-05-01',
 '2001-07-09',
 '1997-07-15',
 '2013-08-22',
 '1993-12-04',
 '1996-03-09',
 '2000-08-20',
 '1997-07-10',
 '1994-04-13',
 '2009-10-19',
 '2011-04-25',
 '2020-02-21',
 '2005-07-06',
 '2010-05-06',
 '2015-08-17',
 '2011-08-14',
 '2019-09-22',
 '1994-08-12',
 '2016-04-25',
 '1999-09-03',
 '1995-06-13',
 '2018-02-08',
 '2010-08-05',
 '2020-05-06',
 '2011-07-27',
 '2016-06-26',
 '2014-02-15',
 '2000-03-01',
 '2002-11-24',
 '2008-05-28',
 '2011-12-29',
 '2013-03-31',
 '2019-11-19',
 '2007-06-23',
 '1997-04-12',
 '2011-02-15',
 '1996-03-10',
 '2014-04-08',
 '2008-04-26',
 '2004-04-25',
 '1996-05-02',
 '1996-06-21',
 '1994-09-12',
 '2000-09-12',
 '1997-01-18',
 '2003-12-23',
 '2006-02-05',
 '2017-10-04',
 '2002-07-22',
 '2011-05-06',
 '2004-06-10',
 '1997-04-01',
 '2015-03-24',
 '1993-05-26',
 '1997-08-08',
 '1997-11-13',
 '2011-01-29',
 '1994-06-16',
 '1991-10-19',
 '1995-06-28',
 '2003-11-11',
 '1996-12-27',
 '2008-05-06',
 '2002-06-07',
 '2007-04-05',
 '1991-04-02',
 '2001-11-19',
 '2000-12-25',
 '2014-10-13',
 '2012-03-29',
 '2019-11-18',
 '1991-07-28',
 '2000-03-20',
 '2006-06-11',
 '2017-01-15',
 '2001-10-22',
 '2019-12-20',
 '1995-01-24',
 '2007-04-27',
 '2019-11-01',
 '2015-03-06',
 '2004-02-23',
 '1996-10-23',
 '1991-06-28',
 '2013-11-01',
 '2016-03-03',
 '1997-03-18',
 '2001-01-25',
 '2001-12-05',
 '2009-08-09',
 '1994-06-12',
 '2002-02-26',
 '2018-06-06',
 '1992-08-25',
 '2001-06-27',
 '2009-01-04',
 '2010-08-13',
 '2020-07-24',
 '2008-03-24',
 '2019-09-18',
 '1992-07-11',
 '2014-07-05',
 '2007-09-24',
 '2000-03-30',
 '1999-09-05',
 '2006-04-23',
 '2002-10-10',
 '2007-08-24',
 '2009-01-28',
 '2011-01-30',
 '1991-07-01',
 '2013-01-24',
 '2012-07-17',
 '2014-07-30',
 '1995-12-09',
 '2013-02-20',
 '1997-01-14',
 '2016-06-03',
 '2016-06-30',
 '2017-02-13',
 '2013-10-10',
 '1995-11-21',
 '1991-06-10',
 '1990-12-11',
 '1996-12-14',
 '2003-08-23',
 '1995-12-31',
 '2004-03-30',
 '1995-01-01',
 '2008-10-28',
 '2010-01-05',
 '1991-11-28',
 '2004-03-07',
 '2004-06-22',
 '2010-03-28',
 '1999-10-08',
 '2004-06-18',
 '2001-05-07',
 '2016-07-20',
 '1998-02-14',
 '2012-11-24',
 '2016-12-16',
 '1999-12-05',
 '2006-12-25',
 '2009-09-27',
 '2008-07-15',
 '2008-09-26',
 '2004-07-27',
 '1990-04-23',
 '1994-02-07',
 '2017-08-16',
 '2014-08-18',
 '2010-09-24',
 '2012-01-01',
 '1998-04-07',
 '1998-01-04',
 '2005-01-09',
 '2006-10-25',
 '2007-01-09',
 '1993-01-25',
 '2008-08-02',
 '1993-06-14',
 '1995-06-21',
 '2005-03-08',
 '2019-11-03',
 '2013-01-07',
 '2008-12-15',
 '1999-04-22',
 '2003-02-16',
 '1990-11-25',
 '2004-01-30',
 '1994-03-10',
 '1997-04-08',
 '1993-08-27',
 '2003-10-18',
 '2013-07-03',
 '2018-04-26',
 '2013-05-07',
 '2019-03-13',
 '2009-11-10',
 '1991-07-18',
 '1995-10-13',
 '2009-11-26',
 '2015-01-10',
 '2009-06-07',
 '2010-09-26',
 '2019-12-10',
 '2000-02-07',
 '2008-01-26',
 '1994-04-14',
 '2000-01-11',
 '2017-05-11',
 '2018-10-04',
 '1969-12-31',
 '2005-09-01',
 '1991-08-03',
 '1996-03-05',
 '1996-10-29',
 '1993-03-22',
 '2016-03-21',
 '1994-07-03',
 '2008-02-03',
 '1991-11-25',
 '1996-07-14',
 '2007-08-01',
 '2001-03-09',
 '2012-11-21',
 '2002-02-10',
 '2009-11-07',
 '1993-08-17',
 '2000-06-07',
 '2009-11-05',
 '2001-08-07',
 '2016-12-03',
 '2000-11-25',
 '2019-04-27',
 '1999-11-01',
 '1997-01-30',
 '2009-04-12',
 '1990-01-08',
 '2003-01-20',
 '1995-07-20',
 '1994-10-09',
 '2006-12-03',
 '1990-10-27',
 '1995-02-14',
 '1995-06-12',
 '2010-12-03',
 '2009-11-06',
 '2020-09-21',
 '2017-01-01',
 '1992-07-25',
 '2011-07-12',
 '2017-11-03',
 '2019-10-16',
 '1997-09-17',
 '1996-08-01',
 '2020-01-21',
 '2003-11-07',
 '2000-04-09',
 '2012-04-02',
 '1992-03-11',
 '1994-02-18',
 '1993-11-26',
 '1995-02-21',
 '2002-11-15',
 '1999-11-19',
 '2014-04-10',
 '1997-12-28',
 '1998-02-27',
 '2005-02-24',
 '2016-03-08',
 '1994-06-24',
 '2005-01-24',
 '1994-09-29',
 '1999-10-04',
 '2016-06-10',
 '1999-07-12',
 '2001-10-26',
 '1990-04-14',
 '1993-06-09',
 '2012-02-13',
 '1997-02-03',
 '2005-03-03',
 '2016-09-18',
 '2011-10-06',
 '2016-05-15',
 '1995-06-15',
 '1998-05-05',
 '1996-09-23',
 '2005-10-09',
 '1992-01-05',
 '2013-06-03',
 '1997-04-26',
 '2005-06-10',
 '1992-10-17',
 '1995-03-28',
 '1995-02-13',
 '2014-12-18',
 '2009-02-21',
 '2000-04-25',
 '1993-02-09',
 '1995-05-05',
 '2016-02-15',
 '2007-07-11',
 '2007-07-31',
 '2013-05-15',
 '1990-03-23',
 '2002-08-13',
 '1993-10-25',
 '1993-09-11',
 '2005-09-24',
 '2011-03-27',
 '2011-11-07',
 '2016-10-12',
 '1994-04-08',
 '2008-10-26',
 '2015-11-18',
 '2003-03-21',
 '1998-07-15',
 '2013-05-08',
 '1995-07-23',
 '2012-02-28',
 '2004-08-17',
 '2001-11-14',
 '1992-06-01',
 '1994-03-26',
 '2011-02-26',
 '2018-05-27',
 '2003-10-03',
 '2010-05-19',
 '2006-11-05',
 '1998-07-11',
 '2013-04-18',
 '2019-06-24',
 '2001-09-21',
 '1995-08-12',
 '2011-07-20',
 '2013-10-25',
 '1992-11-13',
 '1996-01-20',
 '2004-11-18',
 '2001-04-28',
 '2007-03-27',
 '1998-02-15',
 '2019-03-26',
 '2003-03-29',
 '2015-07-19',
 '2005-10-26',
 '1997-11-06',
 '1992-02-27',
 '1997-08-23',
 '2012-09-22',
 '2001-12-16',
 '2015-03-07',
 '1990-01-27',
 '2005-06-18',
 '1993-01-30',
 '2000-05-12',
 '2014-11-08',
 '2010-03-17',
 '2005-10-19',
 '1993-01-12',
 '2011-05-23',
 '2014-06-23',
 '2005-11-22',
 '2020-06-18',
 '1998-02-08',
 '1990-09-03',
 '2015-04-03',
 '1990-12-28',
 '2005-05-24',
 '1990-08-17',
 '2013-04-19',
 '2001-01-06',
 '2007-05-23',
 '1998-04-03',
 '1994-07-31',
 '2007-05-25',
 '2004-12-06',
 '2008-07-27',
 '1996-10-10',
 '2000-09-06',
 '2009-10-08',
 '2010-08-04',
 '2000-02-02',
 '1999-05-03',
 '2019-04-13',
 '2014-07-27',
 '1994-10-18',
 '2017-11-23',
 '2018-11-13',
 '1997-11-22',
 '1994-01-06',
 '2003-01-23',
 '2001-08-05',
 '2011-07-01',
 '1991-02-07',
 '2020-04-06',
 '2001-07-07',
 '1990-11-03',
 '1990-06-27',
 '1999-07-19',
 '2014-09-27',
 '1998-11-04',
 '2019-01-01',
 '2020-08-20',
 '2006-04-24',
 '2017-10-07',
 '1999-11-21',
 '2014-01-27',
 '2004-02-13',
 '2009-08-23',
 '2000-05-02',
 '2005-01-04',
 '2016-09-15',
 '2008-10-16',
 '2003-10-10',
 '2017-09-07',
 '1993-03-11',
 '1993-10-08',
 '2014-01-16',
 '2005-09-06',
 '1992-07-27',
 '1994-10-06',
 '2006-06-28',
 '1990-08-29',
 '2006-06-22',
 '2016-07-06',
 '2001-05-21',
 '2009-07-29',
 '1998-12-27',
 '2008-11-04',
 '1997-12-12',
 '2011-12-17',
 '1992-05-24',
 '2005-09-28',
 '1999-05-16',
 '2005-04-05',
 '2000-10-04',
 '1992-04-09',
 '2002-01-19',
 '1996-02-04',
 '1992-11-22',
 '2008-03-16',
 '2001-12-18',
 '2017-08-05',
 '1995-02-03',
 '2015-06-25',
 '1994-03-18',
 '2006-02-07',
 '2009-03-04',
 '2000-11-23',
 '2000-02-28',
 '2006-03-26',
 '2001-05-23',
 '1991-08-08',
 '2009-04-04',
 '2010-11-21',
 '1995-12-08',
 '2015-06-18',
 '2016-04-15',
 '2006-08-10',
 '2012-11-28',
 '2013-02-13',
 '2017-02-04',
 '2019-01-05',
 '2020-05-30',
 '2009-12-14',
 '2015-07-07',
 '1990-02-17',
 '2013-09-15',
 '1991-03-05',
 '2003-09-11',
 '2006-05-24',
 '2003-06-28',
 '2016-03-14',
 '2013-06-11',
 '1997-03-23',
 '2002-07-04',
 '2020-01-09',
 '2017-06-23',
 '1991-02-09',
 '1998-11-26',
 '2011-08-27',
 '1997-02-16',
 '2000-02-14',
 '2016-04-09',
 '1993-05-11',
 '1997-10-02',
 '1990-11-02',
 '2016-04-18',
 '1998-12-06',
 '2000-01-27',
 '2019-06-01',
 '2013-10-18',
 '2000-02-03',
 '1991-02-05',
 '1997-08-18',
 '2010-01-31',
 '1991-10-24',
 '2015-10-16',
 '2015-04-27',
 '2005-04-04',
 '1993-02-19',
 '1997-06-01',
 '1999-06-15',
 '2013-08-14',
 '2017-10-28',
 '2020-08-11',
 '2013-04-03',
 '1995-07-27',
 '2015-06-15',
 '1991-10-26',
 '2000-01-15',
 '1997-09-24',
 '2007-01-01',
 '2004-04-22',
 '2020-05-05',
 '2000-02-27',
 '2013-02-07',
 '1994-04-30',
 '1997-12-31',
 '1999-11-12',
 '1991-03-31',
 '2002-10-16',
 '1994-10-26',
 '2005-02-02',
 '2005-03-05',
 '1995-08-30',
 '1991-04-21',
 '2006-10-29',
 '2011-06-29',
 '2004-12-21',
 '1997-11-28',
 '1992-04-14',
 '2015-10-29',
 '2018-03-20',
 '2016-08-21',
 '2000-01-19',
 '1994-03-12',
 '2000-02-08',
 '2009-01-21',
 '2012-12-17',
 '2003-10-21',
 '2012-09-28',
 '2014-08-24',
 '1993-05-01',
 '2018-12-29',
 '2010-12-24',
 '2011-01-19',
 '2015-07-27',
 '2006-03-31',
 '1991-08-07',
 '2005-02-26',
 '2007-01-20',
 '2003-08-31',
 '2016-08-22',
 '1999-12-23',
 '2011-07-23',
 '2005-05-02',
 '1993-02-18',
 '2005-03-01',
 '1996-07-03',
 '2009-09-28',
 '2018-12-24',
 '2003-04-21',
 '2006-11-30',
 '1993-02-04',
 '2009-10-03',
 '1999-04-16',
 '2012-03-18',
 '2007-02-13',
 '2016-07-29',
 '1992-07-14',
 '1995-03-30',
 '2007-05-04',
 '2001-06-12',
 '2004-08-25',
 '1998-09-20',
 '2016-04-16',
 '2020-05-23',
 '1997-02-26',
 '2017-11-11',
 '2005-12-18',
 '2014-07-04',
 '2009-09-23',
 '2000-09-11',
 '2006-01-17',
 '1993-10-07',
 '2012-10-29',
 '1996-08-13',
 '2007-08-30',
 '1994-07-06',
 '2006-04-04',
 '2013-09-03',
 '1999-09-13',
 '2011-04-02',
 '2019-09-09',
 '2001-05-17',
 '2017-05-12',
 '2016-01-20',
 '2005-06-05',
 '2015-09-12',
 '2011-05-19',
 '1999-01-14',
 '2008-07-26',
 '2010-06-21',
 '2005-06-23',
 '1999-11-22',
 '2000-11-02',
 '1990-12-27',
 '2019-01-14',
 '2003-06-14',
 '2020-07-17',
 '2018-02-25',
 '1997-07-12',
 '2020-05-02',
 '1993-10-19',
 '1993-08-22',
 '2020-08-13',
 '2000-07-02',
 '2009-07-01',
 '1997-12-20',
 '2002-12-04',
 '2009-12-29',
 '1992-03-27',
 '1998-09-10',
 '1996-05-30',
 '2017-03-16',
 '2002-03-13',
 '2004-03-14',
 '2001-04-07',
 '1999-10-30',
 '1999-10-14',
 '2018-09-26',
 '1999-08-08',
 '2018-09-25',
 '2018-05-10',
 '2014-06-28',
 '2005-10-18',
 '2017-02-16',
 '1995-05-03',
 '2000-02-22',
 '2008-05-31',
 '1991-02-17',
 '2012-06-16',
 '1999-01-20',
 '2018-02-28',
 '2011-08-30',
 '2017-08-03',
 '2014-04-16',
 '2012-08-06',
 '2013-04-22',
 '2018-11-29',
 '2007-05-18',
 '2009-01-01',
 '2006-06-24',
 '1996-06-25',
 '2014-11-17',
 '2006-08-02',
 '1993-07-31',
 '2004-08-21',
 '2010-07-22',
 '2008-08-06',
 '1993-03-23',
 '1994-05-18',
 '2001-12-27',
 '2007-02-12',
 '1993-05-14',
 '1991-02-13',
 '2018-09-29',
 '1997-03-12',
 '2009-01-09',
 '2013-03-17',
 '2005-09-03',
 '2006-07-23',
 '2005-12-26',
 '2017-07-31',
 '1996-03-19',
 '2008-07-11',
 '2001-06-14',
 '2018-03-23',
 '2011-08-31',
 '2007-08-19',
 '2015-02-05',
 '2010-02-17',
 '1994-06-13',
 '2007-01-31',
 '1996-09-28',
 '2003-11-02',
 '2012-07-04',
 '2018-09-08',
 '2003-07-30',
 '2011-09-04',
 '2016-10-30',
 '2009-12-24',
 '1997-06-24',
 '2012-07-06',
 '1997-08-17',
 '2011-04-06',
 '2006-03-30',
 '1992-07-13',
 '2017-06-29',
 '1994-02-09',
 '1991-05-21',
 '2009-08-02',
 '2003-02-13',
 '1996-05-01',
 '2010-03-07',
 '2011-04-15',
 '1996-02-01',
 '1996-05-17',
 '1992-05-07',
 '2016-02-24',
 '2001-04-27',
 '1990-07-05',
 '2006-08-23',
 '1993-08-19',
 '2016-01-22',
 '1997-09-22',
 '2007-07-30',
 '2010-07-03',
 '2006-10-27',
 '1995-12-11',
 '2013-05-02',
 '2007-11-27',
 '2003-07-27',
 '2000-11-24',
 '2007-05-19',
 '2018-02-09',
 '2019-09-03',
 '2016-01-02',
 '2008-01-13',
 '1994-04-02',
 '1991-03-08',
 '2003-01-29',
 '1996-12-06',
 '2013-10-05',
 '2016-05-05',
 '1990-03-19',
 '2007-05-16',
 '1998-09-04',
 '1999-10-18',
 '2002-06-18',
 '2001-01-01',
 '1991-02-02',
 '2020-02-05',
 '2006-09-19',
 '1990-06-07',
 '1993-09-27',
 '2015-07-31',
 '1999-08-04',
 '2007-10-16',
 '1995-08-02',
 '1996-03-20',
 '2017-07-19',
 '2013-11-18',
 '2010-07-17',
 '1998-02-07',
 '1995-02-20',
 '2010-07-06',
 '2010-06-08',
 '2000-10-13',
 '1998-05-03',
 '1993-10-12',
 '1991-08-11',
 '2002-09-05',
 '2014-08-13',
 '1996-08-09',
 '2006-11-16',
 '2001-05-28',
 '1995-02-09',
 '2004-03-22',
 '1995-08-20',
 '2000-02-20',
 '2004-03-11',
 '2009-04-24',
 '2018-12-10',
 '1992-06-15',
 '1992-09-27',
 '2015-08-08',
 '2012-09-07',
 '2003-02-01',
 '1998-07-09',
 '1994-08-21',
 '2000-07-05',
 '2011-06-21',
 '2013-10-03',
 '2003-06-04',
 '2001-08-15',
 '2003-04-11',
 '2002-03-05',
 '1991-12-29',
 '2014-09-02',
 '2010-08-25',
 '2000-12-22',
 '1991-11-14',
 '2018-02-18',
 '1998-01-31',
 '1990-06-15',
 '2010-08-21',
 '2013-06-14',
 '1990-09-20',
 '2005-09-21',
 '2009-05-01',
 '1998-08-18',
 '2012-09-11',
 '2015-03-16',
 '2008-12-05',
 '2010-04-11',
 '1997-02-22',
 '1999-01-06',
 '2012-12-13',
 '2002-10-02',
 '2020-02-15',
 '1993-08-18',
 '2001-05-12',
 '1990-02-19',
 '1997-02-13',
 '2004-07-28',
 '1997-07-13',
 '1993-06-30',
 '2018-09-02',
 '2002-08-01',
 '2001-08-16',
 '1992-09-15',
 '2010-03-27',
 '1990-12-25',
 '2009-05-19',
 '2017-06-22',
 '2010-10-28',
 '1995-05-10',
 '1994-02-24',
 '1993-12-27',
 '2006-11-19',
 '2009-08-27',
 '2017-09-29',
 '2020-06-03',
 '2018-06-23',
 '1990-04-10',
 '1996-03-26',
 '2010-11-24',
 '2018-10-23',
 '2010-09-12',
 '2012-05-02',
 '2015-04-21',
 '1991-09-29',
 '2009-07-19',
 '2013-03-21',
 '2008-04-28',
 '1994-06-09',
 '2007-03-21',
 '1996-08-20',
 '2016-07-01',
 '2020-01-24',
 '2010-10-30',
 '1994-10-02',
 '2017-05-18',
 '2003-11-09',
 '2014-08-25',
 '1998-01-29',
 '1995-12-30',
 '2007-05-21',
 '1999-07-14',
 '2015-08-07',
 '2014-02-21',
 '1990-08-28',
 '2003-07-31',
 '2006-12-18',
 '2020-02-04',
 '2015-04-15',
 '2000-02-23',
 '2015-09-03',
 '1994-11-21',
 '2002-01-27',
 '2011-02-21',
 '2011-03-24',
 '2017-02-03',
 '2010-10-24',
 '2015-06-30',
 '2005-05-16',
 '1992-05-05',
 '2016-08-23',
 '2013-04-04',
 '1999-07-18',
 '2002-07-05',
 '1998-10-11',
 '2014-12-10',
 '2011-02-11',
 '1992-03-28',
 '2006-04-06',
 '2005-07-10',
 '1998-06-13',
 '2016-07-12',
 '2009-05-22',
 '2015-03-12',
 '1995-12-23',
 '2010-08-02',
 '1990-10-07',
 '2006-04-22',
 '2017-01-31',
 '2014-02-18',
 '2010-09-22',
 '2007-09-08',
 '1996-09-22',
 '2011-12-03',
 '1996-06-28',
 '2006-03-20',
 '1992-06-29',
 '2006-12-27',
 '2009-03-06',
 '2018-06-13',
 '2019-09-13',
 '1991-06-08',
 '2010-03-10',
 '2017-11-20',
 '2014-06-27',
 '1991-12-18',
 '1990-10-16',
 '1999-10-15',
 '2014-03-18',
 '2001-04-03',
 '2015-07-02',
 '2007-12-25',
 '2008-09-02',
 '1998-10-20',
 '1993-10-11',
 '1993-10-28',
 '2003-05-12',
 '2015-04-01',
 '2018-10-29',
 '2001-08-31',
 '1994-05-11',
 '2012-11-03',
 '1999-08-26',
 '2001-01-15',
 '2018-10-02',
 '2004-06-24',
 '2000-11-03',
 '2004-10-03',
 '2017-05-22',
 '2015-04-24',
 '2018-04-13',
 '2005-04-06',
 '2015-09-04',
 '1999-12-02',
 '2012-01-18',
 '2013-05-31',
 '1994-08-14',
 '1996-06-30',
 '2013-12-18',
 '2002-06-09',
 '2019-11-16',
 '2010-09-08',
 '2020-07-14',
 '2014-12-16',
 '1997-07-25',
 '2015-01-28',
 '1997-01-26',
 '2004-09-14',
 '2007-09-25',
 '1996-04-20',
 '2013-01-02',
 '2020-03-11',
 '1994-06-05',
 '1995-05-02',
 '2011-12-14',
 '2013-09-05',
 '2008-05-08',
 '1992-01-28',
 '1996-03-21',
 '2014-01-20',
 '1993-07-01',
 '1995-02-17',
 '1992-07-16',
 '2003-04-20',
 '2011-03-22',
 '2016-01-15',
 '2019-06-18',
 '1991-07-06',
 '1995-02-23',
 '2010-12-30',
 '1992-11-28',
 '2012-03-02',
 '2019-04-04',
 '2019-06-15',
 '2011-08-25',
 '1993-07-25',
 '2003-09-05',
 '1996-01-02',
 '1998-12-09',
 '2003-02-24',
 '1997-10-03',
 '2007-09-05',
 '2008-09-24',
 '2016-02-10',
 '2019-04-06',
 '2006-07-03',
 '2003-12-17',
 '2000-10-09',
 '1992-10-10',
 '2003-11-01',
 '2012-10-04',
 '1991-03-15',
 '2004-05-09',
 '2017-12-09',
 '2013-04-23',
 '2005-04-19',
 '1992-11-12',
 '1992-07-17',
 '1997-03-13',
 '1992-02-28',
 '2003-07-15',
 '1997-02-23',
 '1996-07-12',
 '1998-03-17',
 '2016-11-26',
 '1998-07-19',
 '1990-07-19',
 '2011-03-08',
 '1994-09-05',
 '2014-10-25',
 '2016-09-02',
 '2019-03-07',
 '1990-06-14',
 '2015-02-27',
 '2008-07-09',
 '2006-08-22',
 '2018-10-25',
 '2010-12-17',
 '2016-03-31',
 '2009-12-03',
 '2005-12-16',
 '2004-05-22',
 '2011-09-08',
 '2009-06-06',
 '2006-03-03',
 '1994-07-18',
 '2019-03-06',
 '2003-03-09',
 '2013-07-21',
 '1990-10-06',
 '1997-08-07',
 '2003-10-26',
 '2003-06-29',
 '1996-01-06',
 '1997-05-01',
 '2006-04-10',
 '1999-07-15',
 '2020-03-02',
 '2015-02-03',
 '2002-02-16',
 '2016-12-06',
 '1996-11-04',
 '1992-11-16',
 '2001-11-29',
 '1997-09-26',
 '2013-08-26',
 '2006-06-13',
 '1994-12-04',
 '2009-10-06',
 '2005-01-07',
 '2017-06-06',
 '2015-07-10',
 '1991-09-20',
 '2019-05-13',
 '2013-09-26',
 '1990-09-07',
 '2000-11-29',
 '1999-07-29',
 '2013-01-22',
 '2007-11-23',
 '1996-02-29',
 '1990-08-09',
 '1998-04-22',
 '1991-07-30',
 '1998-05-16',
 '1997-03-25',
 '2009-05-31',
 '2018-08-21',
 '2018-09-12',
 '2013-11-16',
 '2020-08-24',
 '2002-12-06',
 '2007-09-18',
 '2017-08-01',
 '2009-06-11',
 '2002-03-02',
 '2016-03-01',
 '1992-07-10',
 '1997-09-14',
 '1990-01-10',
 '2016-06-29',
 '2000-12-26',
 '2016-08-29',
 '2015-11-06',
 '2002-02-25',
 '2004-07-19',
 '2005-10-16',
 '1997-10-30',
 '1993-09-19',
 '2002-06-24',
 '1997-12-11',
 '2006-05-17',
 '2018-01-29',
 '2018-05-03',
 '2009-09-13',
 '2017-09-11',
 '2015-09-01',
 '2009-12-17',
 '2005-01-16',
 '2018-07-20',
 '2017-07-24',
 '1998-06-19',
 '2019-06-27',
 '2012-05-24',
 '2000-07-03',
 '2007-02-01',
 '2009-01-16',
 '1994-12-08',
 '2015-06-11',
 '1990-03-17',
 '1995-06-17',
 '2014-11-12',
 '2013-01-03',
 '2014-09-26',
 '1992-05-30',
 '1996-07-28',
 '1992-06-16',
 '2004-01-09',
 '2006-01-14',
 '2003-11-22',
 '2001-06-28',
 '2011-11-29',
 '2002-05-26',
 '2015-10-27',
 '2002-06-20',
 '2019-05-23',
 '2004-01-24',
 '1992-09-22',
 '2002-11-14',
 '1992-12-17',
 '2013-09-09',
 '2010-08-11',
 '2020-10-02',
 '1994-06-06',
 '2009-06-13',
 '2019-05-08',
 '2004-06-12',
 '1999-06-04',
 '2010-02-26',
 '1999-04-18',
 '2019-12-23',
 '2007-11-15',
 '2005-03-29',
 '2006-06-15',
 '1992-09-14',
 '2019-06-04',
 '2006-08-04',
 '2013-01-16',
 '2004-10-08',
 '1991-06-13',
 '2015-05-19',
 '2007-04-20',
 '2011-06-03',
 '1993-07-11',
 '2001-01-19',
 '1995-04-17',
 '2011-11-02',
 '1999-02-01',
 '2014-02-11',
 '1993-04-09',
 '1991-01-16',
 '2005-09-07',
 '2001-07-13',
 '1992-10-30',
 '2013-08-15',
 '2004-07-18',
 '2018-07-23',
 '2013-05-21',
 '2018-05-28',
 '1998-06-16',
 '2007-09-28',
 '1996-12-22',
 '1998-01-27',
 '1990-02-06',
 '2020-07-11',
 '1998-10-06',
 '2006-04-26',
 '1993-08-29',
 '2007-02-04',
 '2009-07-25',
 '2004-12-01',
 '2003-01-14',
 '2005-04-30',
 '2018-09-15',
 '2001-08-29',
 '2003-11-26',
 '2012-10-06',
 '2020-07-12',
 '2001-05-16',
 '1996-11-11',
 '2014-09-14',
 '2006-05-21',
 '2004-02-12',
 '1993-12-19',
 '1995-06-25',
 '1993-09-26',
 '2018-03-19',
 '2015-02-15',
 '1994-01-28',
 '2005-01-27',
 '2005-06-06',
 '2010-10-04',
 '2006-01-06',
 '2002-03-04',
 '1997-12-18',
 '2017-08-11',
 '2013-03-26',
 '2000-07-23',
 '1991-07-22',
 '1995-12-14',
 '1997-02-08',
 '1992-12-20',
 '1990-04-08',
 '2015-03-09',
 '1993-08-26',
 '2010-05-27',
 '1999-07-13',
 '2011-10-03',
 '1999-08-29',
 '2002-12-25',
 '1996-09-18',
 '2004-08-14',
 '1999-08-14',
 '1992-11-02',
 '1991-03-26',
 '2000-08-21',
 '2007-07-02',
 '1991-11-09',
 '2008-02-15',
 '2000-04-11',
 '1995-01-04',
 '1997-08-24',
 '2019-05-07',
 '1996-11-01',
 '1996-09-10',
 '2001-07-12',
 '2008-08-23',
 '1992-02-21',
 '2018-06-16',
 '1990-11-24',
 '1990-07-06',
 '2001-04-16',
 '2002-03-03',
 '2009-01-29',
 '1995-03-03',
 '1995-10-24',
 '1990-12-22',
 '1990-02-05',
 '2018-08-10',
 '1996-10-17',
 '1994-04-01',
 '2015-01-06',
 '2015-12-19',
 '2014-10-30',
 '1996-02-09',
 '1996-09-08',
 '1991-04-17',
 '1995-09-03',
 '1991-11-20',
 '2001-08-19',
 '2007-10-08',
 '2001-02-27',
 '2013-07-10',
 '1992-07-20',
 '2009-08-01',
 '2009-06-01',
 '1990-11-30',
 '2020-06-10',
 '1996-01-24',
 '2009-11-22',
 '1999-01-31',
 '2016-07-08',
 '1999-04-23',
 '2012-04-17',
 '2012-09-20',
 '2000-03-17',
 '2013-05-13',
 '1997-11-02',
 '2013-02-17',
 '2004-12-29',
 '2018-05-01',
 '1995-12-01',
 '1992-08-22',
 '2007-07-23',
 '2009-06-18',
 '2013-02-11',
 '2013-11-20',
 '2000-03-03',
 '1999-06-10',
 '2014-12-11',
 '2011-04-24',
 '1999-07-02',
 '2020-08-31',
 '2018-10-31',
 '2002-12-31',
 '2000-01-28',
 '2005-09-17',
 '2011-09-05',
 '1990-08-30',
 '2009-11-13',
 '2011-11-23',
 '1990-08-03',
 '2013-03-09',
 '1995-07-10',
 '2008-01-28',
 '1999-11-14',
 '1990-06-25',
 '2004-01-21',
 '1994-12-12',
 '2006-04-05',
 '2016-06-04',
 '2002-02-01',
 '1994-10-07',
 '2019-09-21',
 '1998-12-26',
 '2010-10-26',
 '2006-07-25',
 '2020-06-13',
 '2015-08-30',
 '2020-08-26',
 '1995-11-18',
 '2009-10-29',
 '2011-09-16',
 '2001-07-11',
 '2019-11-23',
 '2015-06-29',
 '1998-09-11',
 '2001-10-21',
 '2017-10-14',
 '2014-01-12',
 '2019-08-18',
 '2008-12-27',
 '2009-12-02',
 '1992-07-02',
 '2020-07-03',
 '2007-03-08',
 '1991-01-04',
 '1991-03-20',
 '2014-11-05',
 '2002-12-24',
 '2004-12-07',
 '2016-02-17',
 '1992-01-30',
 '2020-03-31',
 '1992-11-23',
 '1990-03-04',
 '2015-07-13',
 '2000-04-27',
 '1994-12-31',
 '2004-07-16',
 '2000-01-06',
 '2012-12-20',
 '2016-06-19',
 '2015-04-23',
 '2012-10-31',
 '2019-02-02',
 '1998-11-03',
 '2019-05-22',
 '1990-01-19',
 '2003-12-24',
 '2015-05-28',
 '1992-09-16',
 '2011-05-27',
 '2015-11-01',
 '2000-06-05',
 '2012-07-12',
 '2009-10-23',
 '1998-06-27',
 '2008-03-13',
 '2019-09-05',
 '2012-08-19',
 '1999-02-02',
 '2007-09-22',
 '2009-06-17',
 '2019-03-16',
 '2002-05-14',
 '1996-02-07',
 '2010-04-09',
 '2005-06-19',
 '1997-10-22',
 '2012-04-25',
 '1990-11-19',
 '2010-10-10',
 '1997-02-01',
 '1999-05-07',
 '2000-11-17',
 '2008-08-28',
 '2008-11-05',
 '2002-06-28',
 '2000-02-25',
 '1994-05-08',
 '1997-10-19',
 '2012-03-26',
 '1998-04-30',
 '2001-11-25',
 '1990-12-04',
 '2003-11-25',
 '2005-03-10',
 '2001-03-16',
 '2010-03-21',
 '1996-12-05',
 '2017-12-20',
 '2007-11-26',
 '1999-08-27',
 '1992-12-23',
 '1998-03-26',
 '2002-01-25',
 '2015-08-26',
 '2001-08-17',
 '1992-08-28',
 '2017-04-02',
 '2013-10-09',
 '2011-05-28',
 '1995-11-24',
 '2014-10-20',
 '2014-07-20',
 '2010-03-16',
 '2000-05-23',
 '2020-03-30',
 '2014-09-05',
 '2020-06-22',
 '2008-01-29',
 '1991-04-15',
 '2019-07-20',
 '1998-01-06',
 '2014-02-03',
 '2006-03-17',
 '1991-03-23',
 '2013-05-03',
 '2009-02-03',
 '1993-12-17',
 '2016-12-29',
 '1991-08-17',
 '2007-04-28',
 '2014-12-07',
 '2011-08-07',
 '2000-01-09',
 '2010-07-14',
 '2009-02-23',
 '2007-05-12',
 '1993-04-27',
 '1998-01-09',
 '1990-10-30',
 '1992-01-23',
 '2012-08-20',
 '1994-09-20',
 '2009-12-12',
 '2016-02-23',
 '2002-10-27',
 '1996-11-30',
 '2001-07-05',
 '2005-11-06',
 '2016-05-28',
 '2000-09-18',
 '1999-01-04',
 '2004-07-01',
 '1993-04-15',
 '1998-08-06',
 '2018-11-25',
 '2018-11-21',
 '2001-02-03',
 '2007-10-28',
 '1996-02-08',
 '2000-04-21',
 '1997-05-22',
 '2017-10-30',
 '2013-07-02',
 '2002-12-11',
 '1996-07-05',
 '2011-02-02',
 '2001-01-08',
 '1997-06-28',
 '2006-09-23',
 '2002-11-12',
 '2017-03-06',
 '1990-11-04',
 '2008-08-27',
 '1995-05-01',
 '2006-10-04',
 '2004-03-25',
 '2012-11-19',
 '2009-12-18',
 '2017-12-11',
 '1995-05-17',
 '2014-05-17',
 '2000-06-06',
 '2017-04-23',
 '2004-08-26',
 '1993-08-20',
 '2012-12-03',
 '2003-04-26',
 '2012-06-19',
 '2004-01-18',
 '2020-08-29',
 '2005-06-13',
 '2005-12-25',
 '2016-01-05',
 '1994-06-08',
 '2011-03-12',
 '2008-12-09',
 '2015-10-31',
 '2013-02-23',
 '2016-07-04',
 '2013-01-06',
 '2000-10-18',
 '2020-05-21',
 '2016-01-06',
 '2008-10-14',
 '2007-05-15',
 '1998-01-21',
 '2007-05-01',
 '2018-08-24',
 '2006-11-07',
 '2009-05-13',
 '2020-01-26',
 '2005-02-13',
 '2006-03-06',
 '2017-04-18',
 '1994-11-30',
 '2003-03-28',
 '2012-09-24',
 '2008-02-01',
 '2009-09-15',
 '1993-12-01',
 '2011-10-31',
 '1990-02-25',
 '2010-04-15',
 '2005-06-21',
 '2005-08-03',
 '2005-08-18',
 '2015-09-02',
 '2008-02-11',
 '2014-08-01',
 '2002-10-23',
 '2006-02-02',
 '2009-03-21',
 '1993-03-12',
 '2006-10-15',
 '2005-10-29',
 '2013-10-17',
 '2017-07-17',
 '2003-12-05',
 '2018-02-27',
 '1996-01-21',
 '1998-04-04',
 '2006-04-14',
 '2019-03-12',
 '2010-01-19',
 '2020-06-28',
 '2008-02-13',
 '2003-05-26',
 '1999-02-05',
 '2015-12-22',
 '1996-12-29',
 '2011-08-21',
 '2012-07-29',
 '2006-08-19',
 '2000-03-09',
 '2013-08-29',
 '1997-06-12',
 '2006-02-01',
 '2007-12-28',
 '2001-09-16',
 '1996-02-16',
 '1991-12-24',
 '1991-06-30',
 '2018-09-01',
 '2014-12-05',
 '2006-02-28',
 '2003-02-03',
 '2014-11-07',
 '1998-03-29',
 '2006-01-16',
 '2018-10-05',
 '2005-07-26',
 '2015-04-09',
 '2018-06-26',
 '2018-09-27',
 '1991-01-11',
 '2018-09-09',
 '2004-03-04',
 '2005-07-31',
 '1995-04-26',
 '2017-07-10',
 '2002-10-28',
 '2004-02-24',
 '2006-02-22',
 '2015-05-22',
 '2019-04-17',
 '2002-09-06',
 '2016-10-03',
 '2009-02-14',
 '2007-03-09',
 '1990-03-24',
 '2013-02-02',
 '1998-12-08',
 '2003-08-30',
 '2014-05-26',
 '2005-10-04',
 '1991-08-25',
 '2003-07-14',
 '1999-03-17',
 '2012-08-04',
 '1999-01-25',
 '2016-08-15',
 '1993-03-28',
 '2002-04-23',
 '2008-03-21',
 '2012-11-02',
 '1995-02-12',
 '2006-10-16',
 '2013-12-28',
 '2015-11-08',
 '2009-06-25',
 '2014-05-22',
 '2011-07-18',
 '1993-01-07',
 '2019-10-27',
 '1999-04-27',
 '1991-04-26',
 '1997-10-20',
 '2020-04-30',
 '2019-03-24',
 '2007-01-29',
 '2015-05-21',
 '1991-07-26',
 '1991-07-29',
 '2019-03-25',
 '2014-04-25',
 '1997-04-22',
 '2013-10-23',
 '1990-05-12',
 '2009-04-22',
 '2012-11-11',
 '2014-12-14',
 '2000-11-16',
 '1999-09-28',
 '2016-08-31',
 '2018-12-12',
 '1999-02-11',
 '2015-03-13',
 '2007-03-20',
 '2016-05-03',
 '2020-04-12',
 '2012-11-30',
 '1994-02-04',
 '2005-04-26',
 '1995-09-10',
 '1992-08-10',
 '2001-06-07',
 '2018-11-02',
 '2003-12-07',
 '1994-10-14',
 '1996-03-15',
 '1996-09-27',
 '1998-11-29',
 '2017-09-17',
 '2001-11-13',
 '1992-07-06',
 '2014-06-05',
 '2018-08-14',
 '1998-02-26',
 '1996-11-17',
 '1994-11-23',
 '2020-08-28',
 '2016-02-29',
 '2011-07-07',
 '2011-01-23',
 '2012-05-27',
 '1996-11-02',
 '2006-10-09',
 '2019-02-03',
 '2005-07-13',
 '2017-07-05',
 '1996-08-30',
 '1997-10-04',
 '2001-07-02',
 '1999-12-21',
 '1991-06-04',
 '1996-03-03',
 '1997-03-17',
 '2019-11-21',
 '1991-03-22',
 '1991-07-14',
 '1995-02-01',
 '2020-08-03',
 '1995-11-20',
 '2006-12-01',
 '2009-06-05',
 '2002-07-13',
 '1999-09-10',
 '2004-09-16',
 '2017-10-03',
 '2009-12-21',
 '2018-08-08',
 '2015-02-24',
 '2019-02-22',
 '2003-10-31',
 '1993-05-05',
 '2010-09-06',
 '2015-06-01',
 '2007-03-29',
 '2006-06-01',
 '2018-04-28',
 '1997-01-20',
 '2016-07-26',
 '1990-05-29',
 '1998-11-21',
 '1992-03-06',
 '2010-08-16',
 '1994-01-19',
 '2011-11-18',
 '1999-09-16',
 '2012-06-10',
 '2009-08-16',
 '1991-01-18',
 '2003-04-24',
 '2015-10-04',
 '1992-02-04',
 '2015-07-17',
 '2018-08-11',
 '2009-02-07',
 '2008-01-03',
 '2006-08-05',
 '1999-07-26',
 '2015-09-27',
 '1997-06-18',
 '2001-03-12',
 '2019-09-24',
 '2016-10-11',
 '2006-07-14',
 '2014-05-25',
 '1996-07-30',
 '2009-01-25',
 '2005-08-14',
 '1998-10-05',
 '2010-07-27',
 '2004-03-01',
 '1994-08-27',
 '2000-03-04',
 '1996-01-22',
 '1998-05-18',
 '2019-07-03',
 '2005-02-10',
 '2006-05-15',
 '2010-08-10',
 '2013-05-09',
 '2017-04-29',
 '1993-03-02',
 '2019-01-20',
 '2001-09-22',
 '2006-10-30',
 '1999-05-30',
 '1992-12-21',
 '2005-08-31',
 '2007-03-18',
 '1992-02-02',
 '1996-09-02',
 '2002-09-02',
 '1992-08-06',
 '1992-10-19',
 '2017-05-06',
 '1999-09-30',
 '1997-10-01',
 '1990-03-03',
 '2007-06-27',
 '1990-03-13',
 '2018-11-22',
 '1990-01-09',
 '1990-12-17',
 '2007-11-10',
 '2001-10-04',
 '1992-04-08',
 '1993-02-13',
 '1993-07-23',
 '2015-09-15',
 '2005-09-30',
 '2013-10-04',
 '2005-11-30',
 '2019-04-03',
 '1997-09-18',
 '2001-02-08',
 '1998-06-02',
 '2017-06-26',
 '2014-06-09',
 '1998-05-10',
 '2005-07-28',
 '2007-04-12',
 '2007-09-07',
 '1998-02-28',
 '1996-06-04',
 '2020-06-14',
 '1998-05-07',
 '2001-04-11',
 '2017-11-16',
 '1995-03-12',
 '2013-10-12',
 '2015-06-04',
 '1994-09-13',
 '2003-09-02',
 '1998-04-14',
 '1996-05-18',
 '2013-03-11',
 '2020-04-11',
 '2013-04-20',
 '2017-10-20',
 '2016-05-25',
 '2017-01-19',
 '1994-01-24',
 '1995-04-15',
 '2011-06-27',
 '1998-12-10',
 '2002-09-23',
 '2008-05-13',
 '2014-09-03',
 '1994-02-06',
 '2019-03-01',
 '2001-02-15',
 '2004-10-31',
 '1999-12-29',
 '2012-03-28',
 '2002-03-11',
 '2011-06-23',
 '2018-04-05',
 '1993-11-19',
 '2012-02-16',
 '2011-10-08',
 '2003-07-06',
 '2014-08-20',
 '2010-04-30',
 '1999-10-24',
 '1998-02-17',
 '2002-01-30',
 '2011-01-08',
 '2001-05-11',
 '2004-04-28',
 '2007-09-10',
 '1991-09-06',
 '2014-05-11',
 '2007-09-11',
 '2012-09-23',
 '2012-06-14',
 '2013-07-05',
 '1992-05-13',
 '1996-06-18',
 '2015-05-20',
 '2017-08-22',
 '2005-04-18',
 '2014-02-17',
 '1992-09-09',
 '2019-02-08',
 '2017-11-13',
 '2017-08-08',
 '1995-05-28',
 '1996-02-14',
 '1994-08-08',
 '2010-01-11',
 '2018-12-07',
 '2002-10-11',
 '2020-03-16',
 '2019-05-20',
 '2016-10-26',
 '2015-11-30',
 '2017-01-25',
 '2000-06-12',
 '2012-02-25',
 '2011-09-11',
 '2015-02-22',
 '2011-12-07',
 '2003-04-30',
 '1992-06-17',
 '2017-12-03',
 '1997-08-29',
 '1998-01-25',
 '2005-06-12',
 '1995-05-14',
 '1992-07-07',
 '1994-03-04',
 '2003-04-29',
 '1998-03-07',
 '2018-11-19',
 '1995-10-27',
 '2004-06-03',
 '2006-05-14',
 '2005-05-21',
 '2018-06-24',
 '1996-11-25',
 '1994-05-29',
 '1990-05-03',
 '2019-09-27',
 '2016-05-27',
 '1997-11-11',
 '2020-10-05',
 '2012-01-27',
 '1992-07-23',
 '2008-07-06',
 '1998-09-22',
 '1999-06-07',
 '2001-10-07',
 '2006-11-26',
 '2007-08-16',
 '2000-01-24',
 '1997-03-02',
 '1991-01-01',
 '1991-07-20',
 '2008-12-24',
 '1993-08-08',
 '1993-09-04',
 '1990-05-11',
 '1992-10-08',
 '2008-08-03',
 '2014-06-24',
 '1992-11-04',
 '2000-08-31',
 '2019-08-24',
 '2005-07-20',
 '2011-06-19',
 '1992-11-29',
 '1992-10-14',
 '1993-10-20',
 '2000-07-13',
 '2013-05-24',
 '1994-09-18',
 '2013-10-20',
 '2005-05-08',
 '2015-08-04',
 '1999-09-11',
 '2005-12-08',
 '2012-09-10',
 '2020-07-23',
 '1999-10-09',
 '2000-04-19',
 '1992-12-10',
 '2018-08-04',
 '2004-05-26',
 '2003-06-16',
 '2012-10-01',
 '1993-03-01',
 '2014-11-27',
 '2008-03-20',
 '1999-02-12',
 '1992-07-18',
 '2018-09-18',
 '2013-02-16',
 '2008-04-20',
 '2003-06-08',
 '1996-03-07',
 '2001-01-11',
 '1998-03-05',
 '1991-03-25',
 '1998-04-18',
 '2000-10-05',
 '1995-02-16',
 '1999-12-18',
 '1992-03-18',
 '2010-04-16',
 '2013-05-25',
 '2001-11-01',
 '1996-05-25',
 '2002-01-09',
 '2007-04-13',
 '1997-02-10',
 '1992-05-09',
 '2009-01-12',
 '2007-06-07',
 '2015-09-14',
 '2001-01-24',
 '2015-10-12',
 '2009-01-03',
 '1998-01-15',
 '2004-02-27',
 '2003-06-01',
 '2014-11-01',
 '2006-01-04',
 '2019-08-27',
 '2014-07-23',
 '1992-09-28',
 '2002-12-29',
 '2019-07-31',
 '1995-05-11',
 '1990-11-17',
 '2018-12-04',
 '2006-05-22',
 '2014-06-20',
 '2003-09-12',
 '2010-03-01',
 '1999-04-04',
 '2006-10-19',
 '1999-08-17',
 '2020-03-10',
 '2006-01-12',
 '1995-02-19',
 '2012-11-17',
 '2004-01-22',
 '1992-01-16',
 '2018-01-04',
 '1997-11-03',
 '2003-09-01',
 '1998-06-03',
 '1991-12-08',
 '2013-07-16',
 '2003-02-11',
 '2006-01-22',
 '2020-06-08',
 '2019-10-24',
 '2009-04-02',
 '1996-05-06',
 '2001-05-08',
 '2002-05-28',
 '1993-01-29',
 '1991-04-24',
 '1990-07-14',
 '2002-10-29',
 '2004-05-02',
 '1990-10-11',
 '1997-05-17',
 '1998-04-09',
 '2000-08-14',
 '2002-03-29',
 '1999-12-26',
 '2003-08-06',
 '2001-11-22',
 '2010-12-23',
 '2019-03-03',
 '1999-04-24',
 '1994-08-31',
 '2015-01-13',
 '2013-09-11',
 '2005-04-27',
 '2007-10-23',
 '2017-12-08',
 '2001-03-18',
 '2016-01-19',
 '1991-03-07',
 '1999-08-21',
 '1998-05-25',
 '2004-03-17',
 '2011-11-09',
 '2010-11-22',
 '1997-12-08',
 '2002-09-12',
 '2019-05-14',
 '2013-08-19',
 '1997-03-27',
 '2003-04-10',
 '2008-11-03',
 '2019-11-13',
 '2010-10-19',
 '1992-05-15',
 '2001-09-13',
 '2015-12-20',
 '2010-04-27',
 '2005-06-22',
 '2005-05-26',
 '2020-08-12',
 '2010-02-22',
 '2017-08-19',
 '2009-05-30',
 '2010-07-05',
 '1992-05-17',
 '2002-02-21',
 '2010-12-19',
 '2002-07-29',
 '2008-09-20',
 '2019-08-31',
 '2005-02-23',
 '2012-02-09',
 '2013-07-30',
 '1994-02-25',
 '2001-07-08',
 '1992-05-14',
 '1991-10-10',
 '2001-06-15',
 '2015-07-11',
 '2006-06-26',
 '2002-07-08',
 '2006-08-28',
 '1996-07-04',
 '2010-11-14',
 '2003-01-28',
 '1991-08-28',
 '2018-07-26',
 '1990-04-13',
 '2003-10-16',
 '1990-12-09',
 '1996-05-21',
 '2014-02-24',
 '2000-06-21',
 '2005-03-12',
 '2015-08-27',
 '2016-04-01',
 '2016-07-14',
 '2003-07-25',
 '2006-08-08',
 '2004-05-28',
 '1993-01-04',
 '1991-05-10',
 '2003-03-11',
 '2011-09-10',
 '1998-04-24',
 '1990-08-13',
 '2002-08-16',
 '1997-10-23',
 '2013-01-11',
 '1990-09-15',
 '2018-04-15',
 '1992-12-27',
 '2008-03-17',
 '2007-03-22',
 '1991-08-02',
 '2011-06-09',
 '2004-12-03',
 '1993-03-05',
 '2016-01-26',
 '2020-02-28',
 '1990-02-04',
 '2019-08-13',
 '2004-08-13',
 '2006-07-30',
 '1998-03-12',
 '1990-05-25',
 '2016-11-01',
 '2008-05-22',
 '2013-06-28',
 '1993-11-20',
 '2003-03-23',
 '1992-09-08',
 '2017-02-15',
 '2020-07-04',
 '2005-05-06',
 '2001-02-11',
 '2003-10-30',
 '1998-12-31',
 '2012-10-24',
 '1991-05-05',
 '2007-05-30',
 '2020-04-17',
 '2002-12-03',
 '2002-12-08',
 '2010-04-01',
 '2017-12-30',
 '2000-03-21',
 '1992-08-16',
 '2005-10-10',
 '1997-06-05',
 '2009-12-16',
 '2001-08-22',
 '2009-08-13',
 '2002-11-10',
 '1995-03-20',
 '1991-11-24',
 '2015-06-19',
 '2000-12-07',
 '2014-06-02',
 '1992-02-03',
 '2010-07-26',
 '1992-11-15',
 '1994-01-23',
 '2003-05-07',
 '1998-12-28',
 '1994-04-07',
 '1997-08-04',
 '2001-09-04',
 '2013-12-19',
 '2002-06-17',
 '2002-10-18',
 '2000-04-17',
 '2011-03-31',
 '2016-10-15',
 '2014-05-09',
 '2016-08-04',
 '2014-09-09',
 '1992-03-10',
 '2015-06-02',
 '2000-07-28',
 '1994-07-05',
 '2014-03-13',
 '2020-09-07',
 '2001-09-05',
 '2020-09-08',
 '1998-11-30',
 '2017-06-30',
 '2005-04-10',
 '1994-07-15',
 '2014-08-05',
 '2001-12-03',
 '1996-06-15',
 '2001-08-08',
 '2017-07-13',
 '2005-01-13',
 '2000-09-21',
 '2002-08-20',
 '2014-02-06',
 '1998-06-11',
 '2012-11-18',
 '2019-10-20',
 '1990-10-10',
 '2004-12-30',
 '1990-01-05',
 '2014-10-28',
 '2002-09-11',
 '2016-05-24',
 '2019-07-28',
 '2000-07-09',
 '2000-04-10',
 '2008-06-16',
 '2002-06-04',
 '1997-07-20',
 '2015-03-02',
 '2000-08-17',
 '2015-10-13',
 '2019-07-08',
 '2002-08-10',
 '2010-05-10',
 '1991-08-27',
 '1996-06-11',
 '2004-04-06',
 '1998-01-03',
 '1990-04-18',
 '2005-10-31',
 '2013-11-08',
 '2001-03-04',
 '1992-02-07',
 '1997-03-11',
 '1992-11-19',
 '2018-05-14',
 '2011-05-31',
 '1996-06-07',
 '2012-03-03',
 '2012-07-27',
 '1996-10-19',
 '2015-05-05',
 '2002-04-28',
 '2000-12-19',
 '2003-02-22',
 '1998-03-15',
 '2006-09-14',
 '2020-07-25',
 '1995-09-09',
 '1992-02-23',
 '2013-12-24',
 '1998-05-29',
 '2013-04-06',
 '2008-12-23',
 '2000-04-30',
 '2015-04-04',
 '2003-04-22',
 '1990-09-08',
 '2006-07-01',
 '1995-08-31',
 '2000-06-20',
 '1991-08-16',
 '2005-09-09',
 '2001-09-17',
 '2014-11-22',
 '2012-05-11',
 '2000-12-05',
 '2015-10-15',
 '1992-01-06',
 '2010-03-08',
 '2007-01-08',
 '1996-02-03',
 '2013-07-01',
 '2013-12-15',
 '2003-12-14',
 '1996-04-03',
 '2004-03-10',
 '2005-04-23',
 '2017-02-22',
 '2009-08-29',
 '2015-09-18',
 '1990-02-11',
 '2002-08-30',
 '2000-06-29',
 '1994-11-16',
 '1991-06-06',
 '2005-05-03',
 '2001-07-31',
 '2002-10-13',
 '2011-06-13',
 '2009-05-15',
 '2016-12-13',
 '2002-04-25',
 '2001-08-30',
 '1994-05-30',
 '1995-06-11',
 '1999-07-31',
 '1998-02-13',
 '1995-07-09',
 '1997-02-18',
 '2012-03-16',
 '1991-05-07',
 '2017-06-09',
 '2017-01-14',
 '2018-06-18',
 '2009-11-14',
 '2012-07-03',
 '2019-04-23',
 '2019-05-06',
 '2001-11-27',
 '1998-01-12',
 '1998-09-06',
 '2004-12-02',
 '1997-07-19',
 '2008-01-23',
 '2018-10-22',
 '2011-07-26',
 '1999-11-25',
 '2015-09-22',
 '2010-07-08',
 '2019-09-04',
 '2010-10-21',
 '1993-07-16',
 '2018-01-20',
 '1996-07-27',
 '2016-03-02',
 '2015-06-09',
 '2011-08-23',
 '1991-07-11',
 '1997-05-21',
 '2019-10-10',
 '1994-01-29',
 '1996-07-07',
 '2018-08-03',
 '2000-02-06',
 '2012-01-04',
 '1996-06-19',
 '1994-12-15',
 '1996-10-05',
 '2011-10-28',
 '1992-05-10',
 '2011-01-04',
 '2009-08-17',
 '1997-06-30',
 '2012-09-30',
 '2014-04-17',
 '2000-09-01',
 '2006-07-24',
 '2016-09-26',
 '2019-01-06',
 '1990-05-07',
 '1993-07-26',
 '2019-06-30',
 '1990-05-21',
 '2003-08-24',
 '2001-03-08',
 '2010-12-28',
 '2007-05-24',
 '1990-02-15',
 '1990-10-17',
 '2005-05-11',
 '1994-07-19',
 '1992-11-08',
 '1999-06-17',
 '2015-10-02',
 '1997-11-16',
 '2011-02-07',
 '2013-11-07',
 '2006-12-14',
 '2006-11-15',
 '1996-04-17',
 '2003-09-21',
 '2018-01-28',
 '2010-12-06',
 '2004-07-10',
 '2016-12-27',
 '1998-07-18',
 '2002-01-29',
 '1995-07-29',
 '2002-03-15',
 '2013-04-21',
 '2006-12-16',
 '2009-11-21',
 '2011-10-01',
 '2003-07-22',
 '1996-08-17',
 '1993-06-15',
 '2008-01-22',
 '2018-04-02',
 '2016-06-20',
 '2010-04-08',
 '2019-07-05',
 '1992-08-15',
 '1990-07-30',
 '2019-02-26',
 '1993-05-03',
 '1990-03-21',
 '2020-09-10',
 '2004-07-11',
 '2016-09-04',
 '2019-11-24',
 '1997-12-22',
 '2015-12-17',
 '2018-01-25',
 '2005-06-07',
 '2013-12-26',
 '2014-08-22',
 '2020-03-29',
 '2003-09-09',
 '2006-01-05',
 '1995-05-24',
 '2007-12-21',
 '2007-04-08',
 '1998-12-23',
 '2016-12-26',
 '2013-03-02',
 '2013-08-18',
 '2013-03-16',
 '2003-03-10',
 '1994-03-19',
 '1991-10-13',
 '1995-05-13',
 '2019-12-14',
 '2005-12-14',
 '1995-06-01',
 '2000-05-16',
 '2019-09-16',
 '1990-05-18',
 '2003-06-22',
 '2012-08-31',
 '1994-02-13',
 '2005-12-20',
 '2013-05-30',
 '2018-06-04',
 '2011-12-24',
 '2014-09-18',
 '1993-07-20',
 '2003-04-03',
 '2004-10-20',
 '1997-07-26',
 '1999-12-27',
 '1992-04-02',
 '2011-09-02',
 '1999-05-09',
 '2002-03-06',
 '1996-04-26',
 '2005-11-09',
 '1991-08-30',
 '2017-10-24',
 '2019-06-29',
 '1997-09-01',
 '1996-03-04',
 '2015-12-05',
 '2017-06-18',
 '2001-11-08',
 '2013-01-31',
 '2000-10-12',
 '2007-01-12',
 '2001-10-15',
 '2019-01-10',
 '2008-09-05',
 '2015-07-29',
 '2017-12-28',
 '2003-02-19',
 '2000-09-14',
 '2011-02-19',
 '2005-02-19',
 '2018-12-13',
 '2012-09-06',
 '1992-04-07',
 '2010-10-23',
 '2005-06-08',
 '2016-01-23',
 '2001-06-29',
 '2011-12-06',
 '1993-05-08',
 '2019-07-16',
 '2009-03-13',
 '2006-06-08',
 '1998-03-28',
 '2012-03-05',
 '1998-06-20',
 '2010-04-19',
 '2002-07-31',
 '1992-02-22',
 '2013-04-15',
 '1991-01-10',
 '2014-06-15',
 '2017-05-01',
 '1993-08-06',
 '1997-02-12',
 '2004-09-06',
 '2004-03-20',
 '2002-03-20',
 '1991-10-02',
 '1996-06-29',
 '2006-07-22',
 '2002-10-20',
 '2004-11-02',
 '2014-09-08',
 '2016-03-11',
 '1993-11-06',
 '2007-06-02',
 '1991-03-21',
 '2012-12-18',
 '1996-03-14',
 '2000-05-27',
 '1994-07-28',
 '2018-03-12',
 '2009-09-06',
 '2013-06-30',
 '1997-07-24',
 '1990-01-14',
 '2013-10-21',
 '2006-02-04',
 '2009-05-04',
 '1990-05-06',
 '2016-03-27',
 '1991-09-18',
 '1996-07-01',
 '2010-03-11',
 '1992-09-26',
 '2005-02-21',
 '2017-05-02',
 '2009-07-21',
 '1994-08-24',
 '2007-11-18',
 '1999-12-04',
 '2009-09-11',
 '1995-07-05',
 '2010-05-28',
 '2020-01-16',
 '2015-10-14',
 '1996-04-27',
 '2014-08-29',
 '1999-10-16',
 '2002-07-30',
 '2008-12-16',
 '2007-11-17',
 '2014-12-26',
 '1999-06-25',
 '2009-05-08',
 '2005-08-15',
 '2019-01-17',
 '1997-04-21',
 '1995-11-02',
 '2014-07-13',
 '1992-11-14',
 '1997-08-20',
 '2001-11-10',
 '2012-09-14',
 '2019-08-05',
 '1994-12-02',
 '1992-11-21',
 '1993-12-23',
 '2006-09-05',
 '2019-12-18',
 '2003-05-19',
 '2000-06-17',
 '2015-03-15',
 '2004-08-31',
 '2013-12-29',
 '2004-01-28',
 '2011-09-18',
 '1990-03-18',
 '2006-09-26',
 '2002-06-05',
 '2020-02-25',
 '2004-01-23',
 '1994-11-26',
 '1999-12-07',
 '1995-01-20',
 '1992-04-06',
 '2010-06-14',
 '2000-01-26',
 '2007-12-29',
 '1993-12-16',
 '2018-04-04',
 '2008-05-24',
 '2008-04-27',
 '2005-08-28',
 '2009-07-05',
 '1993-10-09',
 '1990-10-13',
 '2013-11-02',
 '2010-10-03',
 '2007-06-16',
 '2012-10-05',
 '2017-09-03',
 '2000-12-11',
 '2012-04-11',
 '2005-05-18',
 '2007-10-19',
 '2000-04-22',
 '1998-09-23',
 '2017-12-29',
 '1991-12-05',
 '2002-01-07',
 '2009-08-11',
 '2014-04-15',
 '2018-07-24',
 '2004-09-29',
 '2011-09-09',
 '2006-04-17',
 '2011-10-15',
 '2015-09-21',
 '1999-01-05',
 '2011-05-13',
 '2004-03-19',
 '1995-10-31',
 '2003-01-05',
 '2013-01-30',
 '1990-12-01',
 '2011-09-25',
 '2016-10-05',
 '1990-08-23',
 '2015-12-08',
 '2009-09-09',
 '2009-07-30',
 '2001-07-03',
 '2013-08-07',
 '2014-06-18',
 '2013-08-20',
 '2017-11-09',
 '1997-05-03',
 '2007-02-09',
 '2010-12-20',
 '2018-10-10',
 '2005-06-11',
 '2007-08-29',
 '2014-04-14',
 '1998-04-11',
 '2007-05-05',
 '1992-08-07',
 '2019-01-11',
 '2011-04-22',
 '2011-07-10',
 '2011-03-28',
 '2018-07-02',
 '2015-01-18',
 '2017-12-07',
 '2018-08-27',
 '1995-01-13',
 '1997-03-21',
 '2016-05-22',
 '2013-07-29',
 '1996-11-20',
 '1992-02-05',
 '1996-04-22',
 '2003-06-15',
 '1991-09-02',
 '1996-03-31',
 '2002-04-15',
 '1993-12-02',
 '2011-03-29',
 '2002-02-13',
 '2004-10-11',
 '2012-02-05',
 '2007-10-21',
 '2007-11-20',
 '2013-11-03',
 '2006-05-23',
 '1997-08-16',
 '1998-02-16',
 '2009-06-24',
 '2003-03-06',
 '2009-01-19',
 '2005-12-22',
 '2015-12-21',
 '2017-05-29',
 '2010-03-20',
 '2002-10-25',
 '2012-09-05',
 '2003-09-30',
 '2004-03-09',
 '2020-07-22',
 '1995-09-29',
 '2014-10-03',
 '2012-03-01',
 '2013-12-08',
 '1992-06-27',
 '2011-10-13',
 '1997-03-16',
 '2020-08-02',
 '2011-10-22',
 '2012-07-10',
 '2015-09-06',
 '2015-10-01',
 '1998-09-30',
 '1999-07-03',
 '2000-10-02',
 '2019-12-02',
 '2018-03-31',
 '2010-02-05',
 '2017-02-24',
 '2011-03-05',
 '1999-12-09',
 '2002-04-13',
 '2010-01-10',
 '2007-08-27',
 '2004-12-15',
 '1994-08-09',
 '1999-11-27',
 '2000-03-13',
 '2013-04-26',
 '2013-12-12',
 '2004-12-22',
 '2004-12-25',
 '2020-05-28',
 '2017-07-23',
 '1999-12-17',
 '2016-01-18',
 '2011-12-04',
 '2015-07-22',
 '2011-07-15',
 '2011-04-16',
 '2005-12-01',
 '2004-05-06',
 '2015-06-07',
 '1990-11-29',
 '1999-05-15',
 '2001-01-31',
 '2000-08-19',
 '1999-04-25',
 '2008-09-11',
 '1992-03-20',
 '2014-07-08',
 '1996-12-07',
 '2012-01-21',
 '1994-12-13',
 '1996-05-20',
 '2001-03-19',
 '1992-02-12',
 '1994-10-15',
 '2007-12-17',
 '2011-12-23',
 '2010-03-02',
 '2011-03-02',
 '2007-03-01',
 '2003-05-09',
 '1996-07-16',
 '2013-01-25',
 '1997-05-07',
 '1999-08-16',
 '1994-02-15',
 '2012-05-29',
 '2013-09-10',
 '2019-07-06',
 '2009-11-16',
 '1993-12-05',
 '2004-01-03',
 '2009-03-20',
 '2011-03-13',
 '2008-07-22',
 '2004-06-20',
 '2009-04-14',
 '2013-07-11',
 '1999-04-08',
 '1997-05-24',
 '2004-02-21',
 '1997-10-08',
 '1990-10-03',
 '1999-04-21',
 '2012-11-26',
 '2011-05-03',
 '2020-06-09',
 '1990-05-30',
 '1994-10-28',
 '1994-12-14',
 '2017-01-17',
 '1992-08-26',
 '2007-05-27',
 '2019-12-25',
 '1997-06-02',
 '1992-04-01',
 '2007-10-03',
 '2017-01-03',
 '2005-09-26',
 '2017-02-27',
 '1991-09-25',
 '2018-08-15',
 '2005-11-23',
 '1995-04-19',
 '2012-06-04',
 '2005-08-16',
 '2004-11-03',
 '1996-11-28',
 '2006-01-25',
 '2017-10-29',
 '1993-04-16',
 '2005-07-22',
 '2010-10-12',
 '2001-10-20',
 '2020-07-02',
 '2015-04-18',
 '2004-10-04',
 '2000-08-22',
 '2015-07-05',
 '2003-11-16',
 '1996-05-19',
 '2004-02-20',
 '2006-03-16',
 '2014-05-31',
 '1992-07-08',
 '2011-06-04',
 '2007-08-23',
 '2004-06-01',
 '2013-03-05',
 '1995-09-07',
 '2012-08-13',
 '2019-01-18',
 '2017-10-31',
 '2015-02-10',
 '2014-08-03',
 '2002-06-23',
 '2008-11-14',
 '1998-05-27',
 '2007-02-05',
 '2015-03-22',
 '2019-12-29',
 '1999-01-27',
 '1993-04-29',
 '1995-03-07',
 '2009-01-07',
 '2008-04-10',
 '2012-06-28',
 '2003-03-31',
 '2016-02-22',
 '2019-08-17',
 '1999-03-20',
 '2008-02-27',
 '1993-03-17',
 '2016-10-19',
 '2013-04-24',
 '2017-10-02',
 '2009-11-19',
 '1996-10-04',
 '2005-09-11',
 '1996-09-30',
 '2016-10-24',
 '2016-01-17',
 '2003-06-20',
 '1999-03-15',
 '2013-02-01',
 '1991-08-19',
 '2011-11-25',
 '1997-08-27',
 '2005-07-08',
 '2008-03-01',
 '2008-08-20',
 '1997-05-26',
 '2003-09-17',
 '1994-11-14',
 '2008-09-27',
 '2007-08-09',
 '1995-09-06',
 '2005-08-05',
 '2018-04-27',
 '2000-03-10',
 '1997-06-17',
 '2000-03-05',
 '2010-08-24',
 '2013-09-23',
 '1994-02-16',
 '2001-10-12',
 '2005-03-06',
 '2016-10-27',
 '2001-03-06',
 '2008-02-21',
 '1995-08-23',
 '1992-07-05',
 '2014-05-30',
 '1997-01-17',
 '1999-04-07',
 '2017-05-10',
 '2012-07-28',
 '2018-11-26',
 '2009-03-26',
 '2013-02-22',
 '1995-07-11',
 '1997-09-02',
 '2017-10-17',
 '2010-03-31',
 '2018-10-12',
 '2004-04-04',
 '1990-04-06',
 '2008-02-12',
 '2011-11-19',
 '2017-05-23',
 '2011-03-10',
 '2019-09-12',
 '2020-04-13',
 '2002-10-03',
 '2014-07-01',
 '2006-09-03',
 '1994-05-20',
 '1995-04-18',
 '2014-06-07',
 '2000-07-22',
 '1998-08-16',
 '2002-06-02',
 '1996-01-08',
 '1999-03-05',
 '1994-04-17',
 '1992-12-28',
 '1992-12-06',
 '2015-03-30',
 '2015-11-09',
 '1993-10-31',
 '1992-10-12',
 '2012-03-24',
 '1993-05-06',
 '2008-04-14',
 '1994-01-12',
 '1991-05-03',
 '2010-02-12',
 '1995-02-27',
 '2014-01-09',
 '2006-02-23',
 '2003-04-23',
 '1993-03-09',
 '1996-10-20',
 '1996-03-23',
 '1995-03-15',
 '1992-12-12',
 '1990-06-29',
 '2017-09-23',
 '1990-03-06',
 '2019-12-16',
 '2016-12-24',
 '2020-03-04',
 '1999-06-06',
 '2008-12-19',
 '1995-07-24',
 '2012-02-26',
 '1998-01-22',
 '1997-08-28',
 '2007-09-16',
 '1999-09-12',
 '2003-05-03',
 '2001-12-24',
 '2006-09-24',
 '2004-05-10',
 '1990-08-19',
 '1995-10-01',
 '2008-11-21',
 '2008-03-07',
 '2017-04-25',
 '2000-10-27',
 '2001-02-22',
 '2003-08-11',
 '2016-05-16',
 '2020-05-24',
 '2004-02-05',
 '2009-02-27',
 '2001-04-10',
 '1997-06-03',
 '2003-06-19',
 '2004-04-07',
 '2018-12-19',
 '2003-08-26',
 '1995-08-07',
 '2000-08-23',
 '1991-09-01',
 '1999-03-30',
 '2013-12-20',
 '1995-09-12',
 '2006-08-30',
 '2007-08-13',
 '2011-11-24',
 '2005-11-17',
 '1991-11-04',
 '2005-04-14',
 '1998-08-11',
 '1999-03-03',
 '2016-06-28',
 '2008-09-19',
 '2007-02-25',
 '2019-08-28',
 '2011-02-01',
 '2000-12-21',
 '2003-06-17',
 '1992-01-12',
 '2016-05-23',
 '1997-07-09',
 '1998-09-09',
 '1993-10-17',
 '2016-11-17',
 '1994-04-22',
 '2020-03-12',
 '2005-04-13',
 '1990-07-16',
 '2003-12-06',
 '2017-05-30',
 '1999-06-24',
 '2019-11-22',
 '2002-09-16',
 '2007-01-25',
 '2013-11-12',
 '2002-03-09',
 '2012-01-19',
 '1991-10-08',
 '2002-01-05',
 '1992-12-26',
 '2018-12-22',
 '2011-05-11',
 '2008-12-29',
 '2008-01-06',
 '2019-08-14',
 '1995-10-19',
 '2013-12-14',
 '2000-06-08',
 '1990-04-12',
 '2005-06-20',
 '2013-09-04',
 '1999-04-29',
 '2003-12-30',
 '2012-05-13',
 '2013-10-13',
 '1998-05-21',
 '2015-12-26',
 '2008-01-16',
 '2008-06-02',
 '1995-02-11',
 '2000-11-01',
 '2000-04-28',
 '1995-10-12',
 '2004-03-08',
 '1992-03-09',
 '2014-07-21',
 '1994-12-03',
 '2016-12-14',
 '2002-01-16',
 '2003-03-05',
 '1999-11-15',
 '2019-06-10',
 '2009-07-18',
 '1992-04-05',
 '2020-05-27',
 '2010-07-10',
 '1994-09-07',
 '2009-04-23',
 '1999-05-31',
 '2014-12-22',
 '2007-03-13',
 '2015-11-21',
 '2018-03-14',
 '2000-10-11',
 '2013-07-20',
 '1993-03-16',
 '1990-03-08',
 '2019-03-27',
 '1998-10-21',
 '2015-06-12',
 '2007-07-15',
 '2018-02-06',
 '2016-08-01',
 '1990-01-17',
 '2008-06-08',
 '2016-09-01',
 '2000-03-27',
 '2009-09-19',
 '2007-09-29',
 '2018-01-08',
 '2001-05-09',
 '2014-01-08',
 '2011-09-29',
 '1996-03-24',
 '1999-03-04',
 '2005-10-25',
 '2003-02-15',
 '2014-10-06',
 '2005-09-25',
 '1993-07-27',
 '2006-09-25',
 '1997-09-10',
 '2004-07-02',
 '1998-06-25',
 '2019-11-02',
 '2013-11-06',
 '2015-03-18',
 '1998-08-14',
 '1996-05-14',
 '1994-10-23',
 '2003-05-15',
 '2018-04-24',
 '1999-05-28',
 '1992-07-31',
 '2006-09-22',
 '2008-09-18',
 '2002-06-30',
 '1998-11-14',
 '2000-07-18',
 '1991-03-18',
 '1996-10-01',
 '2009-06-30',
 '1998-02-09',
 '2017-05-28',
 '2002-03-22',
 '1997-12-29',
 '2001-03-22',
 '2008-04-16',
 '1993-05-18',
 '1995-09-01',
 '2004-06-06',
 '2019-08-12',
 '1990-06-19',
 '2011-10-09',
 '1994-10-22',
 '2018-01-21',
 '2018-12-11',
 '2018-12-02',
 '2016-09-09',
 '2000-09-28',
 '2001-03-29',
 '1998-02-01',
 '2008-02-08',
 '1997-04-13',
 '2010-06-23',
 '2001-03-25',
 '2002-12-18',
 '2018-12-08',
 '1995-03-08',
 '2000-09-04',
 '1994-05-27',
 '2003-10-09',
 '2002-07-21',
 '2003-08-14',
 '2009-04-28',
 '2005-03-19',
 '2013-10-22',
 '2005-08-21',
 '2004-03-18',
 '2003-07-13',
 '1992-10-28',
 '2010-01-04',
 '1992-11-01',
 '2000-01-01',
 '2002-11-22',
 '1991-05-06',
 '2013-05-16',
 '2007-11-01',
 '1994-08-17',
 '2008-01-11',
 '2012-03-11',
 '2010-02-04',
 '2012-08-26',
 '2008-08-25',
 '2000-09-13',
 '1993-04-01',
 '2000-08-01',
 '1995-12-20',
 '1997-10-16',
 '1994-07-22',
 '2012-01-30',
 '2013-12-17',
 '2018-01-18',
 '1996-02-25',
 '2008-07-23',
 '1992-09-30',
 '2011-06-15',
 '1995-01-31',
 '1996-07-23',
 '2014-10-09',
 '2014-07-31',
 '2014-01-24',
 '2010-03-15',
 '2010-06-10',
 '2001-07-20',
 '2011-03-30',
 '1999-02-21',
 '1995-02-26',
 '2006-11-04',
 '2006-03-05',
 '2013-03-18',
 '2019-11-10',
 '1993-10-05',
 '2010-06-19',
 '1996-10-06',
 '2007-10-20',
 '2020-03-13',
 '1991-04-16',
 '2010-10-08',
 '2020-01-20',
 '2003-05-25',
 '1996-07-02',
 '1997-01-07',
 '1991-07-17',
 '2015-03-08',
 '2003-09-27',
 '2016-08-10',
 '1993-09-22',
 '1990-01-20',
 '2014-06-11',
 '2018-07-15',
 '1990-02-02',
 '2020-04-20',
 '1990-02-14',
 '1998-11-06',
 '2002-09-28',
 '1994-07-25',
 '2007-02-23',
 '1993-06-26',
 '1991-05-15',
 '2002-02-27',
 '2001-08-04',
 '2020-02-27',
 '2010-05-23',
 '2016-07-17',
 '2012-03-30',
 '1992-05-02',
 '2020-10-10',
 '2008-12-21',
 '2011-06-06',
 '2006-02-03',
 '1993-04-11',
 '2009-09-10',
 '2016-07-07',
 '1996-04-11',
 '2012-12-04',
 '2001-06-09',
 '2004-12-10',
 '2010-04-14',
 '2001-06-11',
 '2006-12-22',
 '1998-07-27',
 '2006-11-22',
 '1995-01-23',
 '1998-05-01',
 '2006-03-04',
 '2006-04-01',
 '1999-05-11',
 '2010-09-05',
 '2004-02-07',
 '1999-02-10',
 '2020-04-16',
 '2006-09-12',
 '1992-06-25',
 '2015-09-28',
 '2003-08-08',
 '2012-10-13',
 '2020-03-07',
 '2018-11-01',
 '2000-09-10',
 '2007-06-28',
 '2012-04-07',
 '2017-12-10',
 '2001-01-30',
 '2003-07-16',
 '1999-09-06',
 '1994-08-01',
 '2018-08-16',
 '1993-07-28',
 '2001-05-22',
 '2015-09-08',
 '2008-08-16',
 '1998-12-14',
 '2018-03-05',
 '2020-09-24',
 '1991-06-05',
 '2009-10-01',
 '2000-10-06',
 '2014-03-25',
 '1999-10-13',
 '2007-10-09',
 '2006-06-30',
 '1999-10-06',
 '2014-02-27',
 '1996-05-15',
 '2010-10-14',
 '1994-06-14',
 '2013-11-19',
 '2001-02-19',
 '2020-08-01',
 '2015-05-10',
 '2012-07-14',
 '2013-09-12',
 '2010-12-01',
 '1990-10-20',
 '1997-04-09',
 '2019-08-11',
 '2006-03-18',
 '2007-09-09',
 '2004-08-04',
 '2015-01-11',
 '2014-08-27',
 '1996-05-31',
 '1995-09-30',
 '2020-07-29',
 '2016-03-06',
 '2016-04-29',
 '1990-12-23',
 '2000-03-12',
 '2000-08-09',
 '1998-10-28',
 '2012-12-21',
 '2018-11-08',
 '2014-07-18',
 '1998-06-18',
 '2006-09-07',
 '2013-02-04',
 '2019-04-11',
 '1993-11-12',
 '2014-12-30',
 '1999-02-24',
 '2018-02-17',
 '2004-08-24',
 '2010-06-12',
 '2020-02-17',
 '2010-09-30',
 '2014-01-28',
 '2000-02-18',
 '2015-10-08',
 '2019-02-21',
 '2019-11-08',
 '2011-02-17',
 '2006-06-20',
 '1995-03-17',
 '1999-08-05',
 '2000-12-02',
 '2003-09-16',
 '2011-11-14',
 '2009-05-28',
 '1993-05-31',
 '2000-11-11',
 '2008-10-07',
 '2016-08-14',
 '2010-05-05',
 '2019-05-11',
 '1997-03-03',
 '2017-01-24',
 '2015-09-16',
 '1999-07-05',
 '2016-05-19',
 '2003-08-20',
 '1995-06-22',
 '1996-02-26',
 '1999-09-25',
 '2019-11-04',
 '2019-08-29',
 '2015-10-28',
 '2016-11-23',
 '1990-09-27',
 '2008-03-12',
 '2003-06-07',
 '2019-09-02',
 '1999-01-23',
 '2019-02-23',
 '2001-08-03',
 '2002-05-11',
 '2007-07-03',
 '2020-04-18',
 '2020-03-08',
 '2019-06-17',
 '2016-09-07',
 '2016-04-10',
 '2017-04-10',
 '2000-01-08',
 '2010-01-18',
 '2014-04-11',
 '2011-01-14',
 '2004-10-16',
 '2007-08-14',
 '2012-08-08',
 '2020-05-04',
 '1991-09-22',
 '2004-11-28',
 '2013-04-11',
 '1992-02-19',
 '2017-08-15',
 '2009-08-19',
 '2001-07-17',
 '2010-01-06',
 '1995-04-01',
 '2010-02-20',
 '1993-07-19',
 '2014-02-26',
 '2005-09-29',
 '1997-06-20',
 '1997-04-29',
 '2016-12-19',
 '1990-09-29',
 '2007-03-25',
 '1999-08-12',
 '2014-09-06',
 '2011-05-24',
 '1999-07-16',
 '2008-01-14',
 '1993-06-02',
 '2009-11-12',
 '2002-08-29',
 '2006-01-19',
 '1992-09-06',
 '2017-10-22',
 '2008-07-10',
 '2007-12-13',
 '2019-08-30',
 '2014-02-16',
 '1998-12-07',
 '1990-01-29',
 '1992-05-16',
 '2008-02-14',
 '2004-07-08',
 '2007-06-06',
 '2016-11-08',
 '1997-03-08',
 '1990-08-21',
 '2009-09-01',
 '2018-03-30',
 '1990-01-28',
 '2009-11-23',
 '1999-11-30',
 '2008-12-18',
 '2012-07-25',
 '1997-01-04',
 '2008-12-07',
 '2013-12-01',
 '1997-03-09',
 '2004-04-18',
 '1991-08-15',
 '2006-04-29',
 '2016-12-23',
 '2014-12-08',
 '1995-04-24',
 '2015-05-11',
 '2019-03-31',
 '2005-05-20',
 '1992-10-07',
 '1990-11-06',
 '1999-11-11',
 '1999-10-25',
 '2007-04-01',
 '2016-11-07',
 '2006-08-03',
 '2000-02-21',
 '1996-01-30',
 '2019-08-20',
 '2002-03-26',
 '2000-11-15',
 '2018-05-23',
 '2001-05-27',
 '1995-06-23',
 '2001-04-09',
 '2005-11-04',
 '2011-08-04',
 '1991-02-01',
 '2009-11-15',
 '2015-05-03',
 '2001-06-17',
 '1997-11-01',
 '2001-03-05',
 '2002-04-20',
 '2014-07-22',
 '2009-07-24',
 '2011-02-27',
 '1998-10-14',
 '1998-10-27',
 '2017-02-21',
 '2006-12-20',
 '1993-11-15',
 '1999-10-21',
 '2003-04-14',
 '2018-06-20',
 '2007-05-07',
 '1999-06-16',
 '1993-11-18',
 '1991-08-24',
 '1999-03-07',
 '2004-12-18',
 '2016-12-18',
 '2000-05-15',
 '1991-08-26',
 '2010-06-20',
 '2008-10-12',
 '2011-06-25',
 '1990-07-26',
 '1993-11-09',
 '2002-04-11',
 '2009-09-07',
 '2009-05-17',
 '2002-08-26',
 '2008-05-19',
 '2010-06-24',
 '2019-01-29',
 '1990-09-26',
 '1997-01-08',
 '1999-02-25',
 '1998-09-03',
 '1997-01-05',
 '1990-09-01',
 '1993-11-11',
 '1999-06-11',
 '1997-06-26',
 '1999-06-08',
 '2018-11-04',
 '1997-08-06',
 '2002-09-15',
 '2005-03-14',
 '2010-04-12',
 '2010-06-06',
 '1991-12-10',
 '1994-12-17',
 '1993-04-26',
 '2017-06-17',
 '2002-09-01',
 '1992-04-23',
 '2011-04-26',
 '1995-11-05',
 '2016-10-01',
 '2003-12-08',
 '2005-04-29',
 '1994-10-05',
 '2010-09-15',
 '1994-12-24',
 '2013-12-27',
 '1998-01-10',
 '2005-07-15',
 '1997-07-23',
 '2009-12-13',
 '2001-03-07',
 '1997-03-01',
 '2015-04-05',
 '2019-06-20',
 '2014-12-06',
 '2001-05-02',
 '2010-06-04',
 '2014-12-21',
 '2002-11-27',
 '1995-03-01',
 '1998-11-01',
 '2016-12-15',
 '2015-09-07',
 '1994-12-25',
 '2007-09-01',
 '1996-12-15',
 '1994-02-23',
 '2014-02-08',
 '2016-05-21',
 '2019-12-27',
 '2005-09-27',
 '2013-04-05',
 '2001-07-19',
 '1992-07-03',
 '2015-04-08',
 '2017-10-13',
 '2017-12-31',
 '2002-12-13',
 '2015-05-24',
 '2003-08-15',
 '2017-11-12',
 '2007-03-26',
 '2002-06-25',
 '1990-05-02',
 '2010-06-05',
 '1992-04-12',
 '2013-08-02',
 '1994-09-16',
 '2018-01-22',
 '2018-09-28',
 '2004-04-14',
 '2007-07-04',
 '1996-03-25',
 '1995-04-20',
 '1993-08-01',
 '2004-10-06',
 '1999-12-13',
 '2009-06-02',
 '2011-02-22',
 '2013-01-05',
 '2001-05-05',
 '1994-03-15',
 '2013-02-05',
 '2003-08-02',
 '1998-02-24',
 '1994-11-29',
 '2009-06-14',
 '1998-08-01',
 '2000-05-01',
 '2005-08-29',
 '2011-02-20',
 '2007-05-10',
 '2001-01-29',
 '1993-09-03',
 '2013-12-05',
 '2009-03-19',
 '1997-04-07',
 '2007-03-16',
 '2013-01-01',
 '2013-09-17',
 '1995-04-07',
 '2013-12-23',
 '2009-09-03',
 '1990-06-18',
 '2011-12-18',
 '2016-01-09',
 '2009-07-07',
 '1996-08-31',
 '2000-08-24',
 '1990-11-28',
 '2011-11-05',
 '2011-02-06',
 '2000-05-05',
 '2001-11-02',
 '2005-01-14',
 '2019-08-08',
 '1993-10-15',
 '1991-09-07',
 '2005-05-05',
 '2001-11-23',
 '2011-12-16',
 '2000-12-09',
 '1995-01-12',
 '2002-11-28',
 '2019-02-09',
 '2020-07-26',
 '2007-10-11',
 '2005-07-16',
 '1994-11-11',
 '2003-03-27',
 '2001-01-20',
 '1992-03-17',
 '1994-09-23',
 '2014-07-17',
 '2010-06-15',
 '1996-04-25',
 '2001-01-09',
 '1993-03-27',
 '2010-05-25',
 '2018-11-18',
 '1998-06-05',
 '2016-04-21',
 '1996-02-02',
 '2012-09-17',
 '2015-06-03',
 '2015-02-01',
 '1998-11-17',
 '2007-01-07',
 '2018-11-15',
 '2002-01-13',
 '2001-03-13',
 '2005-06-28',
 '2003-04-04',
 '2007-02-11',
 '1994-03-09',
 '2012-04-09',
 '2012-07-11',
 '1990-04-21',
 '2008-06-21',
 '1996-04-24',
 '1995-07-15',
 '2010-06-30',
 '2002-07-20',
 '2010-09-20',
 '1996-11-08',
 '2009-03-25',
 '1997-02-20',
 '2019-08-26',
 '2008-04-29',
 '2017-03-30',
 '2014-04-30',
 '2003-05-31',
 '1991-01-14',
 '2011-06-28',
 '2014-10-04',
 '2019-01-22',
 '2017-12-16',
 '2002-06-12',
 '1996-05-28',
 '1999-08-01',
 '2013-03-14',
 '2006-05-27',
 '2015-12-15',
 '2005-04-21',
 '2018-09-05',
 '2000-03-31',
 '2002-06-14',
 '1994-06-21',
 '2003-12-31',
 '1991-04-28',
 '2017-10-05',
 '1997-06-25',
 '2006-07-17',
 '2016-02-07',
 '1998-08-31',
 '1994-03-22',
 '2016-05-12',
 '2009-03-03',
 '1994-08-19',
 '2019-07-25',
 '2018-10-01',
 '2001-04-19',
 '2009-10-04',
 '1990-04-15',
 '1994-06-01',
 '2002-01-08',
 '2008-06-05',
 '1997-07-07',
 '2016-05-07',
 '2002-04-07',
 '2008-08-01',
 '2000-06-22',
 '2011-01-26',
 '2017-11-01',
 '2009-06-20',
 '2012-08-29',
 '2013-10-08',
 '1996-04-06',
 '1999-12-24',
 '2010-09-03',
 '1996-05-27',
 '1999-03-21',
 '2008-08-21',
 '2018-09-11',
 '2003-09-10',
 '1990-09-11',
 '2005-02-09',
 '2007-08-31',
 '1997-08-11',
 '2008-10-20',
 '2002-08-07',
 '2006-03-21',
 '2018-08-22',
 '2004-01-08',
 '2015-07-15',
 '2012-10-11',
 '2018-10-15',
 '1998-04-17',
 '1997-02-21',
 '2010-01-15',
 '2017-07-12',
 '2012-06-20',
 '2008-01-19',
 '2004-10-21',
 '1995-10-25',
 '2010-06-03',
 '2014-06-25',
 '2002-06-11',
 '1990-03-02',
 '2002-12-15',
 '1997-12-14',
 '2007-11-11',
 '2014-03-31',
 '2004-03-05',
 '2013-08-12',
 '1993-03-14',
 '2020-06-19',
 '2002-04-10',
 '2005-12-15',
 '1995-08-08',
 '1994-09-28',
 '2007-01-30',
 '2016-08-25',
 '1993-12-15',
 '2017-03-17',
 '2015-01-15',
 '2002-11-02',
 '1995-12-22',
 '1996-11-05',
 '2013-06-26',
 '1991-12-13',
 '2017-01-10',
 '1995-02-25',
 '2011-06-10',
 '1994-11-08',
 '2004-04-17',
 '2003-05-01',
 '2016-09-19',
 '2019-07-26',
 '1993-01-06',
 '2010-01-13',
 '2017-11-05',
 '2004-08-10',
 '2007-05-20',
 '2018-12-06',
 '2005-06-25',
 '2002-04-12',
 '2008-03-27',
 '2008-05-17',
 '2008-09-13',
 '2003-02-05',
 '2011-10-30',
 '2009-02-18',
 '2007-10-14',
 '1992-12-03',
 '2008-12-13',
 '2004-12-14',
 '2016-12-17',
 '2019-09-14',
 '2007-12-15',
 '2003-04-09',
 '2006-03-15',
 '2019-12-31',
 '1996-01-26',
 '2019-12-22',
 '1998-05-15',
 '2018-02-24',
 '2015-01-01',
 '2005-02-06',
 '2008-09-14',
 '2008-01-05',
 '1900-01-01',
 '2015-12-06',
 '2005-08-02',
 '2002-04-18',
 '2010-11-26',
 '1998-07-29',
 '1993-04-05',
 '1991-12-01',
 '2009-05-24',
 '2019-06-13',
 '2007-02-22',
 '2004-09-09',
 '1995-11-17',
 '2018-05-12',
 '2005-12-27',
 '2018-04-03',
 '1996-09-14',
 '1993-01-27',
 '2011-04-03',
 '2002-03-17',
 '1997-05-16',
 '2001-05-06',
 '1996-08-15',
 '2006-07-28',
 '1991-06-14',
 '1992-03-04',
 '1996-08-18',
 '2016-05-01',
 '2011-05-09',
 '2000-01-04',
 '1992-11-26',
 '2015-10-22',
 '2014-07-26',
 '2007-05-14',
 '2001-08-01',
 '2005-03-13',
 '1991-08-12',
 '1993-09-20',
 '2010-09-10',
 '2017-07-30',
 '2020-09-29',
 '1998-04-28',
 '1996-04-02',
 '2001-01-27',
 '2016-10-18',
 '2006-01-01',
 '2006-01-30',
 '2012-05-30',
 '1995-01-14',
 '1991-11-18',
 '2005-08-20',
 '2003-05-02',
 '1993-08-07',
 '2000-01-12',
 '1995-08-17',
 '1995-02-28',
 '2019-07-17',
 '1992-06-11',
 '1996-01-17',
 '2020-05-15',
 '2014-06-10',
 '2002-12-27',
 '2000-08-28',
 '2005-03-22',
 '2018-07-31',
 '2008-07-07',
 '1998-09-21',
 '2016-01-07',
 '1993-07-08',
 '2008-07-29',
 '1999-03-10',
 '1997-09-20',
 '2013-11-29',
 '2016-10-25',
 '1993-02-28',
 '1999-09-15',
 '2020-02-07',
 '2012-12-23',
 '2009-06-12',
 '2019-04-24',
 '2015-01-24',
 '2004-02-11',
 '1995-11-04',
 '1996-12-30',
 '2016-07-11',
 '2012-05-17',
 '1992-01-22',
 '2020-01-29',
 '2012-06-17',
 '1993-09-29',
 '2011-11-11',
 '2003-08-19',
 '2012-02-06',
 '2017-04-17',
 '1999-02-14',
 '2008-01-10',
 '2000-08-10',
 '1993-03-21',
 '2009-08-24',
 '2019-11-26',
 '1995-12-15',
 '1995-02-05',
 '2002-05-17',
 '1990-08-22',
 '2012-05-07',
 '2004-04-19',
 '2008-05-15',
 '2017-09-08',
 '2016-11-10',
 '1999-05-29',
 '2014-08-26',
 '2019-05-25',
 '2012-12-28',
 '1992-02-09',
 '1999-10-12',
 '2001-12-25',
 '2001-07-16',
 '1992-04-27',
 '2004-06-30',
 '2010-09-09',
 '2000-05-13',
 '1994-04-12',
 '2000-11-05',
 '2018-01-30',
 '2004-01-11',
 '1998-03-25',
 '2015-03-26',
 '1991-08-09',
 '1999-05-24',
 '2012-05-26',
 '1997-11-07',
 '2017-05-04',
 '2010-02-15',
 '1998-03-03',
 '2018-10-11',
 '2017-04-08',
 '1992-12-08',
 '1994-03-25',
 '2013-03-06',
 '2008-07-17',
 '2017-04-07',
 '2018-11-27',
 '2019-09-10',
 '2019-08-25',
 '1998-10-01',
 '1994-04-26',
 '2013-07-06',
 '1996-07-25',
 '2018-12-03',
 '1996-08-05',
 '2009-11-09',
 '1997-09-16',
 '1993-12-03',
 '2010-01-29',
 '1991-10-28',
 '2006-11-02',
 '2011-02-12',
 '2019-04-16',
 '1996-02-22',
 '1992-03-31',
 '1992-10-20',
 '2002-01-01',
 '2000-02-04',
 '2000-08-08',
 '1991-07-27',
 '2018-10-06',
 '1993-11-17',
 '2015-01-27',
 '2012-03-22',
 '2014-04-23',
 '1997-08-12',
 '1992-05-12',
 '1990-02-03',
 '1992-05-08',
 '2019-04-14',
 '1994-10-24',
 '1999-08-09',
 '2012-01-08',
 '2011-12-25',
 '1999-06-19',
 '2003-08-01',
 '2016-08-18',
 '2017-01-08',
 '2006-10-08',
 '2003-03-13',
 '2015-06-08',
 '2015-05-08',
 '2003-12-16',
 '2013-10-15',
 '2008-11-29',
 '2020-06-25',
 '2006-04-19',
 '2000-10-23',
 '2009-02-16',
 '1998-01-14',
 '2020-05-26',
 '1991-03-28',
 '2004-09-04',
 '2000-05-03',
 '2000-04-04',
 '2014-05-04',
 '2005-03-28',
 '2015-02-08',
 '1991-01-29',
 '2006-05-09',
 '1990-10-15',
 '2011-05-25',
 '2011-04-17',
 '2002-12-14',
 '1993-08-09',
 '1992-04-04',
 '1991-12-30',
 '2020-01-05',
 '2011-09-22',
 '2007-11-12',
 '2007-08-07',
 '2014-10-02',
 '2016-09-05',
 '2012-06-22',
 '1991-04-18',
 '2001-10-18',
 '2004-02-22',
 '1997-05-20',
 '2013-07-04',
 '2003-05-30',
 '2002-09-10',
 '1992-10-26',
 '2011-04-07',
 '2014-03-08',
 '2006-03-10',
 '2003-10-27',
 '2011-04-09',
 '1991-03-12',
 '2018-05-11',
 '2004-02-17',
 '1994-03-05',
 '2009-11-18',
 '1997-04-25',
 '2018-06-27',
 '2005-03-26',
 '1998-05-08',
 '2002-05-15',
 '1996-12-23',
 '2002-12-26',
 '2009-09-30',
 '2012-05-01',
 '2016-03-12',
 '2016-02-03',
 '2015-05-12',
 '2003-06-26',
 '1990-12-18',
 '1999-11-10',
 '2018-10-17',
 '2016-10-23',
 '2008-12-14',
 '2003-05-29',
 '2002-06-01',
 '2009-01-14',
 '2004-01-16',
 '1995-05-30',
 '2017-09-14',
 '2002-05-16',
 '2000-02-15',
 '2005-11-10',
 '1992-10-04',
 '1992-01-25',
 '1995-01-30',
 '1990-12-24',
 '1995-08-09',
 '2007-07-19',
 '2001-08-28',
 '1991-05-31',
 '2007-12-22',
 '1998-07-20',
 '1993-11-10',
 '2020-04-26',
 '2005-01-23',
 '2016-08-16',
 '1999-05-05',
 '2013-02-26',
 '2007-01-10',
 '1994-11-01',
 '2018-04-16',
 '1999-10-02',
 '1991-06-18',
 '2007-07-21',
 '2006-10-11',
 '1999-03-27',
 '2004-05-23',
 '1995-08-24',
 '2020-04-29',
 '2017-09-30',
 '2004-04-30',
 '2007-09-20',
 '1995-06-08',
 '1997-12-01',
 '1995-06-26',
 '2004-08-07',
 '1993-11-01',
 '2004-10-02',
 '2004-06-13',
 '2001-11-05',
 '1997-04-10',
 '1996-01-07',
 '2002-04-09',
 '2003-09-04',
 '2011-10-20',
 '2015-12-30',
 '2001-04-20',
 '2010-02-13',
 '1997-09-28',
 '2012-08-09',
 '2002-08-31',
 '2017-06-12',
 '1996-10-18',
 '1990-01-11',
 '2017-12-05',
 '1996-10-09',
 '2006-04-12',
 '1991-08-20',
 '2011-03-16',
 '1991-10-20',
 '2006-01-11',
 '2006-06-27',
 '2012-04-01',
 '2005-04-24',
 '2007-08-25',
 '1994-12-09',
 '2007-06-21',
 '1991-06-19',
 '2020-01-12',
 '1990-12-26',
 '2009-08-08',
 '2009-09-12',
 '2013-06-10',
 '2011-11-08',
 '1996-02-12',
 '1996-07-31',
 '2002-04-27',
 '1992-01-18',
 '2004-03-29',
 '2012-01-23',
 '2013-09-30',
 '2005-05-23',
 '2001-02-20',
 '1994-06-25',
 '2000-02-09',
 '1994-05-07',
 '1999-09-07',
 '1995-11-27',
 '2006-09-02',
 '2011-10-16',
 '2010-03-30',
 '2008-12-25',
 '2013-06-08',
 '2011-06-07',
 '2008-11-23',
 '2004-02-09',
 '2015-04-28',
 '1990-08-02',
 '2016-09-03',
 '2012-12-10',
 '2004-01-17',
 '2018-01-14',
 '2013-06-24',
 '1997-11-19',
 '1994-09-17',
 '1993-09-02',
 '1990-01-04',
 '1998-11-23',
 '1992-05-06',
 '2017-06-28',
 '1990-04-03',
 '2016-11-13',
 '2007-10-22',
 '1996-12-19',
 '1995-01-11',
 '2001-02-26',
 '2010-09-07',
 '2016-02-26',
 '2005-08-24',
 '1999-03-29',
 '2019-10-01',
 '1995-12-25',
 '2012-08-07',
 '2016-09-17',
 '2014-08-07',
 '1996-02-21',
 '2010-05-09',
 '2018-08-13',
 '2001-06-21',
 '2003-09-25',
 '2011-12-08',
 '1992-07-12',
 '2000-03-25',
 '1997-11-12',
 '2013-06-02',
 '1996-06-22',
 '2005-03-17',
 '2005-01-05',
 '2010-03-23',
 '2016-09-25',
 '1996-06-14',
 '2012-12-16',
 '2001-02-01',
 '1996-12-12',
 '2008-02-25',
 '2002-04-24',
 '1997-01-22',
 '2006-09-10',
 '1997-09-12',
 '2003-02-08',
 '1998-09-18',
 '2002-07-12',
 '1999-07-01',
 '2011-05-14',
 '2002-01-10',
 '2012-07-09',
 '2008-04-09',
 '2003-05-20',
 '1995-12-18',
 '1998-10-10',
 '1991-03-09',
 '2000-07-27',
 '1992-12-15',
 '1998-06-23',
 '1994-09-30',
 '2000-09-15',
 '1991-03-04',
 '1991-05-26',
 '2002-11-07',
 '2006-11-03',
 '2010-09-19',
 '2003-10-22',
 '1996-08-08',
 '2009-12-06',
 '2011-08-12',
 '1991-06-11',
 '2005-08-26',
 '2014-06-19',
 '1990-06-12',
 '2015-03-23',
 '1995-12-21',
 '2015-03-04',
 '2018-11-16',
 '2018-07-30',
 '2000-02-05',
 '2012-04-24',
 '2018-01-27',
 '2007-11-06',
 '1992-11-09',
 '1994-07-21',
 '2009-07-14',
 '2018-06-14',
 '2001-02-21',
 '2007-10-25',
 '2005-02-22',
 '2020-02-01',
 '2006-08-12',
 '1991-06-17',
 '1997-06-16',
 '2008-03-18',
 '2001-01-14',
 '2015-01-26',
 '1992-01-11',
 '1999-11-07',
 '2013-07-14',
 '2015-11-24',
 '2020-01-25',
 '2000-04-29',
 '2019-10-13',
 '2019-11-09',
 '1996-12-28',
 '2019-07-04',
 '2004-12-27',
 '2010-06-01',
 '2016-12-28',
 '2019-07-11',
 '2012-02-03',
 '1993-01-15',
 '2002-08-05',
 '2005-04-09',
 '1992-03-26',
 '1994-12-07',
 '2013-01-04',
 '1999-03-22',
 '2009-01-02',
 '1998-08-15',
 '1997-01-19',
 '2001-05-14',
 '1995-01-17',
 '1998-08-21',
 '2004-09-27',
 '2017-02-11',
 '1994-05-24',
 '2012-05-15',
 '2009-04-11',
 '2020-09-06',
 '2001-10-17',
 '2014-10-08',
 '2020-02-26',
 '2014-10-07',
 '2016-05-10',
 '2017-05-19',
 '1995-06-27',
 '2006-04-20',
 '1998-05-17',
 '2015-06-06',
 '2014-08-21',
 '2018-09-07',
 '1996-04-10',
 '1990-09-19',
 '2015-03-27',
 '2003-07-29',
 '2002-10-24',
 '2020-08-22',
 '2015-05-04',
 '1991-07-12',
 '1992-04-17',
 '1996-01-03',
 '2019-03-10',
 '1998-06-28',
 '2008-06-23',
 '1992-04-15',
 '1991-02-15',
 '1990-09-10',
 '1998-04-27',
 '1998-10-30',
 '2010-01-07',
 '2006-02-18',
 '2015-01-08',
 '2018-10-07',
 '1994-02-19',
 '1996-12-11',
 '2017-10-09',
 '1999-07-08',
 '1999-03-09',
 '2014-02-28',
 '2006-11-25',
 '2007-02-16',
 '2018-08-05',
 '1990-06-24',
 '1991-06-24',
 '2008-10-10',
 '1993-08-05',
 '1991-01-21',
 '2007-12-09',
 '2014-09-07',
 '1996-09-06',
 '2017-07-20',
 '2011-01-20',
 '1994-02-28',
 '2008-06-30',
 '2011-02-25',
 '2004-05-14',
 '1998-09-24',
 '1996-10-28',
 '2011-09-07',
 '1999-11-26',
 '1990-03-26',
 '1999-11-16',
 '2014-04-12',
 '2016-06-06',
 '2004-03-23',
 '2008-04-22',
 '1991-12-16',
 '2015-05-23',
 '2009-02-15',
 '1997-12-21',
 '2007-07-18',
 '2017-03-02',
 '2007-07-12',
 '2000-04-14',
 '1995-01-27',
 '1996-05-16',
 '2002-07-24',
 '2012-04-29',
 '1996-01-14',
 '2011-03-03',
 '1990-04-22',
 '2011-11-21',
 '2016-03-23',
 '1998-03-19',
 '1995-12-12',
 '1994-10-16',
 '2015-07-24',
 '1999-12-31',
 '2020-02-10',
 '2017-02-02',
 '1991-02-14',
 '2000-09-29',
 '2020-08-06',
 '1992-05-27',
 '2017-09-16',
 '2012-08-17',
 '1999-05-01',
 '2009-07-15',
 '2000-12-08',
 '2019-05-21',
 '2005-10-11',
 '2002-05-24',
 '2020-06-15',
 '1993-03-25',
 '1993-10-24',
 '2015-12-25',
 '2007-01-16',
 '1991-10-01',
 '1991-04-25',
 '2007-07-24',
 '2007-10-31',
 '2008-08-31',
 '2017-04-27',
 '2008-12-03',
 '2001-11-18',
 '1997-06-22',
 '2008-11-15',
 '2007-03-11',
 '2000-10-17',
 '2015-07-09',
 '1995-04-05',
 '1992-08-04',
 '2016-09-22',
 '2009-10-30',
 '2005-05-04',
 '1998-06-15',
 '2005-06-02',
 '1994-08-10',
 '1994-04-25',
 '2010-10-17',
 '2019-02-17',
 '1995-08-18',
 '2017-05-27',
 '1994-05-04',
 '2015-05-31',
 '2005-11-27',
 '2018-04-20',
 '2013-05-18',
 '1997-09-08',
 '2004-09-03',
 '1993-11-30',
 '2017-12-06',
 '2016-09-24',
 '2005-01-28',
 '2018-04-17',
 '1990-10-28',
 '2002-08-22',
 '1998-12-25',
 '2020-05-22',
 '2017-03-15',
 '2011-01-02',
 '1993-06-08',
 '1992-08-14',
 '2005-03-30',
 '2003-06-10',
 '1993-09-08',
 '2002-12-01',
 '1992-09-21',
 '2020-01-31',
 '2019-04-02',
 '1995-08-06',
 '2006-04-09',
 '1998-02-02',
 '2013-12-22',
 '1999-12-03',
 '2000-03-02',
 '2017-11-10',
 '1991-11-22',
 '2017-01-22',
 '2018-08-12',
 '1990-07-15',
 '1998-05-24',
 '2015-10-11',
 '2017-10-23',
 '2018-06-29',
 '1998-10-31',
 '2010-07-09',
 '1995-10-11',
 '2014-02-04',
 '2020-09-14',
 '2020-08-21',
 '1992-06-20',
 '1998-05-20',
 '2001-07-18',
 '2017-03-31',
 '2014-06-04',
 '2012-10-20',
 '1998-10-02',
 '2001-04-06',
 '1994-11-10',
 '1991-11-08',
 '2008-11-13',
 '1998-06-14',
 '2020-02-13',
 '2003-07-18',
 '2008-03-14',
 '2016-09-06',
 '1993-08-13',
 '2011-04-01',
 '2008-05-10',
 '2012-06-03',
 '1999-06-28',
 '2008-09-03',
 '1993-06-05',
 '1991-12-12',
 '1999-04-12',
 '2007-01-02',
 '2019-03-18',
 '2010-02-19',
 '2010-03-13',
 '2007-06-14',
 '2006-02-11',
 '2016-05-18',
 '2011-11-15',
 '2013-10-19',
 '2012-03-13',
 '2010-09-01',
 '2002-11-01',
 '1993-03-26',
 '2018-04-12',
 '1993-01-08',
 '2014-06-06',
 '2007-08-03',
 '2011-09-06',
 '1992-01-24',
 '2019-03-15',
 '2000-11-04',
 '2006-02-17',
 '2014-07-19',
 '2016-04-13',
 '2008-02-09',
 '2019-05-02',
 '1991-07-19',
 '1996-04-01',
 '2011-01-13',
 '2008-05-12',
 '1995-06-18',
 '2020-09-02',
 '1990-05-09',
 '2018-09-16',
 '1999-10-23',
 '1997-07-04',
 '2010-12-29',
 '2009-08-14',
 '2009-08-05',
 '2004-05-27',
 '2006-06-17',
 '2005-10-23',
 '1990-11-15',
 '2008-03-15',
 '2006-08-20',
 '2005-10-22',
 '2015-11-16',
 '1994-07-02',
 '2015-11-12',
 '2020-06-26',
 '2017-02-08',
 '2017-04-20',
 '2001-09-07',
 '1992-03-05',
 '2004-09-19',
 '2005-07-24',
 '2000-07-30',
 '2014-05-19',
 '2008-06-19',
 '2008-10-23',
 '1998-08-03',
 '2011-12-05',
 '2019-10-02',
 '1992-06-02',
 '1994-04-27',
 '1999-12-28',
 '1993-07-18',
 '2020-01-02',
 '2014-03-14',
 '2006-07-29',
 '2007-10-30',
 '1990-04-20',
 '1990-04-11',
 '1992-01-31',
 '2005-06-04',
 '2017-02-14',
 '1997-07-05',
 '1994-01-17',
 '2018-06-07',
 '2001-06-02',
 '2009-01-20',
 '1991-01-12',
 '1990-06-16',
 '2004-07-25',
 '1997-10-15',
 '2016-06-11',
 '1996-11-03',
 '2006-09-27',
 '2013-03-24',
 '2008-04-01',
 '1990-06-10',
 '2005-07-14',
 '2015-02-12',
 '2018-07-07',
 '2012-06-30',
 '1996-11-26',
 '2009-05-16',
 '1990-01-07',
 '2018-08-06',
 '1997-12-04',
 '1996-11-21',
 '1998-05-22',
 '2011-11-01',
 '1996-08-04',
 '2011-11-17',
 '2002-06-19',
 '2017-12-04',
 '1995-05-04',
 '2019-06-28',
 '2014-06-30',
 '2007-07-20',
 '2019-05-15',
 '1990-08-20',
 '1998-04-21',
 '1992-12-22',
 '2001-08-09',
 '2010-12-26',
 '2010-05-30',
 '2019-03-29',
 '2019-09-07',
 '2013-05-11',
 '2004-01-12',
 '2001-10-03',
 '2012-03-08',
 '1996-11-18',
 '2008-11-20',
 '1991-10-17',
 '1997-02-14',
 '1992-09-20',
 '1991-03-16',
 '2004-02-02',
 '2015-02-23',
 '1991-03-27',
 '2007-04-17',
 '1994-03-24',
 '1992-11-25',
 '2006-12-13',
 '2002-02-12',
 '2001-01-26',
 '2011-05-22',
 '1990-01-26',
 '2019-12-28',
 '2007-09-26',
 '1998-12-03',
 '1998-06-12',
 '1999-05-23',
 '2017-01-30',
 '1992-11-20',
 '2020-05-09',
 '2002-05-01',
 '1994-09-22',
 '2004-04-12',
 '2002-01-21',
 '1996-10-16',
 '2004-08-02',
 '2003-06-27',
 '2017-12-23',
 '1998-12-11',
 '2015-10-17',
 '1990-08-16',
 '2015-09-29',
 '1999-08-03',
 '2000-05-29',
 '2020-05-08',
 '1991-02-12',
 '2006-05-10',
 '2008-06-17',
 '1992-10-15',
 '2011-06-17',
 '2001-12-28',
 '2016-01-14',
 '2003-10-25',
 '1994-01-02',
 '2016-10-28',
 '2012-04-04',
 '1992-11-24',
 '2019-03-09',
 '2019-02-18',
 '2003-10-07',
 '2002-07-18',
 '2014-03-04',
 '2015-08-24',
 '2018-11-03',
 '1994-09-03',
 '2009-10-10',
 '1994-01-27',
 '2010-03-29',
 '1993-08-11',
 '2000-12-18',
 '2014-04-29',
 '2017-10-21',
 '2008-06-27',
 '2016-07-19',
 '2011-11-03',
 '2008-09-12',
 '1999-04-17',
 '1990-12-16',
 '2008-12-06',
 '2013-05-19',
 '2020-08-18',
 '1998-05-11',
 '2018-09-24',
 '1997-04-06',
 '2008-09-16',
 '2000-06-11',
 '2013-04-28',
 '1994-08-06',
 '2002-03-19',
 '2012-05-19',
 '2016-05-09',
 '2014-06-14',
 '2013-04-30',
 '2004-04-23',
 '2006-11-08',
 '1993-10-02',
 '2003-10-17',
 '2002-11-20',
 '2009-05-18',
 '1997-06-06',
 '2016-03-30',
 '2019-10-08',
 '2020-10-06',
 '2005-02-17',
 '2004-05-07',
 '2007-11-13',
 '2004-09-22',
 '1992-12-14',
 '2009-10-24',
 '2008-08-10',
 '1993-10-16',
 '2011-08-28',
 '1991-02-03',
 '2020-01-27',
 '2004-11-30',
 '2012-01-15',
 '1994-06-17',
 '2017-07-06',
 '2002-04-02',
 '2013-04-09',
 '2009-07-09',
 '1996-09-09',
 '2007-01-14',
 '1995-05-26',
 '2006-01-02',
 '2008-01-25',
 '2004-02-04',
 '2013-06-07',
 '2006-03-09',
 '1991-07-16',
 '2010-04-04',
 '2017-11-28',
 '2015-11-29',
 '2013-12-11',
 '2017-08-28',
 '2012-08-14',
 '2014-02-07',
 '2016-11-15',
 '1994-05-02',
 '2010-04-17',
 '1997-08-05',
 '1994-01-11',
 '2004-06-25',
 '1997-09-13',
 '2009-04-06',
 '1999-02-08',
 '2009-06-15',
 '2009-08-20',
 '1998-01-17',
 '1993-12-06',
 '1993-06-21',
 '2006-08-17',
 '2009-05-14',
 '2016-08-09',
 '2019-07-10',
 '2016-06-18',
 '2000-03-29',
 '2002-12-16',
 '1993-05-16',
 '2017-11-02',
 '2012-12-06',
 '2016-08-13',
 '2002-06-08',
 '2003-11-06',
 '2009-08-31',
 '2014-05-18',
 '1996-06-03',
 '2001-08-06',
 '2003-08-03',
 '2008-03-22',
 '2019-08-03',
 '2018-06-28',
 '1993-04-25',
 '1992-10-13',
 '2003-01-25',
 '2005-12-21',
 '1990-04-05',
 '1993-05-13',
 '1996-09-15',
 '1995-06-10',
 '2014-12-01',
 '2002-06-26',
 '2013-03-15',
 '2019-03-11',
 '2008-10-15',
 '1992-11-27',
 '2001-08-12',
 '1993-01-03',
 '2013-08-04',
 '2012-05-28',
 '1998-04-19',
 '2002-07-11',
 '1999-04-14',
 '1990-04-30',
 '2017-04-28',
 '1991-04-13',
 '1991-08-01',
 '1996-07-09',
 '2009-02-17',
 '2019-05-26',
 '2011-08-19',
 '1996-01-19',
 '2015-03-01',
 '2016-03-19',
 '2020-03-14',
 '2007-05-11',
 '1993-06-22',
 '2014-02-20',
 '2003-10-20',
 '1992-04-16',
 '1995-09-13',
 '2018-03-13',
 '2018-05-15',
 '2016-05-17',
 '2003-03-08',
 '2015-08-06',
 '2014-01-10',
 '1994-12-28',
 '2018-08-18',
 '2015-12-09',
 '1998-08-02',
 '1994-07-12',
 '2015-07-14',
 '2007-01-23',
 '2014-07-24',
 '2014-03-05',
 '1992-09-12',
 '2006-08-06',
 '2004-04-01',
 '2003-07-02',
 '2019-02-07',
 '2017-08-12',
 '2002-03-30',
 '2009-02-01',
 '1999-05-04',
 '2016-06-16',
 '1999-07-09',
 '1991-05-25',
 '2007-12-02',
 '1990-06-06',
 '1990-09-24',
 '2018-10-24',
 '1995-07-22',
 '2009-10-31',
 '2003-05-10',
 '2012-06-06',
 '2009-03-29',
 '2001-05-15',
 '2010-01-30',
 '1993-07-07',
 '2006-01-28',
 '2003-11-28',
 '2015-08-03',
 '2010-02-24',
 '2011-08-16',
 '1999-12-01',
 '2018-02-26',
 '2019-04-22',
 '1994-09-15',
 '2001-04-12',
 '2014-03-28',
 '2007-10-26',
 '1996-05-24',
 '2015-08-20',
 '1995-11-01',
 '2010-07-11',
 '1995-08-26',
 '2003-12-04',
 '2019-04-05',
 '2010-08-01',
 '2017-02-19',
 '1991-09-27',
 '2009-04-19',
 '2015-07-12',
 '1996-11-16',
 '2013-11-28',
 '2016-01-28',
 '2004-09-20',
 '1991-07-02',
 '1993-09-23',
 '2005-12-11',
 '2005-04-07',
 '1994-08-11',
 '2008-10-04',
 '2012-12-24',
 '1993-08-03',
 '2002-04-16',
 '2002-08-27',
 '1996-07-26',
 '1999-09-21',
 '2007-06-15',
 '2014-05-24',
 '2007-08-15',
 '1991-11-15',
 '1995-05-27',
 '1996-12-13',
 '1995-10-16',
 '2010-11-03',
 '1995-05-06',
 '1998-08-29',
 '1994-10-13',
 '1999-04-10',
 '2007-07-01',
 '2017-01-12',
 '2019-08-16',
 '2001-04-21',
 '1991-09-24',
 '2000-02-29',
 '2000-07-16',
 '1991-04-30',
 '2001-11-07',
 '2020-03-17',
 '2018-05-29',
 '2007-12-19',
 '2020-04-22',
 '1992-11-06',
 '2009-10-27',
 '1994-06-20',
 '1992-12-09',
 '1992-08-02',
 '2002-08-19',
 '2000-11-26',
 '1994-07-04',
 '2009-08-21',
 '2008-09-09',
 '1993-01-09',
 '1996-10-07',
 '1996-08-29',
 '2019-07-22',
 '1999-12-10',
 '2016-12-11',
 '2017-07-02',
 '2001-02-04',
 '2018-02-10',
 '2002-12-05',
 '2020-02-24',
 '2019-12-30',
 '2008-11-16',
 '2019-01-08',
 '2008-05-26',
 '2007-05-29',
 '2015-04-30',
 '2011-09-27',
 '1990-11-08',
 '1999-05-08',
 '1998-07-13',
 '2016-08-12',
 '1991-08-04',
 '1997-12-19',
 '2018-04-30',
 '1990-04-19',
 '2016-07-03',
 '2007-04-30',
 '1996-03-17',
 '2010-08-28',
 '2008-06-24',
 '1999-01-02',
 '2009-04-29',
 '2016-07-23',
 '2009-04-01',
 '2000-07-07',
 '1993-08-28',
 '2006-09-21',
 '1994-06-11',
 '2012-09-01',
 '2000-11-09',
 '2012-05-21',
 '1991-10-05',
 '2014-07-06',
 '2010-08-30',
 '1994-08-07',
 '2004-03-03',
 '2005-11-18',
 '2005-01-11',
 '2019-07-13',
 '1990-07-02',
 '2004-10-26',
 '2017-08-09',
 '2015-08-16',
 '2001-07-30',
 '2020-02-19',
 '2006-06-14',
 '2004-09-07',
 '2019-01-07',
 '1998-07-17',
 '2009-02-09',
 '2019-02-15',
 '1996-11-27',
 '2005-10-12',
 '2018-05-26',
 '2001-03-21',
 '2014-09-16',
 '2015-03-11',
 '2001-10-28',
 '1990-05-31',
 '2011-10-18',
 '2008-09-29',
 '2008-07-03',
 '2019-01-02',
 '2008-01-27',
 '2001-04-08',
 '1991-10-12',
 '2009-03-23',
 '2009-12-20',
 '2002-03-08',
 '2003-02-10',
 '2019-03-19',
 '2001-02-02',
 '1995-11-14',
 '1993-05-15',
 '1992-10-27',
 '1998-09-15',
 '1994-07-13',
 '2001-10-10',
 '2020-01-08',
 '2012-04-30',
 '2013-08-23',
 '2015-01-17',
 '2020-04-25',
 '2014-04-07',
 '2002-10-06',
 '1993-03-15',
 '1990-05-08',
 '1996-12-09',
 '1993-12-29',
 '1992-07-21',
 '2014-09-19',
 '2011-12-10',
 '1993-06-03',
 '1991-09-04',
 '2012-10-19',
 '2000-01-22',
 '2005-07-03',
 '1998-11-05',
 '1994-09-11',
 '1997-12-25',
 '2020-07-27',
 '2013-08-10',
 '2013-06-29',
 '2012-10-09',
 '2006-10-10',
 '1994-01-09',
 '2004-04-29',
 '2018-07-22',
 '2015-08-23',
 '2010-07-16',
 '2014-11-26',
 '2014-02-22',
 '2014-08-31',
 '1998-02-22',
 '2010-01-09',
 '1999-05-17',
 '2015-07-28',
 '2001-01-21',
 '1997-05-05',
 '2014-05-20',
 '2000-11-18',
 '2005-04-01',
 '1997-09-03',
 '2001-07-14',
 '2008-02-16',
 '2010-02-08',
 '2004-02-25',
 '2012-01-11',
 '1991-01-22',
 '1998-12-29',
 '2017-12-14',
 '1999-03-19',
 '2018-08-26',
 '1993-02-26',
 '2011-07-31',
 '2017-01-05',
 '2009-06-09',
 '1991-06-02',
 '2005-09-22',
 '1997-04-27',
 '2006-10-24',
 '2008-04-17',
 '2012-02-15',
 '2014-03-27',
 '1995-07-25',
 '2012-03-15',
 '1998-11-16',
 '1994-10-12',
 '2010-03-09',
 '2018-03-21',
 '2016-11-30',
 '2006-01-23',
 '2014-10-26',
 '2010-08-31',
 '2009-05-20',
 '2009-06-23',
 '2009-05-07',
 '2012-06-13',
 '2009-07-02',
 '2009-07-08',
 '2004-11-11',
 '1990-09-22',
 '1993-05-20',
 '2014-11-30',
 '1999-05-19',
 '2010-01-21',
 '2008-08-22',
 '1991-08-21',
 '2010-02-10',
 '2000-11-07',
 '1990-07-13',
 '1998-04-26',
 '2008-01-15',
 '1992-01-14',
 '1999-06-30',
 '2010-04-07',
 '2013-05-23',
 '2004-02-15',
 '1997-10-25',
 '1998-05-06',
 '1997-12-10',
 '2003-12-27',
 '1991-01-31',
 '2018-01-02',
 '2017-03-03',
 '1998-07-30',
 '1995-10-22',
 '1996-09-26',
 '1990-02-26',
 '2009-11-24',
 '1992-03-07',
 '1993-12-11',
 '2004-11-20',
 '1997-07-29',
 '2010-12-11',
 '2019-09-01',
 '2002-03-18',
 '2017-08-20',
 '2009-03-05',
 '2015-06-05',
 '1994-06-04',
 '2006-10-23',
 '2018-10-27',
 '1998-04-08',
 '2013-10-30',
 '2004-09-23',
 '2013-10-06',
 '2015-07-04',
 '1997-02-05',
 '2013-12-13',
 '2008-12-28',
 '2018-08-09',
 '1993-01-31',
 '2003-08-25',
 '2000-10-19',
 '2006-02-12',
 '1993-09-14',
 '2017-05-31',
 '2004-08-22',
 '2005-04-08',
 '2018-04-10',
 '2010-08-09',
 '1994-11-05',
 '1996-11-24',
 '2019-12-12',
 '2012-07-21',
 '2005-09-05',
 '2002-09-20',
 '2008-03-03',
 '2015-11-03',
 '2016-11-06',
 '2016-10-13',
 '2008-06-06',
 '2002-01-11',
 '2012-09-02',
 '2017-08-06',
 '1991-09-21',
 '2012-06-25',
 '2018-12-28',
 '2001-07-24',
 '2008-03-30',
 '2010-08-23',
 '1991-09-08',
 '2009-08-25',
 '2012-04-27',
 '2007-06-30',
 '2014-02-10',
 '2013-11-17',
 '2000-10-24',
 '1991-10-07',
 '2012-11-23',
 '2011-03-15',
 '2011-05-12',
 '2001-12-12',
 '1990-08-01',
 '2000-01-30',
 '2018-02-13',
 '2007-12-06',
 '2018-04-09',
 '2018-01-19',
 '2017-11-14',
 '2013-02-08',
 '1994-02-12',
 '1998-10-19',
 '2006-07-11',
 '1996-02-20',
 '2015-12-16',
 '2003-11-13',
 '2004-08-11',
 '2002-09-30',
 '1996-11-07',
 '2002-01-20',
 '2006-08-29',
 '2002-09-07',
 '1990-07-27',
 '2008-02-28',
 '2006-02-06',
 '2009-06-16',
 '1998-12-24',
 '2016-11-25',
 '2020-07-06',
 '2013-04-08',
 '1997-08-13',
 '2009-12-26',
 '1995-06-16',
 '1994-12-05',
 '2019-04-26',
 '2020-05-31',
 '2006-11-23',
 '2019-09-20',
 '2006-11-09',
 '2007-06-13',
 '2011-08-26',
 '1998-12-18',
 '2014-05-23',
 '2005-01-19',
 '1998-07-23',
 '1991-10-03',
 '2010-11-04',
 '2003-03-26',
 '1990-01-15',
 '2002-02-18',
 '1993-10-06',
 '2003-11-14',
 '2010-11-12',
 '2006-12-23',
 '2020-06-20',
 '2020-01-14',
 '2007-10-06',
 '2014-05-02',
 '1991-09-16',
 '1997-09-07',
 '2011-08-10',
 '1995-12-26',
 '2018-02-04',
 '1999-11-09',
 '2015-03-21',
 '2006-08-21',
 '2018-06-03',
 '2008-01-07',
 '1990-05-23',
 '2000-11-21',
 '2014-07-02',
 '1996-10-24',
 '2010-04-26',
 '1994-06-28',
 '2018-03-18',
 '2017-12-24',
 '1997-04-15',
 '2001-12-22',
 '2008-09-15',
 '1997-09-04',
 '1995-09-28',
 '2002-11-08',
 '1997-02-02',
 '1997-06-09',
 '2005-05-25',
 '1995-01-02',
 '2015-09-11',
 '2005-06-01',
 '2017-04-15',
 '1997-12-07',
 '2002-09-03',
 '1995-09-25',
 '2011-10-02',
 '2008-10-06',
 '2006-05-25',
 '2009-07-23',
 '1996-04-15',
 '1999-07-22',
 '1997-03-14',
 '2005-08-10',
 '1991-09-28',
 '1996-01-28',
 '1998-05-23',
 '2017-05-08',
 '1992-05-25',
 '2018-02-20',
 '2011-04-18',
 '2004-02-18',
 '2000-06-04',
 '2019-06-11',
 '2012-12-22',
 '2010-03-06',
 '2015-12-28',
 '1995-10-23',
 '2020-10-01',
 '1998-02-19',
 '2018-10-14',
 '1993-01-21',
 '1998-10-26',
 '1994-04-15',
 '2014-01-06',
 '1992-06-12',
 '1999-01-29',
 '2009-12-09',
 '1995-09-15',
 '2002-11-13',
 '2001-09-28',
 '2013-07-13',
 '1994-07-14',
 '2010-07-29',
 '2011-07-17',
 '1996-10-27',
 '1993-06-06',
 '2011-09-19',
 '2018-02-01',
 '2010-11-27',
 '1992-05-29',
 '2011-08-18',
 '2020-02-18',
 '1992-04-18',
 '2004-05-31',
 '2002-05-09',
 '2020-06-04',
 '2008-10-25',
 '2014-07-11',
 '2011-01-07',
 '2019-12-06',
 '2013-06-18',
 '2013-12-04',
 '2005-04-12',
 '1997-05-23',
 '2017-02-12',
 '2016-03-05',
 '2006-01-18',
 '1993-04-10',
 '1996-07-06',
 '1994-08-30',
 '2001-04-26',
 '1991-12-09',
 '2002-01-06',
 '1998-10-07',
 '1999-05-06',
 '2010-11-13',
 '2015-08-11',
 '1993-09-28',
 '2006-10-31',
 '2002-02-20',
 '2011-12-01',
 '2003-01-09',
 '2013-01-28',
 '1999-02-22',
 '2006-12-11',
 '1994-11-12',
 '2019-03-28',
 '2012-11-22',
 '2020-01-01',
 '2008-11-06',
 '2010-03-26',
 '1991-10-18',
 '1993-01-10',
 '2011-07-11',
 '1993-07-21',
 '2005-11-15',
 '1992-02-25',
 '1993-09-10',
 '1993-08-10',
 '1990-10-24',
 '2005-02-12',
 '1992-10-03',
 '2009-06-21',
 '2006-08-11',
 '2002-06-13',
 '1992-08-05',
 '2010-09-13',
 '2007-06-18',
 '1994-09-02',
 '2004-03-26',
 '2009-04-30',
 '2007-03-30',
 '2014-09-12',
 '1996-06-16',
 '2009-03-24',
 '2018-07-08',
 '2000-06-24',
 '2004-12-31',
 '2001-01-28',
 '2006-05-13',
 '1999-02-16',
 '2017-08-23',
 '1998-11-08',
 '2009-05-29',
 '2010-12-27',
 '2014-01-03',
 '1993-04-19',
 '2020-08-04',
 '2009-12-10',
 '2004-11-22',
 '2012-06-18',
 '2001-12-02',
 '1995-09-18',
 '2012-01-20',
 '2017-09-27',
 '2007-03-17',
 '1994-07-26',
 '1995-12-04',
 '2001-05-31',
 '2009-08-12',
 '2002-09-25',
 '1991-04-06',
 '2007-01-28',
 '1999-09-09',
 '2012-11-06',
 '2000-09-25',
 '2013-05-12',
 '2007-12-24',
 '2019-08-09',
 '1999-06-09',
 '2001-12-14',
 '2018-06-21',
 '2005-06-14',
 '2000-06-16',
 '1998-09-02',
 '2009-02-10',
 '1990-01-02',
 '2015-11-19',
 '1995-03-02',
 '2009-10-20',
 '2017-05-14',
 '2018-08-28',
 '1992-08-20',
 '1993-08-30',
 '2006-05-06',
 '2010-09-23',
 '2006-07-10',
 '2003-07-08',
 '2001-04-05',
 '2015-12-18',
 '1995-06-14',
 '1997-11-29',
 '2011-11-10',
 '2015-12-10',
 '1994-12-11',
 '2014-07-29',
 '1992-03-30',
 '1996-02-13',
 '2016-05-20',
 '2003-11-27',
 '1993-12-26',
 '1992-02-01',
 '1996-10-25',
 '2005-08-04',
 '1993-08-31',
 '1994-07-16',
 '1991-10-27',
 '2005-09-12',
 '2004-11-23',
 '1994-08-03',
 '2008-03-10',
 '2000-03-16',
 '1994-12-29',
 '1997-10-10',
 '2014-04-02',
 '2005-01-20',
 '2017-01-04',
 '1995-01-10',
 '2018-07-06',
 '2013-08-01',
 '1997-06-19',
 '2009-01-22',
 '2000-06-25',
 '2007-03-23',
 '2017-11-26',
 '1995-10-26',
 '2003-06-13',
 '2005-12-29',
 '2012-05-08',
 '1993-09-13',
 '2001-11-28',
 '1992-12-16',
 '1992-10-21',
 '2005-08-19',
 '2012-09-16',
 '1995-06-20',
 '1997-03-15',
 '2000-05-14',
 '1996-12-25',
 '2018-04-22',
 '2010-09-14',
 '2017-08-27',
 '2017-02-18',
 '2014-06-22',
 '2004-07-05',
 '1995-07-07',
 '2013-05-01',
 '2006-05-28',
 '2019-01-15',
 '2008-04-12',
 '2006-01-29',
 '2017-07-28',
 '2007-04-21',
 '2016-01-04',
 '2012-02-24',
 '2016-08-02',
 '2006-11-12',
 '2020-01-30',
 '2002-07-19',
 '1990-01-03',
 '2016-12-04',
 '1996-10-08',
 '2001-06-01',
 '1999-08-25',
 '2007-04-16',
 '2003-03-20',
 '2000-04-23',
 '2012-03-25',
 '1994-04-09',
 '2018-12-30',
 '1997-01-29',
 '2006-08-07',
 '2018-02-11',
 '2002-09-14',
 '1991-11-03',
 '2006-09-18',
 '2003-02-04',
 '2000-10-01',
 '1994-10-08',
 '2017-10-11',
 '2004-08-15',
 '2017-04-04',
 '1992-02-15',
 '2001-04-13',
 '2003-08-07',
 '2019-01-04',
 '2016-02-25',
 '1998-10-25',
 '1996-01-04',
 '2002-11-11',
 '2006-12-19',
 '2005-12-30',
 '2019-10-14',
 '1997-04-28',
 '2020-04-08',
 '2017-10-16',
 '2011-01-16',
 '2001-12-19',
 '2001-08-26',
 '2002-10-07',
 '1990-07-04',
 '2011-01-06',
 '2011-12-11',
 '2009-07-17',
 '2014-04-28',
 '2005-11-02',
 '2016-03-16',
 '2013-07-31',
 '1993-01-11',
 '1991-05-18',
 '1997-10-13',
 '1994-08-13',
 '1996-03-30',
 '2009-09-22',
 '2005-09-13',
 '2003-01-18',
 '2012-08-03',
 '2014-12-29',
 '2006-02-10',
 '2002-12-19',
 '1994-07-17',
 '1998-03-27',
 '2012-01-10',
 '1993-01-01',
 '2010-07-28',
 '1991-02-22',
 '2020-03-19',
 '2006-03-24',
 '2018-04-25',
 '1994-04-29',
 '2007-06-19',
 '2006-06-25',
 '2001-11-30',
 '2019-03-30',
 '2016-04-08',
 '1995-02-02',
 '2002-03-07',
 '1994-02-03',
 '2003-04-08',
 '1993-02-10',
 '2011-05-05',
 '2001-02-28',
 '2007-01-11',
 '1994-12-21',
 '2002-11-06',
 '2015-07-25',
 '2006-10-21',
 '2002-10-14',
 '1998-01-02',
 '2003-11-29',
 '2012-05-25',
 '2012-02-22',
 '2010-11-18',
 '1990-06-03',
 '1994-01-10',
 '2004-03-28',
 '1995-08-27',
 '2014-03-21',
 '2006-12-15',
 '1990-10-09',
 '1990-03-09',
 '2008-02-10',
 '1997-05-04',
 '2018-05-18',
 '1993-03-18',
 '2017-11-06',
 '2014-09-25',
 '2010-09-28',
 '2006-04-16',
 '2020-02-02',
 '2000-05-08',
 '2003-04-07',
 '2017-10-26',
 '2007-04-14',
 '1999-04-26',
 '2013-06-21',
 '2006-07-18',
 '1990-04-26',
 '2001-07-23',
 '1999-02-23',
 '2000-06-26',
 '1995-07-16',
 '2018-10-09',
 '1995-03-23',
 '1990-04-02',
 '2008-04-23',
 '2014-11-02',
 '2002-09-04',
 '2018-07-17',
 '1990-05-14',
 '2000-12-29',
 '1998-10-18',
 '2007-01-22',
 '2017-07-22',
 '2003-10-04',
 '2012-04-06',
 '2014-08-19',
 '2012-11-25',
 '1992-09-25',
 '2015-03-10',
 '1993-07-05',
 '2015-01-02',
 '2004-06-23',
 '2003-09-28',
 '2005-08-08',
 '1992-11-07',
 '2020-07-28',
 '2010-07-21',
 '2014-03-17',
 '2000-08-04',
 '2005-08-30',
 '2018-01-23',
 '1992-08-27',
 '2001-09-26',
 '2008-07-13',
 '1990-03-12',
 '2014-07-15',
 '1998-11-18',
 '1990-07-07',
 '1994-08-18',
 '2012-06-23',
 '2013-11-14',
 '2010-11-29',
 '2001-03-31',
 '1995-04-03',
 '2016-12-25',
 '1998-08-09',
 '2016-06-17',
 '1996-09-07',
 '2007-11-21',
 '2014-05-28',
 '1990-05-15',
 '2011-08-17',
 '2010-10-20',
 '2001-03-14',
 '2014-01-05',
 '2006-11-27',
 '2019-04-07',
 '2001-10-29',
 '2006-07-02',
 '2001-11-12',
 '1991-01-13',
 '1995-11-12',
 '1990-08-24',
 '2019-07-12',
 '2002-04-22',
 '1993-04-30',
 '2015-05-16',
 '2018-03-08',
 '2005-04-20',
 '2008-11-26',
 '2011-03-18',
 '2002-10-30',
 '2010-12-18',
 '2003-07-20',
 '1995-03-29',
 '2010-07-02',
 '2010-12-25',
 '1998-12-22',
 '1991-03-02',
 '2003-03-15',
 '2016-02-05',
 '1990-09-12',
 '1997-01-10',
 '1998-06-08',
 '2004-09-30',
 '2012-12-05',
 '2017-03-28',
 '2011-08-05',
 '1999-05-12',
 '2006-08-15',
 '2002-09-24',
 '2003-07-21',
 '2020-04-09',
 '2011-05-15',
 '2013-11-23',
 '2016-01-27',
 '2009-02-26',
 '1990-08-07',
 '2012-11-14',
 '2010-12-14',
 '1999-08-23',
 '1990-02-07',
 '1998-08-20',
 '2008-02-06',
 '1990-03-15',
 '2011-01-27',
 '1990-02-18',
 '1992-10-16',
 '2014-09-17',
 '2019-07-18',
 '2017-02-20',
 '2017-09-21',
 '2006-09-11',
 '2015-06-17',
 '1994-09-14',
 '2014-05-21',
 '2006-10-26',
 '1993-04-13',
 '2012-05-03',
 '2000-05-10',
 '1993-10-18',
 '2019-12-11',
 '2019-10-12',
 '1991-12-11',
 '1993-06-13',
 '2004-02-01',
 '1998-07-16',
 '2006-01-24',
 '2004-03-06',
 '1994-03-28',
 '1991-01-07',
 '2017-05-05',
 '1998-07-31',
 '2014-10-11',
 '2005-10-20',
 '1999-01-08',
 '1991-01-02',
 '1993-06-12',
 '2019-07-19',
 '2017-11-30',
 '1991-03-29',
 '2002-06-03',
 '2019-05-10',
 '1995-11-09',
 '1992-06-24',
 '1998-11-22',
 '2010-02-21',
 '1997-11-25',
 '2018-06-09',
 '1993-05-29',
 '2009-08-22',
 '2004-06-28',
 '2004-07-14',
 '1995-06-24',
 '1995-05-12',
 '2010-08-22',
 '2019-10-30',
 '2016-10-16',
 '2011-11-22',
 '2006-03-27',
 '2001-03-15',
 '1993-09-18',
 '1997-06-08',
 '1992-12-18',
 '1998-03-30',
 '2016-03-25',
 '1996-11-19',
 '2007-01-27',
 '2011-06-18',
 '2006-11-10',
 '2015-07-23',
 '2008-02-18',
 '1992-02-08',
 '2002-09-27',
 '2000-01-25',
 '1990-06-08',
 '2000-01-31',
 '2009-03-15',
 '2012-01-31',
 '2001-11-16',
 '2003-04-27',
 '2010-03-14',
 '2001-10-14',
 '2017-02-28',
 '2005-05-27',
 '1993-11-14',
 '2020-03-05',
 '1991-04-20',
 '2011-06-05',
 '2004-07-26',
 '2008-06-14',
 '1998-11-27',
 '2013-12-31',
 '1999-02-04',
 '2006-08-27',
 '2018-07-28',
 '2013-02-19',
 '2002-08-02',
 '1990-08-08',
 '2014-02-09',
 '1995-03-22',
 '2003-03-16',
 '1998-02-06',
 '2015-01-12',
 '2011-04-23',
 '2012-04-21',
 '1990-02-27',
 '2000-08-05',
 '2006-03-13',
 '1997-11-15',
 '2005-10-01',
 '2020-03-06',
 '1997-02-28',
 '1990-07-17',
 '1990-07-29',
 '2010-07-31',
 '2019-03-05',
 '2004-10-19',
 '2020-08-30',
 '2019-06-03',
 '2007-02-03',
 '2002-12-17',
 '2000-05-20',
 '2016-09-28',
 '1996-09-05',
 '2009-05-05',
 '2019-06-19',
 '2016-11-27',
 '2014-12-23',
 '1990-05-16',
 '2009-03-31',
 '2019-02-20',
 '2004-06-27',
 '1996-10-03',
 '1999-08-07',
 '1999-10-20',
 '2005-02-27',
 '2003-09-15',
 '1991-01-23',
 '1991-02-11',
 '1995-12-05',
 '2011-05-17',
 '2019-06-06',
 '2010-07-07',
 '2018-04-06',
 '2010-03-03',
 '2016-07-21',
 '2008-11-18',
 '2016-10-29',
 '2002-12-02',
 '1997-06-27',
 '2008-07-30',
 '2008-12-20',
 '2003-11-21',
 '2019-10-21',
 '2015-07-03',
 '1995-11-11',
 '1990-05-17',
 '2018-10-21',
 '1997-01-15',
 '2018-09-23',
 '1991-07-10',
 '2000-05-06',
 '1997-03-10',
 '2019-05-03',
 '2002-06-27',
 '1991-11-05',
 '2018-08-02',
 '2014-11-10',
 '2006-08-31',
 '1990-10-25',
 '2018-11-23',
 '2013-10-24',
 '2008-05-02',
 '1992-01-27',
 '1996-07-29',
 '2007-08-20',
 '2015-04-07',
 '2009-11-29',
 '1996-04-07',
 '1999-10-11',
 '2012-03-12',
 '1993-02-11',
 '2007-03-07',
 '2004-06-19',
 '2011-01-25',
 '2015-08-22',
 '2003-08-27',
 '2014-10-15',
 '1998-09-14',
 '1990-07-23',
 '2009-09-14',
 '2016-12-08',
 '1994-10-31',
 '2004-09-11',
 '1995-05-31',
 '2019-08-01',
 '2018-11-05',
 '1996-01-11',
 '2001-03-27',
 '1990-07-03',
 '2007-01-17',
 '2013-04-16',
 '2016-11-04',
 '2006-07-20',
 '1994-02-14',
 '1999-07-11',
 '2000-10-20',
 '2018-06-02',
 '2006-11-20',
 '2010-06-25',
 '2020-04-03',
 '2014-11-18',
 '2014-09-15',
 '2012-08-30',
 '2007-05-22',
 '2003-10-24',
 '1999-08-06',
 '1992-03-23',
 '1992-08-24',
 '2002-08-23',
 '1990-09-28',
 '2018-04-07',
 '2007-08-06',
 '1991-03-24',
 '2008-12-31',
 '2020-05-20',
 '1997-01-13',
 '2015-02-09',
 '2000-10-21',
 '1995-08-10',
 '2019-07-15',
 '1995-10-21',
 '2012-07-19',
 '2016-12-01',
 '2010-10-09',
 '2012-06-21',
 '2006-03-01',
 '2009-05-06',
 '2003-04-02',
 '2004-01-13',
 '2016-07-02',
 '2006-04-03',
 '1998-07-01',
 '2016-02-02',
 '1998-03-04',
 '2008-06-15',
 '1995-01-06',
 '2003-12-26',
 '1990-10-23',
 '1999-11-18',
 '2011-03-14',
 '2010-04-10',
 '2015-02-17',
 '2013-11-26',
 '2001-12-23',
 '1992-04-10',
 '2006-01-09',
 '1994-10-11',
 '2014-03-30',
 '1993-11-13',
 '1994-07-01',
 '1999-11-29',
 '1991-06-20',
 '2004-03-12',
 '1990-06-02',
 '2006-01-21',
 '1993-08-23',
 '1990-04-24',
 '2016-01-25',
 '1999-09-22',
 '1997-03-28',
 '2008-06-11',
 '2009-09-05',
 '2012-06-26',
 '2019-01-21',
 '2007-12-12',
 '2001-11-17',
 '2001-01-13',
 '2014-12-25',
 '2000-12-12',
 '2013-10-14',
 '2017-11-24',
 '2012-01-26',
 '2009-07-26',
 '2020-07-16',
 '2012-06-08',
 '2007-11-22',
 '2016-06-22',
 '2013-10-27',
 '1991-12-26',
 '2012-04-05',
 '2009-05-26',
 '1994-04-20',
 '2002-05-06',
 '2019-04-29',
 '2002-03-28',
 '2005-12-28',
 '2003-04-17',
 '2018-03-28',
 '1999-12-30',
 '2015-07-26',
 '2014-10-21',
 '1990-02-12',
 '2011-12-31',
 '2012-02-23',
 '2006-11-21',
 '2006-10-12',
 '2008-11-19',
 '2007-08-08',
 '1997-08-03',
 '2000-07-19',
 '1993-11-21',
 '2004-03-02',
 '2013-11-21',
 '2017-06-27',
 '2013-04-27',
 '2010-11-23',
 '2007-11-07',
 '2017-03-11',
 '2009-04-20',
 '2011-09-28',
 '2017-06-01',
 '2001-06-13',
 '1996-08-24',
 '2018-07-05',
 '1990-03-01',
 '2008-09-10',
 '2018-06-12',
 '2003-07-03',
 '2014-01-14',
 '2015-02-25',
 '1999-11-24',
 '2004-12-23',
 '2020-07-05',
 '2011-07-25',
 '2014-06-21',
 '2001-12-17',
 '2016-04-02',
 '2004-06-15',
 '2004-11-25',
 '2008-10-01',
 '2016-04-03',
 '2020-03-09',
 '2002-09-13',
 '2008-04-04',
 '2004-05-05',
 '2009-11-27',
 '2009-03-08',
 '1991-04-27',
 '1991-09-14',
 '1992-09-10',
 '1994-05-21',
 '1996-09-29',
 '2013-02-27',
 '2004-12-11',
 '2011-12-13',
 '2015-07-30',
 '2011-02-10',
 '1990-07-24',
 '2013-06-17',
 '1990-08-11',
 '2009-08-03',
 '2004-01-04',
 '2013-09-01',
 '1998-05-31',
 '2007-01-26',
 '2014-04-06',
 '2000-12-23',
 '2012-11-20',
 '2017-03-23',
 '1999-12-14',
 '2010-04-24',
 '2005-10-14',
 '1995-07-19',
 '2004-11-10',
 '2015-06-21',
 '2000-07-15',
 '1994-04-28',
 '2003-07-07',
 '2005-01-31',
 '2015-05-18',
 '2018-05-22',
 '2016-08-07',
 '1993-06-19',
 '1996-05-05',
 '2019-11-06',
 '1995-08-16',
 '2002-05-05',
 '2013-05-20',
 '1990-03-28',
 '2004-09-12',
 '2007-04-29',
 '2007-09-15',
 '2001-05-13',
 '1998-01-30',
 '2005-11-14',
 '2001-08-25',
 '1992-08-08',
 '2005-02-03',
 '1998-11-09',
 '2002-01-04',
 '2005-12-04',
 '1990-01-16',
 '2004-12-09',
 '2000-10-03',
 '1996-01-18',
 '2007-03-31',
 '1992-02-24',
 '2001-04-02',
 '1991-10-15',
 '2010-10-05',
 '2012-11-16',
 '1996-10-14',
 '2011-07-28',
 '1993-09-12',
 '2011-01-05',
 '2005-11-24',
 '1998-05-13',
 '2017-07-11',
 '2018-01-26',
 '2005-10-05',
 '2016-06-24',
 '2015-12-27',
 '1996-05-04',
 '2019-02-14',
 '2010-01-01',
 '2009-12-30',
 '1994-04-10',
 '2016-02-01',
 '2018-07-03',
 '2008-10-17',
 '1997-11-20',
 '2010-05-13',
 '2019-09-26',
 '2008-12-26',
 '1993-10-29',
 '2007-06-12',
 '2003-08-22',
 '2013-03-23',
 '2004-11-06',
 '2001-06-08',
 '1997-07-16',
 '2003-08-05',
 '2012-08-16',
 '2011-10-26',
 '2016-10-21',
 '2009-12-07',
 '1993-11-03',
 '1994-07-10',
 '2014-04-01',
 '2003-10-08',
 '2003-08-10',
 '1997-11-17',
 '1997-05-08',
 '1990-10-21',
 '1996-12-08',
 '1996-05-07',
 '2017-06-21',
 '2020-09-15',
 '2005-05-10',
 '1998-12-20',
 '2020-07-08',
 '2017-02-23',
 '1993-03-03',
 '2007-11-03',
 '1999-04-03',
 '2013-04-14',
 '1994-12-01',
 '2003-08-09',
 '2019-10-29',
 '1995-07-14',
 '2011-11-27',
 '2014-08-04',
 '2008-03-31',
 '1999-01-09',
 '2006-05-12',
 '2007-06-17',
 '2004-10-25',
 '1994-10-03',
 '2010-04-05',
 '2015-04-20',
 '1992-06-30',
 '2014-04-24',
 '1994-08-28',
 '2017-11-22',
 '2013-04-12',
 '2015-02-04',
 '1999-07-10',
 '2019-10-28',
 '2020-05-03',
 '2020-04-04',
 '1998-03-02',
 '1994-10-27',
 '2004-04-20',
 '2005-11-12',
 '1997-04-02',
 '1992-10-05',
 '2014-12-20',
 '2013-05-28',
 '2001-06-23',
 '2007-11-02',
 '2000-03-23',
 '2017-02-10',
 '2017-03-07',
 '1999-04-01',
 '2003-03-25',
 '2018-01-11',
 '2003-11-17',
 '2015-10-21',
 '1991-12-31',
 '2018-12-23',
 '1995-06-09',
 '2020-02-06',
 '1992-03-13',
 '1997-07-30',
 '1994-05-13',
 '2003-12-01',
 '2013-08-06',
 '1999-06-27',
 '2007-11-19',
 '1992-11-17',
 '2014-04-26',
 '2007-12-26',
 '1995-10-17',
 '1995-10-18',
 '2011-06-14',
 '1991-06-12',
 '2010-02-07',
 '1993-09-30',
 '2016-03-07',
 '1996-04-08',
 '2012-05-04',
 '2004-11-04',
 '1992-03-29',
 '2004-08-23',
 '2019-01-16',
 '2007-10-24',
 '1997-11-21',
 '2018-02-05',
 '2018-05-16',
 '2011-07-03',
 '2010-10-01',
 '1990-08-15',
 '2010-03-12',
 '1999-06-18',
 '2002-01-24',
 '1996-09-21',
 '2019-09-30',
 '2001-02-09',
 '2015-10-20',
 '2004-08-30',
 '1992-04-30',
 '1991-10-16',
 '1995-09-17',
 '2011-05-10',
 '1990-10-29',
 '2020-04-28',
 '1996-05-13',
 '2015-06-23',
 '1998-06-21',
 '2001-04-29',
 '2006-09-08',
 '1996-04-14',
 '2002-02-11',
 '1991-01-24',
 '2017-09-01',
 '2006-07-15',
 '2006-12-31',
 '2012-04-13',
 '2019-02-12',
 '2011-01-03',
 '2002-10-15',
 '1995-08-29',
 '1997-02-27',
 '2019-04-19',
 '2007-04-04',
 '2019-07-27',
 '2004-11-13',
 '1990-07-25',
 '1991-10-23',
 '2000-12-14',
 '2013-09-16',
 '2004-09-26',
 '2008-11-24',
 '2004-12-08',
 '2016-11-02',
 '1999-10-28',
 '1992-09-01',
 '2005-08-09',
 '2008-04-08',
 '2001-01-23',
 '2007-12-05',
 '2013-12-06',
 '2004-08-09',
 '2009-02-04',
 '1997-10-26',
 '1998-08-19',
 '2013-03-19',
 '2008-12-04',
 '1991-08-22',
 '2003-04-16',
 '2018-11-17',
 '2000-05-30',
 '1998-06-17',
 '1997-10-24',
 '1996-10-02',
 '2006-02-20',
 '2001-12-30',
 '2005-09-14',
 '2010-10-02',
 '2011-08-15',
 '2007-09-03',
 '1999-02-18',
 '1993-09-06',
 '1996-07-13',
 '1993-04-07',
 '1996-03-22',
 '2002-10-01',
 '2003-10-28',
 '2011-06-12',
 '1996-06-08',
 '1997-01-21',
 '2018-02-22',
 '2013-07-24',
 '2014-08-15',
 '2001-12-15',
 '1993-12-13',
 '2001-05-20',
 '2016-10-22',
 '2014-03-10',
 '1999-09-27',
 '2009-01-06',
 '1998-08-07',
 '1990-02-16',
 '2017-03-27',
 '2011-10-24',
 '2013-03-03',
 '2010-04-21',
 '2003-01-16',
 '2003-12-29',
 '1996-06-02',
 '1998-02-23',
 '2018-10-08',
 '2015-01-29',
 '2017-05-26',
 '1993-04-17',
 '1993-03-31',
 '2012-01-03',
 '2004-05-17',
 '2020-07-19',
 '2002-04-19',
 '2019-02-05',
 '2014-10-01',
 '1995-11-22',
 '2005-06-29',
 '2014-10-05',
 '2014-11-24',
 '1991-06-03',
 '2010-04-02',
 '1991-03-06',
 '1991-05-20',
 '2020-09-04',
 '1996-02-27',
 '1990-11-20',
 '1991-10-21',
 '2014-01-13',
 '2001-08-10',
 '2007-09-04',
 '2009-06-03',
 '2003-05-22',
 '2016-03-10',
 '2013-01-26',
 '1994-02-02',
 '2019-12-08',
 '1995-11-28',
 '1991-01-05',
 '2001-09-03',
 '1994-01-08',
 '1991-09-03',
 '2014-04-03',
 '2016-05-14',
 '2017-04-13',
 '2011-08-08',
 '1991-12-03',
 '2009-01-30',
 '2009-10-13',
 '2000-11-08',
 '1993-01-17',
 '1996-08-02',
 '2018-05-20',
 '2006-08-13',
 '1993-08-02',
 '1994-05-17',
 '2016-06-23',
 '2002-01-28',
 '1998-10-17',
 '1993-05-30',
 '2019-06-05',
 '2012-08-15',
 '2014-08-17',
 '2005-10-06',
 '2011-02-03',
 '2010-01-22',
 '2003-10-13',
 '2007-12-20',
 '1998-06-09',
 '2002-10-22',
 '2020-01-28',
 '1996-09-12',
 '2016-11-20',
 '2017-05-16',
 '1997-04-11',
 '1992-02-13',
 '2001-03-30',
 '2018-06-11',
 '1990-12-20',
 '1996-05-12',
 '1991-01-30',
 '2009-12-28',
 '1995-09-20',
 '2015-02-13',
 '2005-07-21',
 '2004-10-22',
 '2009-04-08',
 '2018-06-10',
 '2008-06-07',
 '1994-06-23',
 '1999-08-18',
 '2002-03-10',
 '2012-08-02',
 '1997-11-10',
 '2011-10-17',
 '2015-10-23',
 '2005-03-07',
 '2008-07-25',
 '2019-06-07',
 '2003-10-11',
 '2010-02-18',
 '1992-05-18',
 '2017-06-02',
 '2018-04-18',
 '2001-06-05',
 '1990-07-10',
 '2018-07-13',
 '2006-12-12',
 '2004-02-16',
 '2002-10-17',
 '2016-09-23',
 '2009-09-21',
 '2007-01-03',
 '2012-03-07',
 '1995-08-03',
 '2004-05-08',
 '1994-10-20',
 '1996-05-03',
 '2004-05-30',
 '2002-07-17',
 '2007-04-18',
 '2001-07-01',
 '2000-04-07',
 '2008-07-24',
 '2007-04-07',
 '2010-05-17',
 '2005-09-20',
 '2002-11-17',
 '2010-05-16',
 '1992-09-23',
 '2016-06-21',
 '2001-10-24',
 '2000-02-11',
 '2002-11-23',
 '2012-03-20',
 '2008-08-17',
 '2008-11-09',
 '1995-03-13',
 '2007-01-06',
 '2017-04-19',
 '1994-03-08',
 '2004-01-06',
 '2018-01-01',
 '1990-01-13',
 '2020-05-29',
 '2013-06-13',
 '2000-05-04',
 '2019-02-25',
 '2011-12-19',
 '2000-08-12',
 '1997-04-17',
 '2020-08-08',
 '1999-01-28',
 '2019-05-01',
 '1992-12-05',
 '2020-05-14',
 '2009-06-26',
 '2005-08-07',
 '2001-09-24',
 '1992-08-03',
 '2006-07-21',
 '2012-02-20',
 '1998-11-11',
 '2010-11-11',
 '2012-05-31',
 '2014-01-18',
 '2001-05-10',
 '2020-08-10',
 '1995-11-10',
 '2015-01-21',
 '2015-11-25',
 '2010-12-13',
 '2018-02-16',
 '2000-12-03',
 '2002-02-14',
 '2008-12-02',
 '2014-06-12',
 '2001-01-12',
 '2002-09-19',
 '1990-06-22',
 '2000-03-18',
 '2020-05-16',
 '2018-09-13',
 '1997-10-17',
 '2011-09-13',
 '2011-07-30',
 '2018-09-06',
 '2002-10-08',
 '1999-10-10',
 '2012-07-20',
 '2012-11-05',
 '2015-12-03',
 '2020-09-09',
 '2002-07-15',
 '1997-08-01',
 '1990-08-04',
 '1998-11-13',
 '1995-09-14',
 '2006-09-04',
 '1990-10-14',
 '2008-07-02',
 '2006-06-02',
 '2015-06-22',
 '2008-03-08',
 '2002-05-02',
 '2016-05-08',
 '1996-08-03',
 '1996-07-10',
 '2010-01-12',
 '2007-11-30',
 '2011-05-29',
 '1996-08-21',
 '2008-02-02',
 '1995-04-09',
 '2009-04-07',
 '2000-11-27',
 '1994-04-21',
 '2009-06-29',
 '2010-10-07',
 '2008-06-04',
 '2020-04-05',
 '2005-03-24',
 '1993-03-10',
 '2019-01-26',
 '2014-04-18',
 '1995-09-16',
 '2001-04-18',
 '1991-12-14',
 '1990-11-26',
 '1994-06-18',
 '2009-10-28',
 '2006-05-20',
 '2002-11-16',
 '2008-07-16',
 '1998-11-25',
 '1990-08-05',
 '1991-02-26',
 '2010-04-23',
 '1990-07-21',
 '2015-09-19',
 '2014-05-03',
 '2008-03-19',
 '2003-02-21',
 '2015-04-10',
 '2002-05-25',
 '1994-08-29',
 '1992-11-11',
 '2016-10-08',
 '2001-12-11',
 '1990-06-28',
 '1998-09-27',
 '2011-11-06',
 '2012-04-20',
 '1990-12-19',
 '2016-08-24',
 '1996-08-22',
 '2018-03-25',
 '2001-04-17',
 '2017-09-20',
 '2014-09-29',
 '2014-01-29',
 '2016-01-13',
 '1997-01-25',
 '1999-09-17',
 '2015-04-19',
 '2000-10-26',
 '2013-10-11',
 '2020-08-19',
 '1998-12-04',
 '1991-11-13',
 '1995-01-26',
 '2002-05-03',
 '1999-12-06',
 '2019-05-19',
 '2018-03-22',
 '2004-08-06',
 '1997-04-05',
 '2017-07-29',
 '2016-09-21',
 '2008-07-21',
 '2008-03-29',
 '1990-03-10',
 '1996-12-16',
 '2007-03-06',
 '2009-07-10',
 '1996-06-23',
 '2004-04-03',
 '2005-03-23',
 '1995-12-27',
 '2014-09-13',
 '2009-12-08',
 '2018-08-30',
 '1994-09-21',
 '2016-02-06',
 '2016-08-26',
 '2017-03-19',
 '2004-10-14',
 '2020-09-26',
 '2006-01-31',
 '2019-09-08',
 '2014-09-11',
 '1997-02-17',
 '2015-04-14',
 '2012-03-27',
 '2018-07-25',
 '2002-07-25',
 '1990-01-25',
 '2000-11-20',
 '1992-01-02',
 '1991-05-16',
 '1997-07-01',
 '2006-07-12',
 '1994-03-27',
 '1992-12-11',
 '2018-10-19',
 '2015-05-29',
 '2004-06-21',
 '2010-10-31',
 '2001-09-11',
 '2005-11-26',
 '2000-09-26',
 '1990-01-06',
 '1999-08-31',
 '2014-04-04',
 '2015-03-20',
 '2000-08-30',
 '1993-03-29',
 '1997-10-21',
 '2012-05-22',
 '2004-06-02',
 '1994-02-27',
 '2002-08-06',
 '1998-09-12',
 '1995-02-18',
 '2012-06-02',
 '1993-06-20',
 '1991-06-26',
 '2002-05-12',
 '2005-07-29',
 '2003-07-28',
 '2014-12-03',
 '2005-01-02',
 '1993-05-07',
 '1995-03-21',
 '1995-11-30',
 '2017-07-04',
 '2019-10-26',
 '2004-04-08',
 '1991-10-06',
 '1999-02-17',
 '2012-08-21',
 '1991-10-31',
 '2000-04-15',
 '2019-02-10',
 '2019-05-16');
{code}",2020-11-11T03:18:13.356+0000,2020-11-24T16:13:31.001+0000,Fixed,Major
CALCITE-2012,Replace LocalInterval by Interval in Druid adapter,CALCITE,Task,Closed,[],2,"[<JIRA IssueLink: id='12517440'>, <JIRA IssueLink: id='12520096'>]","CALCITE-1617 introduced LocalInterval as a proper way to close the gap between the semantics of SQL timestamp type and Druid instants.

After that, CALCITE-1947 introduced 'timestamp with local time zone' type in Calcite and mapped the Druid time column to this type. Thus, we do not need anymore the LocalInterval class and we can use Joda Interval, since the column represents an Instant rather than a LocalDateTime.",2017-10-13T17:02:27.417+0000,2017-12-09T18:21:28.000+0000,Fixed,Major
ZOOKEEPER-829,Add /zookeeper/sessions/* to allow inspection/manipulation of client sessions,ZOOKEEPER,New Feature,Open,[],2,"[<JIRA IssueLink: id='12361876'>, <JIRA IssueLink: id='12333188'>]","For some use cases in HBase (HBASE-1316 in particular) we'd like the ability to forcible expire someone else's ZK session. Patrick and I discussed on IRC and came up with an idea of creating nodes in /zookeeper/sessions/<session id> that can be read in order to get basic stats about a session, and written in order to manipulate one. The manipulation we need in HBase is the ability to write a command like ""kill"", but others might be useful as well.",2010-07-29T17:25:19.017+0000,2012-12-13T08:09:27.831+0000,,Major
HDDS-6202,Avoid using jmh-generator-annprocess since it is GPL2.0,HDDS,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12631619'>],"jmh-generator-annprocess is GPL2.0.  It is incompatible with Apache License 2.0.  It is currently only used by genesis (a micro-benchmark tool).

We may have to rewrite the tool or move it out from the main Ozone code base.",2022-01-19T08:54:19.323+0000,2022-01-20T09:15:29.174+0000,Implemented,Major
SPARK-30724,Support 'like any' and 'like all' operators,SPARK,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12579720'>, <JIRA IssueLink: id='12599726'>]","In Teradata/Hive and PostgreSQL 'like any' and 'like all' operators are mostly used when we are matching a text field with numbers of patterns. For example:

Teradata / Hive 3.0:
{code:sql}
--like any
select 'foo' LIKE ANY ('%foo%','%bar%');

--like all
select 'foo' LIKE ALL ('%foo%','%bar%');
{code}

PostgreSQL:

{code:sql}
-- like any
select 'foo' LIKE ANY (array['%foo%','%bar%']);

-- like all
select 'foo' LIKE ALL (array['%foo%','%bar%']);
{code}
",2020-02-04T08:33:09.592+0000,2020-11-16T06:00:43.922+0000,Fixed,Major
FLUME-1618,Make Flume NG build and tests work with Hadoop 2.0 & Hbase 0.96,FLUME,Task,Resolved,[],4,"[<JIRA IssueLink: id='12388651'>, <JIRA IssueLink: id='12358501'>, <JIRA IssueLink: id='12358448'>, <JIRA IssueLink: id='12389519'>]",Add hadoop 2.0 support for Flume NG,2012-10-04T03:56:09.261+0000,2014-06-10T18:38:41.378+0000,Duplicate,Major
IVY-1320,"Retrieve task automatically decompresses tar.gz files, but doesn't remove gz suffix",IVY,Bug,Resolved,[],1,[<JIRA IssueLink: id='12345498'>],,2011-11-15T01:42:17.427+0000,2012-02-14T23:10:41.220+0000,Fixed,Major
ZOOKEEPER-1665,Support recursive deletion in multi,ZOOKEEPER,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12365671'>],"Use case in HBase is that we need to recursively delete multiple subtrees:
{code}
    ZKUtil.deleteChildrenRecursively(watcher, acquiredZnode);
    ZKUtil.deleteChildrenRecursively(watcher, reachedZnode);
    ZKUtil.deleteChildrenRecursively(watcher, abortZnode);
{code}
To achieve high consistency, it is desirable to use multi for the above operations.

This JIRA adds support for recursive deletion in multi.",2013-03-14T04:08:29.105+0000,2014-04-02T18:38:38.731+0000,Won't Fix,Major
YETUS-540,Remove unnecessary semicolons in audience annotations,YETUS,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12512357'>, <JIRA IssueLink: id='12512356'>]","do the same improvement done in HBASE-17401:

{quote}
InterfaceStability and InterfaceAudience contains unnecessary semicolons after the attribute annotation definitions. These semicolons can be removed.
{quote}",2017-08-19T05:05:46.746+0000,2017-10-27T04:47:38.475+0000,Fixed,Trivial
HCATALOG-546,Rework HCatalog's JMS Notifications ,HCATALOG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12361056'>, <JIRA IssueLink: id='12359919'>]","In 0.4.1, the NotificationListener listens for metastore operations and emits JMS notifications containing the entire metastore-objects (Database/Table/Partitions) in Java-serialized form. The assumption at the time was that consumers might need access to the whole object. This policy poses a couple of problems:

1. The notifications are verbose, since it conveys a bunch of information that's available from querying the metastore anyway.

2. Consumers of these JMS notifications (e.g. Oozie) would now be dependent on the Java class definitions of metastore-objects. If they change, Oozie would also need to be restarted (with updated libs), to consume the notifications.

Ideally, the notifications should convey only the minimum information that identifies the metastore-change unambiguously. (Everything else can be queried for.) They should be backward compatible. If new fields are added, existing consumers shouldn't break (unless they intend to consume the new fields). Also, the notification-format ought to be pluggable.

For the initial rework, we're proposing to use a JSON-string to represent the notification-content. We're also proposing a helper-class for the likes of Oozie to use, that converts the strings to POJOs, in a backward-compatible fashion.

I'll attach sample notifications and a tentative patch shortly.",2012-11-07T00:01:06.005+0000,2019-10-01T22:09:34.299+0000,Fixed,Major
TEZ-3530,"Tez UI: Add query details page, and link the page from All Queries table",TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12490130'>, <JIRA IssueLink: id='12485894'>, <JIRA IssueLink: id='12486011'>]",,2016-11-07T13:53:08.375+0000,2017-08-22T00:03:10.628+0000,Fixed,Major
TEZ-3833,Tasks should report codec errors during shuffle as fetch failures,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12514869'>],Do the equivalent of https://issues.apache.org/jira/browse/MAPREDUCE-6633 so that compression errors do not prove fatal for the DAG/tasks.,2017-09-14T16:28:08.254+0000,2018-01-05T00:17:53.693+0000,Fixed,Major
HCATALOG-92,HCatLoader unnecessarily adds 'innertuple' as a name of tuple in a bag,HCATALOG,Bug,Open,[],2,"[<JIRA IssueLink: id='12342984'>, <JIRA IssueLink: id='12342985'>]","Starting from 0.9, pig has done away with names of tuple in aa bag. HCatloader unnecessarily adds it, which results in subsequent failure. 
{code}
CREATE TABLE tmp_pig (
  mymap       map<string, string>,
  mytuple     struct<num:int,str:string,dbl:double>,
  bagofmap    array<map<string,string>>,
  rownum      int

A = load 'default.complex' using org.apache.hcatalog.pig.HCatLoader();
store A into 'tmp_pig'
        using org.apache.hcatalog.pig.HCatStorer
       ('',
        'mymap: map[],mytuple: (num: int,str: chararray,dbl: double),bagofmap: {innertuple: (innerfield: map[])},rownum: int');
)
{code}

this results in following stacktrace:
{code}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1115: Schema provided in store statement doesn't match with the Schemareturned by Pig run-time. Schema provided in HCatStorer: {mymap: map[],mytuple: (num: int,str: chararray,dbl: double),bagofmap: {(innerfield: map[])},rownum: int} Schema received from Pig runtime: {mymap: map[],mytuple: (num: int,str: chararray,dbl: double),bagofmap: {innertuple: (innerfield: map[])},rownum: int}
{code}",2011-09-05T00:06:16.247+0000,2012-02-01T02:04:09.582+0000,,Major
ZOOKEEPER-2787,CLONE - ZK Shell/Cli re-executes last command on exit,ZOOKEEPER,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12504135'>, <JIRA IssueLink: id='12504137'>, <JIRA IssueLink: id='12504136'>]","In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell. In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run. 
{noformat}
[zk: zookeeper.example.com:2181(CONNECTED) 0] ls /blah
[foo]
[zk: zookeeper.example.com:2181(CONNECTED) 1] <ctrl+d> [foo]
$
{noformat}",2017-05-20T23:07:10.210+0000,2019-11-06T18:52:02.268+0000,Won't Fix,Major
TEZ-793,Staging directories accumulate and pollute the home directory,TEZ,Sub-task,Open,[],4,"[<JIRA IssueLink: id='12389870'>, <JIRA IssueLink: id='12389867'>, <JIRA IssueLink: id='12382605'>, <JIRA IssueLink: id='12382476'>]","Right now, most of the examples use 
{noformat}
Path stagingDir = new Path(fs.getWorkingDirectory(), UUID.randomUUID().toString());
{noformat}
to get the staging directory. This results in home directories filled with directories that have to be manually deleted one by one. The MRRSleep example is the only example which uses TEZ_AM_STAGING_DIR, but that defaults to '/tmp/tez/staging', which breaks for more than one user.

It would be good to have a standardized way of handling staging directories for Tez. Optimally, it would be nice if Tez would handle the lifecycle of its staging directory itself.

",2014-02-04T18:21:31.691+0000,2014-06-20T23:42:38.108+0000,,Major
HCATALOG-244,Security in Hcat ,HCATALOG,Improvement,Open,"[<JIRA Issue: key='HCATALOG-245', id='12540478'>, <JIRA Issue: key='HCATALOG-260', id='12542814'>]",4,"[<JIRA IssueLink: id='12347551'>, <JIRA IssueLink: id='12347548'>, <JIRA IssueLink: id='12347546'>, <JIRA IssueLink: id='12347547'>]","Hcat needs it's own security model, and it should work with security features in hdfs and hbase. 

A design document is at https://cwiki.apache.org/confluence/display/HCATALOG/Hcat+Security+Design",2012-01-31T00:50:45.297+0000,2012-01-31T01:20:19.365+0000,,Major
IMPALA-452,Add support for string concatenation operator using || construct,IMPALA,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12507081'>],"User has requested that we support || for string concatenation, otherwise they are forced to use the concat function.

Thanks

Hari",2013-07-02T16:02:30.000+0000,2020-12-21T17:49:03.861+0000,Fixed,Major
IMPALA-6653,Unicode support for Kudu table names,IMPALA,Bug,Open,[],2,"[<JIRA IssueLink: id='12605387'>, <JIRA IssueLink: id='12597924'>]","It is possible to create a Kudu table containing unicode characters in its in Impala by specifying the kudu.table_name attribute. When trying to select from this table you receive an error that the underlying table does not exist.

The example below shows a table being created successfully, but failing on a select * statement.

{{[jh-kafka-2:21000] > create table test2( a int primary key) stored as kudu TBLPROPERTIES('kudu.table_name' = 'impala::kudutest.😀');}}
{{Query: create table test2( a int primary key) stored as kudu TBLPROPERTIES('kudu.table_name' = 'impala::kudutest.😀')}}
{{WARNINGS: Unpartitioned Kudu tables are inefficient for large data sizes.}}{{Fetched 0 row(s) in 0.64s}}
{{[jh-kafka-2:21000] > select * from test2;}}
{{Query: select * from test2}}
{{Query submitted at: 2018-03-13 08:23:29 (Coordinator: https://jh-kafka-2:25000)}}
{{ERROR: AnalysisException: Failed to load metadata for table: 'test2'}}
{{CAUSED BY: TableLoadingException: Error loading metadata for Kudu table impala::kudutest.????}}
{{CAUSED BY: ImpalaRuntimeException: Error opening Kudu table 'impala::kudutest.????', Kudu error: The table does not exist: table_name: ""impala::kudutest.????""}}",2018-03-13T15:49:29.506+0000,2020-12-23T01:39:27.761+0000,,Major
OOZIE-2729,Use MiniMRYARNCluster in tests,OOZIE,Improvement,In Progress,[],2,"[<JIRA IssueLink: id='12486475'>, <JIRA IssueLink: id='12499315'>]","{{MiniMRCluster}} is deprecated, we should replace it using {{MiniYARNCluster}}.
Also {{MiniDFSCluster}} should be created via builder.",2016-11-08T14:09:27.094+0000,2022-02-08T15:09:58.276+0000,,Major
HCATALOG-380,"If pig script does load then order by, hive-site.xml doesn't seem to propagate properly ",HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12350883'>],"Table is not partitioned and has an RCFile for the data.

The following pig script will cause the MR jobs to fail:
a = load 'default.table' USING org.apache.hcatalog.pig.HCatLoader();
b = order a by id;
dump b;

If I prefix the order by with a foreach statement, then the job will pass.


The MR job fails with the following exception:


Error: java.lang.ClassNotFoundException: javax.jdo.JDOException at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:346) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:333) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:371) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:278) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:248) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:114) at org.apache.hcatalog.mapreduce.InitializeInput.createHiveMetaClient(InitializeInput.java:58) at org.apache.hcatalog.mapreduce.InitializeInput.getSerializedHcatKeyJobInfo(InitializeInput.java:85) at org.apache.hcatalog.mapreduce.InitializeInput.setInput(InitializeInput.java:73) at org.apache.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:40) at org.apache.hcatalog.pig.HCatLoader.setLocation(HCatLoader.java:116) at org.apache.pig.impl.builtin.SampleLoader.setLocation(SampleLoader.java:98) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.mergeSplitSpecificConf(PigInputFormat.java:134) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.createRecordReader(PigInputFormat.java:112) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:489) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370) at org.apache.hadoop.mapred.Child$4.run(Child.java:255) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093) at org.apache.hadoop.mapred.Child.main(Child.java:249) ",2012-04-19T18:09:18.986+0000,2012-05-09T23:43:43.573+0000,Fixed,Major
HDDS-348,Parallalize container ops in ContainerStateMachine,HDDS,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12545095'>, <JIRA IssueLink: id='12560586'>]",Currently all the ops in ContainerStateMachine#applyTransaction are processed sequentially by the ContainerStateMachine. However these ops can be parallelized by having a configurable no of executors where a certain executor for a container can be chosen from the pool of executors by hashing the containerID.,2018-08-12T09:07:56.086+0000,2019-05-11T06:03:04.311+0000,Duplicate,Major
PHOENIX-6122,Upgrade jQuery to 3.5.1,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12598932'>, <JIRA IssueLink: id='12598552'>]",Following HBase we should also upgrade to jQuery 3.5.1 to avoid security vulnerabilities.,2020-09-11T08:41:16.926+0000,2020-09-29T00:16:05.398+0000,Fixed,Major
HDDS-664,Creating hive table on Ozone fails,HDDS,Bug,Open,[],1,[<JIRA IssueLink: id='12545831'>],"Modified HIVE_AUX_JARS_PATH to include Ozone jars. Tried creating Hive external table on Ozone. It fails with ""Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)""
{code:java}
-bash-4.2$ beeline
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
Enter username for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:
Enter password for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:
18/10/15 21:36:55 [main]: INFO jdbc.HiveConnection: Connected to ctr-e138-1518143905142-510793-01-000004.hwx.site:10000
Connected to: Apache Hive (version 3.1.0.3.0.3.0-63)
Driver: Hive JDBC (version 3.1.0.3.0.3.0-63)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.0.3.0.3.0-63 by Apache Hive
0: jdbc:hive2://ctr-e138-1518143905142-510793> create external table testo3 ( i int, s string, d float) location ""o3://bucket2.volume2/testo3"";
Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)
0: jdbc:hive2://ctr-e138-1518143905142-510793> {code}
 ",2018-10-15T21:50:45.697+0000,2021-10-20T20:33:50.200+0000,,Major
AVRO-160,file format should be friendly to streaming,AVRO,Improvement,Closed,[],1,[<JIRA IssueLink: id='12328499'>],"It should be possible to stream through an Avro data file without seeking to the end.

Currently the interpretation is that schemas written to the file apply to all entries before them.  If this were changed so that they instead apply to all entries that follow, and the initial schema is written at the start of the file, then streaming could be supported.

Note that the only change permitted to a schema as a file is written is to, if it is a union, to add new branches at the end of that union.  If it is not a union, no changes may be made.  So it is still the case that the final schema in a file can read every entry in the file and thus may be used to randomly access the file.
",2009-10-20T20:30:35.446+0000,2010-03-01T17:09:02.892+0000,Fixed,Major
AMBARI-14138,Upgrade Ambari Views to Hadoop 2.7.1 jar which has the fix for HDFS-7931,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12450583'>],"Multiple views fail to initialize with below exception when accessing HDFS. 

From the application log:
Cannot find uri with key to create KeyProvider
{code}
15/10/30 16:39:53 ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
15/10/30 16:39:53 INFO impl.NMClientAsyncImpl: Processing Event EventType: START_CONTAINER for Container container_e01_1446222705122_0001_01_000008
15/10/30 16:39:53 INFO impl.ContainerManagementProtocolProxy: Opening proxy : os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:25454
15/10/30 16:39:53 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 9 for hbase on 172.22.93.94:8020
15/10/30 16:39:53 ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
{code}

HBASE_THRIFT and HBASE_MASTER 
{code}
15/10/30 16:40:43 ERROR appmaster.SliderAppMaster: Role instance RoleInstance{role='HBASE_THRIFT', id='container_e01_1446222705122_0001_01_000008', container=ContainerID=container_e01_1446222705122_0001_01_000008 nodeID=os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:25454 http=os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:8042 priority=1073741828 resource=<memory:3072, vCores:1>, createTime=1446223193693, startTime=1446223193777, released=false, roleId=4, host=os-r6-jhtlqs-hbase-slider-1-re-7.novalocal, hostURL=http://os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:8042, state=5, placement=null, exitCode=0, command='python ./infra/agent/slider-agent/agent/main.py --label container_e01_1446222705122_0001_01_000008___HBASE_THRIFT --zk-quorum os-r6-jhtlqs-hbase-slider-1-re-1.novalocal:2181,os-r6-jhtlqs-hbase-slider-1-re-2.novalocal:2181,os-r6-jhtlqs-hbase-slider-1-re-3.novalocal:2181 --zk-reg-path /registry/users/hbase/services/org-apache-slider/hbasesliderapp > <LOG_DIR>/slider-agent.out 2>&1 ; ', diagnostics='', output=null, environment=[LANGUAGE=""en_US.UTF-8"", AGENT_WORK_ROOT=""$PWD"", AGENT_LOG_ROOT=""<LOG_DIR>"", PYTHONPATH=""./infra/agent/slider-agent/"", LC_ALL=""en_US.UTF-8"", SLIDER_PASSPHRASE=""kNR1KYBtgRDN3aVHmnh8HVStB3XNwQwPPJ5kbo3xAG9y2h35hp"", MALLOC_ARENA_MAX=""4"", LANG=""en_US.UTF-8""]} failed
15/10/30 16:40:43 INFO agent.AgentProviderService: Removing component status for label container_e01_1446222705122_0001_01_000008___HBASE_THRIFT
15/10/30 16:40:43 INFO appmaster.SliderAppMaster: Container Completion for containerID=container_e01_1446222705122_0001_01_000002, state=COMPLETE, exitStatus=0, diagnostics=
15/10/30 16:40:43 INFO appmaster.SliderAppMaster: Unregistering component container_e01_1446222705122_0001_01_000008
15/10/30 16:40:43 INFO state.AppState: Failed container in role[1] : HBASE_MASTER
15/10/30 16:40:43 INFO state.AppState: Duration 50550 and startTimeThreshold 60
15/10/30 16:40:43 INFO state.AppState: Current count of failed role[1] HBASE_MASTER =  1
15/10/30 16:40:43 INFO state.RoleHistory: Finished container for node 1, released=false, shortlived=true
15/10/30 16:40:43 INFO state.AppState: Removing node ID container_e01_1446222705122_0001_01_000002
15/10/30 16:40:43 ERROR appmaster.SliderAppMaster: Role instance RoleInstance{role='HBASE_MASTER', id='container_e01_1446222705122_0001_01_000002', container=ContainerID=container_e01_1446222705122_0001_01_000002 nodeID=os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:25454 http=os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:8042 priority=1073741825 resource=<memory:3072, vCores:1>, createTime=1446223192797, startTime=1446223193381, released=false, roleId=1, host=os-r6-jhtlqs-hbase-slider-1-re-7.novalocal, hostURL=http://os-r6-jhtlqs-hbase-slider-1-re-7.novalocal:8042, state=5, placement=null, exitCode=0, command='python ./infra/agent/slider-agent/agent/main.py --label container_e01_1446222705122_0001_01_000002___HBASE_MASTER --zk-quorum os-r6-jhtlqs-hbase-slider-1-re-1.novalocal:2181,os-r6-jhtlqs-hbase-slider-1-re-2.novalocal:2181,os-r6-jhtlqs-hbase-slider-1-re-3.novalocal:2181 --zk-reg-path /registry/users/hbase/services/org-apache-slider/hbasesliderapp > <LOG_DIR>/slider-agent.out 2>&1 ; ', diagnostics='', output=null, environment=[LANGUAGE=""en_US.UTF-8"", AGENT_WORK_ROOT=""$PWD"", AGENT_LOG_ROOT=""<LOG_DIR>"", PYTHONPATH=""./infra/agent/slider-agent/"", LC_ALL=""en_US.UTF-8"", SLIDER_PASSPHRASE=""kNR1KYBtgRDN3aVHmnh8HVStB3XNwQwPPJ5kbo3xAG9y2h35hp"", MALLOC_ARENA_MAX=""4"", LANG=""en_US.UTF-8""]} failed
{code}

The fix in HDFS requires Ambari-views to upgrade their HDFS dependency version to Hadoop 2.7.1

",2015-12-01T17:45:21.418+0000,2015-12-04T18:46:20.188+0000,Fixed,Critical
SPARK-23130,Spark Thrift does not clean-up temporary files (/tmp/*_resources and /tmp/hive/*.pipeout),SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12524472'>, <JIRA IssueLink: id='12524471'>]","Spark Thrift is not cleaning up /tmp for files & directories named like:
 /tmp/hive/*.pipeout
 /tmp/*_resources

There are such a large number that /tmp quickly runs out of inodes *causing the partition to be unusable and many services to crash*. This is even true when the only jobs submitted are routine service checks.

Used `strace` to show that Spark Thrift is responsible:
{code:java}
strace.out.118864:04:53:49 open(""/tmp/hive/55ad7fc1-f79a-4ad8-8e02-26bbeaa86bbc7288010135864174970.pipeout"", O_RDWR|O_CREAT|O_EXCL, 0666) = 134
strace.out.118864:04:53:49 mkdir(""/tmp/b6dfbf9e-2f7c-4c25-95a1-73c44318ecf4_resources"", 0777) = 0
{code}
*Those files were left behind, even days later.*

----
Example files:
{code:java}
# stat /tmp/hive/55ad7fc1-f79a-4ad8-8e02-26bbeaa86bbc7288010135864174970.pipeout
  File: ‘/tmp/hive/55ad7fc1-f79a-4ad8-8e02-26bbeaa86bbc7288010135864174970.pipeout’
  Size: 0         	Blocks: 0          IO Block: 4096   regular empty file
Device: fe09h/65033d	Inode: 678         Links: 1
Access: (0644/-rw-r--r--)  Uid: ( 1000/    hive)   Gid: ( 1002/  hadoop)
Access: 2017-12-19 04:53:49.126777260 -0600
Modify: 2017-12-19 04:53:49.126777260 -0600
Change: 2017-12-19 04:53:49.126777260 -0600
 Birth: -

# stat /tmp/b6dfbf9e-2f7c-4c25-95a1-73c44318ecf4_resources
  File: ‘/tmp/b6dfbf9e-2f7c-4c25-95a1-73c44318ecf4_resources’
  Size: 4096      	Blocks: 8          IO Block: 4096   directory
Device: fe09h/65033d	Inode: 668         Links: 2
Access: (0700/drwx------)  Uid: ( 1000/    hive)   Gid: ( 1002/  hadoop)
Access: 2017-12-19 04:57:38.458937635 -0600
Modify: 2017-12-19 04:53:49.062777216 -0600
Change: 2017-12-19 04:53:49.066777218 -0600
 Birth: -
{code}
Showing the large number:
{code:java}
# find /tmp/ -name '*_resources' | wc -l
68340
# find /tmp/hive -name ""*.pipeout"" | wc -l
51837
{code}",2018-01-17T09:49:21.314+0000,2019-05-21T04:12:44.400+0000,Incomplete,Major
ORC-569,Empty positions list in first row index entry,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12586388'>, <JIRA IssueLink: id='12585998'>]","When writing string columns with orc.dictionary.key.threshold = 0 the first row group's position list is empty. This results in:

{code}
java.lang.IndexOutOfBoundsException: Index: 0
{code}

when the application uses predicate push down and selects the first row group.",2019-11-08T23:22:58.619+0000,2020-04-21T00:10:11.168+0000,Fixed,Major
INFRA-17600,sync for hbase-site repo appears broken between gitbox <-> github and gitpubsub,INFRA,Bug,Closed,[],1,[<JIRA IssueLink: id='12551581'>],"Earlier today HBase moved to gitbox. While attempting to update our website to point to the new repo URLs, we've hit a snag where changes to the gitbox repo aren't showing up in the website. While investigating I noticed that those same commits aren't present in the github version of the repo.

hbase-site repo on gitbox:
https://gitbox.apache.org/repos/asf?p=hbase-site.git;a=summary

hbase-site repo on github:
https://github.com/apache/hbase-site/commits/asf-site

the latter is missing commits 3bda20fd2b811f4ae6c63cb78068de3ca21456a5 and 6ad2151f9a7fa3d1e6b493d489b347dfbbd39102.

if it helps at all wrt timing, this is the jenkins job that did the push:

https://builds.apache.org/view/H-L/view/HBase/job/hbase_generate_website/1552/console

and this is the published site I would expect to see updated as a result:

http://hbase.apache.org/source-repository.html",2019-01-09T20:59:17.596+0000,2019-01-11T13:22:51.964+0000,Fixed,Major
NIFI-5176,NiFi needs to be buildable on Java 11,NIFI,Sub-task,Resolved,[],11,"[<JIRA IssueLink: id='12557830'>, <JIRA IssueLink: id='12558025'>, <JIRA IssueLink: id='12564307'>, <JIRA IssueLink: id='12562125'>, <JIRA IssueLink: id='12558526'>, <JIRA IssueLink: id='12562531'>, <JIRA IssueLink: id='12566401'>, <JIRA IssueLink: id='12537715'>, <JIRA IssueLink: id='12566400'>, <JIRA IssueLink: id='12537713'>, <JIRA IssueLink: id='12566473'>]","While retaining a source/target comptability of 1.8, NiFi needs to be buildable on Java 11.

The following issues have been encountered while attempting to run a Java 1.8-built NiFi on Java 11:
||Issue||Solution||
|groovy-eclipse-compiler not working with Java 10|-Switched to gmaven-plus- Updated to maven-compiler-plugin:3.8.1, groovy-eclipse-compiler:3.4.0-01, and groovy-eclipse-batch:2.5.4-01 (See NIFI-5254 for reference)|
|Antler3 issue with ambiguous method calls|Explicit cast to ValidationContext needed in TestHL7Query.java|
|jaxb2-maven-plugin not compatible with Java 9|Switched to maven-jaxb-plugin|
|-nifi-enrich-processors uses package com.sun.jndi.dns, which does not exist-|-Required addition of- -add-modules=jdk.naming.dns --add-exports=jdk.naming.dns/com.sun.jndi.dns=ALL-UNNAMED, which prevents the usage of compiling with the --release option (to compile only against public APIs in the JDK) from being used. Not an optimal solution.-|
|groovy-eclipse-batch:2.4.13-01 could not find JDK base classes|Updated to groovy-eclipse-batch:2.5.4-01 and groovy-all:2.5.4 (See NIFI-5254 for reference)|
|maven-surefire-plugin:2.20.1 throws null pointer exceptions|Updated to maven-surefire-plugin:2.22.0|
|okhttp client builder requires X509TrustManager on Java 9+|Added methods to return TrustManager instances with the SSLContext created by SSLContextFactory and updated HttpNotificationService to use the paired TrustManager|
|nifi-runtime groovy tests aren't running|Added usage of build-helper-maven-plugin to explicitly add src/test/groovy to force groovy compilation of test sources. groovy-eclipse-compiler skips src/test/groovy if src/test/java doesn't exist, which is the case for nifi-runtime. (See NIFI-5341 for reference)|
|hbase-client depends on jdk.tools:jdk.tools|Excluded this dependency {color:#f79232}*(needs live testing)* {color}|
|HBase client 1.1.2 does not allow running on Java 9+|Updated to HBase client 1.1.11, passes unit tests (See HBASE-17944 for reference) *{color:#f79232}(needs live testing){color}*|
|powermock:1.6.5 does not support Java 10|Updated to powermock:2.0.2 and mockito:2.28.2 (See NIFI-6360 for reference)|
|com.yammer.metrics:metrics-core:2.2.0 does not support Java 10|Upgrading com.yammer.metrics:metrics-core:2.2.0 to io.dropwizard.metrics:metrics-jvm:4.0.0 (See NIFI-5373 for reference)|",2018-05-08T18:37:03.232+0000,2019-08-15T17:29:17.331+0000,Fixed,Major
IMPALA-9408,Execute the TIMESTAMP roadmap,IMPALA,Task,Open,[],3,"[<JIRA IssueLink: id='12605365'>, <JIRA IssueLink: id='12605363'>, <JIRA IssueLink: id='12605364'>]","This issue is intended for tracking the addition and/or alteration of different TIMESTAMP types in order to eventually reach the desired state as specified in the [design doc|https://docs.google.com/document/d/1gNRww9mZJcHvUDCXklzjFEQGpefsuR_akCDfWsdE35Q/edit] for TIMESTAMP types.

It's a sister issue to HIVE-21348 & SPARK-30905 - I found no comparable issue for Impala (and I was hoping to find out the status of this roadmap on Impala-side) - and related to IMPALA-5049.",2020-02-20T19:18:51.369+0000,2020-12-22T20:53:10.029+0000,,Major
CALCITE-1657,Release Calcite 1.12.0,CALCITE,Bug,Closed,[],9,"[<JIRA IssueLink: id='12496785'>, <JIRA IssueLink: id='12498191'>, <JIRA IssueLink: id='12495588'>, <JIRA IssueLink: id='12495460'>, <JIRA IssueLink: id='12495718'>, <JIRA IssueLink: id='12495719'>, <JIRA IssueLink: id='12495457'>, <JIRA IssueLink: id='12495459'>, <JIRA IssueLink: id='12495458'>]","Release Apache Calcite 1.12.0. The plan is to release in TBD, target code freeze on TBD.",2017-02-24T18:08:27.376+0000,2017-03-24T03:20:17.176+0000,Fixed,Major
TEZ-3077,TezClient.waitTillReady should support timeout,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12455558'>],Also preWarm.,2016-01-26T23:54:10.189+0000,2016-07-09T19:16:08.165+0000,Fixed,Major
TEZ-3462,Task attempt failure during container shutdown loses useful container diagnostics,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12482159'>],"When a nodemanager kills a task attempt due to excessive memory usage it will send a SIGTERM followed by a SIGKILL.  It also sends a useful diagnostic message with the container completion event to the RM which will eventually make it to the AM on a subsequent heartbeat.

However if the JVM shutdown processing causes an error in the task (e.g.: filesystem being closed by shutdown hook) then the task attempt can report a failure before the useful NM diagnostic makes it to the AM.  The AM then records some other error as the task failure reason, and by the time the container completion status makes it to the AM it does not associate that error with the task attempt and the useful information is lost.",2016-10-06T13:57:24.255+0000,2017-03-14T03:49:54.841+0000,Fixed,Major
INFRA-23096,Several hbase nodes are full again,INFRA,Task,Closed,[],2,"[<JIRA IssueLink: id='12638449'>, <JIRA IssueLink: id='12637296'>]","hbase2, hbase3, hbase4, hbase7, hbase9

This time it is /home instead of /tmp.

I tried to run du through the script console but it takes too long...

Please helpe checking if it is still some very big stderr or stdout file generated by surefire plugin, as we relocate them from /tmp to the workspace.

Thanks.",2022-04-06T00:15:23.289+0000,2022-04-21T16:18:01.056+0000,Fixed,Major
MJAVADOC-444,Add an 'aggregated-no-fork' goal,MJAVADOC,Improvement,Closed,[],1,[<JIRA IssueLink: id='12556499'>],Currently you can call maven-javadoc-plugin via {{mvn clean package javadoc:aggregate}} which results in deleting all previously created artifacts in {{target}} folder. So it would be helpful having a separate goal without forking the life cycle.,2016-01-12T12:09:00.372+0000,2019-03-11T18:56:19.861+0000,Fixed,Critical
SPARK-24506,Spark.ui.filters not applied to /sqlserver/ url,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12536823'>],"With Spark.ui.filters applied, the web ui's for master/history/worker/storage/executors/stages.etc prompt for http auth but /sqlserver/ tab is not prompting for http auth",2018-06-09T13:20:11.412+0000,2018-06-21T12:49:32.302+0000,Fixed,Major
TEZ-3675,Handle changes to ResourceCalculatorProcessTree in YARN-3427 for Hadoop 3.x,TEZ,Task,Closed,[],1,[<JIRA IssueLink: id='12499376'>],"Removing some APIs. We'll likely need to shim this class.
Ideally, create the shim for 2.6 to use the old methods, and everything else uses the new methods.

Alternately, change the min hadoop version to 2.7",2017-03-29T19:49:04.378+0000,2017-08-22T00:02:57.613+0000,Fixed,Major
KYLIN-2788,HFile is not written to S3,KYLIN,Bug,Closed,[],1,[<JIRA IssueLink: id='12515912'>],"I set kylin.hbase.cluster.fs to s3 bucket where hbase lives.

Step ""Convert Cuboid Data to HFile"" finished without errors. Statistics at the end of the job said that it has written lot's of data to s3.

But there is no hfiles in kylin_metadata folder (kylin_metadata /kylin-1e436685-7102-4621-a4cb-6472b866126d/<table name>/hfile), but only _temporary folder and _SUCCESS file.

_temporary contains hfiles inside attempt folders. it looks like there were not copied from _temporary to result dir. But there is no errors neither in kylin log, nor in reducers' logs.

Then loading empty hfiles produces empty segments.",2017-08-14T06:19:19.484+0000,2017-11-03T16:48:56.646+0000,Fixed,Major
YETUS-913,Docker mode pre-commit can't use JAVA_HOME defined in dockerfile,YETUS,Bug,Open,[],1,[<JIRA IssueLink: id='12569896'>],"Over in HBase we use different JDKs depending on the branch, due to our compatibility promises in major release lines. For test-patch runs, this changes the set of JDKs we install in each branch's Dockerfile.

While recently trying to chase down why we were getting JDK8 used to test a branch that's JDK7+, I realized that the docker bootstrapping _always_ overwrites the JAVA_HOME in the image with JAVA_HOME on the host. This includes when the host doesn't define a JAVA_HOME, effectively preventing a JAVA_HOME set by the image (either via ENV or by installing some package that sets it).

Workaround:

You can prevent this by replacing the {{docker_do_env_adds}} function in your personality. e.g. this version will only set JAVA_HOME in the docker instance if the host defines it. We then make sure all of our uses of test-patch unset JAVA_HOME before invocation.

{code}

# work around yetus overwriting JAVA_HOME from our docker image
function docker_do_env_adds
{
  declare k

  for k in ""${DOCKER_EXTRAENVS[@]}""; do
    if [[ ""JAVA_HOME"" == ""${k}"" ]]; then
      if [ -n ""${JAVA_HOME}"" ]; then
        DOCKER_EXTRAARGS+=(""--env=JAVA_HOME=${JAVA_HOME}"")
      fi
    else
      DOCKER_EXTRAARGS+=(""--env=${k}=${!k}"")
    fi
  done
}
{code}",2019-09-15T20:09:26.764+0000,2019-09-16T13:43:43.495+0000,,Major
AVRO-544,Allow the HttpServer to serve forever without a call to Thread.sleep(),AVRO,New Feature,Closed,[],2,"[<JIRA IssueLink: id='12332872'>, <JIRA IssueLink: id='12332435'>]",One way would be to expose the join() method on the HttpServer: http://jetty.codehaus.org/jetty/jetty-6/apidocs/org/mortbay/jetty/Server.html#join%28%29,2010-05-18T02:32:18.470+0000,2010-09-08T21:07:51.649+0000,Fixed,Major
CALCITE-1446,"Support INTERSECT, EXCEPT, MINUS, each with DISTINCT, ALL options",CALCITE,New Feature,Open,"[<JIRA Issue: key='CALCITE-1447', id='13013028'>, <JIRA Issue: key='CALCITE-1448', id='13013029'>, <JIRA Issue: key='CALCITE-1449', id='13013030'>, <JIRA Issue: key='CALCITE-1450', id='13013031'>, <JIRA Issue: key='CALCITE-1451', id='13013032'>, <JIRA Issue: key='CALCITE-1452', id='13013033'>]",3,"[<JIRA IssueLink: id='12483859'>, <JIRA IssueLink: id='12483390'>, <JIRA IssueLink: id='12483391'>]",,2016-10-17T23:19:55.512+0000,2019-02-22T01:19:33.707+0000,,Major
AVRO-1235,Avro does not handle corrupt records,AVRO,Bug,Open,[],1,[<JIRA IssueLink: id='12363382'>],"As per PIG-3015 (see https://issues.apache.org/jira/browse/PIG-3015?focusedCommentId=13547011&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13547011), Avro is not handling corrupt records.

This could be a serious issue.",2013-01-21T23:59:35.954+0000,2020-06-01T17:01:49.341+0000,,Critical
TEZ-2852,TestVertexImpl fails due to race in AsyncDispatcher,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12479586'>, <JIRA IssueLink: id='12476178'>]","https://builds.apache.org/job/Tez-Build-Hadoop-2.4/176/console
{noformat}
Running org.apache.tez.dag.app.dag.impl.TestVertexImpl
Tests run: 90, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 5.632 sec <<< FAILURE!
testVertexTaskAttemptOutputFailure(org.apache.tez.dag.app.dag.impl.TestVertexImpl)  Time elapsed: 0.049 sec  <<< FAILURE!
java.lang.AssertionError: expected:<OUTPUT_WRITE_ERROR> but was:<UNKNOWN_ERROR>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.tez.dag.app.dag.impl.TestVertexImpl.testVertexTaskAttemptOutputFailure(TestVertexImpl.java:3452)

{noformat}",2015-09-24T13:32:14.049+0000,2017-03-14T03:50:06.035+0000,Fixed,Critical
BIGTOP-1270,BigPetStore: Productionize the Hive portion,BIGTOP,New Feature,Resolved,[],4,"[<JIRA IssueLink: id='12391759'>, <JIRA IssueLink: id='12388734'>, <JIRA IssueLink: id='12389040'>, <JIRA IssueLink: id='12389039'>]","The hive portion of the BigPetStore blueprint app builds a ""view"" over the cleaned data that Mahout can then use to do product recommendations.

The hive code in bigpetstore only runs locally - lets add the necessary configuration hooks and/or (if we have to) externalize the hive script itself from java so that its easy to run directly on a cluster.

And lets actually run it on some kind of a cluster at scale.  The contract for the hive portion is an output file with three numbers like this: 

{noformat}
100 30021 1
100 212341 1
...
{noformat}

Signifying that customer=100 likes both of the products ""30021"" and ""212341"".  

",2014-04-16T10:13:16.346+0000,2014-07-18T02:49:42.273+0000,Won't Fix,Major
YETUS-934,Precommit does not accept a caller's value of MAVEN_ARGS,YETUS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12579894'>],"We're trying to increase parallelism within maven over on HBASE-23779. The patch doesn't appear to work, seems like Yetus doesn't propagate the value of {{MAVEN_ARGS}}.",2020-02-06T19:17:03.800+0000,2020-02-07T21:33:25.435+0000,Invalid,Major
PHOENIX-6523,Support for HBase Registry Implementations through Phoenix connection URL,PHOENIX,Improvement,Open,[],2,"[<JIRA IssueLink: id='12620440'>, <JIRA IssueLink: id='12620441'>]","https://issues.apache.org/jira/browse/HBASE-23305

https://issues.apache.org/jira/browse/HBASE-18095

 

HBase now supports a zookeeper-less connection strategy using a Master Registry implementation. 

 

For this to work, the client simply needs to set a list of <host:port>s of the HMaster quorum

 
{code:java}
<property>
   <name>hbase.masters</name>
   <value>master1:16000,master2:16001,master3:16000</value>
</property>
{code}
 

To support opting into this from a Phoenix connection URL, we can introduce a ""connector type"". We'll leverage the *+* char of [JDBC URL grammar|https://docs.oracle.com/cd/E17952_01/connector-j-8.0-en/connector-j-reference-jdbc-url-format.html] to specify the connection type. Connections will start to look something like this:
{code:java}
jdbc:phoenix+zk:hostname1,2,3...:<properties> 
jdbc:phoenix+hrpc:hostname1,2,3...:<properties>
jdbc:phoenix+bigtable:hostname1,2,3...:<properties>{code}
Above are examples of opting into hrpc/zk/bigtable registry implementations of HBase.

 

If no connector is specified, the driver will default to a Zookeeper based connection.

 

 

 ",2021-08-03T19:58:12.687+0000,2021-11-10T10:02:04.180+0000,,Major
FLINK-16809,Support setting CallerContext on YARN deployments,FLINK,Improvement,Open,[],5,"[<JIRA IssueLink: id='12584099'>, <JIRA IssueLink: id='12584100'>, <JIRA IssueLink: id='12584101'>, <JIRA IssueLink: id='12584102'>, <JIRA IssueLink: id='12627174'>]","Now the Spark and hive have the Call Context to meet the HDFS Job Audit requirement.

I think the flink also should to add it, and in our cluster the flink job may have big pressure to the HDFS, it's will be helpful to find the root job.

 ",2020-03-26T12:38:11.200+0000,2022-07-15T10:39:26.170+0000,,Not a Priority
TEZ-3090,"MRInput should make dagIdentifier, vertexIdentifier, etc available to the InputFormat jobConf",TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12456474'>],,2016-02-02T02:16:23.672+0000,2016-05-18T04:57:44.781+0000,Fixed,Major
SPARK-17355,Work around exception thrown by HiveResultSetMetaData.isSigned,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12479263'>],"Attempting to use Spark SQL's JDBC data source against the Hive ThriftServer results in a {{java.sql.SQLException: Method not supported}} exception from {{org.apache.hive.jdbc.HiveResultSetMetaData.isSigned}}. Here are two user reports of this issue:

- https://stackoverflow.com/questions/34067686/spark-1-5-1-not-working-with-hive-jdbc-1-2-0
- https://stackoverflow.com/questions/32195946/method-not-supported-in-spark

I have filed HIVE-14684 to attempt to fix this in Hive by implementing the {{isSigned}} method, but in the meantime / for compatibility with older JDBC drivers I think we should add special-case error handling to work around this bug.",2016-09-01T05:48:57.604+0000,2016-09-01T23:46:13.521+0000,Fixed,Major
SPARK-11424,Guard against MAPREDUCE-5918 by ensuring RecordReader is only closed once in *HadoopRDD,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12447493'>],"MAPREDUCE-5918 is a bug where an instance of a decompressor ends up getting placed into a pool multiple times. Since the pool is backed by a list instead of a set, this can lead to the same decompressor being used in multiple threads or places at the same time, which is not safe because those decompressors will overwrite each other's buffers. Sometimes this buffer sharing will lead to exceptions but other times it will just result in invalid / garbled results.

That Hadoop bug is fixed in Hadoop 2.7 but is still present in many Hadoop versions that we wish to support. As a result, I think that we should try to work around this issue in Spark via defensive programming to prevent RecordReaders from being closed multiple times.

So far, I've had a hard time coming up with explanations of exactly how double-close()s occur in practice, but I do have a couple of explanations that work on paper.

For instance, it looks like https://github.com/apache/spark/pull/7424, added in 1.5, introduces at least one extremely-rare corner-case path where Spark could double-close() a LineRecordReader instance in a way that triggers the bug. Here are the steps involved in the bad execution that I brainstormed up:

* The task has finished reading input, so we call close(): https://github.com/apache/spark/blob/v1.5.1/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala#L168.
* While handling the close call and trying to close the reader, reader.close() throws an exception: https://github.com/apache/spark/blob/v1.5.1/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala#L190
* We don't set reader = null after handling this exception, so the TaskCompletionListener also ends up calling NewHadoopRDD.close(), which, in turn, closes the record reader again: https://github.com/apache/spark/blob/v1.5.1/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala#L156

In this hypothetical situation, LineRecordReader.close() could fail with an exception if its InputStream failed to close: https://github.com/apache/hadoop/blob/release-1.2.1/src/mapred/org/apache/hadoop/mapred/LineRecordReader.java#L212
I googled for ""Exception in RecordReader.close()"" and it looks like it's possible for a closed Hadoop FileSystem to trigger an error there:

* https://spark-project.atlassian.net/browse/SPARK-757
* https://issues.apache.org/jira/browse/SPARK-2491
* http://apache-spark-user-list.1001560.n3.nabble.com/Any-issues-with-repartition-td13462.html

Looking at https://issues.apache.org/jira/browse/SPARK-3052, it seems like it's possible to get spurious exceptions there when there is an error reading from Hadoop. If the Hadoop FileSystem were to get into an error state _right_ after reading the last record then it looks like we could hit the bug here in 1.5. Again, this might be really unlikely but we should modify Spark's code so that we can 100% rule it out.

*TL;DR:* We can rule out one rare but potential cause of stream corruption via defensive programming.
",2015-10-30T18:15:52.438+0000,2015-11-03T22:19:07.498+0000,Fixed,Critical
TEZ-2346,TEZ-UI: Lazy load other info / counter data,TEZ,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12422157'>, <JIRA IssueLink: id='12421876'>, <JIRA IssueLink: id='12447354'>]",,2015-04-20T18:27:11.277+0000,2016-05-18T04:55:31.143+0000,Fixed,Critical
SPARK-27881,Support CAST (... FORMAT <pattern>) expression,SPARK,Improvement,In Progress,[],2,"[<JIRA IssueLink: id='12562767'>, <JIRA IssueLink: id='12562766'>]","SQL:2016 standard supports the following Cast functions
{noformat}
CAST(<datetime> AS <char string type> [FORMAT <template>])
CAST(<char string> AS <datetime type> [FORMAT <template>])
{noformat}
to specify patterns to use during cast.

Oracle DB 18 already supports a similar formatting option during cast: [https://docs.oracle.com/en/database/oracle/oracle-database/18/sqlrf/CAST.html]

There is an initiative in Impala: IMPALA-4018 and Hive HIVE-21575 to support.
 (Please note that accepting the {{CAST (... FORMAT <pattern>)}} is just one part of these tickets, the other part regarding patterns is covered here: SPARK-27882.)",2019-05-30T13:24:37.227+0000,2020-03-16T22:52:08.530+0000,,Minor
HCATALOG-500,HCatStorer should honor user-specified path for external tables ,HCATALOG,Improvement,Closed,[],1,[<JIRA IssueLink: id='12373490'>],"For the partitions created by HCatStorer, currently the output path format is fixed to ""table_root/part_col_name1=part_col_val1/part_col_name2=part_col_val2"". In our use case, we need HCatStorer to write to a different directory structure to be backward compatible. Therefore, we want HCatStorer be able to write to a external location if the user application specifies so.",2012-09-10T22:45:09.546+0000,2013-08-06T20:08:15.544+0000,Fixed,Major
SQOOP-1172,Make Sqoop compatible with HBase 0.95+,SQOOP,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12373778'>],We should prepare Sqoop to be compatible with upcoming new major release of HBase (that is currently available in version 0.95). I've did preliminary investigation and it seems that there is no change in API that would break Sqoop. Entire HBase has been however broken from one monolithic archive into multiple smaller pieces that our build will have to take into consideration.,2013-08-12T20:39:34.606+0000,2013-08-13T19:34:30.995+0000,Fixed,Major
THRIFT-1736,Visual Studio top level project files within msvc,THRIFT,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12359537'>, <JIRA IssueLink: id='12359536'>, <JIRA IssueLink: id='12359535'>]","improve visual studio support by introducing a top level solution file within top level msvc folder.

see libusb for a good example:
http://www.libusb.org/browser/libusb/msvc
",2012-10-20T19:55:56.132+0000,2016-02-19T02:17:44.781+0000,Won't Fix,Major
TEZ-1264,Support for limiting output records in OnFileSortedOutput,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12391013'>]," When we are limiting on unsorted output, we can stop after reaching the count in the Processor. But if limiting has to be done on sorted output in map phase it is not possible as sorting is done by OnFileSortedOutput. If limiting was supported as part of the output, then we can limit records before writing to each part file after Partitioner is applied.

",2014-07-07T22:27:33.165+0000,2014-12-09T03:37:03.050+0000,,Major
YETUS-561,Ability to limit user process counts and Docker container's RAM usage,YETUS,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12518543'>, <JIRA IssueLink: id='12518487'>, <JIRA IssueLink: id='12518738'>]","Hadoop is blowing up nodes due to unit tests that consume all of RAM.  In an attempt to keep nodes alive, Yetus needs the ability to put an upper limit on the amount that a Docker container can use.",2017-10-24T15:15:07.075+0000,2018-01-17T16:12:20.814+0000,Fixed,Critical
IMPALA-5515,Implement SHOW TBLPROPERTIES,IMPALA,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12506660'>],[HIVE-2530],2017-06-15T14:07:24.509+0000,2017-08-21T17:20:02.905+0000,Won't Do,Minor
TEZ-3531,Tez UI: All Queries table: Improve searchability,TEZ,Bug,Closed,[],4,"[<JIRA IssueLink: id='12490293'>, <JIRA IssueLink: id='12485897'>, <JIRA IssueLink: id='12490130'>, <JIRA IssueLink: id='12487274'>]","- Improve searchability - Add Queue Name, DAG ID, App ID, Tabled Read, Tables Written, OperationID, Queue",2016-11-07T14:08:27.661+0000,2017-08-22T00:02:25.562+0000,Fixed,Major
TEZ-2144,Compressing MRInput Split Distributor payload,TEZ,Improvement,Open,[],2,"[<JIRA IssueLink: id='12424597'>, <JIRA IssueLink: id='12409609'>]","Pig sets the input split information in user payload and when running against a table with 10s of 1000s of partitions, DAG submission fails with 

java.io.IOException: Requested data length 305844060 is longer than maximum
configured RPC length 67108864",2015-02-25T21:40:10.188+0000,2015-05-13T07:36:08.002+0000,,Major
SPARK-11157,Allow Spark to be built without assemblies,SPARK,Umbrella,Resolved,"[<JIRA Issue: key='SPARK-13294', id='12938677'>, <JIRA Issue: key='SPARK-13575', id='12945602'>, <JIRA Issue: key='SPARK-13576', id='12945603'>, <JIRA Issue: key='SPARK-13577', id='12945604'>, <JIRA Issue: key='SPARK-13578', id='12945606'>, <JIRA Issue: key='SPARK-13808', id='12948965'>, <JIRA Issue: key='SPARK-13579', id='12945607'>]",3,"[<JIRA IssueLink: id='12456124'>, <JIRA IssueLink: id='12487651'>, <JIRA IssueLink: id='12463815'>]","For reasoning, discussion of pros and cons, and other more detailed information, please see attached doc.

The idea is to be able to build a Spark distribution that has just a directory full of jars instead of the huge assembly files we currently have.

Getting there requires changes in a bunch of places, I'll try to list the ones I identified in the document, in the order that I think would be needed to not break things:

* make streaming backends not be assemblies

Since people may depend on the current assembly artifacts in their deployments, we can't really remove them; but we can make them be dummy jars and rely on dependency resolution to download all the jars.

PySpark tests would also need some tweaking here.

* make examples jar not be an assembly

Probably requires tweaks to the {{run-example}} script. The location of the examples jar would have to change (it won't be able to live in the same place as the main Spark jars anymore).

* update YARN backend to handle a directory full of jars when launching apps

Currently YARN localizes the Spark assembly (depending on the user configuration); it needs to be modified so that it can localize all needed libraries instead of a single jar.

* Modify launcher library to handle the jars directory

This should be trivial

* Modify {{assembly/pom.xml}} to generate assembly or a {{libs}} directory depending on which profile is enabled.

We should keep the option to build with the assembly on by default, for backwards compatibility, to give people time to prepare.

Filing this bug as an umbrella; please file sub-tasks if you plan to work on a specific part of the issue.",2015-10-16T20:16:01.443+0000,2016-11-29T07:30:51.699+0000,Fixed,Major
TEZ-3714,Tez UI: Hive Queries page: Use Dag ID and App ID if they are published form Hive side.,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12503355'>],"As of now Hive Queries pages does a reverse lookup to display DAG and App IDs. i.e. For each query, the UI looks for DAGs where callerId = hive query id.
This change prevents this ATS fetch when the data is available.",2017-05-08T15:23:25.378+0000,2017-08-22T00:03:02.638+0000,Fixed,Major
HCATALOG-551,Provide ability to overwrite partitions when using HCatStorer,HCATALOG,New Feature,Open,[],1,[<JIRA IssueLink: id='12390948'>],"Right now when you use HCatStorer to write data to a partition that already exists it results in  partition already exists error.

It would be great if you could provide a argument to the HCatStorer so that it overwrites the partition data if it already exists instead of throwing an error.
",2012-11-13T19:40:53.483+0000,2014-07-07T05:18:26.707+0000,,Major
PARQUET-1191,Type.hashCode() takes originalType into account but Type.equals() does not,PARQUET,Bug,Resolved,[],1,[<JIRA IssueLink: id='12525351'>],"Taking originalType into account in Type.hashCode() but ignoring it in Type.equals() is inconsistent and violates hashCode-equals contract.

If two Type instances that are equal according to equals() but have different logical types were ever put in a hash map, then both of the following cases are possible:

* The two instances may accidentally have the same hash, which is consistent with the equals() method but is pure coincidence and has a very low probablility.
* The two instances may have different hashes and end up in different buckets of a hash map, leading to a situation where we can't find a value in the hash map despite that it's equals() would return true.

We should decide whether originalType is needed for an equality check or not. If it is, then it should be added to equals(). Otherwise it should be removed from hashCode().",2018-01-10T14:43:42.449+0000,2018-01-26T09:48:40.648+0000,Fixed,Major
OOZIE-2723,JSON.org license is now CatX,OOZIE,Bug,Closed,[],1,[<JIRA IssueLink: id='12485996'>],"per [update resolved legal|http://www.apache.org/legal/resolved.html#json]:

{quote}
CAN APACHE PRODUCTS INCLUDE WORKS LICENSED UNDER THE JSON LICENSE?

No. As of 2016-11-03 this has been moved to the 'Category X' license list. Prior to this, use of the JSON Java library was allowed. See Debian's page for a list of alternatives.
{quote}

Looks like this is on branch-4.3 and later. (maybe earlier if it was brought in transitively.",2016-11-04T03:25:42.149+0000,2016-12-02T21:05:10.388+0000,Fixed,Blocker
AVRO-720,Generated code for arrays should accept extensions of array content type,AVRO,Improvement,Open,[],3,"[<JIRA IssueLink: id='12336623'>, <JIRA IssueLink: id='12336621'>, <JIRA IssueLink: id='12336622'>]",Discovered when I upgraded a server from 1.3 to 1.4: trying to stuff a GenericData.Array<Utf8> object into a List<CharSequence> does not work; Avro should generate List<? implements CharSequence> instead. The same holds for other array types.,2010-12-28T21:08:27.562+0000,2011-01-07T21:49:05.509+0000,,Major
ZOOKEEPER-2260,Paginated getChildren call,ZOOKEEPER,New Feature,Patch Available,[],4,"[<JIRA IssueLink: id='12453231'>, <JIRA IssueLink: id='12451287'>, <JIRA IssueLink: id='12623376'>, <JIRA IssueLink: id='12453266'>]","Add pagination support to the getChildren() call, allowing clients to iterate over children N at the time.

Motivations for this include:
  - Getting out of a situation where so many children were created that listing them exceeded the network buffer sizes (making it impossible to recover by deleting)[1]
 - More efficient traversal of nodes with large number of children [2]

I do have a patch (for 3.4.6) we've been using successfully for a while, but I suspect much more work is needed for this to be accepted. 


[1] https://issues.apache.org/jira/browse/ZOOKEEPER-272
[2] https://issues.apache.org/jira/browse/ZOOKEEPER-282",2015-08-28T00:29:18.731+0000,2022-02-03T08:36:24.117+0000,,Major
SLIDER-1143,"Clean Intermediate data (generated while apps running),when destry the app",SLIDER,Bug,Open,[],1,[<JIRA IssueLink: id='12496707'>],"When create and run the apps,eg,hbase,storm,etc, We will create a zk directory to support the service woriking. But if we want to destroy the unused apps,we should destroy the directroy or data generated when running.

For example:
1)create and start a storm service, created a zk path to hold the metadata
[zk: localhost:2181(CONNECTED) 2] ls /
[services, registry, storm]
zk: localhost:2181(CONNECTED) 1] ls /storm
[workerbeats, errors, supervisors, storms, assignments]

2) destroy the stopped storm
slider destroy demo-storm --foce

[zk: localhost:2181(CONNECTED) 1] ls /storm
[workerbeats, errors, supervisors, storms, assignments]

3) the zk path not deleted when destroy it
",2016-06-17T03:01:44.554+0000,2017-03-09T03:33:50.870+0000,,Minor
RANGER-2183,Use INodeAttribute information to authorize HDFS access,RANGER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12540797'>],"Currently, Ranger HDFS authorizer uses INodes provided in the authorization API to get the path to the accessed HDFS resource. Such path may not be correct in some cases, notably, if the accessed directory's parent directory is snapshot-enabled, and then parent directory and its contents are removed.",2018-08-04T18:28:32.569+0000,2018-09-27T04:49:28.899+0000,Fixed,Major
RANGER-2432,Upgrade Hadoop Version to 3.1.1 ,RANGER,Improvement,Resolved,"[<JIRA Issue: key='RANGER-2450', id='13236163'>]",2,"[<JIRA IssueLink: id='12560801'>, <JIRA IssueLink: id='12561321'>]",Proposal to upgrade Hadoop version to 3.1.1,2019-05-15T13:07:39.358+0000,2019-08-05T20:22:39.461+0000,Fixed,Major
OOZIE-654,Provide a way to use 'uber' jars with Oozie MR actions,OOZIE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12421544'>, <JIRA IssueLink: id='12354877'>]","Right now, say if you have a custom MR code in a jar that has a {{lib/}} folder inside which carries more dependent jars (a structure known as 'uber' jars), and you submit the job via a regular 'hadoop jar' command, these lib/*.jars get picked up by the framework because the supplied jar is specified explicitly via conf.setJarByClass or conf.setJar. That is, if this user uber jar goes to the JT as the mapred.jar, then  it is handled by the framework properly and the lib/*.jars are all considered and placed on the classpath.

Distributed cache jars do not have this effect, and that is cause the MR framework does not consider them as uber jars and does not extract and use their internal lib/ directories.

We should have a way in oozie to let users promote one of their jars as uber jars, as an option.

Proposal: Have an optional oozie-prefixed config, or an optional element in the MR action XML, that lets user specify what class should be loaded to be set as setJarByClass(...). This will have to be a class available in the higher level of the uber jar (not under lib/) but can be any class inside the targeted jar really (just not from a jar under lib/). We then set this as jobConf.setJarByClass(loadedCls), and then run the job.

Thoughts?",2012-01-17T22:34:15.637+0000,2015-04-16T18:51:04.625+0000,Fixed,Minor
HCATALOG-163,NPE when illustrating a relation,HCATALOG,Bug,Resolved,[],1,[<JIRA IssueLink: id='12363420'>],"It seems that outputSchema hasn't been initialized while illustrating a relation in pig like below
{noformat}
register lib/hive-common-0.7.1.jar;                                         
register lib/hive-metastore-0.7.1.jar;
register lib/libfb303.jar; 
register lib/libthrift.jar;      
register lib/hive-serde-0.7.1.jar; 
register lib/hcatalog-0.2.0.jar;
A = LOAD 'orders' USING org.apache.hcatalog.pig.HCatLoader();
ILLUSTRATE A;
{noformat}

so NPE would throw
{noformat}
java.lang.NullPointerException
        at org.apache.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:274)
        at org.apache.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:238)
        at org.apache.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:61)
        at org.apache.pig.impl.io.ReadToEndLoader.getNextHelper(ReadToEndLoader.java:210)
        at org.apache.pig.impl.io.ReadToEndLoader.getNext(ReadToEndLoader.java:190)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNext(POLoad.java:129)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:267)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:194)
        at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:257)
        at org.apache.pig.pen.ExampleGenerator.readBaseData(ExampleGenerator.java:222)
        at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:154)
        at org.apache.pig.PigServer.getExamples(PigServer.java:1245)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:165)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)
{noformat}

",2011-11-18T07:30:59.495+0000,2013-03-25T19:03:59.151+0000,Fixed,Major
CALCITE-814,RexBuilder reverses precision and scale of DECIMAL literal,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12432088'>],"RexBuilder L850
{code}
 typeFactory.createSqlType(
              SqlTypeName.DECIMAL, scale, precision);
{code}
However, the createSqlType is accepting
{code}
RelDataType createSqlType(
      SqlTypeName typeName,
      int precision,
      int scale);
{code}
Need to swap the precision and scale",2015-07-24T01:13:25.383+0000,2015-09-01T03:02:23.547+0000,Fixed,Major
GORA-111,Clean up build log output to remove numerous stack traces ,GORA,Bug,Closed,[],1,[<JIRA IssueLink: id='12350235'>],"We have many many stack traces which appear on when Gora is built locally and in Jenkins. Although these are not making the build fail, we should get round to removing them if possible.",2012-04-05T19:50:19.586+0000,2014-07-01T14:03:07.303+0000,Not A Problem,Major
BIGTOP-418,Package MAPREDUCE-2858 (MRv2 WebApp Security),BIGTOP,Bug,Closed,[],1,[<JIRA IssueLink: id='12348619'>],"A new server has been added to yarn. It is a web proxy that sits in front of the AM web UI. The server is controlled by the yarn.web-proxy.address config. If that config is set, and it points to an address that is different then the RM web interface then a separate proxy server needs to be launched.

This can be done by running

yarn-daemon.sh start proxyserver

If a separate proxy server is needed other configs also may need to be set, if security is enabled. 
yarn.web-proxy.principal 
yarn.web-proxy.keytab

The proxy server is stateless and should be able to support a VIP or other load balancing sitting in front of multiple instances of this server.",2012-02-24T17:01:45.633+0000,2013-06-21T23:55:19.979+0000,Fixed,Major
OOZIE-1964,Hive Server 2 action doesn't return Hadoop Job IDs,OOZIE,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12409937'>],"Beeline currently doesn't support getting the Hadoop Job IDs for jobs launched by Hive Server 2.  When/If Beeline ever adds support for this, we should update the Hive Server 2 action to parse the IDs and return them back to the Oozie server like most actions do.",2014-08-05T20:26:14.403+0000,2015-05-18T07:11:13.114+0000,Fixed,Major
YETUS-369,Allow setting multijdk by branch; e.g. jdk8 for master branch but jdk7 for older branches ,YETUS,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12463222'>],"Over in hbase, we'd like to have our master branch be jdk8 only but for older branches from which we are still making releases, we'd like to default jdk7. Mighty [~busbey] notes that this is not possible currently in yetus so am filing that I foresee projects needing this facility of yovely yetus.",2016-04-08T18:12:20.296+0000,2022-05-06T18:19:58.493+0000,Won't Fix,Major
IMPALA-8525,preads should use hdfsPreadFully rather than hdfsPread,IMPALA,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12584923'>, <JIRA IssueLink: id='12572134'>, <JIRA IssueLink: id='12569583'>]","Impala preads (only enabled if {{use_hdfs_pread}} is true) use the {{hdfsPread}} API from libhdfs, which ultimately invokes {{PositionedReadable#read(long position, byte[] buffer, int offset, int length)}} in the HDFS-client.

{{PositionedReadable}} also exposes the method {{readFully(long position, byte[] buffer, int offset, int length)}}. The difference is that {{#read}} will ""Read up to the specified number of bytes"" whereas {{#readFully}} will ""Read the specified number of bytes"". So there is no guarantee that {{#read}} will read *all* of the request bytes.

Impala calls {{hdfsPread}} inside {{hdfs-file-reader.cc}} and invokes it inside a while loop until all the requested bytes have been read from the file. This can cause a few performance issues:

(1) if the underlying {{FileSystem}} does not support ByteBuffer reads (HDFS-2834) (e.g. S3A does not support this feature) then {{hdfsPread}} will allocate a Java array equal in size to specified length of the buffer; the call to {{PositionedReadable#read}} may only fill up the buffer partially; Impala will repeat the call to {{hdfsPread}} since the buffer was not filled, which will cause another large array allocation; this can result in a lot of wasted time doing unnecessary array allocations

(2) given that Impala calls {{hdfsPread}} in a while loop, there is no point in continuously calling {{hdfsPread}} when a single call to {{hdfsPreadFully}} will achieve the same thing (this doesn't actually affect performance much, but is unnecessary)

Prior solutions to this problem have been to introduce a ""chunk-size"" to Impala reads (https://gerrit.cloudera.org/#/c/63/ - S3: DiskIoMgr related changes for S3). However, with the migration to {{hdfsPreadFully}} the chunk-size is no longer necessary.

Furthermore, preads are most effective when the data is read all at once (e.g. in 8 MB chunks as specified by {{read_size}}) rather than in smaller chunks (typically 128K). For example, {{DFSInputStream#read(long position, byte[] buffer, int offset, int length)}} opens up remote block readers with a byte range determined by the value of {{length}} passed into the {{#read}} call. Similarly, {{S3AInputStream#readFully}} will issue an HTTP GET request with the size of the read specified by the given {{length}} (although fadvise must be set to RANDOM for this to work).

This work is dependent on exposing {{readFully}} via libhdfs first: HDFS-14564",2019-05-08T15:42:57.523+0000,2020-10-02T00:12:54.184+0000,Fixed,Major
SPARK-7953,Spark should cleanup output dir if job fails,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12426199'>],"MR calls abortTask and abortJob on the {{OutputCommitter}} to clean up the temporary output directories, but Spark doesn't seem to be doing that (when outputting an RDD to a Hadoop FS)

For example: {{PairRDDFunctions.saveAsNewAPIHadoopDataset}} should call {{committer.abortTask(hadoopContext)}} in the finally block inside the writeShard closure. And also {{jobCommitter.abortJob(jobTaskContext, JobStatus.State.FAILED)}} should be called if the job fails.

Additionally, MR removes the output dir if job fails, but Spark doesn't.",2015-05-29T20:05:46.793+0000,2019-05-21T04:13:45.301+0000,Incomplete,Major
CASSANDRA-913,Add Hive support,CASSANDRA,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12332745'>],"http://hadoop.apache.org/hive/ is a project that runs SQL queries against Hadoop map/reduce clusters.  (For analytics; it is too high-latency to run applications against Hive directly).  HIVE-705 added support for backends other than HDFS, with HBase as the first.  Cassandra support should be doable too now.

The Hive storage backends are described in http://wiki.apache.org/hadoop/Hive/StorageHandlers and the HBase backend specifically in http://wiki.apache.org/hadoop/Hive/HBaseIntegration.

I also note that John Sichi, author of the HBase backend, seems like a helpful guy and I imagine would be totally cool with answering questions about implementation details.",2010-03-23T03:55:04.798+0000,2019-04-16T09:33:26.387+0000,Duplicate,Normal
TEZ-1145,Vertices should not start if they have uninitialized custom edges,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12388950'>],"If the vertex is connected to a custom edge and the edge manager has not been set yet, then that vertex should not start. If it does then it will end up starting tasks that dont have all their specifications identified.",2014-05-22T16:48:19.290+0000,2014-09-06T01:36:01.165+0000,Fixed,Major
AMBARI-4255,HBase GC logs can overwrite each other,AMBARI,Bug,Open,[],1,[<JIRA IssueLink: id='12380892'>],"By specifying an explicit gc log file, the two hbase services running on the same host can potentially clobber each other's log files. HBase's launch scripts already create uniquely named log files per process and handle their rotation. The string {{""-Xloggc:<FILE-PATH>""}} is replaced with an appropriately constructed path.

See the replacement logic: https://github.com/apache/hbase/blob/trunk/bin/hbase-daemon.sh#L156-L158",2014-01-09T23:05:27.144+0000,2014-01-09T23:06:33.911+0000,,Major
CALCITE-742,RelFieldTrimmer throws NoSuchElementException in some cases,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12454015'>, <JIRA IssueLink: id='12425745'>, <JIRA IssueLink: id='12446915'>]","RelFieldTrimmer runs into NoSuchElementException in some cases.

Stack trace:
{noformat}
Exception in thread ""main"" java.lang.AssertionError: Internal error: While invoking method 'public org.apache.calcite.sql2rel.RelFieldTrimmer$TrimResult org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(org.apache.calcite.rel.core.Sort,org.apache.calcite.util.ImmutableBitSet,java.util.Set)'
	at org.apache.calcite.util.Util.newInternal(Util.java:743)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:543)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.dispatchTrimFields(RelFieldTrimmer.java:269)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trim(RelFieldTrimmer.java:175)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:947)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:820)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:768)
	at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:109)
	at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:730)
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:145)
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:105)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:607)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:244)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:536)
	... 32 more
Caused by: java.lang.AssertionError: Internal error: While invoking method 'public org.apache.calcite.sql2rel.RelFieldTrimmer$TrimResult org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(org.apache.calcite.rel.core.Sort,org.apache.calcite.util.ImmutableBitSet,java.util.Set)'
	at org.apache.calcite.util.Util.newInternal(Util.java:743)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:543)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.dispatchTrimFields(RelFieldTrimmer.java:269)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimChild(RelFieldTrimmer.java:210)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(RelFieldTrimmer.java:499)
	... 37 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:536)
	... 40 more
Caused by: java.lang.AssertionError: Internal error: While invoking method 'public org.apache.calcite.sql2rel.RelFieldTrimmer$TrimResult org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(org.apache.calcite.rel.core.Project,org.apache.calcite.util.ImmutableBitSet,java.util.Set)'
	at org.apache.calcite.util.Util.newInternal(Util.java:743)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:543)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.dispatchTrimFields(RelFieldTrimmer.java:269)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimChild(RelFieldTrimmer.java:210)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(RelFieldTrimmer.java:499)
	... 45 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:536)
	... 48 more
Caused by: java.lang.AssertionError: Internal error: While invoking method 'public org.apache.calcite.sql2rel.RelFieldTrimmer$TrimResult org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(org.apache.calcite.rel.core.Aggregate,org.apache.calcite.util.ImmutableBitSet,java.util.Set)'
	at org.apache.calcite.util.Util.newInternal(Util.java:743)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:543)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.dispatchTrimFields(RelFieldTrimmer.java:269)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimChild(RelFieldTrimmer.java:210)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(RelFieldTrimmer.java:345)
	... 53 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:536)
	... 56 more
Caused by: java.lang.AssertionError: Internal error: While invoking method 'public org.apache.calcite.sql2rel.RelFieldTrimmer$TrimResult org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(org.apache.calcite.rel.core.Project,org.apache.calcite.util.ImmutableBitSet,java.util.Set)'
	at org.apache.calcite.util.Util.newInternal(Util.java:743)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:543)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.dispatchTrimFields(RelFieldTrimmer.java:269)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimChild(RelFieldTrimmer.java:210)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(RelFieldTrimmer.java:772)
	... 61 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:536)
	... 64 more
Caused by: java.lang.AssertionError: Internal error: While invoking method 'public org.apache.calcite.sql2rel.RelFieldTrimmer$TrimResult org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(org.apache.calcite.rel.core.Project,org.apache.calcite.util.ImmutableBitSet,java.util.Set)'
	at org.apache.calcite.util.Util.newInternal(Util.java:743)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:543)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.dispatchTrimFields(RelFieldTrimmer.java:269)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimChild(RelFieldTrimmer.java:210)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(RelFieldTrimmer.java:345)
	... 69 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:536)
	... 72 more
Caused by: java.util.NoSuchElementException
	at java.util.AbstractList$Itr.next(AbstractList.java:364)
	at java.util.AbstractList.hashCode(AbstractList.java:540)
	at org.apache.calcite.util.Util.hash(Util.java:230)
	at org.apache.calcite.util.Pair.hashCode(Pair.java:87)
	at com.google.common.base.Equivalences$Equals.doHash(Equivalences.java:70)
	at com.google.common.base.Equivalence.hash(Equivalence.java:105)
	at com.google.common.cache.LocalCache.hash(LocalCache.java:1888)
	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3953)
	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4758)
	at org.apache.calcite.rel.type.RelDataTypeFactoryImpl.canonize(RelDataTypeFactoryImpl.java:352)
	at org.apache.calcite.rel.type.RelDataTypeFactoryImpl.createStructType(RelDataTypeFactoryImpl.java:148)
	at org.apache.calcite.rel.type.RelDataTypeFactoryImpl.createStructType(RelDataTypeFactoryImpl.java:172)
	at org.apache.calcite.plan.RelOptUtil.permute(RelOptUtil.java:2589)
	at org.apache.calcite.sql2rel.RelFieldTrimmer.trimFields(RelFieldTrimmer.java:383)
	... 77 more
{noformat}

The query executed is the following one:
{noformat}
SELECT w_warehouse_name, 
       w_warehouse_sq_ft, 
       w_city, 
       w_county, 
       w_state, 
       w_country, 
       ship_carriers, 
       year, 
       Sum(jan_sales)                     AS jan_sales, 
       Sum(feb_sales)                     AS feb_sales, 
       Sum(jan_sales / w_warehouse_sq_ft) AS jan_sales_per_sq_foot, 
       Sum(feb_sales / w_warehouse_sq_ft) AS feb_sales_per_sq_foot, 
       Sum(jan_net)                       AS jan_net, 
       Sum(feb_net)                       AS feb_net 
FROM   (SELECT w_warehouse_name, 
               w_warehouse_sq_ft, 
               w_city, 
               w_county, 
               w_state, 
               w_country, 
               Concat('DHL', ',', 'BARIAN') AS ship_carriers, 
               d_year                       AS year, 
               Sum(CASE 
                     WHEN d_moy = 1 THEN ws_ext_sales_price * ws_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS jan_sales, 
               Sum(CASE 
                     WHEN d_moy = 2 THEN ws_ext_sales_price * ws_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS feb_sales, 
               Sum(CASE 
                     WHEN d_moy = 1 THEN ws_net_paid * ws_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS jan_net, 
               Sum(CASE 
                     WHEN d_moy = 2 THEN ws_net_paid * ws_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS feb_net 
        FROM   web_sales 
               JOIN warehouse 
                 ON web_sales.ws_warehouse_sk = warehouse.w_warehouse_sk 
               JOIN date_dim 
                 ON web_sales.ws_sold_date_sk = date_dim.d_date_sk 
               JOIN time_dim 
                 ON web_sales.ws_sold_time_sk = time_dim.t_time_sk 
               JOIN ship_mode 
                 ON web_sales.ws_ship_mode_sk = ship_mode.sm_ship_mode_sk 
        WHERE  d_year = 2001 
               AND t_time BETWEEN 30838 AND 30838 + 28800 
               AND sm_carrier IN ( 'DHL', 'BARIAN' ) 
        GROUP  BY w_warehouse_name, 
                  w_warehouse_sq_ft, 
                  w_city, 
                  w_county, 
                  w_state, 
                  w_country, 
                  d_year 
        UNION DISTINCT
        SELECT w_warehouse_name, 
               w_warehouse_sq_ft, 
               w_city, 
               w_county, 
               w_state, 
               w_country, 
               Concat('DHL', ',', 'BARIAN') AS ship_carriers, 
               d_year                       AS year, 
               Sum(CASE 
                     WHEN d_moy = 1 THEN cs_sales_price * cs_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS jan_sales, 
               Sum(CASE 
                     WHEN d_moy = 2 THEN cs_sales_price * cs_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS feb_sales, 
               Sum(CASE 
                     WHEN d_moy = 1 THEN cs_net_paid_inc_tax * cs_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS jan_net, 
               Sum(CASE 
                     WHEN d_moy = 2 THEN cs_net_paid_inc_tax * cs_quantity 
                     ELSE Cast(0 AS DECIMAL(7, 2)) 
                   end)                     AS feb_net 
        FROM   catalog_sales 
               JOIN warehouse 
                 ON catalog_sales.cs_warehouse_sk = warehouse.w_warehouse_sk 
               JOIN date_dim 
                 ON catalog_sales.cs_sold_date_sk = date_dim.d_date_sk 
               JOIN time_dim 
                 ON catalog_sales.cs_sold_time_sk = time_dim.t_time_sk 
               JOIN ship_mode 
                 ON catalog_sales.cs_ship_mode_sk = ship_mode.sm_ship_mode_sk 
        WHERE  d_year = 2001 
               AND t_time BETWEEN 30838 AND 30838 + 28800 
               AND sm_carrier IN ( 'DHL', 'BARIAN' ) 
        GROUP  BY w_warehouse_name, 
                  w_warehouse_sq_ft, 
                  w_city, 
                  w_county, 
                  w_state, 
                  w_country, 
                  d_year) x 
GROUP  BY w_warehouse_name, 
          w_warehouse_sq_ft, 
          w_city, 
          w_county, 
          w_state, 
          w_country, 
          ship_carriers, 
          year 
ORDER  BY w_warehouse_name 
LIMIT  100; 
{noformat}

The following smaller query can be also used to reproduce the problem:

{noformat}
EXPLAIN
SELECT w_warehouse_sq_ft, 
       Sum(jan_sales)                     AS jan_sales, 
       Sum(jan_sales / w_warehouse_sq_ft) AS jan_sales_per_sq_foot, 
       Sum(jan_net)                       AS jan_net
FROM   (SELECT w_warehouse_sq_ft, 
               0 AS jan_sales,
               0 AS jan_net
        FROM   web_sales 
               JOIN warehouse 
                 ON web_sales.ws_warehouse_sk = warehouse.w_warehouse_sk 
        UNION
        SELECT w_warehouse_sq_ft, 
               0 AS jan_sales,
               0 AS jan_net
        FROM   web_sales 
               JOIN warehouse 
                 ON web_sales.ws_warehouse_sk = warehouse.w_warehouse_sk) x 
GROUP  BY w_warehouse_sq_ft; 
{noformat}

The problem is in RelFieldTrimmer. The problem seems to be in trimChild method (line 191), and in particular, in the following lines:
{code}
    final ImmutableList<RelCollation> collations =
        RelMetadataQuery.collations(rel);
{code}
trimChild passes down the columns that we need to keep, including the columns on which collations exist.
Currently we take the collations from the parent relation, which does not seem correct, as we end up pointing to positions that do not exist in the child relation; it seems the collations should be taken from the child RelNode. Thus, the call would be:
{code}
    final ImmutableList<RelCollation> collations =
        RelMetadataQuery.collations(input);
{code}
",2015-05-26T09:02:43.695+0000,2016-01-21T22:22:42.992+0000,Duplicate,Major
CALCITE-902,Match nullability when reducing expressions in a Project,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12444237'>],"ReduceExpressionsRule(esp, PROJECT_INSTANCE) does not work with columns that allow null values.

Repro steps:
{code}
@Test
	public void testReduceConstants20() throws Exception {
		HepProgram program = new HepProgramBuilder()
				.addRuleInstance(ReduceExpressionsRule.PROJECT_INSTANCE)
				.addRuleInstance(ReduceExpressionsRule.FILTER_INSTANCE)
				.addRuleInstance(ReduceExpressionsRule.JOIN_INSTANCE).build();

		checkPlanning(program, ""select mgr from emp where mgr=10"");
	}
{code}

failing info.:
{code}
testReduceConstants20(org.apache.calcite.test.RelOptRulesTest)  Time elapsed: 0.019 sec  <<< FAILURE!
java.lang.AssertionError: type mismatch:
type1:
INTEGER NOT NULL
type2:
INTEGER
	at org.apache.calcite.plan.RelOptUtil.eq(RelOptUtil.java:1664)
	at org.apache.calcite.rex.RexUtil.compatibleTypes(RexUtil.java:582)
	at org.apache.calcite.rel.core.Project.isValid(Project.java:182)
	at org.apache.calcite.rel.core.Project.<init>(Project.java:85)
	at org.apache.calcite.rel.logical.LogicalProject.<init>(LogicalProject.java:64)
	at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:123)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$2.onMatch(ReduceExpressionsRule.java:199)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:326)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:515)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:125)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194)
	at org.apache.calcite.test.RelOptTestBase.checkPlanning(RelOptTestBase.java:132)
	at org.apache.calcite.test.RelOptTestBase.checkPlanning(RelOptTestBase.java:84)
	at org.apache.calcite.test.RelOptTestBase.checkPlanning(RelOptTestBase.java:73)
	at org.apache.calcite.test.RelOptRulesTest.testReduceConstants20(RelOptRulesTest.java:794)


Results :

Failed tests:
  RelOptRulesTest.testReduceConstants20:794->RelOptTestBase.checkPlanning:73->RelOptTestBase.checkPlanning:84->RelOptTestBase.checkPlanning:132 type mismatch:
{code}",2015-09-29T00:34:37.719+0000,2015-11-10T08:02:56.293+0000,Fixed,Major
RANGER-962,Ranger plugin should have an option to use X-Forwarded-For address,RANGER,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12549559'>, <JIRA IssueLink: id='12465329'>, <JIRA IssueLink: id='12473900'>]","Ranger plugins use the client IP address provided by the component for authorization and audit log. In cases where a client accesses data through a gateway, like Knox, the components provide Ranger with the IP address of the gateway, instead of the original client IP address. Hence, authorization and audit logs are performed using the IP address of the gateway - instead of the client's IP address.

Ranger plugins should be updated to be able to use client IP address, when available in X-Forwarded-For field of the request. Further X-Forwarded-For field should only be used when the request is through a trusted proxy address.",2016-04-30T00:04:22.427+0000,2018-12-07T02:08:38.814+0000,Fixed,Major
SLIDER-1145,More Hadoop 2.8 changes break Slider mocking,SLIDER,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12471287'>, <JIRA IssueLink: id='12471292'>, <JIRA IssueLink: id='12471286'>]",,2016-06-22T18:57:01.177+0000,2017-03-06T21:23:49.025+0000,Fixed,Blocker
TEZ-2937,Can Processor.close() be called after closing inputs and outputs?,TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12540102'>, <JIRA IssueLink: id='12454852'>, <JIRA IssueLink: id='12448505'>]",Pig hit PIG-4722 as processor.close() which clears some static thread local variables was called when SpillThread was still running the Combiner.  LogicalIOProcessorRuntimeTask.java calls processor.close() before closing the inputs and outputs in both close() and cleanup() methods. Can Tez change to closing the processor after the inputs and outputs are closed? ,2015-11-12T00:37:42.147+0000,2018-08-01T20:08:51.506+0000,Fixed,Major
PHOENIX-6110,Disable Permission ITs on HBase 2.1,PHOENIX,Bug,Closed,[],2,"[<JIRA IssueLink: id='12597616'>, <JIRA IssueLink: id='12617717'>]","The permissions tests have been flakey on Hbase 2.1 ever since we started supporting it.

As we do not have the problems on 2.2 or 2.3, I am reasonably confident that the issue is that permission changes are not synchronous on 2.1 under load.

Since 2.1 is EOL, so there no hope for fixing this issue. Since we do not want drop support support for 2.1 yet, (we may want to do so after 5.1 is released), we should disable the permission-related flakey tests, so that our test results stay relevant.",2020-08-27T16:35:26.875+0000,2021-06-19T07:33:53.861+0000,Fixed,Major
TEZ-263,Stale files left behind for failed jobs,TEZ,Sub-task,Open,[],2,"[<JIRA IssueLink: id='12370857'>, <JIRA IssueLink: id='12389870'>]",,2013-06-19T20:56:56.248+0000,2014-06-20T23:43:05.820+0000,,Major
TEZ-1346,Change Processor construction to make use of contexts,TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12393006'>],"TEZ-1303 made changes for Inputs, Outputs etc but missed the Processor. This is a follow up to change the Processor interface.",2014-07-30T23:28:02.168+0000,2014-09-06T01:35:11.939+0000,Fixed,Blocker
OOZIE-638,Use DistributedCache method that takes conf instead HadoopAccessor trick,OOZIE,Bug,Resolved,[],1,[<JIRA IssueLink: id='12346329'>],"Adding JARs to the distributed cache required a trick of FileSystem pre-initialization in the HadoopAccessor because the addFileToClasspath() method was not taking a configuration and it was creating internally a filesystem with a default configuration.

Since then the DistributedCache API as a method that takes a configuration when adding a JAR to it.

We should move to use that method as there are some edge scenarios when kerberos that this is failing.",2011-12-15T15:05:55.723+0000,2012-03-05T17:23:37.311+0000,Won't Fix,Major
IMPALA-7152,Enable test_drop_partition_from_hive when related Hive change is available in Impala,IMPALA,Improvement,Open,[],1,[<JIRA IssueLink: id='12536073'>],"A Hive issue is found by tests/metadata/test_partition_metadata.py and is skipped until that issue is fixed. Enable it once the Hive fix is available in Impala.

https://issues.apache.org/jira/browse/HIVE-19830

The issue was found when working on https://issues.apache.org/jira/browse/IMPALA-6119

 ",2018-06-08T13:14:16.819+0000,2018-06-08T13:21:35.548+0000,,Minor
PARQUET-83,Hive Query failed if the data type is array<string> with parquet files,PARQUET,Bug,Resolved,[],1,[<JIRA IssueLink: id='12401950'>],"* Created a parquet file from the Avro file which have 1 array data type and rest are primitive types. Avro Schema of the array data type. Eg: 
{code}
{ ""name"" : ""action"", ""type"" : [ { ""type"" : ""array"", ""items"" : ""string"" }, ""null"" ] }
{code}
* Created External Hive table with the Array type as below, 
{code}
create external table paraArray (action Array) partitioned by (partitionid int) row format serde 'parquet.hive.serde.ParquetHiveSerDe' stored as inputformat 'parquet.hive.MapredParquetInputFormat' outputformat 'parquet.hive.MapredParquetOutputFormat' location '/testPara'; 
alter table paraArray add partition(partitionid=1) location '/testPara';
{code}
* Run the following query(select action from paraArray limit 10) and the Map reduce jobs are failing with the following exception.
{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ClassCastException: parquet.hive.writable.BinaryWritable$DicBinaryWritable cannot be cast to org.apache.hadoop.io.ArrayWritable
at parquet.hive.serde.ParquetHiveArrayInspector.getList(ParquetHiveArrayInspector.java:125)
at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:315)
at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)
at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)
at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)
at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:405)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:336)
at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1126)
at org.apache.hadoop.mapred.Child.main(Child.java:264)
]
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:671)
at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)
... 8 more
{code}


This issue has long back posted on Parquet issues list and Since this is related to Parquet Hive serde, I have created the Hive issue here, The details and history of this information are as shown in the link here https://github.com/Parquet/parquet-mr/issues/281.",2014-08-22T09:23:29.644+0000,2015-04-07T20:45:57.812+0000,Fixed,Major
TEZ-3426,Second AM attempt launched for session mode and recovery disabled for certain cases,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12479345'>, <JIRA IssueLink: id='12479420'>]","ApplicationSubmissionContext#setMaxAppAttempts does not fully guarantee that there will be only that many attempts at maximum. There are a few exceptional cases that are not count. Tez should protect itself from accidentally starting the second attempt in session mode and when recovery is disabled since the second attempt will always succeed with no work to do.

{code}
  @Override
  public boolean shouldCountTowardsMaxAttemptRetry() {
    try {
      this.readLock.lock();
      int exitStatus = getAMContainerExitStatus();
      return !(exitStatus == ContainerExitStatus.PREEMPTED
          || exitStatus == ContainerExitStatus.ABORTED
          || exitStatus == ContainerExitStatus.DISKS_FAILED
          || exitStatus == ContainerExitStatus.KILLED_BY_RESOURCEMANAGER);
    } finally {
      this.readLock.unlock();
    }
  }
{code}",2016-09-01T20:13:26.935+0000,2017-03-14T03:49:54.625+0000,Fixed,Critical
TEZ-2094,TEZ-UI Running Task/TaskAttempt are not excluded when choosing other status in the Dropdownlist,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12408390'>],,2015-02-13T05:42:43.980+0000,2015-02-26T11:22:11.977+0000,Invalid,Major
SPARK-25193,insert overwrite doesn't throw exception when drop old data fails,SPARK,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12582902'>, <JIRA IssueLink: id='12582904'>]","dataframe.write.mode(SaveMode.Overwrite).insertInto(s""$databaseName.$tableName"")

Insert overwrite mode will drop old data in hive table if there's old data.

But if data deleting fails, no exception will be thrown and the data folder will be like:

hdfs://uxs_nbp/nba_score/dt=2018-08-15/seq_num=2/part-00000

hdfs://uxs_nbp/nba_score/dt=2018-08-15/seq_num=2/part-000001534916642513.

Two copies of data will be kept.",2018-08-22T06:45:58.003+0000,2020-03-13T07:07:13.236+0000,Duplicate,Major
SLIDER-531,adopt gour's' suggested improvements to registry,SLIDER,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12399115'>],"in YARN-2678 gour suggested improvements to the {{ServiceRecord}} structure for easier python work. These are relatively straightforward, but will change the registry-wide structures. They need to be done now, before any version of the registry ships",2014-10-16T16:47:53.848+0000,2015-04-07T10:42:27.327+0000,Fixed,Major
RANGER-2557,Configure Kerberos for Hive Ranger Client via HS2 configuration,RANGER,Improvement,Open,[],1,[<JIRA IssueLink: id='12569088'>],"In Hive we would like to have possibility to enable Kerberos partially (i.e only Ranger, Atlas and HMS).
However, since hadoop security is a global flag there are many places that need to be commented out to avoid the UGI cluster wide configuration.",2019-09-03T07:48:42.752+0000,2019-09-03T16:32:08.510+0000,,Major
KUDU-442,Kudu SerDe for hive,KUDU,New Feature,Resolved,[],4,"[<JIRA IssueLink: id='12566532'>, <JIRA IssueLink: id='12489699'>, <JIRA IssueLink: id='12560173'>, <JIRA IssueLink: id='12562527'>]","Though Impala is the horse we are betting on, we should probably build a Hive SerDe as well, since Hive still supports many features that Impala doesn't. This is potentially a place to leverage community contribution.",2014-08-06T01:39:44.000+0000,2019-07-28T21:11:39.047+0000,Duplicate,Minor
SLIDER-923,switch to TryOnceThenFail retry policy,SLIDER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12432677'>],"move to an IPC policy of {{TryOnceThenFail}}, as the {{RetryUtils.getDefaultPolicy()}} method signature has (currently) changed on branch-2",2015-07-30T01:02:40.243+0000,2015-09-30T15:52:46.627+0000,Fixed,Minor
TEZ-3187,Pig on tez hang with java.io.IOException: Connection reset by peer,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12463436'>],"We are experiencing occasional application hangs, when testing an existing Pig MapReduce script, executing on Tez.  When this occurs, we find this in the syslog for the executing dag:

016-03-21 16:39:01,643 [INFO] [DelayedContainerManager] |rm.YarnTaskSchedulerService|: No taskRequests. Container's idle timeout delay expired or is new. Releasing container, containerId=container_e11_1437886552023_169758_01_000822, containerExpiryTime=1458603541415, idleTimeout=5000, taskRequestsCount=0, heldContainers=112, delayedContainers=27, isNew=false
2016-03-21 16:39:01,825 [INFO] [DelayedContainerManager] |rm.YarnTaskSchedulerService|: No taskRequests. Container's idle timeout delay expired or is new. Releasing container, containerId=container_e11_1437886552023_169758_01_000824, containerExpiryTime=1458603541692, idleTimeout=5000, taskRequestsCount=0, heldContainers=111, delayedContainers=26, isNew=false
2016-03-21 16:39:01,990 [INFO] [Socket Reader #1 for port 53324] |ipc.Server|: Socket Reader #1 for port 53324: readAndProcess from client 10.102.173.86 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.apache.hadoop.ipc.Server.channelRead(Server.java:2593)
        at org.apache.hadoop.ipc.Server.access$2800(Server.java:135)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1471)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:762)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:607)
2016-03-21 16:39:02,032 [INFO] [DelayedContainerManager] |rm.YarnTaskSchedulerService|: No taskRequests. Container's idle timeout delay expired or is new. Releasing container, containerId=container_e11_1437886552023_169758_01_000811, containerExpiryTime=1458603541828, idleTimeout=5000, taskRequestsCount=0, heldContainers=110, delayedContainers=25, isNew=false

In all cases I've been able to analyze so far, this also correlates with a warning in the node identified in the IOException:

2016-03-21 16:36:13,641 [WARN] [I/O Setup 2 Initialize: {scope-178}] |retry.RetryInvocationHandler|: A failover has occurred since the start of this method invocation attempt.

However, it does not appear that any namenode failover has actually occurred (the most recent failover we see in logs is from 2015).

Attached:
syslog_dag_1437886552023_169758_3.gz: syslog for the dag which hangs
10.102.173.86.logs.gz: aggregated logs from the host identified in the IOException",2016-03-24T16:47:49.072+0000,2016-04-12T19:10:00.375+0000,,Major
YETUS-943,Test patch fails to extract version information from a JDK11 jvm,YETUS,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12599855'>, <JIRA IssueLink: id='12581020'>]","Trying to use pre-commit with AdoptOpenJDK11 in the test matrix, the logic for extracting the version information is incorrect.

For JDK8, the output is
{noformat}
root@ab2cb99dbf34:/# /usr/lib/jvm/jdk8u232-b09/bin/java -version
openjdk version ""1.8.0_232""
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_232-b09)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.232-b09, mixed mode)
{noformat}

which Yetus parses as ""1.8.0_232"".

For JDK11, it's
{noformat}
root@ab2cb99dbf34:/# /usr/lib/jvm/jdk-11.0.6+10/bin/java -version
openjdk version ""11.0.6"" 2020-01-14
OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.6+10)
OpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.6+10, mixed mode)
{noformat}

which Yetus parses as ""2020-01-14"".",2020-02-20T22:38:49.536+0000,2020-10-03T06:37:53.699+0000,Fixed,Minor
IMPALA-5516,test_hbase_inserts failed with OutOfOrderScannerException,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12506700'>],"I saw this test failure on an exhaustive release build.
{code}
04:21:41 FAIL query_test/test_hbase_queries.py::TestHBaseQueries::()::test_hbase_inserts[exec_option: {'disable_codegen': True, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: hbase/none]
04:21:41 =================================== FAILURES ===================================
04:21:41  TestHBaseQueries.test_hbase_inserts[exec_option: {'disable_codegen': True, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: hbase/none] 
04:21:41 query_test/test_hbase_queries.py:58: in test_hbase_inserts
04:21:41     self.run_test_case('QueryTest/hbase-inserts', vector)
04:21:41 common/impala_test_suite.py:390: in run_test_case
04:21:41     result = self.__execute_query(target_impalad_client, query, user=user)
04:21:41 common/impala_test_suite.py:598: in __execute_query
04:21:41     return impalad_client.execute(query, user=user)
04:21:41 common/impala_connection.py:160: in execute
04:21:41     return self.__beeswax_client.execute(sql_stmt, user=user)
04:21:41 beeswax/impala_beeswax.py:173: in execute
04:21:41     handle = self.__execute_query(query_string.strip(), user=user)
04:21:41 beeswax/impala_beeswax.py:339: in __execute_query
04:21:41     self.wait_for_completion(handle)
04:21:41 beeswax/impala_beeswax.py:359: in wait_for_completion
04:21:41     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
04:21:41 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
04:21:41 E    Query aborted:DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?
04:21:41 E   CAUSED BY: OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 2138 number_of_rows: 1024 close_scanner: false next_call_seq: 0 client_handles_partials: true client_handles_heartbeats: true track_scan_metrics: false renew: false
04:21:41 E   	at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2422)
04:21:41 E   	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:33648)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2183)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:183)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:163)
04:21:41 E   
04:21:41 E   CAUSED BY: RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 1 But the nextCallSeq got from client: 0; request=scanner_id: 2138 number_of_rows: 1024 close_scanner: false next_call_seq: 0 client_handles_partials: true client_handles_heartbeats: true track_scan_metrics: false renew: false
04:21:41 E   	at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2422)
04:21:41 E   	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:33648)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2183)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:183)
04:21:41 E   	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:163)
{code}",2017-06-15T15:37:54.746+0000,2017-06-16T06:54:06.895+0000,Not A Bug,Critical
TEZ-684,Uber/Local modes for Tez,TEZ,New Feature,Closed,"[<JIRA Issue: key='TEZ-707', id='12687759'>, <JIRA Issue: key='TEZ-708', id='12687760'>, <JIRA Issue: key='TEZ-709', id='12687762'>, <JIRA Issue: key='TEZ-710', id='12687763'>, <JIRA Issue: key='TEZ-717', id='12688112'>, <JIRA Issue: key='TEZ-870', id='12696559'>, <JIRA Issue: key='TEZ-934', id='12701599'>, <JIRA Issue: key='TEZ-1285', id='12727475'>, <JIRA Issue: key='TEZ-1298', id='12728639'>, <JIRA Issue: key='TEZ-1304', id='12729289'>, <JIRA Issue: key='TEZ-1338', id='12730697'>, <JIRA Issue: key='TEZ-1349', id='12730998'>, <JIRA Issue: key='TEZ-1365', id='12731478'>, <JIRA Issue: key='TEZ-1429', id='12734329'>, <JIRA Issue: key='TEZ-1471', id='12735551'>, <JIRA Issue: key='TEZ-1485', id='12736120'>, <JIRA Issue: key='TEZ-1538', id='12738907'>, <JIRA Issue: key='TEZ-1542', id='12739245'>, <JIRA Issue: key='TEZ-1591', id='12742166'>, <JIRA Issue: key='TEZ-1618', id='12743601'>]",7,"[<JIRA IssueLink: id='12392938'>, <JIRA IssueLink: id='12390406'>, <JIRA IssueLink: id='12403435'>, <JIRA IssueLink: id='12389582'>, <JIRA IssueLink: id='12403430'>, <JIRA IssueLink: id='12393692'>, <JIRA IssueLink: id='12545933'>]","Similarly to MapReduce Uber-mode in Yarn, we plan to create the Uber-mode for Tez. It runs all tasks in local in one process.

Our target is to start DAGAppMaster in local JVM and let it run all tasks in one process. 

Here is my design: 

Once user submits a DAG, Tez starts a instance of DAGAppMaster. This DAGAppMaster will check TezConfiguration before instantiate ContainerLauncher. If ""is_Uber"" is true, DAGAppMaster creates a LocalContainerLauncher. LocalTaskScheduler and LocalTaskSchedulerEventHandler will call LocalContainerLauncher to run all tasks one by one in a single JVM. Communications between ResourceManager and local classes (DAGAppMaster, LocalContainerLauncher, LocalTaskScheduler, and LocalTaskSchedulerEventHandler) are muted.",2013-12-23T23:24:06.564+0000,2018-10-18T03:36:17.550+0000,Fixed,Major
CASSANDRA-3371,Cassandra inferred schema and actual data don't match,CASSANDRA,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12347360'>, <JIRA IssueLink: id='12347138'>]","It's looking like there may be a mismatch between the schema that's being reported by the latest CassandraStorage.java, and the data that's actually returned. Here's an example:

rows = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
DESCRIBE rows;
rows: {key: chararray,columns: {(name: chararray,value: bytearray,photo_owner: chararray,value_photo_owner: bytearray,pid: chararray,value_pid: bytearray,matched_string: chararray,value_matched_string: bytearray,src_big: chararray,value_src_big: bytearray,time: chararray,value_time: bytearray,vote_type: chararray,value_vote_type: bytearray,voter: chararray,value_voter: bytearray)}}
DUMP rows;
(691831038_1317937188.48955,{(photo_owner,1596090180),(pid,6855155124568798560),(matched_string,),(src_big,),(time,Thu Oct 06 14:39:48 -0700 2011),(vote_type,album_dislike),(voter,691831038)})

getSchema() is reporting the columns as an inner bag of tuples, each of which contains 16 values. In fact, getNext() seems to return an inner bag containing 7 tuples, each of which contains two values. 

It appears that things got out of sync with this change:
http://svn.apache.org/viewvc/cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java?r1=1177083&r2=1177082&pathrev=1177083

See more discussion at:
http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/pig-cassandra-problem-quot-Incompatible-field-schema-quot-error-tc6882703.html
",2011-10-17T10:25:03.501+0000,2019-04-16T09:32:48.410+0000,Fixed,Normal
TEZ-990,Add support for a credentials file read by TezClient,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12385657'>],,2014-03-28T08:13:46.202+0000,2014-03-30T08:01:42.257+0000,Fixed,Major
CALCITE-3862,Materialized view rewriting algorithm throws IndexOutOfBoundsException,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12583279'>],"*Repro*
{code:sql}
+    sql(""select \""deptno\"", \""empid\"", \""salary\"", sum(1) ""
+            + ""from \""emps\""\n""
+            + ""group by \""deptno\"", \""empid\"", \""salary\"""",
+        ""select sum(1) ""
+            + ""from \""emps\""\n""
+            + ""join \""depts\"" on \""depts\"".\""deptno\"" = \""empid\"" group by \""empid\"", \""depts\"".\""deptno\"""")
+        .withResultContains(
+            ""EnumerableCalc(expr#0..1=[{inputs}], EXPR$0=[$t1])\n""
+                + ""  EnumerableAggregate(group=[{1}], EXPR$0=[$SUM0($3)])\n""
+                + ""    EnumerableHashJoin(condition=[=($1, $4)], joinType=[inner])\n""
+                + ""      EnumerableTableScan(table=[[hr, m0]])"")
+        .ok();
{code}

*Error*
{code}
Next exception 1: [CIRCULAR REFERENCE SQLException]
            Next exception 2: [CIRCULAR REFERENCE RuntimeException]
            Next exception 3: java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
                at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:310)
                at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:293)
                at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:67)
                at org.apache.calcite.rex.RexBuilder.makeInputRef(RexBuilder.java:853)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewRule$3.visitInputRef(MaterializedViewRule.java:1217)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewRule$3.visitInputRef(MaterializedViewRule.java:1181)
                at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
                at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:277)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewRule.shuttleReferences(MaterializedViewRule.java:1242)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewAggregateRule.rewriteView(MaterializedViewAggregateRule.java:728)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewRule.perform(MaterializedViewRule.java:485)
                at org.apache.calcite.rel.rules.materialize.MaterializedViewOnlyAggregateRule.onMatch(MaterializedViewOnlyAggregateRule.java:63)
                at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:238)
                at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:540)
                at org.apache.calcite.tools.Programs.lambda$standard$3(Programs.java:286)
                at org.apache.calcite.tools.Programs$SequenceProgram.run(Programs.java:346)
                at org.apache.calcite.prepare.Prepare.optimize(Prepare.java:165)
                at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:290)
                at org.apache.calcite.prepare.Prepare.prepareSql(Prepare.java:207)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepare2_(CalcitePrepareImpl.java:634)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepare_(CalcitePrepareImpl.java:498)
                at org.apache.calcite.prepare.CalcitePrepareImpl.prepareSql(CalcitePrepareImpl.java:468)
                at org.apache.calcite.jdbc.CalciteConnectionImpl.parseQuery(CalciteConnectionImpl.java:231)
                at org.apache.calcite.jdbc.CalciteMetaImpl.prepareAndExecute(CalciteMetaImpl.java:550)
                at org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:675)
                at org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)
{code}
 ",2020-03-15T22:48:56.451+0000,2020-05-24T12:00:40.551+0000,Fixed,Major
TEZ-3415,Ability to configure shuffle server listen queue length,TEZ,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12483638'>, <JIRA IssueLink: id='12478065'>]",This tracks updating the Tez custom shuffle handler to have equivalent functionality of MAPREDUCE-6763.,2016-08-19T17:54:29.266+0000,2016-10-19T21:33:45.434+0000,Duplicate,Major
CALCITE-2450,RexSimplify: reorder predicates to a canonical form as a part of RexSimplify,CALCITE,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12578614'>, <JIRA IssueLink: id='12640244'>, <JIRA IssueLink: id='12641582'>]","Certain optimizations are easier to perform when input expressions are in a canonical form.
For instance: more duplicates can be found in AND/OR lists, case branches, etc.

Note: this reordering is supposed to happen in RexSimplify only. In other words, RexBuilder would still produce ""non-canonical"" expressions.

It is expected that {{RexSimplify}} might alter the expression, so if it converts {{5=x}} to {{x=5}} it should be just fine.

The suggested rules are to be discussed, yet the following might be fine:
1) For AND, OR, IN: put ""simpler"" nodes first. The weight of a node could be either {{.toString().length()}} or a number of child nodes or something like that.
The motivation is to simplify logic that handles ""duplicate"" entries. It won't have to consider ""both alternatives"" all over the place.
2) For comparison with literals put literal as the second argument
3) For binary comparison, put node with less weight to the left",2018-08-07T09:19:45.793+0000,2022-06-10T13:30:01.340+0000,Fixed,Major
SPARK-26932,Add a warning for Hive 2.1.1 ORC reader issue,SPARK,Documentation,Resolved,[],1,[<JIRA IssueLink: id='12554930'>],"As of Spark 2.3 and Hive 2.3, both supports using apache/orc as orc writer and reader. In older version of Hive, orc reader(isn't forward-compitaient) implemented by its own.

So Hive 2.2 and older can not read orc table created by spark 2.3 and newer which using apache/orc instead of Hive orc.

I think we should add these information into Spark2.4 orc configuration file : https://spark.apache.org/docs/2.4.0/sql-data-sources-orc.html",2019-02-19T15:12:52.739+0000,2019-03-23T17:42:14.277+0000,Fixed,Minor
SPARK-29295,Duplicate result when dropping partition of an external table and then overwriting,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12570922'>, <JIRA IssueLink: id='12571295'>]","When we drop a partition of a external table and then overwrite it, if we set CONVERT_METASTORE_PARQUET=true(default value), it will overwrite this partition.
But when we set CONVERT_METASTORE_PARQUET=false, it will give duplicate result.

Here is a reproduce code below(you can add it into SQLQuerySuite in hive module):

{code:java}
  test(""spark gives duplicate result when dropping a partition of an external partitioned table"" +
    "" firstly and they overwrite it"") {
    withTable(""test"") {
      withTempDir { f =>
        sql(""create external table test(id int) partitioned by (name string) stored as "" +
          s""parquet location '${f.getAbsolutePath}'"")

        withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> false.toString) {
          sql(""insert overwrite table test partition(name='n1') select 1"")
          sql(""ALTER TABLE test DROP PARTITION(name='n1')"")
          sql(""insert overwrite table test partition(name='n1') select 2"")
          checkAnswer( sql(""select id from test where name = 'n1' order by id""),
            Array(Row(1), Row(2)))
        }

        withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> true.toString) {
          sql(""insert overwrite table test partition(name='n1') select 1"")
          sql(""ALTER TABLE test DROP PARTITION(name='n1')"")
          sql(""insert overwrite table test partition(name='n1') select 2"")
          checkAnswer( sql(""select id from test where name = 'n1' order by id""),
            Array(Row(2)))
        }
      }
    }
  }
{code}

{code}
create external table test(id int) partitioned by (name string) stored as parquet location '/tmp/p';
set spark.sql.hive.convertMetastoreParquet=false;
insert overwrite table test partition(name='n1') select 1;
ALTER TABLE test DROP PARTITION(name='n1');
insert overwrite table test partition(name='n1') select 2;
select id from test where name = 'n1' order by id;
{code}",2019-09-30T01:19:50.653+0000,2020-06-09T07:31:46.593+0000,Fixed,Major
PHOENIX-4496,Fix RowValueConstructorIT and IndexMetadataIT,PHOENIX,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12531532'>, <JIRA IssueLink: id='12523110'>]","{noformat}
[ERROR] Tests run: 46, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 117.444 s <<< FAILURE! - in org.apache.phoenix.end2end.RowValueConstructorIT
[ERROR] testRVCLastPkIsTable1stPkIndex(org.apache.phoenix.end2end.RowValueConstructorIT)  Time elapsed: 4.516 s  <<< FAILURE!
java.lang.AssertionError
        at org.apache.phoenix.end2end.RowValueConstructorIT.testRVCLastPkIsTable1stPkIndex(RowValueConstructorIT.java:1584)
{noformat}

{noformat}
ERROR] Tests run: 14, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 79.381 s <<< FAILURE! - in org.apache.phoenix.end2end.index.IndexMetadataIT
[ERROR] testMutableTableOnlyHasPrimaryKeyIndex(org.apache.phoenix.end2end.index.IndexMetadataIT)  Time elapsed: 4.504 s  <<< FAILURE!
java.lang.AssertionError
        at org.apache.phoenix.end2end.index.IndexMetadataIT.helpTestTableOnlyHasPrimaryKeyIndex(IndexMetadataIT.java:662)
        at org.apache.phoenix.end2end.index.IndexMetadataIT.testMutableTableOnlyHasPrimaryKeyIndex(IndexMetadataIT.java:623)
{noformat}",2017-12-26T07:34:02.475+0000,2018-07-26T01:13:58.143+0000,Fixed,Major
YETUS-896,Add an emoji column to the github vote table,YETUS,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12566924'>],There are no cloor in the vote table. HBase QA result has many rows. If the overall is -1 and it is not easy to find which row is -1. Add a emoji column with color will make it easy. :-),2019-07-23T02:52:26.973+0000,2019-08-21T06:17:42.735+0000,Fixed,Major
KNOX-567,Having Knox Supporting Hadoop/Yarn/HBase/Hive High Availability ,KNOX,New Feature,Closed,[],3,"[<JIRA IssueLink: id='12431059'>, <JIRA IssueLink: id='12514843'>, <JIRA IssueLink: id='12514853'>]","Currently Knox only provides limited support of high availability to the Hadoop ecosystem, i.e.  Knox supports only high availability for webHDFS.  We need to make Knox supports Yarn, HDFS, Hive and HBase high availability as well.  Especially after Knox supports Hadoop ecosystem UIs, having Knox supports at least the HA mode of Hadoop, Hive and HBase becomes even more critical.",2015-07-08T18:16:36.526+0000,2019-03-28T14:19:08.098+0000,Fixed,Major
PHOENIX-1335,Surface support for per cell TTLs,PHOENIX,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12398676'>],"If/when per cell TTL support is committed to HBase (HBASE-10560), we should surface this capability in Phoenix. ",2014-10-10T01:22:01.047+0000,2022-06-11T17:51:03.805+0000,Incomplete,Major
CHUKWA-22,Need index for chukwa sequence files,CHUKWA,New Feature,Open,[],2,"[<JIRA IssueLink: id='12325189'>, <JIRA IssueLink: id='12322948'>]","Chukwa has ability to collect large volume of data, but the lack of index prevents Chukwa front end to serve data straight from HDFS.  This jira is the place holder for designing a indexing service for Chukwa.  The plan is to create indexing service base on available software like lucene or katta.",2009-01-14T21:43:25.105+0000,2013-05-02T02:29:20.086+0000,,Major
PARQUET-596,Parquet need to support empty list and empty map,PARQUET,Bug,Resolved,[],1,[<JIRA IssueLink: id='12465422'>],"In current hive upstream 2.1version, when hive tried to insert empty list or empty map to parquet table, it fails with error:
 parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead

It seems now parquet only support null value, we should find a way to support empty list and empty map values. ",2016-05-02T17:17:50.759+0000,2016-09-08T16:33:28.866+0000,Duplicate,Major
FLUME-1787,Implement Pre-commit testing with Jenkins,FLUME,Bug,Resolved,[],1,[<JIRA IssueLink: id='12372112'>],Hadoop does pre-commit testing. It'd be nice if we could do the same. This was discussed here: http://markmail.org/message/ie4zrpv4kviu6n4y,2012-12-17T17:23:54.046+0000,2013-07-16T00:46:56.037+0000,Fixed,Major
HAMA-559,Add a spilling message queue,HAMA,Sub-task,Resolved,[],6,"[<JIRA IssueLink: id='12350701'>, <JIRA IssueLink: id='12350700'>, <JIRA IssueLink: id='12358028'>, <JIRA IssueLink: id='12359557'>, <JIRA IssueLink: id='12362534'>, <JIRA IssueLink: id='12362536'>]","After HAMA-521 is done, we can add a spilling queue which just holds the messages in RAM that fit into the heap space. The rest can be flushed to disk.
We may call this a HybridQueue or something like that.

The benefits should be that we don't have to flush to disk so often and get faster. However we may have more GC so it is always overall faster.

The requirements for this queue also include:

- The message object once written to the queue (after returning from the write call) could be modified, but the changes should not be reflected in the messages stored in the queue.

- For now let's implement a queue that does not support concurrent reading and writing. This feature is needed when we implement asynchronous communication.






",2012-04-18T18:17:47.588+0000,2013-05-05T21:06:19.142+0000,Fixed,Minor
RANGER-1851,Enhance Ranger Hive Plugin to support authorization for KILL QUERY command,RANGER,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12557965'>, <JIRA IssueLink: id='12623209'>, <JIRA IssueLink: id='12526399'>, <JIRA IssueLink: id='12530628'>]","With the HIVE-17483 JIRA,  Hive has introduced a way to kill query <id> and in hive its a privileged  action for Hive Admin Role. In order for the Ranger Hive Authorizer to support authorization, we need to enhance the ranger hive authorizer. Current Hive implementation is to Kill Query in a HiveService which can be LLAP / HIVESERVER2 , later these HIVE SERVICEs can be grouped into NAME SPACEs and kill query can be run against them. When HiveServer2/LLAP Ranger Plugin sends the request to Ranger for Authorization, it will be sending the HIVE SERVICE in the context with the COMMAND that is executed.  
With all the details proposal is to have 
1) In Ranger Hive Service Definition, we will have a new Resource ""Hive Service"" to authorize.
2) In Ranger Hive Permission Model, we will have a new Permission ""Service Admin"" to group Kill Query operation.
    - ""Service Admin""  permission will enable hive ranger plugin to isolate various admin operations in this case ""Kill Query"" and in future if hive introduces other operations which are done at ""HIVE SERVICE level"" , group them under this and authorize.
   - ""Service Admin"" won't be able to do  DATABASE / TABLE / COLUMN operations as this will all be taken care by the existing DATABASE/TABLE/COLUMN level permission model.

[~madhan.neethiraj] [~vperiasamy][~thejas][~bosco][~sneethiraj]",2017-10-20T17:14:25.199+0000,2021-09-17T12:25:25.819+0000,Fixed,Critical
PHOENIX-5459,Enable running the test suite with JDK11,PHOENIX,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12569373'>, <JIRA IssueLink: id='12569012'>, <JIRA IssueLink: id='12569244'>]","As HBase runs on JDK11, Phoenix should also support JDK11.

A first step is making sure that we can run the tests on JDK11, while still building on JDK8.

Right now, the test suite cannot be run with JDK11, because it tries to run the failsafe tests with a VM setting that prevent JDK11 from starting.",2019-09-02T09:12:27.558+0000,2021-02-10T10:00:29.317+0000,Fixed,Major
SLIDER-431,AM is not reachable when HTTPS is enabled,SLIDER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12396827'>],The RM web proxy has an issue with forwarding SSL traffic and needs to be fixed in order for the Slider AM to be accessible when SSL/HTTPS is enabled (https://issues.apache.org/jira/browse/YARN-2554),2014-09-15T17:33:51.639+0000,2014-10-24T20:26:48.751+0000,Fixed,Major
TEZ-4057,Fix Unsorted broadcast shuffle umasks,TEZ,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12588848'>, <JIRA IssueLink: id='12588850'>]","{code}

    if (numPartitions == 1 && !pipelinedShuffle) {
      //special case, where in only one partition is available.
      finalOutPath = outputFileHandler.getOutputFileForWrite();
      finalIndexPath = outputFileHandler.getOutputIndexFileForWrite(indexFileSizeEstimate);
      skipBuffers = true;
      writer = new IFile.Writer(conf, rfs, finalOutPath, keyClass, valClass,
          codec, outputRecordsCounter, outputRecordBytesCounter);
    } else {
      skipBuffers = false;
      writer = null;
    }
{code}

The broadcast events don't update the file umasks, because they have 1 partition.

{code}
total 8.0K
-rw------- 1 hive hadoop 15 Mar 27 20:30 file.out
-rw-r----- 1 hive hadoop 32 Mar 27 20:30 file.out.index
{code}

ending up with readable index files and unreadable .out files.",2019-03-27T21:11:13.794+0000,2020-08-25T15:00:10.650+0000,Fixed,Major
TEZ-3078,Provide a mechanism for AM to let Client know about the reason for failure,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12455779'>],"When working on HIVE-12959 for LLAP, the requirement is when we submit a query to LLAP task scheduler and if there are no LLAP daemons we should fail the query instead of waiting indefinitely for daemons to show up. For this to work, the task scheduler has to provide a mechanism to let the AM know that the scheduler service cannot proceed further as there are no daemons running. Currently there is no way for the task scheduler to let AM know about this information. The only way right now is to send back exception using TaskSchedulerContext.onError() API. This will kill the AM but AM will restart to recover the DAG. It will be better if there a way to let AM know about daemon status via some status response based on which AM should avoid restarting. It will be even better if we can provide a way for AM to communicate this information back to the client (hive CLI or HiveServer2).",2016-01-28T22:33:23.213+0000,2016-02-06T02:18:20.205+0000,,Major
BIGTOP-3803,Fix Hive3.1.3 Metastore service compatible issue with Hadoop3.3.x when kerberos enabled,BIGTOP,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12646963'>, <JIRA IssueLink: id='12647027'>, <JIRA IssueLink: id='12647117'>]","Fail to start Hive metastore when enable Kerberos in Hadoop 3.3.x
{code:java}
2022-09-02T06:20:04,458 INFO  [main]: zookeeper.ZooKeeper (ZooKeeper.java:<init>(868)) - Initiating client connection, connectString=ambari-agent-01:2181,ambari-agent-02:2181,ambari-server:2181 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@433348bc
2022-09-02T06:20:04,470 INFO  [main]: common.X509Util (X509Util.java:<clinit>(79)) - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-09-02T06:20:04,499 INFO  [main]: zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(237)) - jute.maxbuffer value is 4194304 Bytes
2022-09-02T06:20:04,540 INFO  [main]: zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1653)) - zookeeper.request.timeout value is 0. feature enabled=
2022-09-02T06:20:04,587 ERROR [main]: metastore.HiveMetaStore (HiveMetaStore.java:startMetaStore(9056)) - java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModeStatPathAndBytesable;
	at org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.ensurePath(ZooKeeperTokenStore.java:158)
	at org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.initClientAndPaths(ZooKeeperTokenStore.java:234)
	at org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:471)
	at org.apache.hadoop.hive.metastore.security.MetastoreDelegationTokenManager.startDelegationTokenSecretManager(MetastoreDelegationTokenManager.java:76)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:8952)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:8854)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)

2022-09-02T06:20:04,613 INFO  [main-SendThread(ambari-agent-01:2181)]: zookeeper.Login (Login.java:login(302)) - HiveZooKeeperClient successfully logged in.
2022-09-02T06:20:04,615 INFO  [Thread-6]: zookeeper.Login (Login.java:run(135)) - TGT refresh thread started.
2022-09-02T06:20:04,640 INFO  [main-SendThread(ambari-agent-01:2181)]: client.ZooKeeperSaslClient (SecurityUtils.java:run(128)) - Client will use GSSAPI as SASL mechanism.
2022-09-02T06:20:04,588 ERROR [main]: metastore.HiveMetaStore (HiveMetaStore.java:main(8859)) - Metastore Thrift Server threw an exception...
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModeStatPathAndBytesable;
	at org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.ensurePath(ZooKeeperTokenStore.java:158) ~[hive-exec-3.1.3.jar:3.1.3]
	at org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.initClientAndPaths(ZooKeeperTokenStore.java:234) ~[hive-exec-3.1.3.jar:3.1.3]
	at org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:471) ~[hive-exec-3.1.3.jar:3.1.3]
	at org.apache.hadoop.hive.metastore.security.MetastoreDelegationTokenManager.startDelegationTokenSecretManager(MetastoreDelegationTokenManager.java:76) ~[hive-exec-3.1.3.jar:3.1.3]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:8952) ~[hive-exec-3.1.3.jar:3.1.3]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:8854) [hive-exec-3.1.3.jar:3.1.3]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_332]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_332]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323) [hadoop-common-3.3.4.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236) [hadoop-common-3.3.4.jar:?]

{code}",2022-09-02T07:51:44.519+0000,2022-09-20T16:51:20.558+0000,Fixed,Major
SPARK-35274,old hive table's all columns are read when column pruning applies in spark3.0,SPARK,Question,Open,[],3,"[<JIRA IssueLink: id='12614987'>, <JIRA IssueLink: id='12614986'>, <JIRA IssueLink: id='12614985'>]","I asked this question [before|https://issues.apache.org/jira/browse/SPARK-35190], but perhaps I did not addressed question clearly, so I did not get answer. This time I will show an example to illustrate this question clearly.
{code:java}
import org.apache.spark.sql.SparkSession
import org.apache.spark.scheduler.{SparkListener, SparkListenerTaskEnd} 
val spark = SparkSession.builder().appName(""OrcTest"").getOrCreate()
var inputBytes = 0L
spark.sparkContext.addSparkListener(new SparkListener() {
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = { 
    val metrics = taskEnd.taskMetrics
    inputBytes += metrics.inputMetrics.bytesRead 
  } 
}) 
spark.sql(""create table orc_table_old_schema (_col0 int, _col1 string, _col2 double) STORED AS ORC;"")
spark.sql(""insert overwrite table orc_table_old_schema select 1, 'name1', 1000.05"")
inputBytes = 0L
spark.sql(""select _col2 from orc_table_old_schema"").show()
print(""input bytes for old schema table: "" + inputBytes) // print 1655 

spark.sql(""create table orc_table_new_schema (id int, name string, value double) STORED AS ORC;"")
spark.sql(""insert overwrite table orc_table_new_schema select 1, 'name1', 1000.05"")
inputBytes = 0L
spark.sql(""select value from orc_table_new_schema"").show()
print(""input bytes for new schema table: "" + inputBytes) // print 1641
{code}
This example is run on spark3.0 with default flags. In this example, I create orc table orc_table_old_schema, which schema has no field name and is written before HIVE-4243, to trigger this issue. You can see that input bytes for table orc_table_old_schema is 14 bytes more than table orc_table_new_schema.

The reason is that spark3.0 default use native reader rather than hive reader to read orc table, and native reader read all columns for old hive schema table and read only pruning columns (in this example, only column 'value' is read) for new hive schema table.

The following flags enable native reader: set spark.sql.hive.convertMetastoreOrc=true; set spark.sql.orc.impl=native; both flags value are spark3.0's default value

Then I dig into spark code and find this:  [https://github.com/apache/spark/blob/branch-3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala#L149 |https://github.com/apache/spark/blob/branch-3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala#L149]

It looks like read all columns for old hive schema (which has no field names) is by design for spark3.0

In my company data, some table schema is old hive, while some table schema is new hive. The performance of query reading old hive table decreases a lot when I enable native reader in spark3.0. This is main block for us to switch hive reader to native reader in spark3.0. 

My questions is:

#1 Do you have plan to support column pruning for old hive schema in native orc reader?

#2 If question #1's answer is No. Is there some potential issue if code is fixed to support column pruning?

 ",2021-04-29T13:35:54.720+0000,2021-05-02T01:37:06.893+0000,,Major
SPARK-24120,Show `Jobs` page when `jobId` is missing,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12532905'>, <JIRA IssueLink: id='12532906'>, <JIRA IssueLink: id='12533071'>]","For now, users try to connect {{job}} page without {{jobid}}, Spark UI shows only error page. It's not incorrect but helpless to users. It would be better to redirect to `jobs` page to select proper job. This, actually, happens when users use yarn mode. Because of yarn's bug(YARN-6615), some parameters aren't passed to Spark's driver UI with now the latest version of Yarn. It's also mentioned at SPARK-20772.",2018-04-30T02:52:22.189+0000,2021-05-25T01:50:33.552+0000,Incomplete,Minor
CALCITE-929,Calls to AbstractRelNode may result in NPE,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12446806'>],Calls to methods {{isDistinct}} and {{isKey}} may result in NPE if the value return by the metadata provider is null.,2015-10-21T19:48:45.725+0000,2015-11-10T08:02:49.444+0000,Fixed,Major
RANGER-791,Update ranger to work with HiveAuthorizer interface changes,RANGER,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12452576'>, <JIRA IssueLink: id='12452323'>]","HIVE-12698 is introducing changes to the HiveAuthorizer api. This will be part of Hive 2.0.0 release. 
Change is needed to provide a implementation in ranger of the additional api method being added to the interface.
",2015-12-18T04:57:40.600+0000,2015-12-21T20:49:47.778+0000,Fixed,Major
SPARK-19109,ORC metadata section can sometimes exceed protobuf message size limit,SPARK,Bug,Resolved,[],6,"[<JIRA IssueLink: id='12504732'>, <JIRA IssueLink: id='12518116'>, <JIRA IssueLink: id='12491969'>, <JIRA IssueLink: id='12524457'>, <JIRA IssueLink: id='12524455'>, <JIRA IssueLink: id='12524456'>]","Basically, Spark inherits HIVE-11592 from its Hive dependency. From that issue:

If there are too many small stripes and with many columns, the overhead for storing metadata (column stats) can exceed the default protobuf message size of 64MB. Reading such files will throw the following exception
{code}
Exception in thread ""main"" com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)
        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)
        at com.google.protobuf.CodedInputStream.readRawBytes(CodedInputStream.java:811)
        at com.google.protobuf.CodedInputStream.readBytes(CodedInputStream.java:329)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics.<init>(OrcProto.java:1331)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics.<init>(OrcProto.java:1281)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics$1.parsePartialFrom(OrcProto.java:1374)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics$1.parsePartialFrom(OrcProto.java:1369)
        at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics.<init>(OrcProto.java:4887)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics.<init>(OrcProto.java:4803)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics$1.parsePartialFrom(OrcProto.java:4990)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics$1.parsePartialFrom(OrcProto.java:4985)
        at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics.<init>(OrcProto.java:12925)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics.<init>(OrcProto.java:12872)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics$1.parsePartialFrom(OrcProto.java:12961)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics$1.parsePartialFrom(OrcProto.java:12956)
        at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata.<init>(OrcProto.java:13599)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata.<init>(OrcProto.java:13546)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata$1.parsePartialFrom(OrcProto.java:13635)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata$1.parsePartialFrom(OrcProto.java:13630)
        at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)
        at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)
        at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)
        at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata.parseFrom(OrcProto.java:13746)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl$MetaInfoObjExtractor.<init>(ReaderImpl.java:468)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:314)
        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)
        at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:67)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}

This is fixed in Hive 1.3, so it should be fairly straightforward to pick up the patch.

As a side note: Spark's management of its Hive fork/dependency seems incredibly arcane to me. Surely there's a better way than publishing to central from developers' personal repos.",2017-01-06T20:04:26.727+0000,2018-01-17T06:46:00.433+0000,Fixed,Major
IMPALA-5177,"Error making alter_table rpc, job failure",IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12507590'>],"{code}
20:54:13 FAIL data_errors/test_data_errors.py::TestTimestampErrors::()::test_timestamp_scan_agg_errors[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 1, 'num_nodes': 0} | table_format: text/none]
20:54:13 =================================== FAILURES ===================================
20:54:13  TestTimestampErrors.test_timestamp_scan_agg_errors[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 1, 'num_nodes': 0} | table_format: text/none] 
20:54:13 [gw1] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/bin/../infra/python/env/bin/python
20:54:13 data_errors/test_data_errors.py:152: in test_timestamp_scan_agg_errors
20:54:13     self._setup_test_table(FQ_TBL_NAME)
20:54:13 data_errors/test_data_errors.py:148: in _setup_test_table
20:54:13     self.client.execute(alter_stmt)
20:54:13 common/impala_connection.py:160: in execute
20:54:13     return self.__beeswax_client.execute(sql_stmt, user=user)
20:54:13 beeswax/impala_beeswax.py:173: in execute
20:54:13     handle = self.__execute_query(query_string.strip(), user=user)
20:54:13 beeswax/impala_beeswax.py:337: in __execute_query
20:54:13     handle = self.execute_query_async(query_string, user=user)
20:54:13 beeswax/impala_beeswax.py:333: in execute_query_async
20:54:13     return self.__do_rpc(lambda: self.imp_service.query(query,))
20:54:13 beeswax/impala_beeswax.py:458: in __do_rpc
20:54:13     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
20:54:13 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
20:54:13 E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
20:54:13 E    MESSAGE: 
20:54:13 E   ImpalaRuntimeException: Error making 'alter_table' RPC to Hive Metastore: 
20:54:13 E   CAUSED BY: MetaException: Exception thrown when executing query
{code}

Looks like there is something failing in the Hive rpc.",2017-04-06T21:11:02.524+0000,2017-06-26T19:52:29.201+0000,Fixed,Critical
SPARK-34512,Disable validate default values when parsing Avro schemas,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12624868'>, <JIRA IssueLink: id='12609191'>]","This is a regression problem. How to reproduce this issue:
{code:scala}
  // Add this test to HiveSerDeReadWriteSuite
  test(""SPARK-34512"") {
    withTable(""t1"") {
      hiveClient.runSqlHive(
        """"""
          |CREATE TABLE t1
          |  ROW FORMAT SERDE
          |  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
          |  STORED AS INPUTFORMAT
          |  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
          |  OUTPUTFORMAT
          |  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
          |  TBLPROPERTIES (
          |    'avro.schema.literal'='{
          |      ""namespace"": ""org.apache.spark.sql.hive.test"",
          |      ""name"": ""schema_with_default_value"",
          |      ""type"": ""record"",
          |      ""fields"": [
          |         {
          |           ""name"": ""ARRAY_WITH_DEFAULT"",
          |           ""type"": {""type"": ""array"", ""items"": ""string""},
          |           ""default"": null
          |         }
          |       ]
          |    }')
          |"""""".stripMargin)

      spark.sql(""select * from t1"").show
    }
  }
{code}


{noformat}
org.apache.avro.AvroTypeException: Invalid default for field ARRAY_WITH_DEFAULT: null not a {""type"":""array"",""items"":""string""}
	at org.apache.avro.Schema.validateDefault(Schema.java:1571)
	at org.apache.avro.Schema.access$500(Schema.java:87)
	at org.apache.avro.Schema$Field.<init>(Schema.java:544)
	at org.apache.avro.Schema.parse(Schema.java:1678)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1425)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1413)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:268)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:111)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.determineSchemaOrReturnErrorSchema(AvroSerDe.java:187)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:107)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:83)
	at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:450)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:437)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:831)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:867)
	at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:820)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:800)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:787)

{noformat}
",2021-02-23T23:34:50.832+0000,2021-10-20T08:20:58.134+0000,Fixed,Major
ORC-562,"Don't wrap readerSchema in acidSchema, if readerSchema is already acid",ORC,Bug,Closed,[],1,[<JIRA IssueLink: id='12572631'>],"{code:sql}
create table tbl1 (a int, b string) partitioned by (ds string) stored as orc tblproperties ('transactional'='true');
insert into tbl1 partition (ds) values (1, 'fred', 'today'), (2, 'wilma', 'yesterday');
{code}
As this table is transactional, all the modifications will generate a new delta directory, containing a delta file in orc format. The schema of this file will be
{code:sql}
struct<operation:int,originaltransaction:bigint,bucket:int,rowid:bigint,currenttransaction:bigint,row:struct<a:int,b:string>>
{code}

If I create a new partitioned table with the very same schema, and change the partition location to one of the delta directories, I would assume that I would be able to run queries against the contents of the delta file. 

Right now this is not possible in orc, because the original readerschema is wrapped in acidschema again, regardless that the readerschema is already acid.

{code:sql}
struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<operation:int,originaltransaction:bigint,bucket:int,rowid:bigint,currenttransaction:bigint,row:struct<a:int,b:string>>>
{code}
",2019-10-24T08:05:08.462+0000,2019-11-25T16:16:44.839+0000,Fixed,Major
SPARK-23554,Hive's textinputformat.record.delimiter equivalent in Spark,SPARK,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12528362'>],"It would be great if Spark would support an option similar to Hive's {{textinputformat.record.delimiter }} in spark-csv reader.

We currently have to create Hive tables to workaround this missing functionality natively in Spark.

{{textinputformat.record.delimiter}} was introduced back in 2011 in map-reduce era -
 see MAPREDUCE-2254.

As an example, one of the most common use cases for us involving {{textinputformat.record.delimiter}} is to read multiple lines of text that make up a ""record"". Number of actual lines per ""record"" is varying and so {{textinputformat.record.delimiter}} is a great solution for us to process these files natively in Hadoop/Spark (custom .map() function then actually does processing of those records), and we convert it to a dataframe.. ",2018-03-01T20:57:08.880+0000,2018-04-12T16:34:08.910+0000,Duplicate,Major
BEAM-2543,HBaseIOTest fails tests if run from a directory with spaces,BEAM,Bug,Open,[],1,[<JIRA IssueLink: id='12510413'>],"HBaseIOTest failed Jenkins Java cross-JDK tests since June 22 as can tell from Jenkins history. Both JDK7, OpenJDK7&8 are affected.

Errors:
{code}
2017-06-29\T\18:13:56.187 [ERROR] org.apache.beam.sdk.io.hbase.HBaseIOTest  Time elapsed: 0 s  <<< ERROR!
java.io.IOException: Shutting down
	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:235)
	at org.apache.hadoop.hbase.MiniHBaseCluster.<init>(MiniHBaseCluster.java:97)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1036)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1002)
	at org.apache.beam.sdk.io.hbase.HBaseIOTest.beforeClass(HBaseIOTest.java:102)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.apache.maven.surefire.junitcore.pc.Scheduler$1.run(Scheduler.java:393)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMasterIllegal character in path at index 89: file:/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Java_JDK_Versions_Test/jdk/JDK 1.7 (latest)/label/beam/sdks/java/io/hbase/target/test-data/b11a0828-4628-4fe9-885d-073fb641ddc9
	at org.apache.hadoop.hbase.util.JVMClusterUtil.createMasterThread(JVMClusterUtil.java:143)
	at org.apache.hadoop.hbase.LocalHBaseCluster.addMaster(LocalHBaseCluster.java:220)
	at org.apache.hadoop.hbase.LocalHBaseCluster.<init>(LocalHBaseCluster.java:155)
	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:217)
	... 23 more
Caused by: java.lang.IllegalArgumentException: Illegal character in path at index 89: file:/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Java_JDK_Versions_Test/jdk/JDK 1.7 (latest)/label/beam/sdks/java/io/hbase/target/test-data/b11a0828-4628-4fe9-885d-073fb641ddc9
	at java.net.URI.create(URI.java:859)
	at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:175)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
	at org.apache.hadoop.hbase.fs.HFileSystem.<init>(HFileSystem.java:80)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeFileSystem(HRegionServer.java:613)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:564)
	at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:412)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hbase.util.JVMClusterUtil.createMasterThread(JVMClusterUtil.java:139)
	... 26 more
Caused by: java.net.URISyntaxException: Illegal character in path at index 89: file:/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Java_JDK_Versions_Test/jdk/JDK 1.7 (latest)/label/beam/sdks/java/io/hbase/target/test-data/b11a0828-4628-4fe9-885d-073fb641ddc9
	at java.net.URI$Parser.fail(URI.java:2829)
	at java.net.URI$Parser.checkChars(URI.java:3002)
	at java.net.URI$Parser.parseHierarchical(URI.java:3086)
	at java.net.URI$Parser.parse(URI.java:3034)
	at java.net.URI.<init>(URI.java:595)
	at java.net.URI.create(URI.java:857)
	... 37 more
{code}

Latest build link: https://builds.apache.org/view/Beam/job/beam_PostCommit_Java_JDK_Versions_Test/145/

Looks like the problem is URI parsing failed with space character in between. ",2017-06-29T18:36:23.193+0000,2022-06-03T18:35:29.181+0000,,P3
HCATALOG-621,bin/hcat should include hbase jar and dependencies in the classpath,HCATALOG,Bug,Patch Available,[],2,"[<JIRA IssueLink: id='12367756'>, <JIRA IssueLink: id='12384950'>]","Attempting to use the HBaseHCatStorageHandler results in

{noformat}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/MasterNotRunningException
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hcatalog.common.HCatUtil.getStorageHandler(HCatUtil.java:523)
        at org.apache.hcatalog.cli.SemanticAnalysis.CreateTableHook.postAnalyze(CreateTableHook.java:198)
        at org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.postAnalyze(HCatSemanticAnalyzer.java:244)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:909)
        at org.apache.hcatalog.cli.HCatDriver.run(HCatDriver.java:42)
        at org.apache.hcatalog.cli.HCatCli.processCmd(HCatCli.java:250)
        at org.apache.hcatalog.cli.HCatCli.processLine(HCatCli.java:204)
        at org.apache.hcatalog.cli.HCatCli.processFile(HCatCli.java:223)
        at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:168)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.MasterNotRunningException
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        ... 18 more
{noformat}",2013-02-19T21:28:04.988+0000,2014-03-18T22:49:19.210+0000,,Major
PHOENIX-1637,Use supported region locking protocols when available,PHOENIX,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12408957'>, <JIRA IssueLink: id='12407235'>]","Another major use of HRegion methods in Phoenix is locking. HBase has proposed adding a Region interface, which would be a supportable subset of HRegion methods applications like Phoenix may need. If it exposes read (region) and write (row) locks then we can use them:
- Where we lock META rows when making transactional changes to system tables
- When we process aggregates, sequences, and index or system table updates",2015-02-04T20:38:40.013+0000,2015-02-24T19:07:12.307+0000,Duplicate,Major
SPARK-14743,Improve delegation token handling in secure clusters,SPARK,Improvement,Resolved,[],7,"[<JIRA IssueLink: id='12474034'>, <JIRA IssueLink: id='12475300'>, <JIRA IssueLink: id='12490716'>, <JIRA IssueLink: id='12480167'>, <JIRA IssueLink: id='12490368'>, <JIRA IssueLink: id='12589925'>, <JIRA IssueLink: id='12476738'>]","In a way, I'd consider this a parent bug of SPARK-7252.

Spark's current support for delegation tokens is a little all over the place:
- for HDFS, there's support for re-creating tokens if a principal and keytab are provided
- for HBase and Hive, Spark will fetch delegation tokens so that apps can work in cluster mode, but will not re-create them, so apps that need those will stop working after 7 days
- for anything else, Spark doesn't do anything. Lots of other services use delegation tokens, and supporting them as data sources in Spark becomes more complicated because of that. e.g., Kafka will (hopefully) soon support them.

It would be nice if Spark had consistent support for handling delegation tokens regardless of who needs them. I'd list these as the requirements:

- Spark to provide a generic interface for fetching delegation tokens. This would allow Spark's delegation token support to be extended using some plugin architecture (e.g. Java services), meaning Spark itself doesn't need to support every possible service out there.

This would be used to fetch tokens when launching apps in cluster mode, and when a principal and a keytab are provided to Spark.

- A way to manually update delegation tokens in Spark. For example, a new SparkContext API, or some configuration that tells Spark to monitor a file for changes and load tokens from said file.

This would allow external applications to manage tokens outside of Spark and be able to update a running Spark application (think, for example, a job sever like Oozie, or something like Hive-on-Spark which manages Spark apps running remotely).

- A way to notify running code that new delegation tokens have been loaded.

This may not be strictly necessary; it might be possible for code to detect that, e.g., by peeking into the UserGroupInformation structure. But an event sent to the listener bus would allow applications to react when new tokens are available (e.g., the Hive backend could re-create connections to the metastore server using the new tokens).


Also, cc'ing [~busbey] and [~steve_l] since you've talked about this in the mailing list recently.",2016-04-20T03:09:46.918+0000,2020-06-01T05:18:23.159+0000,Fixed,Major
TEZ-4370,Apache Tez adoption umbrella,TEZ,Bug,Open,[],5,"[<JIRA IssueLink: id='12631061'>, <JIRA IssueLink: id='12631059'>, <JIRA IssueLink: id='12631060'>, <JIRA IssueLink: id='12633931'>, <JIRA IssueLink: id='12630744'>]","Not sure about the long-term purpose of this jira, maybe something like:
1. link tickets from other projects related to tez adoption
2. convert tez tickets as subtasks of this jira, or just link those",2022-01-07T18:25:32.452+0000,2022-02-18T09:58:31.004+0000,,Major
TEZ-349,Add initial support for Tez Sessions,TEZ,Sub-task,Closed,[],3,"[<JIRA IssueLink: id='12374289'>, <JIRA IssueLink: id='12373513'>, <JIRA IssueLink: id='12374320'>]","Requested by [~hagleitn].

Essentially, users of TezClient (Hive in this case) would create a Tez session. This session can then be used to submit multiple DAGs. 

Internally, TezClient can control whether an AM is run within the same process, in unmanaged mode, on the cluster etc. Initially, we'd likely end up creating one AM per submitted DAG. Once TEZ-340 is in place, the same AM could be re-used for multiple non-concurrent DAGs.",2013-08-07T05:15:57.295+0000,2013-12-01T20:21:34.185+0000,Fixed,Major
PHOENIX-5269,PhoenixAccessController should use AccessChecker instead of AccessControlClient for permission checks,PHOENIX,Bug,Closed,[],5,"[<JIRA IssueLink: id='12560269'>, <JIRA IssueLink: id='12560272'>, <JIRA IssueLink: id='12560250'>, <JIRA IssueLink: id='12560771'>, <JIRA IssueLink: id='12561593'>]","PhoenixAccessController should use AccessChecker instead of AccessControlClient for permission checks. 

In HBase, every RegionServer's AccessController maintains a local cache of permissions. At startup time they are initialized from the ACL table. Whenever the ACL table is changed (via grant or revoke) the AC on the ACL table ""broadcasts"" the change via zookeeper, which updates the cache. This is performed and managed by TableAuthManager but is exposed as API by AccessChecker. AccessChecker is the result of a refactor that was committed as far back as branch-1.4 I believe.

Phoenix implements its own access controller and is using the client API AccessControlClient instead. AccessControlClient does not cache nor use the ZK-based cache update mechanism, because it is designed for client side use.

The use of AccessControlClient instead of AccessChecker is not scalable. Every permissions check will trigger a remote RPC to the ACL table, which is generally going to be a single region hosted on a single RegionServer. ",2019-05-05T21:16:07.307+0000,2019-12-21T00:56:11.235+0000,Fixed,Critical
SENTRY-2338,Enable Sentry code disabled by HIVE-18762,SENTRY,Task,Open,[],2,"[<JIRA IssueLink: id='12540584'>, <JIRA IssueLink: id='12540583'>]","Once Sentry integrates with Hive version that contains HIVE-18762, we should enable the code that is disabled because HIVE-18762 is not in the hive version integrated with Sentry right now. ",2018-08-08T20:36:44.728+0000,2019-06-14T18:38:52.302+0000,,Major
CALCITE-2247,Simplify AND and OR conditions using predicates,CALCITE,Improvement,Closed,[],5,"[<JIRA IssueLink: id='12565248'>, <JIRA IssueLink: id='12531423'>, <JIRA IssueLink: id='12534239'>, <JIRA IssueLink: id='12537594'>, <JIRA IssueLink: id='12531818'>]","Simplify expressions like: {code}a = 1 AND (a = 1 OR a = 2){code} to {code}a = 1{code}

Conditions to apply will be:
* in an AND condition there exists a comparison(c) and an OR (o)
* o and c only reference 1 variable

See HIVE-19097 for more info.",2018-04-10T11:55:23.115+0000,2019-07-12T12:46:03.964+0000,Fixed,Major
ZOOKEEPER-1422,Support _HOST substitution in JAAS configuration ,ZOOKEEPER,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12384834'>, <JIRA IssueLink: id='12364040'>]","At the moment a JAAS configuration file needs to be created with the Kerberos principal specified as user/host. It would be much easier for deployment automation if the host portion could be resolved at startup time, as supported in Hadoop (something like user/_HOST instead of user/hostname). A configuration alternative to global JAAS conf would be even better (via direct properties in zoo.cfg?).
",2012-03-16T02:03:42.272+0000,2018-01-18T16:54:09.856+0000,Implemented,Major
SQOOP-384,Sqoop is incompatible with Hadoop prior to 0.21,SQOOP,Bug,Resolved,"[<JIRA Issue: key='SQOOP-412', id='12535662'>, <JIRA Issue: key='SQOOP-413', id='12535663'>]",1,[<JIRA IssueLink: id='12347175'>],"The following are the APIs Sqoop relies upon, which are not available in Hadoop prior to 0.21:
   org.apache.hadoop.conf.Configuration.getInstances
   org.apache.hadoop.mapreduce.lib.db.DBWritable 
   org.apache.hadoop.mapreduce.lib.input.CombineFileSplit 
   org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat 
   org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader ",2011-10-29T00:00:30.631+0000,2012-02-01T19:59:54.961+0000,Fixed,Major
INFRA-10800,git delete remote branch no longer supported?,INFRA,Bug,Closed,[],12,"[<JIRA IssueLink: id='12452703'>, <JIRA IssueLink: id='12449632'>, <JIRA IssueLink: id='12452163'>, <JIRA IssueLink: id='12452162'>, <JIRA IssueLink: id='12450074'>, <JIRA IssueLink: id='12449396'>, <JIRA IssueLink: id='12451884'>, <JIRA IssueLink: id='12451885'>, <JIRA IssueLink: id='12452188'>, <JIRA IssueLink: id='12452739'>, <JIRA IssueLink: id='12449397'>, <JIRA IssueLink: id='12452159'>]","hi,

when we're trying to clean some remote branches we found following error:

Kylin git:(2.x-staging) ✗ git push origin :KYLIN-1112
remote: error: denying ref deletion for refs/heads/KYLIN-1112
To https://git-wip-us.apache.org/repos/asf/incubator-kylin.git
 ! [remote rejected] KYLIN-1112 (deletion prohibited)
error: failed to push some refs to 'https://git-wip-us.apache.org/repos/asf/incubator-kylin.git'

did infra change any policy on this? what is the suggested use of branches now?",2015-11-19T11:19:38.001+0000,2016-01-13T16:07:04.931+0000,Later,Major
ORC-482,Error writing to OrcNewOutputFormat using MapR MultipleOutputs,ORC,Bug,Open,[],1,[<JIRA IssueLink: id='12641239'>],"We reading data from ORC files and writing it back to ORC and Parquet format using MultipleOutputs. Our job is Map only and does not have a reducer. We are getting following type of errors in some mappers which fails the entire job. Let me know if more information is required.

 

Error: java.lang.RuntimeException: Overflow of newLength. smallBuffer.length=1073741824, nextElemLength=300947 ",2019-03-22T09:22:55.763+0000,2022-06-06T05:53:26.187+0000,,Major
TEZ-952,"Port MAPREDUCE-5209, MAPREDUCE-5251",TEZ,Task,Closed,[],2,"[<JIRA IssueLink: id='12384992'>, <JIRA IssueLink: id='12384993'>]",,2014-03-19T18:18:20.842+0000,2014-03-30T08:01:46.660+0000,Fixed,Major
INFRA-2461,Nexus Access for Apache Hadoop Hbase,INFRA,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12329006'>],The Hadoop HBase project needs to be publish maven artifacts - (http://hbase.org) .  Our groupid would be org.apache.hadoop and our artifactid woud be hbase.  Thanks.,2010-01-24T05:08:38.883+0000,2014-02-02T18:47:32.892+0000,Duplicate,Major
APEXCORE-608,Streaming Containers use stale RPC proxy after connection is closed,APEXCORE,Bug,Closed,[],1,[<JIRA IssueLink: id='12491989'>],"When Application is killed and Application Master is terminated, Streaming Containers initiate container exit sequence and use disconnected RPC proxy to report errors back to already terminated Application Master.",2017-01-13T16:18:52.524+0000,2017-05-05T10:46:05.982+0000,Fixed,Minor
AMBARI-22981,Update Hadoop RPC Encryption Properties During Upgrade,AMBARI,Task,Resolved,[],2,"[<JIRA IssueLink: id='12527222'>, <JIRA IssueLink: id='12527073'>]","When *HDP 3.0.0* is installed, clients should have the ability to choose encrypted communication over RPC when talking to core hadoop components. Today, the properties that control this are:
 - {{core-site.xml : hadoop.rpc.protection = authentication}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication}}

The new value of {{privacy}} enables clients to choose an encrypted means of communication. By keeping {{authentication}} first, it will be taken as the default mechanism so that wire encryption is not automatically enabled by accident.

The following properties should be changed to add {{privacy}}:
 - {{core-site.xml : hadoop.rpc.protection = authentication,privacy}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication,privacy}}

The following are cases when this needs to be performed:
 - During Kerberization (this case is covered by AMBARI-22803)
 - During a stack upgrade to any version of *HDP 3.0.0*, they should be automatically merged

Blueprint deployment is not a scenario being covered here.",2018-02-14T07:53:37.987+0000,2018-03-09T16:03:25.632+0000,Fixed,Critical
SENTRY-67,Complete Hive Integration points,SENTRY,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12378687'>, <JIRA IssueLink: id='12378682'>, <JIRA IssueLink: id='12378685'>, <JIRA IssueLink: id='12378681'>]",This issue will be blocked by any Hive change required to make Sentry work with Hive Trunk.,2013-11-19T14:37:31.264+0000,2017-07-10T15:28:56.833+0000,Fixed,Critical
TEZ-1164,Only events for tasks should be buffered in Initializing state,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12389175'>],Other events like eg VertexManagerEvent should be routed as they be needed in getting the vertex out of that state.,2014-06-02T06:06:48.423+0000,2014-09-06T01:35:28.769+0000,Fixed,Major
PHOENIX-5274,ConnectionQueryServiceImpl#ensureNamespaceCreated and ensureTableCreated should use HBase APIs that do not require ADMIN permissions for existence checks,PHOENIX,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12560360'>],"[HBASE-22377|https://issues.apache.org/jira/browse/HBASE-22377] will introduce a new API that does not require ADMIN permissions to check the existence of a namespace.

Currently, CQSI#ensureNamespaceCreated calls HBaseAdmin#getNamespaceDescriptor which eventually on the server causes a call to AccessController#preGetNamespaceDescriptor. This tries to acquire ADMIN permissions on the namespace. We should ideally use the new API provided by HBASE-22377 which does not require the phoenix client to get ADMIN permissions on the namespace. We should acquire ADMIN permissions only in case we need to create the namespace if it doesn't already exist.

Similarly, CQSI#ensureTableCreated should first check the existence of a table before trying to do HBaseAdmin#getTableDescriptor since this requires CREATE and ADMIN permissions.

",2019-05-07T21:34:54.882+0000,2022-06-23T15:02:08.744+0000,Fixed,Major
TEZ-4157,ShuffleHandler: upgrade to Netty4 and remove Netty3 dependency from tez,TEZ,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12635116'>, <JIRA IssueLink: id='12604680'>]","-In the dependency tree, there are 2 occurrences of compile scope direct netty dependencies, however, they're not used at all. I compiled locally successfully without them. E.g. when investigating blackduck alerts (complaining about netty deps for current 3.10.5.Final), it would be cleaner to start from a dependency tree where Tez doesn't depend on netty directly in order to eliminate its responsibility (and move the focus to underlying hadoop for instance).-

Tez depends on netty3 almost only in ShuffleHandler and some related classes. We can eliminate netty3 by upgrading it, but this effort might involve some testing due to fundamental [changes from netty3->netty4|https://netty.io/wiki/new-and-noteworthy-in-4.0.html] + we don't have a reference yet, as [hadoop's ShuffleHandler|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java] is still on netty3.

As per the netty documentation, we can also expect some performance improvement (e.g. Pooled buffers).

Background:
netty4 migration guideline: https://netty.io/wiki/new-and-noteworthy-in-4.0.html
articles of possible performance improvement:
https://blog.twitter.com/engineering/en_us/a/2013/netty-4-at-twitter-reduced-gc-overhead.html
https://developer.squareup.com/blog/upgrading-a-reverse-proxy-from-netty-3-to-4/
 ",2020-04-27T11:32:22.831+0000,2022-03-07T13:54:05.356+0000,Fixed,Major
SPARK-5439,Expose yarn app id for yarn mode,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12602400'>, <JIRA IssueLink: id='12409777'>, <JIRA IssueLink: id='12406569'>]","When submitting Spark apps on YARN, the caller should be able to get back the YARN app ID programmatically. ",2015-01-27T22:53:05.930+0000,2020-11-03T09:09:08.368+0000,Implemented,Major
TEZ-4145,Reduce lock contention in TezSpillRecord,TEZ,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12585704'>],"Observed the following stack trace when debugging one of the jobs.

Though {{TezSpillRecord}} uses {{LocalFileSystem}}, it ends up getting blocked due to FileSystem cache.

It would be good to have a way of passing RawLocalFileSystem to TezSpillRecord.
{noformat}
""Task-Executor-11"" #273 daemon prio=5 os_prio=0 tid=0x00007fe204664800 nid=0x2343d2 waiting on condition [0x00007fe1fcfda000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.ipc.Client.stop(Client.java:1329)
	at org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:113)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.close(ProtobufRpcEngine.java:302)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:677)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.close(ClientNamenodeProtocolTranslatorPB.java:304)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:672)
	at org.apache.hadoop.io.retry.DefaultFailoverProxyProvider.close(DefaultFailoverProxyProvider.java:57)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor.close(RetryInvocationHandler.java:234)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.close(RetryInvocationHandler.java:444)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:677)
	at org.apache.hadoop.hdfs.DFSClient.closeConnectionToNamenode(DFSClient.java:592)
	at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:633)
	- locked <0x00007fed071063a0> (a org.apache.hadoop.hdfs.DFSClient)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1358)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:3463)
	- locked <0x00007fe63d000000> (a org.apache.hadoop.fs.FileSystem$Cache)
	at org.apache.hadoop.fs.FileSystem.closeAllForUGI(FileSystem.java:576)
	at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.callInternal(TaskRunnerCallable.java:299)
	at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.callInternal(TaskRunnerCallable.java:93)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)



""New I/O worker #44"" #80 prio=5 os_prio=0 tid=0x00007fede2a03000 nid=0x233f2b waiting for monitor entry [0x00007fe20cd3d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3345)
	- waiting to lock <0x00007fe63d000000> (a org.apache.hadoop.fs.FileSystem$Cache)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:435)
	at org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.<init>(TezSpillRecord.java:65)
	at org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.<init>(TezSpillRecord.java:58)
	at org.apache.hadoop.hive.llap.shufflehandler.IndexCache.readIndexFileToCache(IndexCache.java:121)
	at org.apache.hadoop.hive.llap.shufflehandler.IndexCache.getIndexInformation(IndexCache.java:70)
	at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle.getMapOutputInfo(ShuffleHandler.java:887)
	at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle.populateHeaders(ShuffleHandler.java:908)
	at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:805)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:142)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

{noformat}
Note: For the IPC sleep issue, it is related to https://issues.apache.org/jira/browse/HADOOP-16126
  ",2020-04-13T11:16:24.451+0000,2020-04-15T14:08:36.025+0000,Fixed,Major
SPARK-19143,API in Spark for distributing new delegation tokens (Improve delegation token handling in secure clusters),SPARK,Improvement,Resolved,[],6,"[<JIRA IssueLink: id='12490716'>, <JIRA IssueLink: id='12532510'>, <JIRA IssueLink: id='12490717'>, <JIRA IssueLink: id='12500341'>, <JIRA IssueLink: id='12490719'>, <JIRA IssueLink: id='12511993'>]","Spin off from SPARK-14743 and comments chain in [recent comments| https://issues.apache.org/jira/browse/SPARK-5493?focusedCommentId=15802179&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15802179] in SPARK-5493.

Spark currently doesn't have a way for distribution new delegation tokens. Quoting [~vanzin] from SPARK-5493 
{quote}
IIRC Livy doesn't yet support delegation token renewal. Once it reaches the TTL, the session is unusable.
There might be ways to hack support for that without changes in Spark, but I'd like to see a proper API in Spark for distributing new delegation tokens. I mentioned that in SPARK-14743, but although that bug is closed, that particular feature hasn't been implemented yet.
{quote}

Other thoughts?",2017-01-10T04:20:12.153+0000,2021-05-25T01:50:31.178+0000,Incomplete,Major
BIGTOP-1366,"Updated, Richer Model for Generating Data for BigPetStore ",BIGTOP,Improvement,Resolved,[],9,"[<JIRA IssueLink: id='12401669'>, <JIRA IssueLink: id='12401673'>, <JIRA IssueLink: id='12401675'>, <JIRA IssueLink: id='12401677'>, <JIRA IssueLink: id='12401676'>, <JIRA IssueLink: id='12401944'>, <JIRA IssueLink: id='12401671'>, <JIRA IssueLink: id='12397887'>, <JIRA IssueLink: id='12400890'>]","BigPetStore uses synthetic data as the basis for its workflow.  BPS's current model for generating customer data is sufficient for basic testing of the Hadoop ecosystem, **but the model is very basic and lacks sufficient complexity for embedding interesting patterns into the data**.  

As a result, **more complex, scalable testing such as testing clustering algorithms in Mahout on non-trivial data or multidimensional data with factors influencing it** is not currently possible.

Efforts are currently underway to incrementally improve the current model (see BIGTOP-1271 and BIGTOP-1272).  

To create a model that can that incorporate **realistic, non-hierarchichal patterns** and input data to generate rich customer/transaction data with interesting correlations will require a re-imagining of the current model and its framework.

To support the improvements to the model in BigPetStore, I have been working on an **alternative ab initio model, developed from scratch**. Since the development of a new model involves substantial R&D work with more specialized tools (mathematical and plotting libraries), I'm doing the current work outside of BPS using the iPython Notebook environment.  Due to the long time frame, the model will be developed on a separate timeline to prevent slowing the development of BPS.  

Once the model has stabilized, I will begin incorporating the model into BPS itself.  One option is to implement the model in using Scala for clean integration with **spark** which is likely to play an increasingly important role in the hadoop ecosystem, and thus will be an important part of bigpetstore as a test/blueprint app.

",2014-07-07T14:08:08.440+0000,2015-03-18T22:47:45.533+0000,Fixed,Minor
ORC-412,[C++] ORC: Char(n) and Varchar(n) writers truncate to n bytes & corrupts multi-byte data,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12545197'>, <JIRA IssueLink: id='12545198'>]","https://github.com/apache/orc/blob/master/java/core/src/java/org/apache/orc/impl/writer/CharTreeWriter.java#L41

{code}
    itemLength = schema.getMaxLength();
    padding = new byte[itemLength];
  }
{code}

https://github.com/apache/orc/blob/master/java/core/src/java/org/apache/orc/impl/writer/VarcharTreeWriter.java#L48

{code}
      if (vector.noNulls || !vector.isNull[0]) {
        int itemLength = Math.min(vec.length[0], maxLength);
{code}

",2018-10-09T07:04:51.307+0000,2019-09-04T02:00:29.643+0000,Fixed,Major
SLIDER-947,build node map from yarn update reports; serve via REST/IPC,SLIDER,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12446801'>, <JIRA IssueLink: id='12447895'>]","Before doing any anti-affinity allocation, update the state of the cluster from the RM events, and make it visible via the IPC and REST APIs.

That way, we and the tests can see what is going on",2015-10-14T14:22:38.168+0000,2016-01-30T20:57:46.344+0000,Fixed,Major
SENTRY-1855,Improve scalability of permission delta updates,SENTRY,Bug,Open,"[<JIRA Issue: key='SENTRY-2063', id='13119796'>, <JIRA Issue: key='SENTRY-2064', id='13119797'>, <JIRA Issue: key='SENTRY-2065', id='13119800'>]",2,"[<JIRA IssueLink: id='12529214'>, <JIRA IssueLink: id='12512669'>]","Looking at the latest stress runs, we noticed that some of the transactions could fail to commit to the database (with Duplicate key exception) after exhausting all the retries.
This problem has become more evident if we have more number of clients connecting to Sentry to issue the permission updates. Was able to reproduce consistently with 15 clients doing 100 operations each.
In the past we introduced exponential backoff (SENTRY-1821) so as part of test run increased the defaults to 750ms sleep and 20 retries. But even after this, the cluster still shows up the transaction failures. This change also increases the latency of every transaction in sentry.
We need to revisit this and come up with a better way to solve this problem.

{code}
2017-07-13 13:18:14,449 ERROR org.apache.sentry.provider.db.service.persistent.TransactionManager: The transaction has reached max retry number, Exception thrown when executing query
javax.jdo.JDOException: Exception thrown when executing query
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:252)
	at org.apache.sentry.provider.db.service.persistent.SentryStore.getRole(SentryStore.java:294)
	at org.apache.sentry.provider.db.service.persistent.SentryStore.alterSentryRoleGrantPrivilegeCore(SentryStore.java:645)
	at org.apache.sentry.provider.db.service.persistent.SentryStore.access$500(SentryStore.java:101)
	at org.apache.sentry.provider.db.service.persistent.SentryStore$11.execute(SentryStore.java:601)
	at org.apache.sentry.provider.db.service.persistent.TransactionManager.executeTransaction(TransactionManager.java:159)
	at org.apache.sentry.provider.db.service.persistent.TransactionManager.access$100(TransactionManager.java:63)
	at org.apache.sentry.provider.db.service.persistent.TransactionManager$2.call(TransactionManager.java:213)
--
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:971)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2530)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1907)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2141)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1773)
	... 33 more
2017-07-13 13:18:14,450 ERROR org.apache.sentry.provider.db.service.thrift.SentryPolicyStoreProcessor: Unknown error for request: TAlterSentryRoleGrantPrivilegeRequest(protocol_version:2, requestorUserName:hive, roleName:2017_07_12_15_06_38_1_2_805, privileges:[TSentryPrivilege(privilegeScope:DATABASE, serverName:server1, dbName:2017_07_12_15_06_38_1_2, tableName:, URI:, action:*, createTime:1499904401222, grantOption:FALSE, columnName:)]), message: The transaction has reached max retry number, Exception thrown when executing query
java.lang.Exception: The transaction has reached max retry number, Exception thrown when executing query
	at org.apache.sentry.provider.db.service.persistent.TransactionManager$ExponentialBackoff.execute(TransactionManager.java:255)
	at org.apache.sentry.provider.db.service.persistent.TransactionManager.executeTransactionBlocksWithRetry(TransactionManager.java:209)
	at org.apache.sentry.provider.db.service.persistent.SentryStore.execute(SentryStore.java:3330)
	at org.apache.sentry.provider.db.service.persistent.SentryStore.alterSentryRoleGrantPrivilege(SentryStore.java:593)
	at org.apache.sentry.provider.db.service.persistent.SentryStore.alterSentryRoleGrantPrivileges(SentryStore.java:633)
	at org.apache.sentry.provider.db.service.thrift.SentryPolicyStoreProcessor.alter_sentry_role_grant_privilege(SentryPolicyStoreProcessor.java:256)
	at org.apache.sentry.provider.db.service.thrift.SentryPolicyService$Processor$alter_sentry_role_grant_privilege.getResult(SentryPolicyService.java:997)
	at org.apache.sentry.provider.db.service.thrift.SentryPolicyService$Processor$alter_sentry_role_grant_privilege.getResult(SentryPolicyService.java:982)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
{code}",2017-07-20T15:06:21.370+0000,2019-06-14T16:41:12.359+0000,,Major
IMPALA-11491,Support BINARY nested in complex types in select list,IMPALA,Sub-task,Open,[],1,[<JIRA IssueLink: id='12645431'>],"The question is how the print BINARY types when the parent complex type is printed to JSON. We can't rely on the existing Hive behavior, as it turned out to be buggy:
HIVE-26454",2022-08-10T16:58:06.769+0000,2022-08-10T16:58:18.201+0000,,Major
REEF-1288,Add a request ID to resource requests / allocated Evaluators,REEF,Improvement,Open,[],1,[<JIRA IssueLink: id='12462038'>],"In many cases, it is desirable to be able to assign an ID to a Evaluator request that is then carried by the allocated Evaluators received in response. ",2016-03-27T23:34:28.233+0000,2016-03-27T23:35:15.063+0000,,Major
YETUS-221,asflicense check exits multi-module Maven build early if there are pre-existing license problems.,YETUS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12451285'>],"The asflicense check runs {{mvn apache-rat:check}} from the root of the project (unless told otherwise).  For a multi-module build, the build exits with failure after the first occurrence of a module with a license problem.  It won't check subsequent modules.  If the patch in question is touching one of those subsequent modules, then asflicense won't check it.  A reviewer might incorrectly conclude that the patch is fine, because the pre-existing license problems are unrelated.",2015-12-08T22:01:10.645+0000,2015-12-15T18:06:35.467+0000,Fixed,Major
SPARK-23658,InProcessAppHandle uses the wrong class in getLogger,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12529187'>],"{{InProcessAppHandle}} uses {{ChildProcAppHandle}} as the class in {{getLogger}}, it should just use its own name.",2018-03-12T22:19:18.307+0000,2018-03-16T00:05:08.536+0000,Fixed,Minor
SPARK-17563,Add org/apache/spark/JavaSparkListener to make Spark-2.0.0 work with Hive-2.X.X,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12480453'>, <JIRA IssueLink: id='12480679'>]","According to https://issues.apache.org/jira/browse/SPARK-14358 JavaSparkListener was deleted from Spark-2.0.0, but Hive-2.X.X uses JavaSparkListener

{code}
package org.apache.hadoop.hive.ql.exec.spark.status.impl;

import ...

public class JobMetricsListener extends JavaSparkListener {
{code}

Configuring Hive-2.X.X on Spark-2.0.0 will give an exception:

{code}
2016-09-16T11:20:57,474 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(593)) - java.lang.NoClassDefFoundError: org/apache/spark/JavaSparkListener
{code}

Please add JavaSparkListener into Spark-2.0.0",2016-09-16T12:07:51.044+0000,2016-09-19T08:26:59.438+0000,Won't Fix,Major
YETUS-260,distclean step is not working,YETUS,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12452870'>, <JIRA IssueLink: id='12452869'>]","In MAPREDUCE-6583, test-patch reported that mapred-default.xml has many trailing whitespaces, because the test-patch disctclean step is not executed. ",2015-12-26T10:04:43.532+0000,2015-12-28T18:59:27.782+0000,Fixed,Critical
BIGTOP-732,Support running multiple HBase region servers,BIGTOP,New Feature,Closed,"[<JIRA Issue: key='BIGTOP-1632', id='12770876'>]",1,[<JIRA IssueLink: id='12359601'>],"Previously on the mailing list I submitted the idea of supporting multiple region server daemons on the same system. This can be done using the local-regionservers.sh and local-masters.sh scripts that we remove from our packaging (see BIGTOP-503), but apparently running multiple region servers in production can be useful. It should be possible through init scripts, and it should play nice with the more traditional use case.

The modified init script template should make it safe and intuitive to run multiple region servers and masters on the same system, but only the regionserver package is using the new template - I don't know of a good reason to run multiple masters in production. Using the init script as before will control a single region-server daemon EXACTLY as it did before. If you specify numbers as additional parameters, you can control multiple daemons: 

{noformat} 
service hbase-regionserver start # Starts a single region server daemon, as before
(all other commands, with no additional parameters, will work as before)

service hbase-regionserver start 1 2 3 4 # Starts a single region server daemon
service hbase-regionserver restart 2 4 # Restarts the even daemons
service hbase-regionserver stop 1 3 # Stops the odd daemons
service hbase-regionserver stop # Stops all region servers in any mode of operation
service hbase-regionserver restart 1 2 3 # Stops all region servers, then starts these 3
{noformat} 

I can see a case being made for changing the behavior of the stop and restart commands - so let me know if you disagree with the path I took. The log files and pid files get put in the same directory, but are also numbered according to their offset. The force-stop and force-reload command should also work as expected. When running a single daemon you can't start multiple daemons, and vice-versa. As recommended by Bruno for LSB-compliance and ease of administration, you can specify the offsets in /etc/hbase/conf/regionserver_offsets instead of on the command-line. Specifying offsets on the command-line anyway will override the file.",2012-10-09T17:48:17.998+0000,2015-01-29T21:35:59.094+0000,Fixed,Critical
SPARK-25174,ApplicationMaster suspends when unregistering itself from RM with extreme large diagnostic message,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12541408'>],"We recently ran into SPARK-18016 which has been fixed in v2.3.0. This JIRA is not about the issue in SPARK-18016 but the side-effect which it brings. When SPARK-18016 occurs, ApplicationMaster fails unregistering itself because the exception contains extreme large error information.

{code:java}
ERROR yarn.ApplicationMaster: User class threw exception: java.lang.RuntimeException: Error while decoding: java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.janino.JaninoRuntimeException: Constant pool has grown past JVM limit of 0xFFFF
/* 001 */ public java.lang.Object generate(Object[] references) {
....

/* 395656 */       mutableRow.update(0, value);
/* 395657 */     }
/* 395658 */
/* 395659 */     return mutableRow;
/* 395660 */   }
/* 395661 */ }
{code}

The above codegen text is included in the final message for AM to wave goodbye to RM, while it ends up crashing the rm's ZKRMStateStore for YARN-6125 not covering the unregisterApplicationMaster's message truncation. We also create an Jira on YARN Side https://issues.apache.org/jira/browse/YARN-8691 

Although SPARK-18016 fixed already, there are maybe other uncaught exceptions will cause this problem. I guess that we should limit the error message's size sent to RM while unregistering AM .",2018-08-21T08:54:45.658+0000,2020-05-17T18:14:04.750+0000,Fixed,Major
PHOENIX-3393,Use Iterables.removeIf instead of Iterator.remove in HBase filters,PHOENIX,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12483806'>],"This is a performance improvement to use Iterables.removeIf in the filterRowCells method of ColumnProjectionFilter as described here:

https://rayokota.wordpress.com/2016/10/20/tips-on-writing-custom-hbase-filters/",2016-10-20T18:57:45.855+0000,2016-10-21T11:44:44.416+0000,Fixed,Minor
TEZ-3856,API to access counters in InputInitializerContext,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12517905'>],Hive would like to publish some counters related to input splits during split generation. Tez doesn't expose TezCounters via InputIntializerContext. This ticket is to expose TezCounters via InputInitializerContext so that counters can be accessed during split generation.,2017-10-18T23:15:25.558+0000,2018-01-05T00:18:00.174+0000,Fixed,Major
INFRA-15373,Various Jenkins bits are in trouble,INFRA,Bug,Closed,[],2,"[<JIRA IssueLink: id='12518536'>, <JIRA IssueLink: id='12518543'>]","
It looks like Jenkins is no longer scheduling jobs.  I attempted to restart the Jenkins agent via the UI on H4 (see below) and that's when things appears to have stopped getting scheduled.  

Also, nodes H1, H2, H10, and H12 need to be kicked.


Additionally, I think I've discovered that the Hadoop HDFS unit tests for branch-2 are causing havoc on build nodes. ( tracking in HDFS-12711)  I'm at 50% condfidence the problem is OOM-killer related.  In majority cases, the nodes become unavailable entirely from Jenkins.  Today, H4 reported back data from inside the container which meant that it wasn't a kernel panic.  So I did a restart of the Jenkins agent but it still never fully came back from what I can tell.

In any case, I'm still trying to reproduce the problem locally but it's tough going.  I'm going to hard set which nodes certain tests run on to try and limit the damage though.  Additionally, I've been working on YETUS-561 in case it is OOM related.  From experiments, that seems to work when OOM actually is the problem: unit tests and the like are sufficiently sacrificed.

Anyway, sorry for the issues and thanks for the help.",2017-10-25T19:53:23.692+0000,2018-08-25T06:16:50.165+0000,Fixed,Blocker
PHOENIX-938,Use higher priority queue for index updates to prevent deadlock,PHOENIX,Bug,Closed,[],3,"[<JIRA IssueLink: id='12391191'>, <JIRA IssueLink: id='12408744'>, <JIRA IssueLink: id='12391422'>]","With our current global secondary indexing solution, a batched Put of table data causes a RS to do a batch Put to other RSs. This has the potential to lead to a deadlock if all RS are overloaded and unable to process the pending batched Put. To prevent this, we should use a higher priority queue to submit these Puts so that they're always processed before other Puts. This will prevent the potential for a deadlock under high load. Note that this will likely require some HBase 0.98 code changes and would not be feasible to implement for HBase 0.94.",2014-04-16T17:11:02.070+0000,2015-11-21T02:17:11.327+0000,Fixed,Major
CALCITE-2226,Druid adapter: Substring operator converter does not handle non-constant literals correctly,CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12530614'>],"Query like the following 
{code}
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table;
{code}
will fail with 
{code}
java.lang.AssertionError: not a literal: $13
	at org.apache.calcite.rex.RexLiteral.findValue(RexLiteral.java:963)
	at org.apache.calcite.rex.RexLiteral.findValue(RexLiteral.java:955)
	at org.apache.calcite.rex.RexLiteral.intValue(RexLiteral.java:938)
	at org.apache.calcite.adapter.druid.SubstringOperatorConversion.toDruidExpression(SubstringOperatorConversion.java:46)
	at org.apache.calcite.adapter.druid.DruidExpressions.toDruidExpression(DruidExpressions.java:120)
	at org.apache.calcite.adapter.druid.DruidQuery.computeProjectAsScan(DruidQuery.java:746)
	at org.apache.calcite.adapter.druid.DruidRules$DruidProjectRule.onMatch(DruidRules.java:308)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:317)
{code}

Druid Substring converter is assuming that index is always a constant literal, which is wrong.  ",2018-03-28T17:09:54.634+0000,2018-07-20T08:18:48.763+0000,Fixed,Major
PARQUET-108,Parquet Memory Management in Java,PARQUET,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12404976'>, <JIRA IssueLink: id='12397933'>]","As discussed in HIVE-7685, Hive + Parquet often runs out memory when writing to many Hive partitions. This is quite problematic for our users.",2014-09-29T15:27:17.160+0000,2015-09-29T05:33:15.046+0000,Fixed,Major
SPARK-6628,"ClassCastException occurs when executing sql statement ""insert into"" on hbase table",SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12429491'>, <JIRA IssueLink: id='12544908'>]","Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 12, vm-17): java.lang.ClassCastException: org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat
        at org.apache.spark.sql.hive.SparkHiveWriterContainer.outputFormat$lzycompute(hiveWriterContainers.scala:72)
        at org.apache.spark.sql.hive.SparkHiveWriterContainer.outputFormat(hiveWriterContainers.scala:71)
        at org.apache.spark.sql.hive.SparkHiveWriterContainer.getOutputName(hiveWriterContainers.scala:91)
        at org.apache.spark.sql.hive.SparkHiveWriterContainer.initWriters(hiveWriterContainers.scala:115)
        at org.apache.spark.sql.hive.SparkHiveWriterContainer.executorSideSetup(hiveWriterContainers.scala:84)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:112)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:93)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:93)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",2015-03-31T07:32:17.337+0000,2019-05-17T11:42:07.979+0000,Not A Problem,Major
SPARK-34897,Support reconcile schemas based on index after nested column pruning,SPARK,Bug,Resolved,[],4,"[<JIRA IssueLink: id='12612848'>, <JIRA IssueLink: id='12614419'>, <JIRA IssueLink: id='12614418'>, <JIRA IssueLink: id='12612863'>]","How to reproduce this issue:
{code:scala}
spark.sql(
  """"""
    |CREATE TABLE `t1` (
    |  `_col0` INT,
    |  `_col1` STRING,
    |  `_col2` STRUCT<`c1`: STRING, `c2`: STRING, `c3`: STRING, `c4`: BIGINT>,
    |  `_col3` STRING)
    |USING orc
    |PARTITIONED BY (_col3)
    |"""""".stripMargin)

spark.sql(""INSERT INTO `t1` values(1, '2', null, '2021-02-01')"")

spark.sql(""SELECT _col2.c1, _col0 FROM `t1` WHERE _col3 = '2021-02-01'"").show
{code}


Error message:
{noformat}
java.lang.AssertionError: assertion failed: The given data schema struct<_col0:int,_col2:struct<c1:string>> has less fields than the actual ORC physical schema, no idea which columns were dropped, fail to read. Try to disable 
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.sql.execution.datasources.orc.OrcUtils$.requestedColumnIds(OrcUtils.scala:159)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$3(OrcFileFormat.scala:180)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2620)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$1(OrcFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:117)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:165)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:94)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:756)
{noformat}

",2021-03-29T08:51:14.729+0000,2021-04-24T12:20:17.286+0000,Fixed,Major
PHOENIX-4529,Users should only require RX access to SYSTEM.SEQUENCE table,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12526531'>],"Currently, users don't need to have Write access to {{SYSTEM.CATALOG}} and other tables, since the code is run on the server side as login user. However for {{SYSTEM.SEQUENCE}}, write permission is still needed. This is a potential security concern, since it allows anyone to modify the sequences created by others. This JIRA is to discuss how we can improve the security of this table. 

Potential options include
1. Usage of HBase Cell Level Permissions (works only with HFile version 3 and above)
2. AccessControl at Phoenix Layer by addition of user column in the {{SYSTEM.SEQUENCE}} table and use it for access control (Can be error-prone for complex scenarios like sequence sharing)

Please advice.
[~tdsilva] [~jamestaylor] [~apurtell] [~ankit@apache.org] [~elserj]",2018-01-11T23:39:46.188+0000,2018-08-11T17:36:37.211+0000,,Major
TEZ-4356,Ignore some exceptions when the task is already in CLOSED state,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12627569'>],"with HIVE-24207, we can be in a situation where there are input related events to process when the task is already CLOSED
in this case, we simply don't care about certain kinds of exceptions and should ignore them in order to avoid task failure

thinking about whether it is unsafe to ignore these exceptions...I'm assuming that processing events for a closed task is just a best-effort behavior and we never rely on that

{code}
java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""lbodor-MBP16.local/192.168.0.53""; destination host is: ""localhost"":54688; 
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:200) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:139) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:105) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.tez.mapreduce.lib.MRReaderMapred.<init>(MRReaderMapred.java:76) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.tez.mapreduce.input.MultiMRInput.initFromEvent(MultiMRInput.java:196) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.tez.mapreduce.input.MultiMRInput.handleEvents(MultiMRInput.java:154) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.handleEvent(LogicalIOProcessorRuntimeTask.java:729) [tez-runtime-internals-0.10.1.jar:0.10.1]
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.access$600(LogicalIOProcessorRuntimeTask.java:110) [tez-runtime-internals-0.10.1.jar:0.10.1]
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask$1.runInternal(LogicalIOProcessorRuntimeTask.java:817) [tez-runtime-internals-0.10.1.jar:0.10.1]
	at org.apache.tez.common.RunnableWithNdc.run(RunnableWithNdc.java:35) [tez-common-0.10.1.jar:0.10.1]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.IOException: java.io.IOException: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""lbodor-MBP16.local/192.168.0.53""; destination host is: ""localhost"":54688; 
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) ~[hive-shims-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) ~[hive-shims-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:449) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:197) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	... 11 more
Caused by: java.io.IOException: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""lbodor-MBP16.local/192.168.0.53""; destination host is: ""localhost"":54688; 
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:141) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:446) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:197) ~[tez-mapreduce-0.10.1.jar:0.10.1]
	... 11 more
{code}",2021-11-29T14:11:33.034+0000,2022-01-12T12:49:15.123+0000,Won't Fix,Major
PARQUET-19,NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat,PARQUET,Bug,Resolved,[],1,[<JIRA IssueLink: id='12392029'>],Make sure the valueObj instance variable is always initialized.  This change is neeeded when running a Hive query that uses the CombineHiveInputFormat and the first file in the combined split is empty.  This can lead to a NullPointerException because the valueObj is null when the CombineHiveInputFormat calls the createValue method.,2014-07-12T00:33:51.739+0000,2015-04-07T20:46:17.875+0000,Fixed,Major
IMPALA-6109,Hbase in minicluster appears to be flaky,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12520476'>],"I saw a bunch of hbase-related tests failing with errors along the lines below:
    metadata.test_compute_stats.TestHbaseComputeStats.test_hbase_compute_stats[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    metadata.test_compute_stats.TestHbaseComputeStats.test_hbase_compute_stats_incremental[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    query_test.test_hbase_queries.TestHBaseQueries.test_hbase_scan_node[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    query_test.test_join_queries.TestJoinQueries.test_joins_against_hbase[batch_size: 0 | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_hbase_queries.TestHBaseQueries.test_hbase_row_key[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    query_test.test_observability.TestObservability.test_scan_summary
    query_test.test_hbase_queries.TestHBaseQueries.test_hbase_filters[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    query_test.test_scanners.TestScannersAllTableFormats.test_scanners[batch_size: 0 | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    query_test.test_mt_dop.TestMtDop.test_mt_dop[mt_dop: 2 | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    query_test.test_hbase_queries.TestHBaseQueries.test_hbase_subquery[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: hbase/none]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 4 | location: CLOSE | action: FAIL | query: select 1 from alltypessmall order by id]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: OPEN | action: CANCEL | query: select row_number() over (partition by int_col order by id) from alltypessmall]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 4 | location: GETNEXT_SCANNER | action: MEM_LIMIT_EXCEEDED | query: select * from alltypes]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: GETNEXT | action: MEM_LIMIT_EXCEEDED | query: select 1 from alltypessmall order by id limit 100]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: PREPARE_SCANNER | action: MEM_LIMIT_EXCEEDED | query: select count(int_col) from alltypessmall group by id]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: OPEN | action: MEM_LIMIT_EXCEEDED | query: select count(*) from alltypessmall]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: CLOSE | action: MEM_LIMIT_EXCEEDED | query: select c from (select id c from alltypessmall order by id limit 10) v where c = 1]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: CLOSE | action: MEM_LIMIT_EXCEEDED | query: select * from alltypessmall union all select * from alltypessmall]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 0 | location: CLOSE | action: MEM_LIMIT_EXCEEDED | query: select 1 from alltypessmall a join alltypessmall b on a.id != b.id]
    failure.test_failpoints.TestFailpoints.test_failpoints[table_format: hbase/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | mt_dop: 4 | location: PREPARE | action: FAIL | query: select 1 from alltypessmall a join alltypessmall b on a.id = b.id]

{noformat}
E    Query aborted:RuntimeException: couldn't retrieve HBase table (functional_hbase.alltypessmall) info:
E   This server is in the failed servers list: localhost/127.0.0.1:16202
E   CAUSED BY: FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:16202
{noformat}

{noformat}

E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: RuntimeException: couldn't retrieve HBase table (functional_hbase.alltypessmall) info:
E   Connection refused
E   CAUSED BY: ConnectException: Connection refused
{noformat}
",2017-10-25T21:56:35.979+0000,2017-11-20T20:38:58.382+0000,Fixed,Critical
SPARK-4313,"""Thread Dump"" link is broken in yarn-cluster mode",SPARK,Bug,Closed,[],1,[<JIRA IssueLink: id='12401138'>],"In yarn-cluster mode, the Web UI is running behind a yarn proxy server. Some features(or bugs?) of yarn proxy server will break the links for thread dump.

1. Yarn proxy server will do http redirect internally, so if opening ""http://example.com:8088/cluster/app/application_1415344371838_0012/executors"", it will fetch ""http://example.com:8088/cluster/app/application_1415344371838_0012/executors/"" and return the content but won't change the link in the browser. Then when a user clicks ""Thread Dump"", it will jump to ""http://example.com:8088/proxy/application_1415344371838_0012/threadDump/?executorId=2"". This is a wrong link. The correct link should be ""http://example.com:8088/proxy/application_1415344371838_0012/executors/threadDump/?executorId=2"".

2. Yarn proxy server has a bug about the URL encode/decode. When a user accesses ""http://example.com:8088/proxy/application_1415344371838_0006/executors/threadDump/?executorId=%3Cdriver%3E"", the yarn proxy server will require ""http://example.com:36429/executors/threadDump/?executorId=%25253Cdriver%25253E"". But Spark web server expects ""http://example.com:36429/executors/threadDump/?executorId=%3Cdriver%3E"". I will report this issue to Hadoop community later.",2014-11-10T07:43:32.529+0000,2014-11-14T21:36:44.930+0000,Fixed,Minor
SPARK-37090,Upgrade libthrift to resolve security vulnerabilities,SPARK,Task,In Progress,[],4,"[<JIRA IssueLink: id='12634787'>, <JIRA IssueLink: id='12634786'>, <JIRA IssueLink: id='12634044'>, <JIRA IssueLink: id='12628623'>]","Currently, Spark uses libthrift 0.12, which has reported high severity security vulnerabilities https://snyk.io/vuln/maven:org.apache.thrift%3Alibthrift
Upgrade to 0.14 to get rid of vulnerabilities.",2021-10-21T19:58:02.083+0000,2022-03-02T06:37:55.444+0000,,Major
HDDS-11,Fix findbugs exclude rules for ozone and hdds projects,HDDS,Bug,Resolved,[],1,[<JIRA IssueLink: id='12532915'>],"Some PreCommit builds are complaining about many findbugs violation in hdds/ozone projects. This is because we have no exclude rule for:

 * The proto files are generated by hadoop-hdds/container-service project (all the other projects with proto file such as hadoop-hdds/common are fine)

 * The generated files under genesis are not excluded.

With fixing these two projects we can reduce the numbers of the findbugs violation.",2018-04-29T15:17:06.018+0000,2018-07-03T21:09:35.406+0000,Fixed,Major
SPARK-12497,thriftServer does not support semicolon in sql ,SPARK,Bug,Closed,[],2,"[<JIRA IssueLink: id='12466844'>, <JIRA IssueLink: id='12466843'>]","0: jdbc:hive2://192.168.128.130:14005> SELECT ';' from tx_1 limit 1 ;
Error: org.apache.spark.sql.AnalysisException: cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in select clause; line 1 pos 8 (state=,code=0)
0: jdbc:hive2://192.168.128.130:14005> 
0: jdbc:hive2://192.168.128.130:14005> select '\;' from tx_1 limit 1 ; 
Error: org.apache.spark.sql.AnalysisException: cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in select clause; line 1 pos 9 (state=,code=0)",2015-12-23T08:44:06.436+0000,2021-02-02T05:44:32.176+0000,Duplicate,Major
SPARK-8712,Hive's Parser does not support distinct aggregations with OVER clause,SPARK,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12429233'>, <JIRA IssueLink: id='12449147'>]","Hive's parser ignores Window spec when a distinct aggregation is used.
{code}
scala> Seq((1, 2, 3)).toDF(""i"", ""j"", ""k"").registerTempTable(""t"")

scala> sql(""select count(distinct j) over (partition by i) from t"").explain(true)
== Parsed Logical Plan ==
'Project [UnresolvedAlias
 CountDistinct
  UnresolvedAttribute [j]
]
 'UnresolvedRelation [t], None

== Analyzed Logical Plan ==
_c0: bigint
Aggregate [COUNT(DISTINCT j#23) AS _c0#27L]
 Subquery t
  Project [_1#19 AS i#22,_2#20 AS j#23,_3#21 AS k#24]
   LocalRelation [_1#19,_2#20,_3#21], [[1,2,3]]

== Optimized Logical Plan ==
Aggregate [COUNT(DISTINCT j#23) AS _c0#27L]
 LocalRelation [j#23], [[2]]

== Physical Plan ==
GeneratedAggregate false, [CombineAndCount(partialSets#28) AS _c0#27L], false
 Exchange SinglePartition
  GeneratedAggregate true, [AddToHashSet(j#23) AS partialSets#28], false
   LocalTableScan [j#23], [[2]]

Code Generation: true
== RDD ==

scala> sql(""select count(j) over (partition by i) from t"").explain(true)
== Parsed Logical Plan ==
'Project [UnresolvedAlias
 WindowExpression
  UnresolvedWindowFunction count
   UnresolvedAttribute [j]
  WindowSpecDefinition UnspecifiedFrame
   UnresolvedAttribute [i]
]
 'UnresolvedRelation [t], None

== Analyzed Logical Plan ==
_c0: bigint
Project [_c0#31L]
 Project [j#23,i#22,_c0#31L,_c0#31L]
  Window [j#23,i#22], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount(j#23) WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING AS _c0#31L], WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
   Project [j#23,i#22]
    Subquery t
     Project [_1#19 AS i#22,_2#20 AS j#23,_3#21 AS k#24]
      LocalRelation [_1#19,_2#20,_3#21], [[1,2,3]]

== Optimized Logical Plan ==
Project [_c0#31L]
 Window [j#23,i#22], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount(j#23) WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING AS _c0#31L], WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
  LocalRelation [j#23,i#22], [[2,1]]

== Physical Plan ==
Project [_c0#31L]
 Window [j#23,i#22], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount(j#23) WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING AS _c0#31L], WindowSpecDefinition ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
  ExternalSort [i#22 ASC], false
   Exchange (HashPartitioning 200)
    LocalTableScan [j#23,i#22], [[2,1]]

Code Generation: true
== RDD ==
{code}",2015-06-29T19:50:31.540+0000,2016-10-10T05:50:34.799+0000,Not A Problem,Major
KUDU-2751,Java tests that start an HMS client fail when run on JDK10+,KUDU,Bug,Resolved,[],1,[<JIRA IssueLink: id='12557439'>],"They may fail on JDK9 as well, with something like this:
{noformat}
MetaException(message:Got exception: java.lang.ClassCastException java.base/[Ljava.lang.Object; cannot be cast to java.base/[Ljava.net.URI;)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1389)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:204)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:129)
	at org.apache.kudu.hive.metastore.TestKuduMetastorePlugin.setUp(TestKuduMetastorePlugin.java:108)
{noformat}

I tracked this down and filed HIVE-21508. We should see if we can find some sort of workaround that isn't necessarily upgrading to a newer Hive artifact (or maybe we should upgrade our Hive dependency).",2019-03-26T08:52:52.247+0000,2020-06-03T14:40:45.817+0000,Fixed,Major
INFRA-8563,Info on status of hbase/hadoop migration of our rsync of download mirrors to svn pubsub,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12400728'>],"No hurry. Was just wondering if any status on migration of hbase, hadoop published downloads over to svnpubsub?  It is just taking a while? Currently our downloads are missing releases.  Will these show up soon?

I tried to checkout the new svnpubsub dir bug got this:

{code}
kalashnikov-3:hbase.git.commit stack$ svn checkout https://dist.apache.org/repos/dist/release/hbase hbase.svnpubsub
Error validating server certificate for 'https://dist.apache.org:443':
 - The certificate is not issued by a trusted authority. Use the
   fingerprint to validate the certificate manually!
Certificate information:
 - Hostname: *.apache.org
 - Valid: from Fri, 11 Apr 2014 00:00:00 GMT until Thu, 07 Apr 2016 23:59:59 GMT
 - Issuer: Thawte, Inc., US
 - Fingerprint: 15:1d:8a:d1:e1:ba:c2:14:66:bc:28:36:ba:80:b5:fc:f8:72:f3:7c
(R)eject, accept (t)emporarily or accept (p)ermanently? a
svn: E175002: Unable to connect to a repository at URL 'https://dist.apache.org/repos/dist/release/hbase'
...
{code}

We got the following note from David McNally but it said we had to do nothing and we'll be told what to do when done.  We are just a little ansy since it is a good few days now.  Thanks

{code}
> From: David Nalley <david@gnsa.us>
> Date: Fri, Oct 24, 2014 at 3:31 PM
> Subject: [NOTICE] Changes to website and release publishing.
> To: ""infrastructure@apache.org"" <infrastructure@apache.org>
>
>
> Hello folks:
>
> My apologies for intruding on your regularly scheduled email flow.
>
> In February of 2012, Infra sent a notice[1] to PMCs that websites and
> release publishing was moving exclusively to svnpubsub (or
> svnpubsub+CMS) and rsync was being deprecated. Despite the very long
> notice time, a large number of projects continue to use rsync for
> publishing artifacts or websites.
>
> Unfortunately, in some of our work to deal with some capacity
> problems, we've discovered that those remaining on rsync end up with
> missing release artifacts and websites. This affects 40+ projects, and
> if you are receiving this message, your project is one of them.
>
> Effective a short while ago, rsync will no longer work as a publishing
> method.
>
> My website is still using rsync, what do I need to do? (hc, openejb,
> synapse, tuscany, velocity, xmlbeans as well as a number of retired
> projects):
>     Nothing at the moment - Infrastructure is going to automatically
> move affected projects' websites on Monday 27 October to begin using
> svnpubsub. You'll receive a notification when this is done with more
> information.
>
> I still publish release artifacts using rsync, what do I need to do?
> (everyone else) :
>     Nothing at the moment - Infrastructure will be migrating all
> projects to using svnpubsub for their release artifacts on 27 October.
> You'll receive a notification when this is done with more information.
>
> Why the rush to do this?:
>     Well, it's not really a rush. This migration was announced over
> 2.5 years ago, and intended to be done in January 2013.
> This week, while dealing with capacity issues on a machine that
> currently serves the US mirror of the website, we deployed an
> additional web server and discovered lots of missing content. That
> content has remained in the legacy site/artifact publishing. Rather
> than continue to maintain two processes, we are discontinuing the
> long-deprecated rsync process.
>
> If you have questions or concerns, please don't hesitate to contact
> infra by email at infrastructure@apache.org or join us in chat at:
> http://s.apache.org/infrachat
>
> --David
> on behalf of Infrastructure
>
> [1] http://s.apache.org/pubsubnotification
{code}",2014-11-02T03:29:12.773+0000,2017-06-18T00:30:15.347+0000,Fixed,Major
PHOENIX-946,Use Phoenix to service Hive queries over HBase data,PHOENIX,Bug,Open,[],1,[<JIRA IssueLink: id='12387002'>],,2014-04-22T23:33:39.189+0000,2014-04-25T00:38:21.704+0000,,Major
CALCITE-1099,Upgrade Cassandra driver to support 3.0 server,CALCITE,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12458977'>, <JIRA IssueLink: id='12458162'>, <JIRA IssueLink: id='12459878'>]","The current version of the Java driver used in the Cassandra adapter does not support Cassandra server 3.0.x. Upgrading is a fairly straightforward process. However, the new version of the driver depends on Guava 15.0 and it looks like Calcite is currently pinned at 14.0.1 for compatibility with Hive (it looks like Spark is stuck on this version as well). Any thoughts on how to resolve this conflict?",2016-02-24T19:43:38.569+0000,2016-03-22T01:18:10.466+0000,Fixed,Minor
YETUS-539,Foreach and switch in RootDocProcessor and StabilityOptions,YETUS,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12512354'>, <JIRA IssueLink: id='12512355'>]","do the same improvement that was done in HBASE-17563

{quote}
To make the code in RootDocProcessor and StabilityOptions more readable change the existing for-loops and if-statements to foreach-loops and switch-statements.
{quote}",2017-08-19T05:02:34.511+0000,2018-01-15T18:50:21.519+0000,Fixed,Major
KUDU-2910,Add client cache/factory implementation to the kudu-client,KUDU,Improvement,Open,[],1,[<JIRA IssueLink: id='12566589'>],"Often integrations should cache and use a shared client for all communication to a given list of masters. This is seen in our own kudu-spark integration in `KuduContext.KuduClientCache`. 

It would be nice to add a generic implementation to the kudu-client so that this code doesn't get re-written over and over. Additionally we can add more complex logic if useful later. 
",2019-07-29T13:16:00.547+0000,2020-06-03T15:30:11.943+0000,,Major
TEZ-4106,Add Exponential Smooth RuntimeEstimator to the speculator,TEZ,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12576144'>, <JIRA IssueLink: id='12576145'>, <JIRA IssueLink: id='12576143'>]","Tez speculator implements start-end runtime estimator. Similar to [MAPREDUCE-7208|https://issues.apache.org/jira/browse/MAPREDUCE-7208], we need to implement an adaptive estimator based on smooth Exponential",2019-12-09T22:00:30.750+0000,2020-02-04T16:52:07.391+0000,Fixed,Major
PHOENIX-1146,Detect stale client region cache on server and retry scans in split regions,PHOENIX,Bug,Closed,[],1,[<JIRA IssueLink: id='12393439'>],"HBase cannot recover correctly from an aggregate scan run on the coprocessor side (see HBASE-116670). This can lead to incorrect query results the first time a query is run after a split occurs (due to the region boundary cache being stale). Phoenix can work around this by:
- detecting on server before the scan starts that the region cache used by the client is out-of-date. This can be done up-front because the start/stop row of the scan should never span across a region boundary. In this case, a DoNotRetryIOException is thrown with some embedded information to cause a StaleRegionBoundaryCacheException to be thrown on the client.
- catching this exception on the client (in ParallelIterators), refreshing the region boundary cache, and re-running the necessary scans based on the new region boundaries.
- detecting if this happens more than N times to prevent any kind of excessive looping due to splits occurring over and over again.

Phoenix has additional requirements above and beyond standard HBase clients, so even if HBase could recover from this situation, Phoenix would likely need this workaround to ensure that a scan does not span across region boundaries. This is required when the client is doing a merge sort on the results of the parallel scans, mainly in ORDER BY (including topN) and local indexing, and potentially GROUP BY if we move toward sorting the distinct groups on the server side.",2014-08-05T06:37:12.346+0000,2015-11-21T02:15:48.745+0000,Fixed,Major
IMPALA-9164,Implement  LOAD DATA  for ACID tables,IMPALA,New Feature,Open,[],1,[<JIRA IssueLink: id='12575306'>],,2019-11-18T12:38:33.722+0000,2019-11-29T16:52:15.189+0000,,Major
LEGAL-325,Request VP Legal ruling on JSR107 License Use,LEGAL,Question,Resolved,[],1,[<JIRA IssueLink: id='12511820'>],"Please provide guidance on whether the ""JSR-000107 JCACHE 2.9 Public Review - Updated Specification License"" as used by the jsr107 spec v1.0.0 https://raw.githubusercontent.com/jsr107/jsr107spec/v1.0.0/LICENSE.txt is acceptable for use in ASF projects.

Full text copied here for convenience:

{noformat}
JSR-000107 JCACHE 2.9 Public Review - Updated Specification

ORACLE AND GREG LUCK ARE WILLING TO LICENSE THIS SPECIFICATION TO YOU ONLY UPON THE CONDITION THAT YOU ACCEPT ALL OF THE TERMS CONTAINED IN THIS LICENSE AGREEMENT (""AGREEMENT""). PLEASE READ THE TERMS AND CONDITIONS OF THIS AGREEMENT CAREFULLY. BY DOWNLOADING THIS SPECIFICATION, YOU ACCEPT THE TERMS AND CONDITIONS OF THIS AGREEMENT. IF YOU ARE NOT WILLING TO BE BOUND BY THEM, SELECT THE ""DECLINE"" BUTTON AT THE BOTTOM OF THIS PAGE AND THE DOWNLOADING PROCESS WILL NOT CONTINUE.
Specification: JSR-000107 Java(tm) Temporary Caching API Specification (""Specification"")
Version: 2.9
Status: Public Review
Release: 8 August 2013

Copyright 2013 ORACLE America, Inc. and Greg Luck
4150 Network Circle, Santa Clara, California 95054, U.S.A
All rights reserved.
NOTICE
The Specification is protected by copyright and the information described therein may be protected by one or more U.S. patents, foreign patents, or pending applications. Except as provided under the following license, no part of the Specification may be reproduced in any form by any means without the prior written authorization of Oracle USA, Inc. (""Oracle""), Greg Luck (""Greg Luck"") and their licensors, if any. Any use of the Specification and the information described therein will be governed by the terms and conditions of this Agreement.

Subject to the terms and conditions of this license, including your compliance with Paragraphs 1 and 2 below, Oracle and Greg Luck hereby grant you a fully-paid, non-exclusive, non-transferable, limited license (without the right to sublicense) under Oracle and Greg Luck's intellectual property rights to:

1.Review the Specification for the purposes of evaluation. This includes: (i) developing implementations of the Specification for your internal, non-commercial use; (ii) discussing the Specification with any third party; and (iii) excerpting brief portions of the Specification in oral or written communications which discuss the Specification provided that such excerpts do not in the aggregate constitute a significant portion of the Technology. 2.Distribute implementations of the Specification to third parties for their testing and evaluation use, provided that any such implementation:
(i) does not modify, subset, superset or otherwise extend the Licensor Name Space, or include any public or protected packages, classes, Java interfaces, fields or methods within the Licensor Name Space other than those required/authorized by the Specification or Specifications being implemented;
(ii) is clearly and prominently marked with the word ""UNTESTED"" or ""EARLY ACCESS"" or ""INCOMPATIBLE"" or ""UNSTABLE"" or ""BETA"" in any list of available builds and in proximity to every link initiating its download, where the list or link is under Licensee's control; and
(iii) includes the following notice:
""This is an implementation of an early-draft specification developed under the Java Community Process (JCP) and is made available for testing and evaluation purposes only. The code is not compatible with any specification of the JCP."" The grant set forth above concerning your distribution of implementations of the specification is contingent upon your agreement to terminate development and distribution of your ""early draft"" implementation as soon as feasible following final completion of the specification. If you fail to do so, the foregoing grant shall be considered null and void. No provision of this Agreement shall be understood to restrict your ability to make and distribute to third parties applications written to the Specification. Other than this limited license, you acquire no right, title or interest in or to the Specification or any other Oracle or Greg Luck intellectual property, and the Specification may only be used in accordance with the license terms set forth herein. This license will expire on the earlier of: (a) two (2) years from the date of Release listed above; (b) the date on which the final version of the Specification is publicly released; or (c) the date on which the Java Specification Request (JSR) to which the Specification corresponds is withdrawn. In addition, this license will terminate immediately without notice from Oracle or Greg Luck if you fail to comply with any provision of this license. Upon termination, you must cease use of or destroy the Specification. ""Licensor Name Space"" means the public class or interface declarations whose names begin with ""java"", ""javax"", ""com.oracle"" or their equivalents in any subsequent naming convention adopted by Oracle or Greg Luck through the Java Community Process, or any recognized successors or replacements thereof

TRADEMARKS
No right, title, or interest in or to any trademarks, service marks, or trade names of Oracle, Greg Luck or their licensors is granted hereunder. Oracle, the Oracle logo, Java are trademarks or registered trademarks of Oracle USA, Inc. in the U.S. and other countries.

DISCLAIMER OF WARRANTIES
THE SPECIFICATION IS PROVIDED ""AS IS"" AND IS EXPERIMENTAL AND MAY CONTAIN DEFECTS OR DEFICIENCIES WHICH CANNOT OR WILL NOT BE CORRECTED BY ORACLE. ORACLE MAKES NO REPRESENTATIONS OR WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO, WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT THAT THE CONTENTS OF THE SPECIFICATION ARE SUITABLE FOR ANY PURPOSE OR THAT ANY PRACTICE OR IMPLEMENTATION OF SUCH CONTENTS WILL NOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADE SECRETS OR OTHER RIGHTS. This document does not represent any commitment to release or implement any portion of the Specification in any product.

THE SPECIFICATION COULD INCLUDE TECHNICAL INACCURACIES OR TYPOGRAPHICAL ERRORS. CHANGES ARE PERIODICALLY ADDED TO THE INFORMATION THEREIN; THESE CHANGES WILL BE INCORPORATED INTO NEW VERSIONS OF THE SPECIFICATION, IF ANY. ORACLE MAY MAKE IMPROVEMENTS AND/OR CHANGES TO THE PRODUCT(S) AND/OR THE PROGRAM(S) DESCRIBED IN THE SPECIFICATION AT ANY TIME. Any use of such changes in the Specification will be governed by the then-current license for the applicable version of the Specification.

LIMITATION OF LIABILITY
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL ORACLE OR ITS LICENSORS BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION, LOST REVENUE, PROFITS OR DATA, OR FOR SPECIAL, INDIRECT, CONSEQUENTIAL, INCIDENTAL OR PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF OR RELATED TO ANY FURNISHING, PRACTICING, MODIFYING OR ANY USE OF THE SPECIFICATION, EVEN IF ORACLE AND/OR ITS LICENSORS HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

You will hold Oracle and Greg Luck (and their licensors) harmless from any claims based on your use of the Specification for any purposes other than the limited right of evaluation as described above, and from any claims that later versions or releases of any Specification furnished to you are incompatible with the Specification provided to you under this license.

RESTRICTED RIGHTS LEGEND
If this Software is being acquired by or on behalf of the U.S. Government or by a U.S. Government prime contractor or subcontractor (at any tier), then the Government's rights in the Software and accompanying documentation shall be only as set forth in this license; this is in accordance with 48 C.F.R. 227.7201 through 227.7202-4 (for Department of Defense (DoD) acquisitions) and with 48 C.F.R. 2.101 and 12.212 (for non-DoD acquisitions).

REPORT
You may wish to report any ambiguities, inconsistencies or inaccuracies you may find in connection with your evaluation of the Specification (""Feedback""). To the extent that you provide Oracle or Greg Luck with any Feedback, you hereby: (i) agree that such Feedback is provided on a non-proprietary and non-confidential basis, and (ii) grant Oracle and Greg Luck a perpetual, non-exclusive, worldwide, fully paid-up, irrevocable license, with the right to sublicense through multiple levels of sublicensees, to incorporate, disclose, and use without limitation the Feedback for any purpose related to the Specification and future versions, implementations, and test suites thereof.

GENERAL TERMS
Any action related to this Agreement will be governed by California law and controlling U.S. federal law. The U.N. Convention for the International Sale of Goods and the choice of law rules of any jurisdiction will not apply.

The Specification is subject to U.S. export control laws and may be subject to export or import regulations in other countries. Licensee agrees to comply strictly with all such laws and regulations and acknowledges that it has the responsibility to obtain such licenses to export, re-export or import as may be required after delivery to Licensee.

This Agreement is the parties' entire agreement relating to its subject matter. It supersedes all prior or contemporaneous oral or written communications, proposals, conditions, representations and warranties and prevails over any conflicting or additional terms of any quote, order, acknowledgment, or other communication between the parties relating to its subject matter during the term of this Agreement. No modification to this Agreement will be binding, unless in writing and signed by an authorized representative of each party.
{noformat}",2017-08-11T16:41:21.761+0000,2018-05-24T07:11:06.780+0000,Resolved,Major
SENTRY-74,Add column-level privileges for Hive/Impala,SENTRY,New Feature,Resolved,"[<JIRA Issue: key='SENTRY-426', id='12740036'>, <JIRA Issue: key='SENTRY-389', id='12733976'>, <JIRA Issue: key='SENTRY-391', id='12733996'>, <JIRA Issue: key='SENTRY-390', id='12733978'>, <JIRA Issue: key='SENTRY-393', id='12734023'>, <JIRA Issue: key='SENTRY-392', id='12734001'>, <JIRA Issue: key='SENTRY-394', id='12734024'>]",21,"[<JIRA IssueLink: id='12396345'>, <JIRA IssueLink: id='12400016'>, <JIRA IssueLink: id='12394871'>, <JIRA IssueLink: id='12394239'>, <JIRA IssueLink: id='12394220'>, <JIRA IssueLink: id='12394238'>, <JIRA IssueLink: id='12394240'>, <JIRA IssueLink: id='12394221'>, <JIRA IssueLink: id='12394241'>, <JIRA IssueLink: id='12396329'>, <JIRA IssueLink: id='12435576'>, <JIRA IssueLink: id='12426741'>, <JIRA IssueLink: id='12426740'>, <JIRA IssueLink: id='12396327'>, <JIRA IssueLink: id='12426743'>, <JIRA IssueLink: id='12426742'>, <JIRA IssueLink: id='12434258'>, <JIRA IssueLink: id='12426750'>, <JIRA IssueLink: id='12426751'>, <JIRA IssueLink: id='12402691'>, <JIRA IssueLink: id='12398289'>]","Currently the finest grain of privilege is at the table/view level. This leads to the unwieldy scenario where a different view has to be created for each combination of columns that need to be restricted. With column level privileges this would not be required.

In the policy file column privileges might potentially look like:

server=server1->db=default->table=employees->column=salary->action=select",2013-12-20T02:16:47.136+0000,2015-08-26T09:58:31.914+0000,Fixed,Major
TEZ-2931,Running DistCp job in Tez with DEBUG logging fails,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12448137'>],"When working on HIVE-12364 I noticed the following exception when tez DEBUG logging is enabled. This fails all hive jobs that use distcp
{code}
Caused by: java.io.IOException: Cannot execute DistCp process: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: application_1446860311492_0003-distcp:%20insert%20overwrite%20directory%20'/tmp/src'%20...src(Stage-1)-tez-dag.pb.txt
        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1180)
        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:554)
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2625)
        ... 36 more
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: application_1446860311492_0003-distcp:%20insert%20overwrite%20directory%20'/tmp/src'%20...src(Stage-1)-tez-dag.pb.txt
        at org.apache.hadoop.fs.Path.initialize(Path.java:206)
        at org.apache.hadoop.fs.Path.<init>(Path.java:172)
        at org.apache.hadoop.fs.Path.<init>(Path.java:94)
        at org.apache.tez.common.TezCommonUtils.getTezTextPlanStagingPath(TezCommonUtils.java:207)
        at org.apache.tez.client.TezClientUtils.localizeDagPlanAsText(TezClientUtils.java:737)
        at org.apache.tez.client.TezClientUtils.createApplicationSubmissionContext(TezClientUtils.java:598)
        at org.apache.tez.client.TezClient.submitDAGApplication(TezClient.java:680)
        at org.apache.tez.client.MRTezClient.submitDAGApplication(MRTezClient.java:47)
        at org.apache.tez.mapreduce.client.YARNRunner.submitJob(YARNRunner.java:648)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:536)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)
        at org.apache.hadoop.tools.DistCp.execute(DistCp.java:162)
        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1177)
        ... 38 more
Caused by: java.net.URISyntaxException: Relative path in absolute URI: application_1446860311492_0003-distcp:%20insert%20overwrite%20directory%20'/tmp/src'%20...src(Stage-1)-tez-dag.pb.txt
        at java.net.URI.checkPath(URI.java:1823)
        at java.net.URI.<init>(URI.java:745)
        at org.apache.hadoop.fs.Path.initialize(Path.java:203)
        ... 55 more

{code}",2015-11-07T02:06:27.530+0000,2017-03-14T03:40:34.049+0000,,Major
TEZ-3719,DAGImpl.computeProgress slows down dispatcher and ipc threads,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12503195'>],"The Vertex readLock is held while computing progress, which prevents scheduling of tasks from happening on that vertex (which needs a writeLock).

Making the locked section do as little work as possible would speedup the AM.",2017-05-10T08:50:59.701+0000,2017-08-22T00:02:49.742+0000,Fixed,Major
AMBARI-2784,Ambari  memory params configuration is not right for yarn and mapreducde,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12373492'>],"Looks like Amabri carried over many memory related parameter from stack 1.x to stack 2.x. Many of those parameters are not applicable in 2.x.

This is in the context of configuration parameters for yarn and mapreduce.
Some paramters need to be set with different default values.

",2013-07-31T17:55:07.174+0000,2013-08-16T21:15:42.559+0000,Fixed,Major
IMPALA-6073,No response from expr-codegen-test/expr-test,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12517831'>],"I checked out HEAD of Impala repo. There is no any code modification on my local repo.
After full build, ./build/debug/exprs/expr-test and ./build/debug/exprs/expr-codegen-test seem to be hung. The other gtests in be are working appropriately. Some threads are waiting and the other threads are on sleep. 

I guess this issue is similar to the deadlock: https://issues.apache.org/jira/browse/HDFS-11851

Please let me know if you have any workaround.

Here are stack traces on expr-codegen-test using GDB:

{code:java}
jinchulkim@ubuntu:~/workspace/Impala/be$ gdb ./build/debug/exprs/expr-codegen-test
...
(gdb) info thread
  Id   Target Id         Frame
  16   Thread 0x7fffe20ae700 (LWP 23374) ""expr-codegen-te"" pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_timedwait.S:238
  15   Thread 0x7fffe21af700 (LWP 23373) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  14   Thread 0x7fffe22b0700 (LWP 23372) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  13   Thread 0x7fffe23b1700 (LWP 23371) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  12   Thread 0x7fffe24b2700 (LWP 23370) ""expr-codegen-te"" sem_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/sem_wait.S:85
  11   Thread 0x7fffe2c95700 (LWP 23369) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  10   Thread 0x7fffe2d96700 (LWP 23368) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  9    Thread 0x7fffe2e97700 (LWP 23367) ""expr-codegen-te"" pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_timedwait.S:238
  8    Thread 0x7fffe915a700 (LWP 23366) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  7    Thread 0x7fffe925b700 (LWP 23365) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  6    Thread 0x7fffe935c700 (LWP 23364) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  5    Thread 0x7fffe945d700 (LWP 23363) ""expr-codegen-te"" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  4    Thread 0x7fffee536700 (LWP 23362) ""expr-codegen-te"" 0x00007ffff037bb9d in nanosleep () at ../sysdeps/unix/syscall-template.S:81
  3    Thread 0x7fffeed37700 (LWP 23361) ""expr-codegen-te"" 0x00007ffff037bb9d in nanosleep () at ../sysdeps/unix/syscall-template.S:81
  2    Thread 0x7fffef538700 (LWP 23356) ""expr-codegen-te"" 0x00007ffff0067dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81
* 1    Thread 0x7fffef5408c0 (LWP 23150) ""expr-codegen-te"" __lll_lock_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135
(gdb) where
#0  __lll_lock_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135
#1  0x00007ffff0376649 in _L_lock_909 () from /lib/x86_64-linux-gnu/libpthread.so.0
#2  0x00007ffff0376470 in __GI___pthread_mutex_lock (mutex=0x7ffff4080600 <jvmMutex>) at ../nptl/pthread_mutex_lock.c:79
#3  0x00007ffff3e7b666 in mutexLock (m=<optimized out>) at /data/2/jenkins/workspace/impala-hadoop-dependency/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/os/posix/mutexes.c:28
#4  0x00007ffff3e73a11 in setTLSExceptionStrings (rootCause=0x0, stackTrace=0x0) at /data/2/jenkins/workspace/impala-hadoop-dependency/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:581
#5  0x00007ffff3e73393 in printExceptionAndFreeV (env=0x307c1d8, exc=0x301adc0, noPrintFlags=<optimized out>, fmt=0x7ffff3e7bf6e ""loadFileSystems"", ap=0x7fffffffaab0) at /data/2/jenkins/workspace/impala-hadoop-dependency/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/exception.c:183
#6  0x00007ffff3e735ef in printExceptionAndFree (env=<optimized out>, exc=<optimized out>, noPrintFlags=<optimized out>, fmt=<optimized out>) at /data/2/jenkins/workspace/impala-hadoop-dependency/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/exception.c:213
#7  0x00007ffff3e74880 in getGlobalJNIEnv () at /data/2/jenkins/workspace/impala-hadoop-dependency/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:463
#8  getJNIEnv () at /data/2/jenkins/workspace/impala-hadoop-dependency/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:528
#9  0x00007ffff7845a1c in impala::JniUtil::Init () at /home/jinchulkim/workspace/Impala/be/src/util/jni-util.cc:105
#10 0x00007ffff7e3c221 in impala::InitCommonRuntime (argc=1, argv=0x7fffffffb1e8, init_jvm=true, test_mode=impala::TestInfo::BE_TEST) at /home/jinchulkim/workspace/Impala/be/src/common/init.cc:236
#11 0x00000000008333fc in main (argc=1, argv=0x7fffffffb1e8) at /home/jinchulkim/workspace/Impala/be/src/exprs/expr-codegen-test.cc:361
(gdb) thr 2
[Switching to thread 2 (Thread 0x7fffef538700 (LWP 23356))]
#0  0x00007ffff0067dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81
81      ../sysdeps/unix/syscall-template.S: No such file or directory.
(gdb) where
#0  0x00007ffff0067dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81
#1  0x00007ffff0067c94 in __sleep (seconds=0) at ../sysdeps/unix/sysv/linux/sleep.c:137
#2  0x00007ffff7e3b304 in LogMaintenanceThread () at /home/jinchulkim/workspace/Impala/be/src/common/init.cc:113
#3  0x00007ffff56e4a8e in boost::detail::function::void_function_invoker0<void (*)(), void>::invoke (function_ptr=...) at /home/jinchulkim/workspace/Impala/toolchain/boost-1.57.0-p3/include/boost/function/function_template.hpp:112
#4  0x00007ffff7b22f22 in boost::function0<void>::operator() (this=0x7fffef537ca0) at /home/jinchulkim/workspace/Impala/toolchain/boost-1.57.0-p3/include/boost/function/function_template.hpp:767
#5  0x00007ffff78c0ba3 in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7fffffffaa60) at /home/jinchulkim/workspace/Impala/be/src/util/thread.cc:352
#6  0x00007ffff78ca32c in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0x2fb47c0, f=@0x2fb47b8: 0x7ffff78c0884 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...)
    at /home/jinchulkim/workspace/Impala/toolchain/boost-1.57.0-p3/include/boost/bind/bind.hpp:457
#7  0x00007ffff78ca26f in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0x2fb47b8)
    at /home/jinchulkim/workspace/Impala/toolchain/boost-1.57.0-p3/include/boost/bind/bind_template.hpp:20
#8  0x00007ffff78ca232 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run()
    (this=0x2fb4600) at /home/jinchulkim/workspace/Impala/toolchain/boost-1.57.0-p3/include/boost/thread/detail/thread.hpp:116
#9  0x000000000090c53a in thread_proxy ()
#10 0x00007ffff0374184 in start_thread (arg=0x7fffef538700) at pthread_create.c:312
#11 0x00007ffff00a137d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb)
{code}

Here are the relevant environment variables:

{code:java}
jinchulkim@ubuntu:~/workspace/Impala/be$ echo $JAVA_HOME
/usr/lib/jvm/java-7-openjdk-amd64

jinchulkim@ubuntu:~/workspace/Impala/be$ echo $BOOST_LIBRARYDIR
/usr/lib/x86_64-linux-gnu

jinchulkim@ubuntu:~/workspace/Impala/be$ echo $LD_LIBRARY_PATH
:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server:/home/jinchulkim/workspace/Impala/toolchain/cdh_components/hadoop-2.6.0-cdh5.14.0-SNAPSHOT//lib/native:/home/jinchulkim/workspace/Impala/toolchain/snappy-1.1.4/lib:/home/jinchulkim/workspace/Impala/../Impala-lzo/build:/home/jinchulkim/workspace/Impala/toolchain/gcc-4.9.2/lib64:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server:/home/jinchulkim/workspace/Impala/toolchain/cdh_components/hadoop-2.6.0-cdh5.14.0-SNAPSHOT//lib/native:/home/jinchulkim/workspace/Impala/toolchain/snappy-1.1.4/lib:/home/jinchulkim/workspace/Impala/../Impala-lzo/build:/home/jinchulkim/workspace/Impala/toolchain/gcc-4.9.2/lib64
{code}",2017-10-18T08:22:24.184+0000,2020-03-13T16:27:47.843+0000,Cannot Reproduce,Major
SPARK-3174,Provide elastic scaling within a Spark application,SPARK,Improvement,Closed,"[<JIRA Issue: key='SPARK-3795', id='12745937'>, <JIRA Issue: key='SPARK-3822', id='12746251'>, <JIRA Issue: key='SPARK-3796', id='12745938'>, <JIRA Issue: key='SPARK-3797', id='12745939'>, <JIRA Issue: key='SPARK-4839', id='12761549'>]",8,"[<JIRA IssueLink: id='12404050'>, <JIRA IssueLink: id='12401364'>, <JIRA IssueLink: id='12404052'>, <JIRA IssueLink: id='12502802'>, <JIRA IssueLink: id='12395118'>, <JIRA IssueLink: id='12412362'>, <JIRA IssueLink: id='12394849'>, <JIRA IssueLink: id='12395121'>]","A common complaint with Spark in a multi-tenant environment is that applications have a fixed allocation that doesn't grow and shrink with their resource needs.  We're blocked on YARN-1197 for dynamically changing the resources within executors, but we can still allocate and discard whole executors.

It would be useful to have some heuristics that
* Request more executors when many pending tasks are building up
* Discard executors when they are idle

See the latest design doc for more information.",2014-08-21T17:36:25.785+0000,2018-09-20T17:21:01.938+0000,Fixed,Major
SPARK-21514,Hive has updated with new support for S3 and InsertIntoHiveTable.scala should update also,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12514743'>, <JIRA IssueLink: id='12510962'>, <JIRA IssueLink: id='12515379'>]","Hive has updated adding new parameters to optimize the usage of S3, now you can avoid the usage of S3 as the stagingdir using the parameters hive.blobstore.supported.schemes & hive.blobstore.optimizations.enabled.

The InsertIntoHiveTable.scala file should be updated with the same improvement to match the behavior of Hive.",2017-07-23T19:23:04.007+0000,2021-05-25T01:53:22.516+0000,Incomplete,Major
BIGTOP-3282,Bump Hive to 2.3.6,BIGTOP,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12580270'>, <JIRA IssueLink: id='12579910'>, <JIRA IssueLink: id='12579911'>, <JIRA IssueLink: id='12579873'>]",,2019-12-06T03:58:32.504+0000,2020-11-03T07:13:40.305+0000,Fixed,Major
KYLIN-3388,"Data may become not correct if mappers fail during the redistribute step, ""distribute by rand()""",KYLIN,Bug,Closed,[],4,"[<JIRA IssueLink: id='12582766'>, <JIRA IssueLink: id='12536153'>, <JIRA IssueLink: id='12536152'>, <JIRA IssueLink: id='12536770'>]",,2018-05-28T05:02:09.356+0000,2020-03-10T08:41:09.762+0000,Fixed,Critical
SLIDER-622,jenkins on windows failing,SLIDER,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12401073'>, <JIRA IssueLink: id='12401058'>]","Jenkins windows builds are failing:

[[https://builds.apache.org/job/slider-develop-windows/]]",2014-11-06T18:40:18.880+0000,2014-11-12T15:32:57.604+0000,Fixed,Major
CALCITE-727,Constant folding involving CASE and NULL,CALCITE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12424760'>, <JIRA IssueLink: id='12428166'>]","There are a few Hive issues relating to constant reduction of NULL and CASE. Calcite should do the same.

HIVE-9645:
{code}
select count(1) from store_sales where null=1;
# should simplify to
select count(1) from store_sales where false;
{code}

HIVE-9644:
{code}
select count(1) from store_sales where (case ss_sold_date when '1998-01-01' then 1 else null end)=1;
# should simplify to
select count(1) from store_sales where ss_sold_date= '1998-01-01' ;
{code}",2015-05-14T17:54:49.957+0000,2015-09-01T03:02:17.342+0000,Fixed,Major
ZOOKEEPER-1178,Add eclipse target for supporting Apache IvyDE,ZOOKEEPER,Improvement,Patch Available,[],1,[<JIRA IssueLink: id='12347289'>],"This patch adds support for Eclipse with Apache IvyDE, which is the extension that integrates Ivy support into Eclipse. This allows the creation of what appear to be fully portable .eclipse and .classpath files. I will be posting a patch shortly.",2011-09-12T16:03:08.099+0000,2022-02-03T08:36:24.498+0000,,Minor
SPARK-12567,Add aes_encrypt and aes_decrypt UDFs,SPARK,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12630871'>, <JIRA IssueLink: id='12453032'>]","AES (Advanced Encryption Standard) algorithm.
Add aes_encrypt and aes_decrypt UDFs.
Ref:
[Hive|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Misc.Functions]
[MySQL|https://dev.mysql.com/doc/refman/5.5/en/encryption-functions.html#function_aes-decrypt]",2015-12-30T00:34:43.076+0000,2022-04-04T09:23:55.607+0000,Fixed,Major
IMPALA-5741,JDBC storage handler,IMPALA,New Feature,Open,[],2,"[<JIRA IssueLink: id='12605384'>, <JIRA IssueLink: id='12555617'>]","In Hive there is a generic JDBC Storage handler that would be beneficial to be replicated into Impala. There are several workloads out that that could make good use of it.

The hive version of the handler is tracked under:
https://issues.apache.org/jira/browse/HIVE-1555

Please evaluate the possibility of including this into the roadmap at some point.",2017-07-28T17:20:03.829+0000,2021-11-14T08:47:39.610+0000,,Major
RANGER-238,Range Hive plugin needs update for changes in HiveAuthorizer interface,RANGER,Bug,Resolved,[],1,[<JIRA IssueLink: id='12407507'>],"HiveAuthorizer has been updated (ref: HIVE-9350) with addition of the following method:
List<HivePrivilegeObject> filterListCmdObjects(List<HivePrivilegeObject>,HiveAuthzContext);

Rager plugin for Hive need to be updated for this change.",2015-02-07T01:43:39.012+0000,2015-04-01T06:26:14.383+0000,Fixed,Major
IMPALA-9030,Handle translated external Kudu tables,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12571581'>],"In HIVE-22158 HMS disallows creating of any managed table which is not transactional. This breaks the behavior for managed Kudu tables created from Impala. When user creates a managed Kudu table, HMS internally translates such table into a external table with an additional property ""TRANSLATED_TO_EXTERNAL"" set to true. The table type is changed to EXTERNAL.

Subsequently, when such a table is dropped or renamed, Catalog thinks such tables as external and does not update Kudu (dropping the table in Kudu or renaming the table in Kudu). This is unexpected from the point of view of user since user may think that they created a managed table and Impala should handle the drop and rename accordingly. The same applies to certain other alter operations like alter table set properties on a managed Kudu table which previously was disallowed now goes through after HIVE-22158",2019-10-09T01:06:46.756+0000,2019-11-04T23:15:01.614+0000,Fixed,Major
IMPALA-10491,Impala parquet scanner should use writer.time.zone when converting Hive timestamps,IMPALA,Improvement,Open,[],2,"[<JIRA IssueLink: id='12608286'>, <JIRA IssueLink: id='12608287'>]","IMPALA-8721 reports some issues with Hive 3 and timezone conversion.

HIVE-21290 fixed some of the issues, and also sets writer.time.zone in the Parquet metadata, which provides a better way to determine how the time zone was written. E.g.

{noformat}
tarmstrong@tarmstrong-Precision-7540:~/impala/impala$ hadoop jar ~/repos/parquet-mr/parquet-tools/target/parquet-tools-1.12.0-SNAPSHOT.jar meta /test-warehouse/asdfgh/000000_0
21/02/08 20:26:44 INFO hadoop.ParquetFileReader: Initiating action with parallelism: 5
21/02/08 20:26:44 INFO hadoop.ParquetFileReader: reading another 1 footers
21/02/08 20:26:44 INFO hadoop.ParquetFileReader: Initiating action with parallelism: 5
file:        hdfs://localhost:20500/test-warehouse/asdfgh/000000_0
creator:     parquet-mr version 1.10.99.7.2.7.0-44 (build 27344fd5fdaa371e364c604f471b340f8bcf8936)
extra:       writer.date.proleptic = false
extra:       writer.time.zone = America/Los_Angeles
extra:       writer.model.name = 3.1.3000.7.2.7.0-44
{noformat}

We should use this timezone when converting timestamps, I think either always or when convert_legacy_hive_parquet_utc_timestamps=true. 

CC [~boroknagyz] [~csringhofer]",2021-02-09T04:35:45.006+0000,2021-02-09T04:36:00.740+0000,,Major
TEZ-2119,Counter for launched containers,TEZ,Improvement,In Progress,[],3,"[<JIRA IssueLink: id='12598256'>, <JIRA IssueLink: id='12424812'>, <JIRA IssueLink: id='12501916'>]","org.apache.tez.common.counters.DAGCounter
                NUM_SUCCEEDED_TASKS=32976
                TOTAL_LAUNCHED_TASKS=32976
                OTHER_LOCAL_TASKS=2
                DATA_LOCAL_TASKS=9147
                RACK_LOCAL_TASKS=23761

It would be very nice to have TOTAL_LAUNCHED_CONTAINERS counter added to this. The difference between TOTAL_LAUNCHED_CONTAINERS and TOTAL_LAUNCHED_TASKS should make it easy to see how much container reuse is happening. It is very hard to find out now.",2015-02-18T23:42:12.179+0000,2020-09-05T14:28:58.337+0000,,Major
ORC-451,Timestamp statistics is wrong if read with useUTCTimestamp=true,ORC,Bug,Open,[],1,[<JIRA IssueLink: id='12550762'>],"We're using external orc tables and a timezone ""Europe/Moscow"" (UTC+3) for both client and server. After switching to hive 3 which uses orc 1.5.x we've got an issue with predicate push down filtering out matching stripes by timestamp. E.g. consider a table (it's orc data is in the attachment):
{quote}{{create external table test_ts (ts timestamp) stored as orc;}}

{{insert into test_ts values (""2018-12-24 18:30:00"");}}

{{// No rows selected}}

{{select * from test_ts where ts < ""2018-12-24 19:00:00"";}}

// the lowest filter to return the value

{{select * from test_ts where ts <= ""2018-12-24 21:30:00"";}}
{quote}
The issue only affects external orc tables statistics. Turning ppd off with _set hive.optimize.index.filter=false;_ helps.

We believe it was the https://jira.apache.org/jira/browse/ORC-341, which introduced it.

org.apache.orc.impl.SerializationUtils utc convertion is rather strange:
{quote}public static long convertToUtc(TimeZone local, long time){
 {color:#cc7832}  int {color}offset = local.getOffset(time - local.getRawOffset()){color:#cc7832};{color}{color:#cc7832}  return {color}time - offset{color:#cc7832};{color}
 }
{quote}
This adds a 3 hour offset to our timestamp in UTC+3 timezone (shouldn't it substract 3 hours, btw?).

If org.apache.orc.impl.TimestampStatisticsImpl is used with useUTCTimestamp=false, the timestamp is converted back in a compatible way via SerializationUtils.convertFromUtc. But hive seems to override default org.apache.orc.OrcFile.ReaderOptions with org.apache.hadoop.hive.ql.io.orc.ReaderOptions which have useUTCTimestamp(true) in it's constructor. With useUTCTimestamp=true evaluatePredicateProto predictate is using  TimestampStatisticsImpl.getMaximumUTC(), which returns the timestamp as is, i.e. in the example it's ""2018-12-24 21:30:00 UTC+3"".

At the same time the predicate is not shifted (the value in this tez log is in UTC+3):
{quote}2018-12-24 22:12:16,205 [INFO] [InputInitializer \{Map 1} #0|#0] |orc.OrcInputFormat|: ORC pushdown predicate: leaf-0 = (LESS_THAN ts 2018-12-24 19:00:00.0), expr = leaf-0
{quote}",2018-12-24T19:26:53.522+0000,2019-01-24T10:09:56.597+0000,,Major
TEZ-1248,Reduce slow-start should special case 1 reducer runs,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12474287'>],"Reducer slow-start has a performance problem for the small cases where there is just 1 reducer for a case with a single wave.

Tez knows the split count and wave count, being able to determine if the cluster has enough spare capacity to run the reducer earlier for lower latency in a N-mapper -> 1 reducer case.",2014-07-01T19:46:40.847+0000,2017-03-14T03:49:59.259+0000,Fixed,Critical
CALCITE-1069,"In Aggregate, deprecate indicators, and allow GROUPING to be used as an aggregate function",CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12504494'>, <JIRA IssueLink: id='12495012'>, <JIRA IssueLink: id='12455662'>]","Grouping sets are currently implemented in Calcite using a bit to indicate each
of the grouping columns. For instance, consider the following group by clause:

GROUP BY CUBE (a, b)

The generated Aggregate operator in Calcite will have a row schema consisting of [a, b, GROUPING(a), GROUPING(b)], where GROUPING( x ) is a boolean field indicator which represents whether x is participating in the group by clause.

In contrast, Hive's implementation stores a single number corresponding to the GROUPING bit vector associated with a row (this is the result of the GROUPING_ID function in RDBMS such as MSSQLServer, Oracle, etc). Thus, the row schema of the Aggregate operator is [a, b, GROUPING_ID(a,b)].

This difference is creating a mismatch between Calcite and Hive. As of now, we work around this mismatch in the Hive side: we create our own GROUPING_ID function applied over those columns. However, we have some issues related to predicates pushdown, constant propagation, join project transpose rule (HIVE-12923)
etc., that we need to continue solving as new rules are added to Hive optimizer. In short, this is making the code on the Hive side harder and harder to maintain. 

This jira is intended to modify the implementation on the Calcite side to that we need not make workarounds/hacks in Hive to support Grouping IDs.",2016-01-27T20:52:43.613+0000,2017-10-02T14:30:45.273+0000,Fixed,Major
TEZ-1169,Allow numPhysicalInputs to be specified for RootInputs,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12390006'>],,2014-06-02T18:21:58.909+0000,2014-09-06T01:35:39.547+0000,Fixed,Major
CALCITE-1822,Push Aggregate that follows Aggregate down to Druid,CALCITE,Bug,Open,[],1,[<JIRA IssueLink: id='12505424'>],"Push Aggregate that follows Aggregate down to Druid. This can occur if the SQL has an aggregate function applied to an aggregate function, or with a sub-query in the FROM clause.

{code}
SELECT MAX(COUNT(*))
FROM Emp
GROUP BY deptno

SELECT MAX(c) FROM (
  SELECT deptno, COUNT(*) AS c
  FROM Emp
  GROUP BY deptno)
{code}

And there are other possibilities where there is a Project and/or a Filter after the first Aggregate and before the second Aggregate.

[~bslim], you wrote:
{quote}
For instance in druid we can do select count distinct as an inner group by that group on the key and the outer one does then count. more complex cases is count distinct from unions of multiple queries
{quote}

Can you please write a SQL statement for each of those cases?",2017-06-01T18:45:35.571+0000,2019-02-22T01:22:03.251+0000,,Major
SPARK-23785,LauncherBackend doesn't check state of connection before setting state,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12530311'>],Found in HIVE-18533 while trying to integration with the {{InProcessLauncher}}. {{LauncherBackend}} doesn't check the state of its connection to the {{LauncherServer}} before trying to run {{setState}} - which sends a {{SetState}} message on the connection.,2018-03-23T20:12:15.399+0000,2018-03-30T17:26:05.579+0000,Fixed,Major
THRIFT-3193,Option to supress date value in @Generated annotation,THRIFT,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12434616'>, <JIRA IssueLink: id='12428134'>]",https://issues.apache.org/jira/browse/THRIFT-1813 added the \@Generated annotation but didn't provide a way to turn off generating the date.  This causes issues in our automation which enforces that the generated files are always unchanged.,2015-06-17T08:00:51.601+0000,2015-08-17T23:42:17.935+0000,Fixed,Major
BIGTOP-464,There's currently a commented block in puppet/modules/hadoop/templates/yarn-site.xml that fails to render in some cases,BIGTOP,Bug,Closed,[],1,[<JIRA IssueLink: id='12349431'>],"bigtop-deploy/puppet/modules/hadoop/templates/yarn-site.xml has a block that's commented with the words ""FIXME: MAPREDUCE-3916"".  Unfortunately, it's only an XML comment, not an erb comment, so it gets rendered anyway, even when hadoop_ps_port isn't set (which in several contexts where the file is imported, it's not).  Attaching a patch that prevents the block from being rendered until MAPREDUCE-3916 is addressed.",2012-03-20T00:36:51.173+0000,2013-06-21T23:55:15.654+0000,Fixed,Major
INFRA-15517,Upgrade jenkins junit plugin to >=1.22,INFRA,Improvement,Closed,[],1,[<JIRA IssueLink: id='12520554'>],"after almost a year of waiting.... the jenkins junit plugin contains the fix to accurately show test suite excecutions: [here|https://issues.jenkins-ci.org/browse/JENKINS-37598?focusedCommentId=314645&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-314645]

I would like to ask to upgrade the [junit plugin|https://wiki.jenkins.io/display/JENKINS/JUnit+Plugin] to at least 1.22

old conversation: INFRA-12493",2017-11-20T13:24:23.150+0000,2018-08-25T06:16:48.665+0000,Fixed,Major
CALCITE-4991,Improve RuleEventLogger to also print input rels in FULL_PLAN mode,CALCITE,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12631712'>, <JIRA IssueLink: id='12631713'>]","After using for some time now the new plan logging capabilities introduced via _RuleEventLogger_ in CALCITE-4704, only printing the new rel produced by a rule is not enough for complex cases, because the input rel(s) used by the rule are not printed, and sometimes they are nowhere in the logs, making it hard to understand what situation caused the appearance of a given plan of interest.

The present ticket aims at additionally printing, for _FULL_PLAN_ marker, the input rels used by a given rule, before printing the output rel produced by the rule application.

The output logs would evolve from:

{noformat}
022-01-21T02:41:27,466 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Apply rule [HiveProjectFilterPullUpConstantsRule] to [rel#45:HiveProject,rel#56:HiveFilter]

2022-01-21T02:41:27,468 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Rule [HiveProjectFilterPullUpConstantsRule] produced [rel#58:HiveProject]
2022-01-21T02:41:27,468 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Full plan for [rel#58:HiveProject]:
HiveProject(month=[CAST(202110):INTEGER])
  HiveFilter(condition=[=($0, 202110)])
    HiveTableScan(table=[[default, test2]], table:alias=[test2])
{noformat}

to:

{noformat}
022-01-21T02:41:27,466 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Apply rule [HiveProjectFilterPullUpConstantsRule] to [rel#45:HiveProject,rel#56:HiveFilter]
2022-01-21T02:41:27,468 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Full plan for rule input [rel#45:HiveProject]:
HiveProject(month=[$0])
  HiveFilter(condition=[=($0, 202110)])
    HiveTableScan(table=[[default, test2]], table:alias=[test2])

2022-01-21T02:41:27,468 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Full plan for rule input [rel#56:HiveFilter]:
HiveFilter(condition=[=($0, 202110)])
  HiveTableScan(table=[[default, test2]], table:alias=[test2])

2022-01-21T02:41:27,468 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Rule [HiveProjectFilterPullUpConstantsRule] produced [rel#58:HiveProject]
2022-01-21T02:41:27,468 DEBUG [02c3a9eb-0565-404f-9c56-a2bbb556c827 main] calcite.RuleEventLogger: call#5: Full plan for [rel#58:HiveProject]:
HiveProject(month=[CAST(202110):INTEGER])
  HiveFilter(condition=[=($0, 202110)])
    HiveTableScan(table=[[default, test2]], table:alias=[test2])
{noformat}",2022-01-21T10:44:22.911+0000,2022-03-19T09:17:10.324+0000,Fixed,Major
TEPHRA-285,Fix TransactionProcessorTest in tephra-hbase-compat-1.4,TEPHRA,Bug,Open,[],4,"[<JIRA IssueLink: id='12529241'>, <JIRA IssueLink: id='12529040'>, <JIRA IssueLink: id='12529039'>, <JIRA IssueLink: id='12529041'>]","TransactionProcessorTest in tephra-hbase-compat-1.4 fails with the following exception on JDK 7. It works fine on JDK 8 though.

This issue was exposed by https://issues.apache.org/jira/browse/TEPHRA-272?focusedCommentId=16393952&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16393952

 
{code:java}
2018-03-08 16:59:50,416 - ERROR [main:o.a.h.h.c.CoprocessorHost@543] - The coprocessor org.apache.tephra.hbase.coprocessor.TransactionProcessor threw java.util.ServiceConfigurationError: org.apache.hadoop.hbase.metrics.MetricRegistries: Provider org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl not found
java.util.ServiceConfigurationError: org.apache.hadoop.hbase.metrics.MetricRegistries: Provider org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl not found
	at java.util.ServiceLoader.fail(ServiceLoader.java:231) ~[na:1.7.0_80]
	at java.util.ServiceLoader.access$300(ServiceLoader.java:181) ~[na:1.7.0_80]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:365) ~[na:1.7.0_80]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:445) ~[na:1.7.0_80]
	at org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.getDefinedImplemantations(MetricRegistriesLoader.java:91) ~[hbase-metrics-api-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.metrics.MetricRegistriesLoader.load(MetricRegistriesLoader.java:49) ~[hbase-metrics-api-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.metrics.MetricRegistries$LazyHolder.<clinit>(MetricRegistries.java:40) ~[hbase-metrics-api-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.metrics.MetricRegistries.global(MetricRegistries.java:48) ~[hbase-metrics-api-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.coprocessor.MetricsCoprocessor.createRegistryForRegionCoprocessor(MetricsCoprocessor.java:114) ~[hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.<init>(RegionCoprocessorHost.java:141) ~[hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.createEnvironment(RegionCoprocessorHost.java:419) [hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.createEnvironment(RegionCoprocessorHost.java:94) [hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.loadInstance(CoprocessorHost.java:272) ~[hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.load(CoprocessorHost.java:229) ~[hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.load(CoprocessorHost.java:188) ~[hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.loadTableCoprocessors(RegionCoprocessorHost.java:376) [hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.<init>(RegionCoprocessorHost.java:238) [hbase-server-1.4.0.jar:1.4.0]
	at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:796) [hbase-server-1.4.0.jar:1.4.0]
	at org.apache.tephra.hbase.coprocessor.TransactionProcessorTest.createRegion(TransactionProcessorTest.java:576) [test-classes/:na]
	at org.apache.tephra.hbase.coprocessor.TransactionProcessorTest.testFamilyDeleteTimestamp(TransactionProcessorTest.java:386) [test-classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_80]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_80]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_80]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_80]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) [junit-4.11.jar:na]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.11.jar:na]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) [junit-4.11.jar:na]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) [junit-4.11.jar:na]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) [junit-4.11.jar:na]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) [junit-4.11.jar:na]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.11.jar:na]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.11.jar:na]
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.11.jar:na]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.11.jar:na]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) [junit-4.11.jar:na]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160) [junit-4.11.jar:na]
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) [junit-rt.jar:na]
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) [junit-rt.jar:na]
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) [junit-rt.jar:na]
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) [junit-rt.jar:na]

java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.metrics.MetricRegistries$LazyHolder
	at org.apache.hadoop.hbase.metrics.MetricRegistries.global(MetricRegistries.java:48)
	at org.apache.hadoop.hbase.metrics.BaseSourceImpl.<init>(BaseSourceImpl.java:118)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.<init>(MetricsRegionAggregateSourceImpl.java:56)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.<init>(MetricsRegionAggregateSourceImpl.java:48)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactoryImpl.getAggregate(MetricsRegionServerSourceFactoryImpl.java:41)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactoryImpl.createRegion(MetricsRegionServerSourceFactoryImpl.java:64)
	at org.apache.hadoop.hbase.regionserver.MetricsRegion.<init>(MetricsRegion.java:35)
	at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:798)
	at org.apache.tephra.hbase.coprocessor.TransactionProcessorTest.createRegion(TransactionProcessorTest.java:576)
	at org.apache.tephra.hbase.coprocessor.TransactionProcessorTest.testFamilyDeleteTimestamp(TransactionProcessorTest.java:386)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}",2018-03-10T01:45:19.612+0000,2019-11-19T16:44:24.158+0000,,Major
YETUS-281,hadoop: use built-in dependency order,YETUS,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12464867'>, <JIRA IssueLink: id='12457166'>, <JIRA IssueLink: id='12456597'>]","With the commit of YETUS-280, hadoop should loosen the reins a bit and drop several hundred lines of code to use the built-in dependency creator.",2016-01-13T23:39:00.141+0000,2016-04-25T20:58:06.399+0000,Fixed,Blocker
AVRO-2035,enable validation of default values in schemas by default,AVRO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12608843'>],"suppose i have the following schema evolution:
{code}
{
  ""name"": ""Bob"",
  ""type"": ""record"",
  ""fields"": [
    {""name"": ""f1"", ""type"": ""int""}
  ]
}
{code}
and then:
{code}
{
  ""name"": ""Bob"",
  ""type"": ""record"",
  ""fields"": [
    {""name"": ""f1"", ""type"": ""int""},
    {""name"": ""f2"", ""type"": ""boolean"", ""default"": ""true""}
  ]
}
{code}

the default value for ""f2"" is specified as the _STRING_ ""true"" (and not the literal boolean true). 

if this default value is ever accessed (when reading a gen1-serialized object as a gen2) we get this:

{code}
org.apache.avro.AvroTypeException: Non-boolean default for boolean: ""true""

	at org.apache.avro.io.parsing.ResolvingGrammarGenerator.encode(ResolvingGrammarGenerator.java:408)
	at org.apache.avro.io.parsing.ResolvingGrammarGenerator.getBinary(ResolvingGrammarGenerator.java:307)
	at org.apache.avro.io.parsing.ResolvingGrammarGenerator.resolveRecords(ResolvingGrammarGenerator.java:285)
	at org.apache.avro.io.parsing.ResolvingGrammarGenerator.generate(ResolvingGrammarGenerator.java:118)
	at org.apache.avro.io.parsing.ResolvingGrammarGenerator.generate(ResolvingGrammarGenerator.java:50)
	at org.apache.avro.io.ResolvingDecoder.resolve(ResolvingDecoder.java:85)
	at org.apache.avro.io.ResolvingDecoder.<init>(ResolvingDecoder.java:49)
	at org.apache.avro.io.DecoderFactory.resolvingDecoder(DecoderFactory.java:307)
	at org.apache.avro.generic.GenericDatumReader.getResolver(GenericDatumReader.java:127)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:142)
{code}

yet Schema.parse() passes for this",2017-05-17T01:27:34.188+0000,2021-02-19T10:07:28.160+0000,Fixed,Major
KNOX-1581,YARN v1 UI - Application - Node and logs links broken,KNOX,Sub-task,Closed,[],7,"[<JIRA IssueLink: id='12547832'>, <JIRA IssueLink: id='12547830'>, <JIRA IssueLink: id='12547870'>, <JIRA IssueLink: id='12547869'>, <JIRA IssueLink: id='12547860'>, <JIRA IssueLink: id='12547890'>, <JIRA IssueLink: id='12547889'>]","[http://localhost:8443/gateway/test/yarn/cluster/app/application_1541711200634_0002]

Under Node ""http://ambari:8042""
 * Redirects to [http://localhost:8443/gateway/nodemanagerui/node?host=ambari&port=8042]
 ** Correct link should be [http://localhost:8443/gateway/test/nodemanager/node?host=ambari&port=8042|http://localhost:8443/gateway/nodemanagerui/node?host=ambari&port=8042]
 * Root cause: There is no nodemanagerui endpoint - Could be the same as KNOX-1579

Under Logs ""Logs""
 * Redirects to [http://localhost:8443/gateway/nodemanagerui/node/containerlogs/container_e01_1541711200634_0002_01_000001/ambari-qa?scheme=http&host=ambari&port=8042]
 ** Correct link should be [http://localhost:8443/gateway/test/yarn/nodemanager/node/containerlogs/container_e01_1541711200634_0002_01_000001/ambari-qa?scheme=http&host=ambari&port=8042]
 ** Which then redirects incorrectly to [http://ambari:19888/jobhistory/logs/ambari:45454/container_e01_1541711200634_0002_01_000001/container_e01_1541711200634_0002_01_000001/ambari-qa?user.name=admin]
 *** Correct link should be [http://localhost:8443/gateway/test/jobhistory/joblogs/ambari:45454/container_e01_1541711200634_0002_01_000001/job_1541711200634_0002/ambari-qa/?user.name=admin]",2018-11-09T12:52:51.220+0000,2019-07-09T18:53:43.194+0000,Fixed,Major
TEZ-3385,DAGClient API should be accessible outside of DAG submission,TEZ,New Feature,Open,[],2,"[<JIRA IssueLink: id='12476349'>, <JIRA IssueLink: id='12476491'>]","  In PIG-4958, I had to resort to  

DAGClient client = new DAGClientImpl(appId, dagID, new TezConfiguration(conf), null);

This is not good as DAGClientImpl is a internal class and not something users should be referring to. Tez needs to have an API to give DAGClient given the appId, dagID and configuration. This is something basic like JobClient.getJob(String jobID). 

",2016-07-28T23:51:14.540+0000,2017-09-01T15:47:24.640+0000,,Major
TEZ-1153,Allow object caching for more than one vertex,TEZ,Improvement,Open,[],3,"[<JIRA IssueLink: id='12393536'>, <JIRA IssueLink: id='12388791'>, <JIRA IssueLink: id='12388792'>]",  It would be good to have option to cache an object for more than 1 vertex by specifying a list of vertex ids,2014-05-27T17:55:55.867+0000,2017-03-14T03:40:09.935+0000,,Major
IMPALA-9923,"Data loading of TPC-DS ORC fails with ""Fail to get checksum""",IMPALA,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12604241'>, <JIRA IssueLink: id='12603436'>, <JIRA IssueLink: id='12598902'>]","{noformat}

INFO  : Loading data to table tpcds_orc_def.store_sales partition (ss_sold_date_sk=null) from hdfs://localhost:20500/test-warehouse/managed/tpcds.store_sales_orc_def
INFO  : 

ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. java.io.IOException: Fail to get checksum, since file /test-warehouse/managed/tpcds.store_sales_orc_def/ss_sold_date_sk=2451646/base_0000003/_orc_acid_version is under construction.
INFO  : Completed executing command(queryId=ubuntu_20200707055650_a1958916-1e85-4db5-b1bc-cc63d80b3537); Time taken: 14.512 seconds
INFO  : OK
Error: Error while compiling statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. java.io.IOException: Fail to get checksum, since file /test-warehouse/managed/tpcds.store_sales_orc_def/ss_sold_date_sk=2451646/base_0000003/_orc_acid_version is under construction. (state=08S01,code=1)
java.sql.SQLException: Error while compiling statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. java.io.IOException: Fail to get checksum, since file /test-warehouse/managed/tpcds.store_sales_orc_def/ss_sold_date_sk=2451646/base_0000003/_orc_acid_version is under construction.
	at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:401)
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:266)
	at org.apache.hive.beeline.Commands.executeInternal(Commands.java:1007)
	at org.apache.hive.beeline.Commands.execute(Commands.java:1217)
	at org.apache.hive.beeline.Commands.sql(Commands.java:1146)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1497)
	at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1355)
	at org.apache.hive.beeline.BeeLine.executeFile(BeeLine.java:1329)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1127)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1082)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:546)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:528)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
Closing: 0: jdbc:hive2://localhost:11050/default;auth=none
{noformat}

https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/11223/",2020-07-07T15:38:18.596+0000,2020-12-04T09:44:00.397+0000,Fixed,Critical
PHOENIX-5435,Annotate HBase WALs with Phoenix Metadata,PHOENIX,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12601259'>, <JIRA IssueLink: id='12567696'>, <JIRA IssueLink: id='12567697'>, <JIRA IssueLink: id='12567360'>]","HBase write-ahead-logs (WALs) drive not only failure recovery, but HBase replication and some HBase backup frameworks. The WALs contain HBase-level metadata such as table and region, but lack Phoenix-level metadata. That means that it's quite difficult to build correct logic that needs to know about Phoenix-level constructs such as multi-tenancy, views, or indexes. 

HBASE-22622 and HBASE-22623 add the capacity for coprocessors to annotate extra key/value pairs of metadata into the HBase WAL. We should have the option to annotate the tuple <tenant_id, table-or-view-name, timestamp>, or some hashed way to reconstruct that tuple into the WAL. It should have a feature toggle so operators who don't need it don't bear the slight extra storage cost. ",2019-08-08T21:50:25.759+0000,2020-12-31T23:56:09.093+0000,Fixed,Major
PARQUET-1434,Release parquet-mr 1.11.0,PARQUET,Task,Resolved,[],9,"[<JIRA IssueLink: id='12550292'>, <JIRA IssueLink: id='12553032'>, <JIRA IssueLink: id='12553837'>, <JIRA IssueLink: id='12555357'>, <JIRA IssueLink: id='12555377'>, <JIRA IssueLink: id='12565279'>, <JIRA IssueLink: id='12554182'>, <JIRA IssueLink: id='12569628'>, <JIRA IssueLink: id='12572218'>]",,2018-10-03T09:39:27.950+0000,2019-12-09T11:20:04.850+0000,Fixed,Major
SLIDER-875,Ability to create an Uber application package with capability to deploy and manage as a single business app,SLIDER,New Feature,Resolved,[],1,[<JIRA IssueLink: id='12478684'>],"A business application as we typically refer to, is one that provides value to an end user. Few examples will be, a CRM application, an online advertising application, and a trucking application (to monitor driving habits of truck drivers).

An end user does not understand (or care about) the numerous application components like HBase, Storm, Spark, Kakfa, Tomcat, MySql, Memcached, or Nodejs that are required to build such a business application. 

Several such business applications are hosted by cloud vendors like AWS, GCE, Azure, and others. From a cluster management point of view, the IT administrator would benefit from an Uber control of the business application as a whole. The business application owner understands the different components (like Tomcat, Memcached, HBase, etc.) of her/his Uber application. As much as they need fine-grain control of each of these individual applications (which is supported today), they would also benefit from a management control for the Uber app. With Docker becoming popular every day, this will provide a platform to the application owners to define a business application as a conglomeration of Docker containers.

Slider currently is viewed (and used) to package individual applications like HBase, Storm, Kafka, Memcached, and Tomcat. Slider should be able to expose the concept of an Uber application package definition. This Uber definition will be composed of config and resource specifications of the individual application components. Additionally, it will have definitions for Uber management and control, like -
# Stop, start and flex of the Uber app
# Dependency specification between the individual applications such that flex of certain components of an application can automatically trigger proportional flex of components in another application
# Cruise control of the Uber app, on top of what SLIDER-868 will provide for an individual app. Ability to define a skyline for the Uber app, over time and other dimensions.
# Resource requirements and planning for the Uber app as a whole. Most of the time, an Uber app is functional only when all (or minimum viable) application components are deployed and available. Tomcat running with MySql, Memcached and HBase still waiting for containers, is a useless business application. Slider should be able to do resource calculation and negotiation for the Uber app as a whole. It can work with YARN to get the minimal viable applications of the Uber app running or not bother to run anything (I smell SLAs for vendors and savings for application owners).
# Ability to define and use multiple YARN labels for the Uber application (in addition to the fine grained label definitions for the individual sub-components of a single app)

I am sure, there are several other benefits which are not identified yet, but this is a start.
",2015-05-14T23:32:31.403+0000,2017-11-06T22:32:25.717+0000,Fixed,Major
INFRA-10315,OOME: can't create native thread,INFRA,Bug,Closed,[],2,"[<JIRA IssueLink: id='12434816'>, <JIRA IssueLink: id='12434814'>]","We see this in hbase builds as of about Aug 18th, 2015 5:10:02 AM. See HBASE-14262.  Did anything change on the H* machines?",2015-08-20T00:54:16.566+0000,2015-12-07T06:21:01.516+0000,Duplicate,Major
SPARK-2421,Spark should treat writable as serializable for keys,SPARK,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12406345'>, <JIRA IssueLink: id='12392795'>, <JIRA IssueLink: id='12391724'>]","It seems that Spark requires the key be serializable (class implement Serializable interface). In Hadoop world, Writable interface is used for the same purpose. A lot of existing classes, while writable, are not considered by Spark as Serializable. It would be nice if Spark can treate Writable as serializable and automatically serialize and de-serialize these classes using writable interface.

This is identified in HIVE-7279, but its benefits are seen global.",2014-07-09T19:56:59.403+0000,2021-12-28T16:52:55.517+0000,Won't Fix,Major
SPARK-12154,Upgrade to Jersey 2,SPARK,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12463537'>, <JIRA IssueLink: id='12466589'>, <JIRA IssueLink: id='12481117'>, <JIRA IssueLink: id='12457340'>]","Fairly self-explanatory, Jersey 1 is a bit old and could use an upgrade. Library conflicts for Jersey are difficult to workaround - see discussion on SPARK-11081. It's easier to upgrade Jersey entirely, but we should target Spark 2.0 since this may be a break for users who were using Jersey 1 in their Spark jobs.",2015-12-05T01:16:04.800+0000,2018-12-29T08:34:45.331+0000,Fixed,Blocker
REEF-42,Extra YARN container causing unexpected memory reservations,REEF,Bug,Open,[],1,[<JIRA IssueLink: id='12401345'>],"Our cluster has 4 nodes, each with 13.67GB of memory. When we launch Surf(a long-running job) with 4 evaluators(7GB each), The available memory becomes 6.67GBX3nodes and 5.67GBX1node(AM is 1GB). But an extra container request(7GB), hangs at RM, because of the following reasons.

# Because Surf is a long-running job, the evaluators that have been allocated do not exit and make room for the extra container. If there was a room, REEF would have been notified of the allocation of the extra container and released it right away.
# To avoid YARN-314, currently we never send a 0-container request, which in effect removes the hanging extra container

As a result, RM infinitely tries to allocate the hanging request, reserving 7GB for each node. So, *Memory Reserved* metric increases and *Memory Available* metric decreases.

The same thing happens when we explicitly request for more than the capacity, say 8GBX5evaluators. But the difference is that the one caused by the extra container is unpredictable.

[~chobrian] and I discussed the tradeoff between the followings.
# Send 0-container requests and address YARN-314 differently by adding another indirection atop AMRMClient or replacing it altogether
# Wait until YARN-314 is resolved since our case is not common and can be discovered and fixed by the system administrator

We think the second approach is better. Once YARN-314 is resolved, I'll create a patch that allows sending 0-container requests. 

Any suggestions are welcome.",2014-11-14T10:29:25.819+0000,2014-11-14T16:20:01.648+0000,,Major
TEZ-1003,Need a input that merges multiple ShuffleMergedInput from VertexGroup,TEZ,Bug,Closed,[],3,"[<JIRA IssueLink: id='12386470'>, <JIRA IssueLink: id='12385822'>, <JIRA IssueLink: id='12386461'>]","In PIG-3835, was trying to do use vertex groups for unions. Union followed by store works fine. But when trying to do groupby, 
 
{code}
A = LOAD '/tmp/data' AS (f1:int,f2:int);
B = LOAD '/tmp/data2' AS (f1:int,f2:int);
C = UNION onschema A,B;
D = GROUP C by f1;
E = FOREACH D GENERATE group, SUM(C.f2);
store E into '/tmp/tezout' using PigStorage();
{code}

ConcatenatedMergedKeyValuesInput on the reduce, had only grouped records within each input and not across all inputs.

i.e If A had records
a 1
b 1
b 2
and B
a 2
a 3
b 3

The records from ConcatenatedMergedKeyValuesInput of A and B were
a {1}, b {1,2}, a {2,3}, b {3} while I am expecting a {1,2,3}, b {1,2,3}


",2014-03-31T09:24:42.994+0000,2014-09-06T01:35:46.813+0000,Fixed,Major
KNOX-1866,Fix the HBase UI proxying,KNOX,Bug,Closed,[],3,"[<JIRA IssueLink: id='12561905'>, <JIRA IssueLink: id='12561906'>, <JIRA IssueLink: id='12561466'>]","There are lots and lots of edges in the HBASEUI service which render the system broken.

I've been able to get everythign working exception the LogLevel servlet. The way this one is written in HBase fundamentally makes it impossible to proxy it as-is with Knox (at least, given my knowledge).

There are some trivial HBase changes that need to go in first to make sure these rules work as expected.",2019-05-24T00:35:27.513+0000,2019-10-21T14:24:24.008+0000,Fixed,Major
RANGER-2504,Remove index related codes from Hive,RANGER,Bug,Open,[],1,[<JIRA IssueLink: id='12565247'>],Hive no longer supports indexes.,2019-07-12T11:55:02.380+0000,2019-07-15T22:57:02.059+0000,,Major
OOZIE-2786,Pass Oozie workflow ID and settings to Spark application configuration,OOZIE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12493593'>],"When using Oozie to launch Spark applications, the Oozie work flow ID and etc do not show up in Spark Application's Environment settings. There is no Spark application ID exposed from Oozie side as well about the Spark applications Oozie launches.

When looking at applications in Spark History Server, it is hard to figure out which Oozie workflow launched it. This makes debugging the workflow hard. We want to be able to let Spark History server display the Oozie workflow ID in the application's 'Environment' settings.

For reference, oozie properties that can be passed to Spark are:
    oozie.job.id
    oozie.HadoopAccessorService.created
    oozie.action.spark.setup.hadoop.conf.dir
    oozie.child.mapreduce.job.tags
    oozie.action.id
    oozie.action.rootlogger.log.level",2017-02-02T21:18:46.316+0000,2018-01-25T21:00:56.741+0000,Fixed,Critical
TEZ-1170,Simplify Vertex Initializing transition,TEZ,Improvement,Closed,[],1,[<JIRA IssueLink: id='12389457'>],"After TEZ-1145 and 1151, a vertex should only need to stay in INITIALZING state when it has an uninitialized edge, or when the parallelism is at -1 (not set yet). Waiting for all RootInputInitializers to complete should not be required - as long as one of them sets parallelism (via a VertexManager).",2014-06-02T18:26:10.894+0000,2014-09-06T01:35:57.172+0000,Fixed,Major
FLUME-1404,Cannot build Apache Flume 1.2.0 with profile hadoop23,FLUME,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12355575'>, <JIRA IssueLink: id='12358390'>]","Here is the output: 
{noformat}
[bruno@p8700 apache-flume-1.2.0]$ mvn -DskipTests -Dhadoop.profile=23  clean install
/usr/lib/jvm/java
[INFO] Scanning for projects...
[ERROR] The build could not read 1 project -> [Help 1]
[ERROR]   
[ERROR]   The project org.apache.flume.flume-ng-sinks:flume-ng-hbase-sink:1.2.0 (/home/bruno/projects/bigtop-git-svn/temp/apache-flume-1.2.0/flume-ng-sinks/flume-ng-hbase-sink/pom.xml) has 1 error
[ERROR]     'dependencies.dependency.version' for org.apache.hadoop:hadoop-test:jar is missing. @ line 88, column 17
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
{noformat}

From http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-test only Apache Hadoop 1.X has a hadoop-test artefact.
This is also why hadoop-test version is only defined for the default Apache Hadoop 1.X profile and not the 0.23 one",2012-07-28T21:51:24.385+0000,2018-11-11T18:45:43.668+0000,Invalid,Major
TEZ-1993,Implement a pluggable InputSizeEstimator for grouping fairly,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12407585'>],"Split grouping is currently done using a file size measurement which is the exact size of the split as it stays at rest on HDFS.

This is not valid for columnar formats and especially suffers from highly compressible data skews.",2015-01-22T21:52:56.705+0000,2015-06-30T04:52:40.757+0000,Fixed,Major
ORC-323,Predicate push down for nested fields,ORC,Improvement,Closed,[],1,[<JIRA IssueLink: id='12529627'>],"*1. Predicate Pushdown For Nested field*

*1.1 Objective*

In the ORC(Optimized Row Columnar) all the primitive type column consist of index. Predicate refer to the column name in where clause and pushdown mean skipping rows groups, strips and block while reading by comparing the meta store in the strips. Meta consist of max, sum ,min value present in the given column. 

Currently predicate pushdown only work for top level column of the schema. 

Extending the Predicate Pushdown for nested structure in hive.  


*1.2 Current state *- 
 
*1.2.1 Schema*
struct<int1:int, complex:struct<int2:int,String1:string>>
 
*1.2.2 Search Argument  *
SearchArgument sarg = SearchArgumentFactory.newBuilder()
       .startAnd()
       .startNot()
       .lessThan(“int2"", PredicateLeaf.Type.LONG, 300000L)
       .end()
       .lessThan(""int2"", PredicateLeaf.Type.LONG, 600000L)
       .end()
       .build();
 
 
 
 
*1.2.3 Pushdown Predicate not supported in Nested field in ORC*
 
private boolean[] populatePpdSafeConversion() {
    if (fileSchema == null || readerSchema == null || readerFileTypes == null) {
      return null;
    }

    boolean[] result = new boolean[readerSchema.getMaximumId() + 1];
    boolean safePpd = validatePPDConversion(fileSchema, readerSchema);
    result[readerSchema.getId()] = safePpd;
    List<TypeDescription> children = readerSchema.getChildren();
    if (children != null) {
      for (TypeDescription child : children) {
        TypeDescription fileType = getFileType(child.getId());
        safePpd = validatePPDConversion(fileType, child);
        result[child.getId()] = safePpd;
      }
    }
    return result;
  }

In populatePpdSafeConversion() this function only check the conversion validation for top level field. So validation of nested field search argument fails.


static int findColumns(SchemaEvolution evolution,
                         String columnName) {
    TypeDescription readerSchema = evolution.getReaderBaseSchema();
    List<String> fieldNames = readerSchema.getFieldNames();
    List<TypeDescription> children = readerSchema.getChildren();
    for (int i = 0; i < fieldNames.size(); ++i) {
      if (columnName.equals(fieldNames.get(i))) {
        TypeDescription result = evolution.getFileType(children.get(i));
        return result == null ? -1 : result.getId();
      }
    }
    return -1;
  }


In findColumns() all the only top level column is referred. “Int2” is nested column due to which  “-1” is return instead of index of “int2”.

*1.2.4 Result -*

PPD is not working for int2 field in the search argument.


*1.3 Expected state - *

*1.3.1 Schema*
struct<int1:int, complex:struct<int2:int,String1:string>>
 
*1.3.2 Query*
Replacing Column name in PredicateLeaf with fully qualified column path.
 
SearchArgument sarg = SearchArgumentFactory.newBuilder()
       .startAnd()
       .startNot()
       .lessThan(“complex.int2"", PredicateLeaf.Type.LONG, 300000L)
       .end()
       .lessThan(""complex.int2"", PredicateLeaf.Type.LONG, 600000L)
       .end()
       .build();
 
*1.3.3 Pushdown Predicate support in Nested field*

https://github.com/apache/orc/pull/232


*1.3.4 Result*

PPD is working for complex.int2 field in the search argument.",2018-03-16T12:20:31.448+0000,2018-05-14T22:24:44.294+0000,Fixed,Minor
SLIDER-829,"when containers are allocated, explicitly cancel the request",SLIDER,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12411663'>, <JIRA IssueLink: id='12411661'>]",The AM needs to cancel outstanding requests made for containers —even after they've been allocated. Without this they will be re-requested again and simply discarded.,2015-03-24T11:12:37.799+0000,2015-03-24T13:50:38.251+0000,Duplicate,Blocker
PHOENIX-1479,IndexSplitTransaction won't compile after HBASE-12550,PHOENIX,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12403774'>, <JIRA IssueLink: id='12402092'>, <JIRA IssueLink: id='12402254'>, <JIRA IssueLink: id='12402089'>]","{code}
[ERROR] /Users/apurtell/src/phoenix/phoenix-core/src/main/java/org/apache/hadoop/hbase/regionserver/IndexSplitTransaction.java:[364,28] method createDaughterRegionFromSplits in class org.apache.hadoop.hbase.regionserver.HRegion cannot be applied to given types;
[ERROR] required: org.apache.hadoop.hbase.HRegionInfo,int
[ERROR] found: org.apache.hadoop.hbase.HRegionInfo
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] /Users/apurtell/src/phoenix/phoenix-core/src/main/java/org/apache/hadoop/hbase/regionserver/IndexSplitTransaction.java:[368,28] method createDaughterRegionFromSplits in class org.apache.hadoop.hbase.regionserver.HRegion cannot be applied to given types;
[ERROR] required: org.apache.hadoop.hbase.HRegionInfo,int
[ERROR] found: org.apache.hadoop.hbase.HRegionInfo
[ERROR] reason: actual and formal argument lists differ in length
{code}
",2014-11-24T22:30:19.630+0000,2016-01-24T03:31:49.631+0000,Fixed,Major
ORC-228,Make MemoryManagerImpl.ROWS_BETWEEN_CHECKS configurable,ORC,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12511720'>, <JIRA IssueLink: id='12511719'>]","currently addedRow() looks like
{noformat}
public void addedRow(int rows) throws IOException {
    rowsAddedSinceCheck += rows;
    if (rowsAddedSinceCheck >= ROWS_BETWEEN_CHECKS) {
      notifyWriters();
    }
  }
{noformat}

it would be convenient for testing to set ROWS_BETWEEN_CHECKS to a low value so that we can generate multiple stripes with very little data.

Currently the only way to do this is to create a new MemoryManager that overrides this method and install it via OrcFile.WriterOptions but this only works when you have control over creating the Writer.
For example _org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testRecordReaderNewBaseAndDelta()_

There is no way to do this via some set of config params to make Hive query for example, create multiple stripes with little data.",2017-08-10T22:43:02.062+0000,2018-05-14T22:24:51.020+0000,Fixed,Major
TEZ-909,Provide support for application tags,TEZ,New Feature,Closed,[],3,"[<JIRA IssueLink: id='12387496'>, <JIRA IssueLink: id='12384022'>, <JIRA IssueLink: id='12422017'>]","YARN-1399 adds support for application tags. With MR, setting mapreduce.job.tags it is set by the MR application master on the ApplicationSubmissionContext. This is mainly used with rolling upgrades and RM HA to determine old child jobs and kill them on a new launch of the parent job (pig script for example). We need to do that in Tez as well. 

Fixing this will introduce a dependency on hadoop 2.4.0.",2014-03-03T19:47:15.813+0000,2016-07-09T19:15:58.825+0000,Fixed,Major
SLIDER-66,AM Restart not working -YARN issues,SLIDER,Bug,Resolved,[],6,"[<JIRA IssueLink: id='12388313'>, <JIRA IssueLink: id='12388311'>, <JIRA IssueLink: id='12388312'>, <JIRA IssueLink: id='12388310'>, <JIRA IssueLink: id='12388309'>, <JIRA IssueLink: id='12391359'>]",AM restart isn't working due to some YARN-side issues,2014-05-19T10:14:25.565+0000,2014-08-18T09:54:47.734+0000,Fixed,Major
ACCUMULO-2632,Research the performance of using compression on the walogs,ACCUMULO,Task,Resolved,[],2,"[<JIRA IssueLink: id='12386079'>, <JIRA IssueLink: id='12388707'>]",Research the performance impact of using compression on data that is being written to the walog.,2014-04-04T17:36:50.535+0000,2015-07-09T22:52:09.427+0000,Later,Minor
SOLR-2565,Prevent IW#close and cut over to IW#commit,SOLR,Improvement,Closed,[],4,"[<JIRA IssueLink: id='12354192'>, <JIRA IssueLink: id='12339858'>, <JIRA IssueLink: id='12339380'>, <JIRA IssueLink: id='12339381'>]","Spinnoff from SOLR-2193. We already have a branch to work on this issue here https://svn.apache.org/repos/asf/lucene/dev/branches/solr2193 

The main goal here is to prevent solr from closing the IW and use IW#commit instead. AFAIK the main issues here are:

The update handler needs an overhaul.

A few goals I think we might want to look at:

1. Expose the SolrIndexWriter in the api or add the proper abstractions to get done what we now do with special casing:
2. Stop closing the IndexWriter and start using commit (still lazy IW init though).
3. Drop iwAccess, iwCommit locks and sync mostly at the Lucene level.
4. Address the current issues we face because multiple original/'reloaded' cores can have a different IndexWriter on the same index.

Eventually this is a preparation for NRT support in Solr which I will create a followup issue for.",2011-06-01T07:32:50.275+0000,2013-05-10T10:41:08.748+0000,Fixed,Major
ACCUMULO-4670,RangeInputSplit drops sasl-enabled boolean,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12507926'>],"When creating the RecordReader for a RangeInputSplit, the Split should encapsulate all of the context of the configuration to invoke that Split (e.g. instance info, connector info, scan iterators, etc).

However, the ""useSasl"" option is not preserved inside of the RangeInputSplit which results in the split always using {{useSasl=false}}. This obviously breaks Kerberos installations:

Offending code: https://github.com/apache/accumulo/blob/f81a8ec7410e789d11941351d5899b8894c6a322/core/src/main/java/org/apache/accumulo/core/client/mapreduce/RangeInputSplit.java#L362-L376

A workaround is to make sure that the Configuration present in the call to {{getRecordReader}} also has {{AccumuloInputFormat.setZooKeeperInstance}} invoked on it (push the ClientConfiguration into the Configuration which will result in it being merged with the details set on the input split).

{code}
      if (useSasl) {
        AccumuloInputFormat.setZooKeeperInstance(jobConf, instanceName, zookeepers, useSasl);
      }
{code}",2017-06-28T23:36:55.477+0000,2021-09-03T22:15:39.814+0000,Incomplete,Critical
SENTRY-392,Authorization for column level security ,SENTRY,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12398481'>, <JIRA IssueLink: id='12394095'>, <JIRA IssueLink: id='12394240'>, <JIRA IssueLink: id='12394234'>]",,2014-08-14T08:34:18.850+0000,2014-11-18T08:52:26.716+0000,Fixed,Major
CALCITE-726,HiveRelNode.CONVENTION not present in TraitSet,CALCITE,Bug,Open,[],1,[<JIRA IssueLink: id='12424866'>],"I have tested the latest Calcite snapshot against Hive.

We get the following exception when we create a Project
operator copy, that we were not getting in 1.2:

{noformat}
junit.framework.AssertionFailedError: Unexpected exception
java.lang.AssertionError
	at
org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.copy(H
iveProject.java:169)
	at org.apache.calcite.rel.core.Project.copy(Project.java:110)
	at
org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:770)
	at
org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:764)
	at
org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:764)
	at
org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:764)
	at org.apache.calcite.plan.hep.HepPlanner.setRoot(HepPlanner.java:150)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan
(CalcitePlanner.java:994)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPr
eJoinOrderingTransforms(CalcitePlanner.java:951)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(C
alcitePlanner.java:820)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(C
alcitePlanner.java:768)
	at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:109)
	at
org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.ja
va:741)
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:145)
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:105)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlann
er.java:607)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.jav
a:244)
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAn
alyzer.java:10051)
	at
org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlann
er.java:207)
	at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAn
alyzer.java:227)
	at
org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(Exp
lainSemanticAnalyzer.java:74)
	at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAn
alyzer.java:227)
...
{noformat}

The assertion that is not met is the following:

{code}
  @Override
  public Project copy(RelTraitSet traitSet, RelNode input, List<RexNode>
exps,
      RelDataType rowType) {
    assert traitSet.containsIfApplicable(HiveRelNode.CONVENTION);
    return new HiveProject(getCluster(), traitSet, input, exps, rowType,
getFlags());
  }
{code}",2015-05-14T17:40:15.000+0000,2021-06-18T08:31:42.146+0000,,Major
CALCITE-831,Rules to push down limits,CALCITE,Bug,Open,"[<JIRA Issue: key='CALCITE-889', id='12875678'>, <JIRA Issue: key='CALCITE-892', id='12882229'>]",5,"[<JIRA IssueLink: id='12436557'>, <JIRA IssueLink: id='12433674'>, <JIRA IssueLink: id='12436556'>, <JIRA IssueLink: id='12443182'>, <JIRA IssueLink: id='12437251'>]","Add rules to push down limits, based on a conversation with [~maryannxue].

Recall that the SQL LIMIT clause becomes a Sort relational expression; the Sort.fetch attribute specifies the limit, or is null for no limit; the Sort has zero or more collations, corresponding to the expressions in the ORDER BY; there may also be a Sort.offset attribute.

A ""naked limit"" is a Sort with 0 sort columns and a not-null fetch clause, e.g. ""LIMIT 10"" without ""ORDER BY"".

Cases:
* SortProjectTransposeRule matches a Sort on a Project, already exists, and already handles offset and fetch. (DONE)
* SortSortMergeRule (proposed) combines two Sort expressions. Among other cases, it handles a naked limit followed by a naked limit, and a sort followed by a naked limit.
* SortUnionTransposeRule pushes a Sort through a Union in some cases. It can push a naked limit through a union all, but needs to keep a limit after the union.
* SortJoinTransposeRule pushes a Sort through a Join in some cases. You could push 'select * from emp join dept using (deptno) order by sal limit 10' because the join to dept is just a 'lookup' and has no filtering effect.
* SortAggregateMergeRule could, if the limit applied to a measure such as sum or count, create a ""topN"" aggregate, for which there are known optimizations.

Non-cases:
* SortFilterTransposeRule would not be very useful. It is not safe to push a limit through a Filter. And you can perform a Sort (without limit) before a Filter but it is more effort, so why bother?",2015-08-07T15:58:17.438+0000,2019-02-22T01:20:02.811+0000,,Major
ACCUMULO-3083,capability balancing,ACCUMULO,New Feature,Open,[],2,"[<JIRA IssueLink: id='12395325'>, <JIRA IssueLink: id='12395197'>]","I was thinking about balancing, in much the way that new container-based (yarn, mesos, etc) resource managers work.  The existing {{Balancers}} try to keep the number of tablets even over presumably homogeneous nodes.

But as clusters age and are improved and expanded in piecemeal, the hardware capabilities will drift apart.  Some nodes may be capable of handling more load than others.  We may even want some nodes to have a trivial number of tablets so the data they serve is always cached.

Tablet servers could advertise an attribute (or several).  Really, it would just be a name, like {{fatnode}} or {{metaonly}}.  These would then be used by the {{Balancer}} to favor or ignore tservers for balancing.
",2014-08-25T22:44:51.179+0000,2014-08-27T23:08:25.767+0000,,Major
SLIDER-1052,Deadlock in slider AM,SLIDER,Bug,Open,[],2,"[<JIRA IssueLink: id='12454460'>, <JIRA IssueLink: id='12455109'>]","I have a hung slider AM in the following state.
The first app attempt failed to start, so this is the 2nd one. -However, the 1st app attempt process is still running on the same machine, and it is in a state where I cannot jstack it even with -F. I will kill it shortly and see what happens.  YARN thinks it's killed.-.nm, it was some other process. The first container was on a different machine and did die.
The 2nd attempt received the container death notification for the first one:
{noformat}
2016-01-07 03:59:41,828 [AMRM Callback Handler Thread] INFO  appmaster.SliderAppMaster - Container Completion for containerID=container_e02_1450721565699_0007_01_000001, state=COMPLETE, exitStatus=-105, diagnostics=Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
{noformat}
Note that is is from the 2nd container (container_e02_1450721565699_0007_02_000001)  logs. Jstack for the 2nd attempt has the deadlock:

{noformat}
Found one Java-level deadlock:
=============================

""AMRM Callback Handler Thread"":
  waiting to lock Monitor@0x00007f1b953b18b8 (Object@0x00000000c022c6f0, a org/apache/slider/server/appmaster/state/AppState),
  which is held by ""main""
""main"":
  waiting to lock Monitor@0x00007f1b953b1128 (Object@0x00000000c00db378, a org/apache/slider/server/appmaster/SliderAppMaster),
  which is held by ""AMRM Callback Handler Thread""

{noformat}

The jstack is with -F, so I cannot actually see thread names in the dump, but these look like it (not sure about the first one):
{noformat}
Thread 11054: (state = BLOCKED)
 - org.apache.slider.server.appmaster.state.AppState.onCompletedNode(org.apache.hadoop.yarn.api.records.ContainerStatus) @bci=0, line=1534 (Interpreted frame)
 - org.apache.slider.server.appmaster.SliderAppMaster.onContainersCompleted(java.util.List) @bci=119, line=1606 (Interpreted frame)
 - org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run() @bci=141, line=300 (Interpreted frame)
...
Thread 10254: (state = BLOCKED)
 - org.apache.hadoop.service.AbstractService.getConfig() @bci=0, line=403 (Interpreted frame)
 - org.apache.slider.server.appmaster.SliderAppMaster.getClusterFS() @bci=5, line=1369 (Interpreted frame)
 - org.apache.slider.server.appmaster.SliderAppMaster.createAndRunCluster(java.lang.String) @bci=1291, line=822 (Interpreted frame)
 - org.apache.slider.server.appmaster.SliderAppMaster.runService() @bci=162, line=576 (Interpreted frame)
 - org.apache.slider.core.main.ServiceLauncher.launchService(org.apache.hadoop.conf.Configuration, java.lang.String[], boolean) @bci=128, line=188 (Interpreted frame)
 - org.apache.slider.core.main.ServiceLauncher.launchServiceRobustly(org.apache.hadoop.conf.Configuration, java.lang.String[]) @bci=4, line=475 (Interpreted frame)
 - org.apache.slider.core.main.ServiceLauncher.launchServiceAndExit(java.util.List) @bci=21, line=403 (Interpreted frame)
 - org.apache.slider.core.main.ServiceLauncher.serviceMain(java.util.List) @bci=143, line=630 (Interpreted frame)
 - org.apache.slider.server.appmaster.SliderAppMaster.main(java.lang.String[]) @bci=24, line=2327 (Interpreted frame)


{noformat}



",2016-01-07T19:41:16.490+0000,2016-01-21T21:38:27.688+0000,,Critical
SPARK-37205,Support mapreduce.job.send-token-conf when starting containers in YARN,SPARK,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12643128'>, <JIRA IssueLink: id='12626901'>]",{{mapreduce.job.send-token-conf}} is a useful feature in Hadoop (see [YARN-5910|https://issues.apache.org/jira/browse/YARN-5910] with which RM is not required to statically have config for all the secure HDFS clusters. Currently it only works for MRv2 but it'd be nice if Spark can also use this feature. I think we only need to pass the config to {{LaunchContainerContext}} in {{Client.createContainerLaunchContext}}.,2021-11-03T23:45:50.577+0000,2022-07-01T16:54:13.052+0000,Fixed,Major
IVY-1026,checksum failed due to incorrect parsing of .sha1 file content,IVY,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12332169'>, <JIRA IssueLink: id='12323230'>]","Ivy failed to download artifacts from maven public repository due a checksum failure. 

The problem exists in class ChecksumHelper.java at line 59.

It checks to see if there is a space in the checksum file content, if so, it does a substring from position 0 to the position index of the space. However, on maven public repository, the sha1 checksum content looks like this (http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar.sha1):

SHA1(serializer-2.7.1.jar)= 4b4b18df434451249bb65a63f2fb69e215a6a020 - 

Thus, it compares the computed checksum against SHA1(serializer-2.7.1.jar) instead of the actual checksum in .sha1 file.

Below is the debug messages:

[ivy:resolve] parameter found as ivy variable: ivy.configurations=*
[ivy:resolve] parameter found as ivy variable: ivy.resolve.default.type.filter=*
[ivy:resolve] parameter found as ivy variable: ivy.dep.file=ivy.xml
[ivy:resolve] using ivy parser to parse file:/C:/Documents%20and%20Settings/linj6/My%20Documents/subversion/system_owner/dia/SecurityService/trunk/mod
ules/wsapi-client/ivy.xml
[ivy:resolve] post 1.3 ivy file: using exact as default matcher
[ivy:resolve] :: resolving dependencies :: au.edu.qut.dia#ws-api-client;1.0 [not transitive]
[ivy:resolve]   confs: [default]
[ivy:resolve]   validate = true
[ivy:resolve]   refresh = false
[ivy:resolve] resolving dependencies for configuration 'default'
[ivy:resolve] == resolving dependencies for au.edu.qut.dia#ws-api-client;1.0 [default]
[ivy:resolve] loadData of au.edu.qut.dia#ws-api-client;1.0 of rootConf=default
[ivy:resolve] == resolving dependencies au.edu.qut.dia#ws-api-client;1.0->xalan#serializer;2.7.1 [default->*]
[ivy:resolve] loadData of xalan#serializer;2.7.1 of rootConf=default
[ivy:resolve]   using myresolvers to resolve xalan#serializer;2.7.1
[ivy:resolve] myresolvers: Checking cache for: dependency: xalan#serializer;2.7.1 {*=[*]}
[ivy:resolve] No entry is found in the ModuleDescriptorCache : C:\Documents and Settings\linj6\.ivy2\cache\xalan\serializer\ivy-2.7.1.xml
[ivy:resolve] pre 1.3 ivy file: using exactOrRegexp as default matcher
[ivy:resolve]   found ivy file in cache for xalan#serializer;2.7.1 (resolved by mvn-repo): C:\Documents and Settings\linj6\.ivy2\cache\xalan\serialize
r\ivy-2.7.1.xml
[ivy:resolve] myresolvers: module revision found in cache: xalan#serializer;2.7.1
[ivy:resolve]   found xalan#serializer;2.7.1 in mvn-repo
[ivy:resolve] Nbr of module to sort : 1
[ivy:resolve] Sort dependencies of : xalan#serializer;2.7.1 / Number of dependencies = 0
[ivy:resolve] Sort done for : xalan#serializer;2.7.1
[ivy:resolve]   resolved ivy file produced in C:\Documents and Settings\linj6\.ivy2\cache\resolved-au.edu.qut.dia-ws-api-client-1.0.xml
[ivy:resolve] :: downloading artifacts ::
[ivy:resolve]    trying http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar
[ivy:resolve]           tried http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar
[ivy:resolve] using commons httpclient 3.x helper
[ivy:resolve] downloading http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar ...
[ivy:resolve]   mvn-repo: downloading http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar
[ivy:resolve]           to C:\Documents and Settings\linj6\.ivy2\cache\xalan\serializer\jars\serializer-2.7.1.jar.part
[ivy:resolve] .......... (271kB)
[ivy:resolve] sha1 file found for http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar: checking...
[ivy:resolve]   mvn-repo: downloading http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar.sha1
[ivy:resolve]           to C:\DOCUME~1\linj6\LOCALS~1\Temp\ivytmp10668sha1
[ivy:resolve] .. (0kB)
[ivy:resolve] WARN:     [FAILED     ] xalan#serializer;2.7.1!serializer.jar: invalid sha1: expected=sha1(serializer-2.7.1.jar)= computed=4b4b18df43445
1249bb65a63f2fb69e215a6a020 (172ms)
[ivy:resolve] WARN:     [FAILED     ] xalan#serializer;2.7.1!serializer.jar: invalid sha1: expected=sha1(serializer-2.7.1.jar)= computed=4b4b18df43445
1249bb65a63f2fb69e215a6a020 (172ms)
[ivy:resolve] WARN: ==== mvn-repo: tried
[ivy:resolve] WARN:   http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar
[ivy:resolve] :: resolution report :: resolve 109ms :: artifacts dl 172ms
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
        ---------------------------------------------------------------------
[ivy:resolve] WARN:     ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] WARN:     ::              FAILED DOWNLOADS            ::
[ivy:resolve] WARN:     :: ^ see resolution messages for details  ^ ::
[ivy:resolve] WARN:     ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] WARN:     :: xalan#serializer;2.7.1!serializer.jar
[ivy:resolve] WARN:     ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] Nbr of module to sort : 1
[ivy:resolve] Sort dependencies of : xalan#serializer;2.7.1 / Number of dependencies = 0
[ivy:resolve] Sort done for : xalan#serializer;2.7.1
[ivy:resolve]   report for au.edu.qut.dia#ws-api-client;1.0 default produced in C:\Documents and Settings\linj6\.ivy2\cache\au.edu.qut.dia-ws-api-clie
nt-default.xml
[ivy:resolve]   resolve done (109ms resolve - 172ms download)

[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: WARNINGS
[ivy:resolve]           [FAILED     ] xalan#serializer;2.7.1!serializer.jar: invalid sha1: expected=sha1(serializer-2.7.1.jar)= computed=4b4b18df43445
1249bb65a63f2fb69e215a6a020 (172ms)
[ivy:resolve]           [FAILED     ] xalan#serializer;2.7.1!serializer.jar: invalid sha1: expected=sha1(serializer-2.7.1.jar)= computed=4b4b18df43445
1249bb65a63f2fb69e215a6a020 (172ms)
[ivy:resolve]   ==== mvn-repo: tried
[ivy:resolve]     http://repo1.maven.org/maven2/xalan/serializer/2.7.1/serializer-2.7.1.jar
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           ::              FAILED DOWNLOADS            ::
[ivy:resolve]           :: ^ see resolution messages for details  ^ ::
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           :: xalan#serializer;2.7.1!serializer.jar
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]
[ivy:resolve]
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

",2009-02-02T07:07:24.610+0000,2010-05-27T03:17:40.981+0000,Duplicate,Critical
PHOENIX-1819,Build a framework to capture and report phoenix client side request level metrics,PHOENIX,New Feature,Closed,[],1,[<JIRA IssueLink: id='12436601'>],"In order to get insight into what phoenix is doing and how much it is doing per request, it would be ideal to get a single log line per phoenix request. The log line could contain request level metrics like:
1) Number of spool files created.
2) Number of parallel scans.
3) Number of serial scans.
4) Query failed - boolean 
5) Query time out - boolean 
6) Query time.
7) Mutation time.
8) Mutation size in bytes.
9) Number of mutations.
10) Bytes allocated by the memory manager.
11) Time spent by threads waiting for the memory to be allocated.
12) Number of tasks submitted to the pool.
13) Number of tasks rejected.
14) Time spent by tasks in the queue.
15) Time taken by tasks to complete - from construction to execution completion.
16) Time taken by tasks to execute.",2015-04-06T22:27:02.665+0000,2015-11-21T02:17:30.011+0000,Fixed,Major
THRIFT-2046,The worktask can be timed out in TThreadPoolServer (Java) when the max# thrift thread is reached,THRIFT,Improvement,Closed,[],3,"[<JIRA IssueLink: id='12370903'>, <JIRA IssueLink: id='12370908'>, <JIRA IssueLink: id='12551155'>]","Once the max# of thrift threads is reached, a new task (workprocess) is attempted to be executed in an infinite loop, then the client may hang.
An improvement is to introduce a task timeout. after a certain time, if the task is still not got queued to be executed. It will be invalidated.",2013-06-19T02:03:12.283+0000,2019-01-03T14:52:02.548+0000,Fixed,Major
INFRA-6593,CMS Staging site for Hive,INFRA,Task,Closed,[],3,"[<JIRA IssueLink: id='12381287'>, <JIRA IssueLink: id='12372689'>, <JIRA IssueLink: id='12372688'>]",Please create a CMS staging site for Apache Hive. Our svn repo is here http://svn.apache.org/repos/asf/hive/cms/,2013-07-26T04:29:46.993+0000,2014-02-02T19:08:20.436+0000,Fixed,Major
REEF-589,REEF crashes when new nodes are added to the clusters dynamically,REEF,Task,Resolved,[],4,"[<JIRA IssueLink: id='12434234'>, <JIRA IssueLink: id='12434239'>, <JIRA IssueLink: id='12434235'>, <JIRA IssueLink: id='12434236'>]","When trying to use REEF with Federation, we found a problem in REEF resource catalog.
If an admin happens to add a new node to the cluster dynamically, and future allocations are done in that node, REEF crashes as it is not able to find that node in the catalog.
Though we found this problem using YARN, it will happen the same for other RMs.",2015-08-13T16:33:54.840+0000,2015-08-14T01:45:26.578+0000,Fixed,Minor
SPARK-23155,YARN-aggregated executor/driver logs appear unavailable when NM is down,SPARK,Improvement,Resolved,[],2,"[<JIRA IssueLink: id='12550527'>, <JIRA IssueLink: id='12525592'>]","Unlike MapReduce JobHistory Server, Spark history server isn't rewriting container log URL's to point to the aggregated yarn.log.server.url location and relies on the NodeManager webUI to trigger a redirect. This fails when the NM is down. Note that NM may be down permanently after decommissioning in traditional environments or when used in a cloud environment such as AWS EMR where either worker nodes are taken away with autoscale, the whole cluster is used to run a single job.",2018-01-19T05:42:48.211+0000,2019-02-01T08:37:55.416+0000,Fixed,Major
ORC-578,IllegalArgumentException: Can't use LongColumnVector to read proleptic Gregorian dates.,ORC,Bug,Closed,[],3,"[<JIRA IssueLink: id='12578079'>, <JIRA IssueLink: id='12579176'>, <JIRA IssueLink: id='12576906'>]","Use of the proleptic Gregorian calendar via ORC-27 and schema evolution can lead to an exception.

{code}
Caused by: java.lang.IllegalArgumentException: Can't use LongColumnVector to read proleptic Gregorian dates.
 at org.apache.orc.impl.TreeReaderFactory$DateTreeReader.nextVector(TreeReaderFactory.java:1157) ~[orc-core-1.5.9-SNAPSHOT.jar:1.5.9-SNAPSHOT]
 at org.apache.orc.impl.ConvertTreeReaderFactory$StringGroupFromDateTreeReader.nextVector(ConvertTreeReaderFactory.java:1702) ~[orc-core-1.5.9-SNAPSHOT.jar:1.5.9-SNAPSHOT]
 at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextVector(TreeReaderFactory.java:2077) ~[orc-core-1.5.9-SNAPSHOT.jar:1.5.9-SNAPSHOT]
 at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:2059) ~[orc-core-1.5.9-SNAPSHOT.jar:1.5.9-SNAPSHOT]
 at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1310) ~[orc-core-1.5.9-SNAPSHOT.jar:1.5.9-SNAPSHOT]
{code}",2019-12-11T21:48:09.744+0000,2020-01-31T17:44:38.833+0000,Fixed,Major
AMBARI-13946,Non NameNode-HA properties still in hdfs-site.xml causing (at least) Balancer and ATS to fail,AMBARI,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12461515'>, <JIRA IssueLink: id='12449181'>]","After enabling NameNode-HA, {{hdfs-site.xml}} does still contain non-HA properties, including

* dfs.namenode.rpc-address
* dfs.namenode.http-address
* dfs.namenode.https-address

This cause the balancer to fail with the following symptoms in Balancer:

{code}
...
15/11/18 15:48:30 INFO balancer.Balancer: namenodes  = [hdfs://daplab2, hdfs://daplab-rt-11.fri.lan:8020]
...
java.io.IOException: Another Balancer is running..  Exiting ...
{code}

And ATS:

{code}
_assert_valid
    self.target_status = self._get_file_status(target)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 292, in _get_file_status
    list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py"", line 210, in run_command
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of 'curl -sS -L -w '%{http_code}' -X GET 'http://pvvsccmn1-brn1:50070/webhdfs/v1/ats/done?op=GETFILESTATUS&user.name=hdfs'' returned status_code=403. 
{
  ""RemoteException"": {
    ""exception"": ""StandbyException"", 
    ""javaClassName"": ""org.apache.hadoop.ipc.StandbyException"", 
    ""message"": ""Operation category READ is not supported in state standby""
  }
}
{code}

These should be removed from the config.

Steps to reproduce: after turning on NameNode HA, {{grep dfs.namenode.rpc-address|dfs.namenode.http-address /etc/hadoop/conf/hdfs-site.xml}} shouldn't return anything
",2015-11-18T16:01:00.030+0000,2016-03-22T09:18:34.226+0000,Fixed,Major
TEZ-4228,TezClassLoader should be used in TezChild and for Configuration objects,TEZ,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12597970'>, <JIRA IssueLink: id='12598061'>]","While testing a 0.10.0 staging artifact, I found that there multiple issues of TEZ-3860/TEZ-4223, which should be fixed before 0.10.0 release. The issues came up while hive precommit tests:
http://ci.hive.apache.org/blue/organizations/jenkins/hive-precommit/detail/PR-1311/11/tests

The [failing hive test|https://github.com/apache/hive/blob/master/ql/src/test/queries/clientpositive/mapjoin_addjar.q] uses ADD JAR functionality:
{code}
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
{code}

1. There are code paths, where Configuration object's classloader is used, so if we add resources to TezClassLoader, we should point Configuration objects to use them, otherwise, we'll face exception as this:  [^syslog_bad.log] . 
The changes in TezUtils solved the problem for container mode as  [^syslog_good.log] 

2. There are codepaths, which use thread context classloader, so calling TezClassLoader.setupTezClassLoader early in TezChild solves issues like this:  [^syslog_bad_context_classloader.log] 

3. The Configuration issue is the same in Hive LLAP, which is solved in 1), example failure reproduced locally is:  [^hive_llap.log].

4. The context classloader problem is also present in LLAP daemons, which is being solved by HIVE-24108",2020-09-01T10:21:53.879+0000,2020-09-03T21:27:36.035+0000,Fixed,Blocker
PHOENIX-5475,Add preWALAppend to DelegateRegionObserver for upcoming HBase 1.5.0,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12569855'>, <JIRA IssueLink: id='12571929'>]","HBase 1.5 SNAPSHOT only, see linked HBase issue.",2019-09-13T17:20:51.465+0000,2019-10-14T19:32:54.058+0000,Duplicate,Major
HCATALOG-451,Partitions are created even when Jobs are aborted,HCATALOG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12355423'>, <JIRA IssueLink: id='12356672'>]","If an MR job using HCatOutputFormat fails, and FileOutputCommitterContainer::abortJob() is called, one would expect that partitions aren't created/registered with HCatalog.

When using dynamic-partitions, one sees that this behaves correctly. But when static-partitions are used, partitions are created regardless of whether the Job succeeded or failed.
(This manifested as a failure when the job is repeated. The retry-job fails to launch since the partitions already exist from the last failed run.)

This is a result of bad code in FileOutputCommitter::cleanupJob(), which seems to do an unconditional partition-add. This can be fixed by adding a check for the output directory before adding partitions (in the !dynamicParititoning case), since the directory is removed in abortJob().

We'll have a patch for this shortly. As an aside, we ought to move the partition-creation into commitJob(), where it logically belongs. cleanupJob() is deprecated and common to both success and failure code paths.",2012-07-19T17:22:34.928+0000,2013-05-02T02:29:55.316+0000,Fixed,Major
SLIDER-799,AM to decide when to relax placement policy from specific host to rack/cluster,SLIDER,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12410132'>, <JIRA IssueLink: id='12409320'>]","If Slider asks for relaxed affinity, YARN only gives it ~1 second for free capacity to appear on a node before it falls back to non-local assignment. While this is OK for analytics throughput, it's suboptimal for placement of code such as HBase region servers.

AM needs to take charge of the placement and decide for itself when to convert from placed to relaxed.",2015-02-27T23:17:50.472+0000,2015-03-26T18:02:01.887+0000,Fixed,Critical
ORC-437,Make acid schema checks case insensitive,ORC,Bug,Closed,[],2,"[<JIRA IssueLink: id='12548110'>, <JIRA IssueLink: id='12569751'>]","When reading from an Orc file, SchemaEvolution evolution tries to determine if this is an Acid compliant format by comparing field names with Acid event names in {{SchemaEvolution.checkAcidSchema}}. Would be good to make this comparison case insensitive.
This requirement comes in from HIVE-20699 where a Hive query is being used to run compaction (and hence write the compacted data to the bucket files via a HiveQL query). Since hive converts all column names to lower case, the compacted files end up with lower case Acid schema columns. The change is much simpler when made in Orc.

",2018-11-14T07:52:22.788+0000,2019-09-12T10:36:32.437+0000,Fixed,Major
ZOOKEEPER-846,zookeeper client doesn't shut down cleanly on the close call,ZOOKEEPER,Bug,Closed,[],2,"[<JIRA IssueLink: id='12333802'>, <JIRA IssueLink: id='12333793'>]","Using HBase 0.20.6 (with HBASE-2473) we encountered a situation where Regionserver
process was shutting down and seemed to hang.

Here is the bottom of region server log:
http://pastebin.com/YYawJ4jA

zookeeper-3.2.2 is used.

Here is relevant portion from jstack - I attempted to attach jstack twice in my email to dev@hbase.apache.org but failed:

""DestroyJavaVM"" prio=10 tid=0x00002aabb849c800 nid=0x6c60 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""regionserver/10.32.42.245:60020"" prio=10 tid=0x00002aabb84ce000 nid=0x6c81 in Object.wait() [0x0000000043755000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1099)
        - locked <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)
        at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1077)
        at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:505)
        - locked <0x00002aaabf5e0c30> (a org.apache.zookeeper.ZooKeeper)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.close(ZooKeeperWrapper.java:681)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:654)
        at java.lang.Thread.run(Thread.java:619)

""main-EventThread"" daemon prio=10 tid=0x0000000043474000 nid=0x6c80 waiting on condition [0x00000000413f3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaabf6e9150> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:414)
",2010-08-12T17:19:53.973+0000,2011-11-23T19:22:15.054+0000,Fixed,Blocker
HCATALOG-10,Shouldn't assume the secure hadoop installation,HCATALOG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12345862'>, <JIRA IssueLink: id='12345863'>]",HCatalog currently only builds against Hadoop 20S. It should successfully build against hadoop-0.20 as well. ,2011-04-25T21:02:36.996+0000,2012-05-17T01:20:23.083+0000,Fixed,Major
IMPALA-8847,Add partition events may contain empty partition object list,IMPALA,Bug,Resolved,[],1,[<JIRA IssueLink: id='12567590'>],"When event polling is ON and when an external application like Hive issues a {{alter table <table> add if not exists partition (<partspec>)}} it is possble that command did not add a partition since it is preexisting. However, metastore still generates a ADD_PARTITION event in such a case with empty list of added partitions. Such events cause a Precondition to fail while processing on the EventsProcessor side and event polling goes into error state.

The fix would be simple. Ignore such events.",2019-08-08T18:31:09.533+0000,2019-08-16T23:01:49.125+0000,Fixed,Major
TEZ-704,Tez MapReduce job hangs if setting mapreduce.reduce.cpu.vcores to be 2,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12393134'>],"When running Hive on Tez in EMR hadoop districution, which set mapreduce.reduce.cpu.vcores to be 2 in mapred-site.xml, all MapReduce jobs get hang, without any progress.

The problem seems like,
Hive On Tez sets mapreduce.reduce.cpu.vcores to 2, so its application
manager asks for a 2 cpu container, but the resource manager could not
allocate a node for it, since all it has are 1 cpu containers.

I saw the same problem when running MapReduce Job on Tez when setting mapreduce.reduce.cpu.vcores to be 2.

This could be a bug in Tez MapReduce code.",2014-01-06T06:58:50.168+0000,2016-04-14T23:29:30.188+0000,,Major
THRIFT-4613,libfb303-0.10.0.jar missing in maven repository,THRIFT,Bug,Closed,[],1,[<JIRA IssueLink: id='12640451'>],"It looks like libfb303 was published to maven repositories for the 0.9.3 release,
 but I don't see any libfb303 artifacts available for the 0.10.0 OR 0.11.0 release.",2018-08-06T18:56:14.421+0000,2022-05-21T16:18:38.085+0000,Abandoned,Major
KNOX-8,Support HBase via HBase/Stargate,KNOX,New Feature,Closed,[],7,"[<JIRA IssueLink: id='12368661'>, <JIRA IssueLink: id='12370825'>, <JIRA IssueLink: id='12368662'>, <JIRA IssueLink: id='12370217'>, <JIRA IssueLink: id='12370219'>, <JIRA IssueLink: id='12531064'>, <JIRA IssueLink: id='12371846'>]","The goal of this new feature is to allow access to HBase through the Knox gateway.  The Stargate subproject of HBase appears to provide REST APIs for HBase.
http://wiki.apache.org/hadoop/Hbase/Stargate
We should evaluate supporting access to HBase via Stargate through Knox.",2013-03-27T22:04:50.340+0000,2019-03-28T14:13:35.461+0000,Fixed,Major
SLIDER-846,"[RM HA] Slider Application Master url from RM UI complains ""This is standby RM""",SLIDER,Bug,Open,[],2,"[<JIRA IssueLink: id='12423651'>, <JIRA IssueLink: id='12423587'>]","Hadoop 2.6 env - RM HA enabled

Steps:
1) Create an application using Slider
2) Go to RM web UI. Click on ApplicationMaster link of the running application. It says -
{color:red}
{code}
This is standby RM. Redirecting to the current active RM: <app_uri>
{code}
{color}

This happens in Slider 0.60 even though the following YARN bugs are fixed in Hadoop 2.6 and needs investigation -

YARN-1525
YARN-1811
YARN-2605
",2015-04-08T17:10:21.311+0000,2015-05-05T20:18:41.963+0000,,Major
INFRA-22855,ci-hbase nodes do not have python,INFRA,Task,Closed,[],2,"[<JIRA IssueLink: id='12633098'>, <JIRA IssueLink: id='12633047'>]","We use python in our builds to do some work and it seems that we do not have python on hbase11-20 nodes.

https://ci-hbase.apache.org/job/Test-Script/1/console",2022-02-08T13:34:15.374+0000,2022-02-10T14:33:00.216+0000,Fixed,Major
HCATALOG-183,Memory leak in HCat 0.1/0.2,HCATALOG,Bug,Open,[],2,"[<JIRA IssueLink: id='12347033'>, <JIRA IssueLink: id='12346355'>]","When one leaves the HCatalog server running for long (in a secure setup), with requests continuously coming in, one sees that the memory footprint of the metastore-server increases continuously, until it culminates in an OutOfMemoryError:

<backtrace>
2011-12-01 18:11:00,620 ERROR api.ThriftHiveMetastore$Processor (ThriftHiveMetastore.java:process(5949)) - Internal error processing get_partition_names
java.lang.OutOfMemoryError: Java heap space 
  at java.util.Arrays.copyOf(Arrays.java:2882)
  at java.lang.StringValue.from(StringValue.java:24)
  at java.lang.String.<init>(String.java:178)
  at com.mysql.jdbc.SingleByteCharsetConverter.toString(SingleByteCharsetConverter.java:286)
  at com.mysql.jdbc.SingleByteCharsetConverter.toString(SingleByteCharsetConverter.java:262)
  at com.mysql.jdbc.ResultSet.getStringInternal(ResultSet.java:5671)
  at com.mysql.jdbc.ResultSet.getString(ResultSet.java:5544)
  at org.apache.commons.dbcp.DelegatingResultSet.getString(DelegatingResultSet.java:213)
  at org.apache.commons.dbcp.DelegatingResultSet.getString(DelegatingResultSet.java:213)
  at org.datanucleus.store.rdbms.mapping.CharRDBMSMapping.getObject(CharRDBMSMapping.java:460)
  at org.datanucleus.store.mapped.mapping.SingleFieldMapping.getObject(SingleFieldMapping.java:216)
  at org.datanucleus.store.rdbms.query.ResultClassROF.processScalarExpression(ResultClassROF.java:583)
  at org.datanucleus.store.rdbms.query.ResultClassROF.getObject(ResultClassROF.java:361)
  at org.datanucleus.store.rdbms.query.legacy.LegacyForwardQueryResult.nextResultSetElement(LegacyForwardQueryResult.java:137)
  at org.datanucleus.store.rdbms.query.legacy.LegacyForwardQueryResult$QueryResultIterator.next(LegacyForwardQueryResult.java:305)
  at org.apache.hadoop.hive.metastore.ObjectStore.listPartitionNames(ObjectStore.java:1200)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$26.run(HiveMetaStore.java:1555)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$26.run(HiveMetaStore.java:1552)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.executeWithRetry(HiveMetaStore.java:309)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partition_names(HiveMetaStore.java:1552)
  ...
</backtrace>

The OOM is preceded by other failures, including a ""GSS initiate failure"" (in spite of a client-side kinit), and an ""Error occurred during processing of request"".",2011-12-16T07:13:15.749+0000,2012-01-12T19:41:45.033+0000,,Major
CALCITE-2555,"RexSimplify: >=(true, null) could be simplified to null",CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12543190'>],"{code:java}
@Test public void simplifyComparisonWithNull() {
  checkSimplify2(ge(trueLiteral, falseLiteral), ""true"", ""true"");
  checkSimplify2(ge(trueLiteral, nullBool), ""null"", ""false"");
  checkSimplify2(ge(nullBool, nullBool), ""null"", ""false"");
}
{code}",2018-09-13T06:32:56.478+0000,2018-12-23T00:12:12.306+0000,Fixed,Major
NIFI-5841,PutHive3Streaming processor Mem Leak,NIFI,Bug,Closed,[],2,"[<JIRA IssueLink: id='12551626'>, <JIRA IssueLink: id='12551604'>]","Nifi versions: 1,7.1 and 1.8.0
 nifi/nifi-nar-bundles/nifi-hive-bundle/nifi-hive3-processors/src/main/java/org/apache/nifi/processors/hive/PutHive3Streaming.java**
 , line 417 seems redundant,
{code:java}
ShutdownHookManager.addShutdownHook(hiveStreamingConnection::close, FileSystem.SHUTDOWN_HOOK_PRIORITY + 1){code}
 

Whereas Hive 3.0.0 did not add a shutdownhook within connect() method, Hive 3.1.* does:

 hive/streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java
{code:java}
ShutdownHookManager.addShutdownHook(streamingConnection::close, FileSystem.SHUTDOWN_HOOK_PRIORITY + 1);{code}
This creates two references to the shutdownhook object per transaction out of which only one is ever cleaned; resulting in a slow/fast degradation of heap space depending on the velocity of transactions.",2018-11-26T14:46:20.084+0000,2019-10-11T08:04:41.505+0000,Fixed,Major
TEZ-1246,"Replace constructors with create() methods for DAG, Vertex, Edge etc in the API",TEZ,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12394522'>],,2014-07-01T07:30:41.590+0000,2014-09-06T01:35:07.835+0000,Fixed,Blocker
SENTRY-338,Sentry policy import tool adds non-compatible comments to grant privilege statements,SENTRY,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12391751'>, <JIRA IssueLink: id='12391629'>]","The policy file to SQL tool generates grant statements with inline comments prefixed with '#'. 
For example, 
{noformat}
GRANT ALL ON TABLE tab1 TO ROLE junior_analyst_role; # server=server1, database=default
{noformat}
Beeline doesn't support that syntax and fails to execute the script.
",2014-07-16T22:59:56.543+0000,2014-08-29T18:50:48.544+0000,Fixed,Major
TEZ-1162,Tez leaks CodecPool buffers,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12389110'>],"Tez Fetcher currently leaks a codec pool (~32kb) for each partition fetched

This causes task failures due to the direct buffers allocated by the codec, which are not freed until a full GC (but whose allocation does not trigger GC).

It is possible to perform the entire shuffle operations without ever triggering a full GC, the following case 

{code}
Container exited with a non-zero exit code 143
], AttemptID:attempt_1399351577718_2330_1_05_000020_3 Info:Container container_1399351577718_2330_01_000310 COMPLETED with diagnostics set to [Container [pid=1734,containerID=container_1399351577718_2330_01_000310] is running beyond physical memory limits. Current usage: 4.1 GB of 4 GB physical memory used; 5.4 GB of 40 GB virtual memory used. Killing container.

container_1399351577718_2330_01_000365/ $ grep -ri ""CodecPool.*brand-new"" syslog* | wc -l

6988
{code}

That is approx ~436Mb leak on a JVM spun up with -Xmx3500m & 4096m container.",2014-06-01T00:08:08.861+0000,2014-09-06T01:35:11.505+0000,Fixed,Major
SLIDER-713,Implement the cluster flex operation via REST,SLIDER,Sub-task,Open,[],4,"[<JIRA IssueLink: id='12408328'>, <JIRA IssueLink: id='12408330'>, <JIRA IssueLink: id='12408329'>, <JIRA IssueLink: id='12408333'>]","Implement the cluster flex operation via REST.

This is identical to the existing IPC flex: push up a new resources.json to AM; the main engineering is tests/migrating existing code. 

its not clear how much funtest tests for for flexing; this will add some",2014-12-10T14:26:24.174+0000,2015-02-27T19:32:25.353+0000,,Major
TEZ-2715,Add API to Reader classes to expose raw key bytes in addition to getCurrentKey,TEZ,Improvement,Open,[],1,[<JIRA IssueLink: id='12434095'>],   This is required for making faster comparisons when comparing multiple inputs for Joins.,2015-08-12T18:46:35.635+0000,2015-08-12T18:46:49.411+0000,,Major
ZOOKEEPER-1535,ZK Shell/Cli re-executes last command on exit,ZOOKEEPER,Bug,Closed,[],3,"[<JIRA IssueLink: id='12504135'>, <JIRA IssueLink: id='12397890'>, <JIRA IssueLink: id='12387344'>]","In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell. In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run. 
{noformat}
[zk: zookeeper.example.com:2181(CONNECTED) 0] ls /blah
[foo]
[zk: zookeeper.example.com:2181(CONNECTED) 1] <ctrl+d> [foo]
$
{noformat}",2012-08-14T23:56:40.333+0000,2017-05-20T23:07:10.721+0000,Fixed,Major
SUREFIRE-1389,Java package regex via command line selects no tests,SUREFIRE,Bug,Closed,[],1,[<JIRA IssueLink: id='12508907'>],"We noticed the following problem over in [HBase|https://issues.apache.org/jira/browse/HBASE-18264?focusedCommentId=16082795&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16082795]

On surefire 2.18.1 and 2.19.1, the following command would have the effect of running all tests which exist in the package {{org.apache.hadoop.hbase.quotas}}:

{noformat}
mvn package -Dtest='org.apache.hadoop.hbase.quotas.*'
{noformat}

Upon upgrading to 2.20, the same command executes no tests.

If helpful, plugin configuration (albeit convoluted) can be found at https://github.com/apache/hbase/blob/branch-2/pom.xml. Happy to provide other info to help debug further -- I haven't tried a trivial reproduction.",2017-07-11T20:04:49.776+0000,2017-09-18T14:17:14.127+0000,Not A Bug,Major
IMPALA-9363,Add support to skip given table storage formats,IMPALA,Improvement,Open,[],1,[<JIRA IssueLink: id='12605438'>],"Catalog by default pulls in all the tables from HMS. Impala however, only can work with some table formats. Such tables are shown in the table listing and when they are queries, Impala also loads them. It is only during querying that we throw an error saying its an unsupported table format. It would be good to have a config which can take in list of storage handlers and skip loading such tables.",2020-02-06T23:32:47.472+0000,2020-12-23T18:12:27.442+0000,,Critical
SQOOP-721,Duplicating rows on export when exporting from compressed files.,SQOOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12360719'>],"It appears that in some situations export will duplicate rows. It seems that this behavior is happening when user is exporting compressed files that are ""big enough"".",2012-11-22T15:27:58.993+0000,2012-11-30T05:42:41.477+0000,Fixed,Blocker
APEXCORE-426,Support work preserving AM recovery,APEXCORE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12463226'>],"On app master failure, the streaming containers should continue running. 

As of 2.2, YARN will automatically terminate all containers and the replacement app master will relaunch them. Once we move to a newer minimum Hadoop version, we should leverage work preserving restart.

The mechanism in Apex containers to locate the new master process are already in place.
 
Test Cases:
1. Kill the app-master - only app-master container id should change, all the other containers id should remain same.
2. Kill the app-master and few other containers, make sure that killed containers are recovered.

",2016-04-10T02:41:14.826+0000,2017-05-05T10:46:20.607+0000,Fixed,Major
ACCUMULO-2266,TServer should ensure wal settings are valid for underlying FS,ACCUMULO,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12381949'>, <JIRA IssueLink: id='12381953'>]","ACCUMULO-2264 revealed a problem in how the tserver handles conflicts between its settings and the restrictions of the underlying fs.

In the case of ACCUMULO-2264, if the tserver is configured with a wal block size less than that allowed by HDFS the tserver sits in an infinite loop.

The tserver should probably be checking for the minimum blocksize (the property is dfs.namenode.fs-limits.min-block-size) and then either issuing a WARN/ERROR to the client and using the minimum or failing loudly and refusing to start. I favor the latter.",2014-01-28T16:16:05.973+0000,2014-01-28T22:57:26.488+0000,Fixed,Critical
RANGER-2871,Add Support of HBASE-20653 features in Ranger Hbase plugin,RANGER,Improvement,Open,[],2,"[<JIRA IssueLink: id='12592002'>, <JIRA IssueLink: id='12592001'>]","Currently the following region server group operations don't have corresponding implementation in RangerAuthorizationCoprocessor :

AddRSGroup, RemoveRSGroup, BalanceRSGroup, RemoveServers, moveServers, listRSGroups, moveServersAndTables, moveTables, getRSGroupInfo, getRSGroupInfoOfTable, getRSGroupOfServer

 

Note: Since current hbase version in ranger master does not have the implementation in Hbase MasterObserver class, the implementation of above method will be blocked untill Ranger usage the hbase version which has this feature support.",2020-06-24T07:01:48.161+0000,2021-09-10T07:00:44.064+0000,,Minor
PHOENIX-5303,Fix index failures with some versions of HBase.,PHOENIX,Test,Closed,[],2,"[<JIRA IssueLink: id='12599037'>, <JIRA IssueLink: id='12561657'>]","The problem was introduced with HBASE-21158. The fix here works regardless of the HBase version.

This must have started very recently, but it's already past the history of the test runs.

Or perhaps it never works in 4.x-HBase-1.5

[~apurtell], in case you have any ideas.
{code:java}
[INFO] Running org.apache.phoenix.hbase.index.covered.TestCoveredColumnIndexCodec
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.403 s <<< FAILURE! - in org.apache.phoenix.hbase.index.covered.TestCoveredColumnIndexCodec
[ERROR] testGeneratedIndexUpdates(org.apache.phoenix.hbase.index.covered.TestCoveredColumnIndexCodec) Time elapsed: 0.16 s <<< FAILURE!
java.lang.AssertionError: Had some index updates, though it should have been covered by the delete
at org.apache.phoenix.hbase.index.covered.TestCoveredColumnIndexCodec.ensureNoUpdatesWhenCoveredByDelete(TestCoveredColumnIndexCodec.java:242)
at org.apache.phoenix.hbase.index.covered.TestCoveredColumnIndexCodec.testGeneratedIndexUpdates(TestCoveredColumnIndexCodec.java:220)
{code}
 

MutableIndexIT fails as well (for non-transactional indexes)

 ",2019-05-25T02:33:18.754+0000,2020-09-21T07:07:19.503+0000,Fixed,Blocker
SPARK-22036,BigDecimal multiplication sometimes returns null,SPARK,Bug,Resolved,[],3,"[<JIRA IssueLink: id='12536771'>, <JIRA IssueLink: id='12522590'>, <JIRA IssueLink: id='12524867'>]","The multiplication of two BigDecimal numbers sometimes returns null. Here is a minimal reproduction:

{code:java}
object Main extends App {
  import org.apache.spark.{SparkConf, SparkContext}
  import org.apache.spark.sql.SparkSession
  import spark.implicits._
  val conf = new SparkConf().setMaster(""local[*]"").setAppName(""REPL"").set(""spark.ui.enabled"", ""false"")
  val spark = SparkSession.builder().config(conf).appName(""REPL"").getOrCreate()
  implicit val sqlContext = spark.sqlContext

  case class X2(a: BigDecimal, b: BigDecimal)
  val ds = sqlContext.createDataset(List(X2(BigDecimal(-0.1267333984375), BigDecimal(-1000.1))))
  val result = ds.select(ds(""a"") * ds(""b"")).collect.head
  println(result) // [null]
}
{code}
",2017-09-16T11:12:06.876+0000,2018-09-20T23:40:29.214+0000,Fixed,Major
DRILL-3492,Add support for encoding of Drill data types into byte ordered format,DRILL,New Feature,Closed,[],1,[<JIRA IssueLink: id='12434800'>],"The following JIRA added this functionality in HBase: https://issues.apache.org/jira/browse/HBASE-8201

We need to port this functionality in Drill so as to allow filtering and pruning of rows during scans.",2015-07-13T18:42:56.176+0000,2015-10-06T17:48:54.347+0000,Fixed,Major
TEZ-2277,modifyACLsStr in DAGAccessControls does not take effect,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12412791'>],"Even if modifyACLsStr in DAGAccessControls constructor is set and that access control is set for the DAG, it does not actually get set in access control at runtime.

See comment in [HIVE-10145|https://issues.apache.org/jira/browse/HIVE-10145?focusedCommentId=14393933&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14393933]",2015-04-03T18:39:00.820+0000,2015-04-06T19:32:01.926+0000,Invalid,Critical
ACCUMULO-4671,ConfiguratorBase.unwrapAuthenticationToken will NPE if the Token is not in the Job's credentials,ACCUMULO,Bug,Resolved,[],1,[<JIRA IssueLink: id='12507927'>],"When Kerberos is enabled, the AuthenticationToken set on the Configuration object is essentially a pointer to the Hadoop-style Token stored (securely) in the Job's credentials object.

However, if by user error, the user thinks that they have serialized a Token (but it got lost along the way), unwrapAuthenticationToken will throw an NPE trying to extract that token. The name of the Token is abstracted away from the user (they shouldn't actually know what it is), so we should not fail hard like this.

It would be easy to return back the original token in this case and update the javadoc accordingly to warn users.",2017-06-28T23:40:26.009+0000,2019-08-27T23:14:49.316+0000,Auto Closed,Major
BIGTOP-1245,DFSCIOTest fails with libhdfs.so.1 missing,BIGTOP,Bug,Resolved,[],1,[<JIRA IssueLink: id='12384406'>],"hadoop@localhost bin]$ sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.0.6-alpha-tests.jar DFSCIOTest -write -nrFiles 1 -fileSize 100
DFSCIOTest.0.0.1
14/03/05 23:03:31 INFO fs.DFSCIOTest: nrFiles = 1
14/03/05 23:03:31 INFO fs.DFSCIOTest: fileSize (MB) = 100
14/03/05 23:03:31 INFO fs.DFSCIOTest: bufferSize = 1000000
File /usr/lib/hadoop/libhdfs/libhdfs.so.1 does not exist",2014-03-06T04:49:05.283+0000,2014-03-07T22:04:44.115+0000,Invalid,Major
AMBARI-16056,Ambari support for bucket caching parameter - hbase.bucketcache.ioengine,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12464634'>],"In order to utilize SSD as a L2 (bucket) cache, we specify a file location to hbase.bucketcache.ioengine parameter.
hbase.bucketcache.ioengine=file:<filepath>
The directory location specified to the above parameter could be non-existent or non-accessible.
Since ambari-agent runs with root permissions, the proposed change here is to read the 'hbase.bucketcache.ioengine' parameter from hbase configuration and see if it points to a file location (by looking at file: prefix). 
In case ioengine points to a file, ambari-agent should create the underlying directory and assign appropriate permissions to hbase:hadoop user.
Activity",2016-04-22T17:59:31.993+0000,2016-04-26T20:50:51.974+0000,Fixed,Major
SPARK-19649,Spark YARN client throws exception if job succeeds and max-completed-applications=0,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12494686'>, <JIRA IssueLink: id='12494685'>]","I believe the patch in SPARK-3877 created a new race condition between YARN and the Spark client.

I typically configure YARN not to keep *any* recent jobs in memory, as some of my jobs get pretty large.

{code}
yarn-site	yarn.resourcemanager.max-completed-applications	0
{code}

The once-per-second call to getApplicationReport may thus encounter a RUNNING application followed by a not found application, and report a false negative.

(typical) Executor log:
{code}
17/01/09 19:31:23 INFO ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
17/01/09 19:31:23 INFO SparkContext: Invoking stop() from shutdown hook
17/01/09 19:31:24 INFO SparkUI: Stopped Spark web UI at http://10.0.0.168:37046
17/01/09 19:31:24 INFO YarnClusterSchedulerBackend: Shutting down all executors
17/01/09 19:31:24 INFO YarnClusterSchedulerBackend: Asking each executor to shut down
17/01/09 19:31:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/09 19:31:24 INFO MemoryStore: MemoryStore cleared
17/01/09 19:31:24 INFO BlockManager: BlockManager stopped
17/01/09 19:31:24 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/09 19:31:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/09 19:31:24 INFO SparkContext: Successfully stopped SparkContext
17/01/09 19:31:24 INFO ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
17/01/09 19:31:24 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/09 19:31:24 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/01/09 19:31:24 INFO AMRMClientImpl: Waiting for application to be successfully unregistered.
17/01/09 19:31:24 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
{code}

Client log:
{code}
17/01/09 19:31:23 INFO Client: Application report for application_1483983939941_0056 (state: RUNNING)
17/01/09 19:31:24 ERROR Client: Application application_1483983939941_0056 not found.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1483983939941_0056 is killed
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1038)
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1081)
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}",2017-02-17T22:56:54.438+0000,2020-05-17T18:14:05.199+0000,Won't Fix,Minor
TEZ-2192,Relocalization does not check for source,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12410267'>]," PIG-4443 spills the input splits to disk if serialized split size is greater than some threshold. It faces issues with relocalization when more than one vertex has job.split file. If a job.split file is already there on container reuse, it is reused causing wrong data to be read.

Either need a way to turn off relocalization or  check the source+timestamp and redownload the file during relocalization. ",2015-03-10T23:32:41.938+0000,2015-06-30T04:52:41.630+0000,Fixed,Blocker
MNG-6237,Property value during the same build should not change,MNG,Bug,Closed,[],1,[<JIRA IssueLink: id='12504064'>],"I've encountered this problem in HIVE-14289.
But now I know more...and I think I probably understand what's going on.

Long story short, I would like to override some project property to use a different version of a dependency...however using
{code}
mvn install -Dhadoop.version=2.8.0
{code}
is not enough...that's what I've found out in that ticket.

The problem is:

 * root project defines a property {{junit.version}}
 * there is a lib project which pulls in a dependency with that property
 * there is an app module which tries to use the lib in some way

in this setup even thru the developer would like to override the version of {{junit.version}}, he can't really do it; because maven references to it anyway.

I've created a samle project in which the root project contains a malformed version number; and in case it tries to use it, will end up with an artifact resolution error.

I'm not sure if the problem is within maven or in some of its plugins...

sample project:
https://github.com/kgyrtkirk/mng-6237
sample output:
https://api.travis-ci.org/jobs/234037973/log.txt?deansi=true",2017-05-19T14:29:22.467+0000,2020-10-17T12:38:04.956+0000,Incomplete,Major
IMPALA-3570,Creating a managed Kudu table should not create an HDFS dir,IMPALA,Bug,Open,[],1,[<JIRA IssueLink: id='12614324'>],Kudu tables don't need HDFS dirs. For some reason creating a table also creates an HDFS dir. Some investigation needs to be done to track down what is creating the dir and prevent that from happening.,2016-05-18T20:47:10.000+0000,2021-04-22T13:30:40.636+0000,,Minor
SPARK-23897,Guava version,SPARK,Dependency upgrade,Open,[],4,"[<JIRA IssueLink: id='12596908'>, <JIRA IssueLink: id='12558016'>, <JIRA IssueLink: id='12587508'>, <JIRA IssueLink: id='12575803'>]","Guava dependency version 14 is pretty old, needs to be updated to at least 16, google cloud storage connector uses newer one which causes pretty popular error with guava; ""java.lang.NoSuchMethodError: com.google.common.base.Splitter.splitToList(Ljava/lang/CharSequence;)Ljava/util/List;"" and causes app to crash",2018-04-08T19:14:43.539+0000,2022-01-20T23:26:19.805+0000,,Minor
OOZIE-2260,"Only set ""java.io.tmpdir"" to ""./tmp""  for the AM",OOZIE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12452666'>, <JIRA IssueLink: id='12452172'>, <JIRA IssueLink: id='12426517'>]","OOZIE-2209 sets java.io.tmpdir to ./tmp for AM, map and reduce for both launcher and action jobs.

MapReduceChildJVM already sets java.io.tmpdir to ./tmp
{code}
Path childTmpDir = new Path(Environment.PWD.$(),
        YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR);
    vargs.add(""-Djava.io.tmpdir="" + childTmpDir);
{code}

So it is only needed to be set for AM in launcher because of uber mode. 
",2015-06-02T21:21:29.861+0000,2016-12-02T21:03:16.470+0000,Fixed,Major
SPARK-37925,Update document to mention the workaround for YARN-11053,SPARK,Documentation,Resolved,[],2,"[<JIRA IssueLink: id='12631752'>, <JIRA IssueLink: id='12631753'>]",,2022-01-17T04:14:06.521+0000,2022-02-05T15:47:39.163+0000,Fixed,Trivial
SPARK-16996,Hive ACID delta files not seen,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12486402'>],"spark-sql seems not to see data stored as delta files in an ACID Hive table.

Actually I encountered the same problem as describe here : http://stackoverflow.com/questions/35955666/spark-sql-is-not-returning-records-for-hive-transactional-tables-on-hdp

For example, create an ACID table with HiveCLI and insert a row :

{code}
set hive.support.concurrency=true;
set hive.enforce.bucketing=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.compactor.initiator.on=true;
set hive.compactor.worker.threads=1;
 CREATE TABLE deltas(cle string,valeur string) CLUSTERED BY (cle) INTO 1 BUCKETS
    ROW FORMAT SERDE  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
    STORED AS 
      INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
      OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
    TBLPROPERTIES ('transactional'='true');

INSERT INTO deltas VALUES(""a"",""a"");
{code}
Then make a query with spark-sql CLI :
{code}
SELECT * FROM deltas;
{code}
That query gets no result and there are no errors in logs.
If you go to HDFS to inspect table files, you find only deltas
{code}
~>hdfs dfs -ls /apps/hive/warehouse/deltas
Found 1 items
drwxr-x---   - me hdfs          0 2016-08-10 14:03 /apps/hive/warehouse/deltas/delta_0020943_0020943
{code}
Then if you run compaction on that table (in HiveCLI) :
{code}
ALTER TABLE deltas COMPACT 'MAJOR';
{code}
As a result, the delta will be compute into a base file :
{code}
~>hdfs dfs -ls /apps/hive/warehouse/deltas
Found 1 items
drwxrwxrwx   - me hdfs          0 2016-08-10 15:25 /apps/hive/warehouse/deltas/base_0020943
{code}
Go back to spark-sql and the same query gets a result :
{code}
SELECT * FROM deltas;
a       a
Time taken: 0.477 seconds, Fetched 1 row(s)
{code}
But next time you make an insert into Hive table : 
{code}
INSERT INTO deltas VALUES(""b"",""b"");
{code}
spark-sql will immediately see changes : 
{code}
SELECT * FROM deltas;
a       a
b       b
Time taken: 0.122 seconds, Fetched 2 row(s)
{code}
Yet there was no other compaction, but spark-sql ""sees"" the base AND the delta file :
{code}
~> hdfs dfs -ls /apps/hive/warehouse/deltas
Found 2 items
drwxrwxrwx   - valdata hdfs          0 2016-08-10 15:25 /apps/hive/warehouse/deltas/base_0020943
drwxr-x---   - valdata hdfs          0 2016-08-10 15:31 /apps/hive/warehouse/deltas/delta_0020956_0020956
{code}",2016-08-10T13:37:15.828+0000,2021-05-25T01:55:27.137+0000,Incomplete,Major
IMPALA-8237,Enabling preads always fetches hedged reads metrics,IMPALA,Bug,Open,[],2,"[<JIRA IssueLink: id='12554615'>, <JIRA IssueLink: id='12554616'>]","In {{HdfsFileReader}} if preads are enabled, we assume that hedged reads are enabled as well, so whenever we close a file we make a libhdfs call to collect a few hedged read metrics from the underlying {{FileSystem}} object. However, as part of IMPALA-5212 we may want to enable preads even when hedged reads are disabled, so making the call to libhdfs to fetch hedged read metrics will be a waste.

Digging through the HDFS code, it seems the HDFS client triggers hedged reads only if {{dfs.client.hedged.read.threadpool.size}} is greater than 0. We can use the same check in {{HdfsFileReader}} to trigger the fetch of hedged read metrics. The issue is that currently libhdfs does not provide a good way of getting the value of {{dfs.client.hedged.read.threadpool.size}}, it provides a method called {{hdfsConfGetInt}}, but that method simply calls {{new Configuration()}} and fetches the value of  {{dfs.client.hedged.read.threadpool.size}} from it. The issue is that calling {{new Configuration}} simply loads the current {{hdfs-site.xml}}, {{core-site.xml}}, etc. which does not take into account the scenario where the default configuration has been modified for specific filesystem objects - e.g. using {{hdfsBuilder}} to set non-default configuration parameters (see HDFS-14301 for more details).",2019-02-21T16:35:33.556+0000,2019-02-21T16:38:40.423+0000,,Major
HDDS-989,Check Hdds Volumes for errors,HDDS,Improvement,Resolved,[],3,"[<JIRA IssueLink: id='12552322'>, <JIRA IssueLink: id='12552321'>, <JIRA IssueLink: id='12552598'>]","HDDS volumes should be checked for errors periodically.

This Jira introduces volume checks on Ozone DN startup and periodically (once every 15 minutes by default). The volume checker logic is borrowed from HDFS, although some code duplication is necessary because the HDFS classes are package-private.

This patch just detects the volume failure. Failure handling will be added in HDDS-1008.",2019-01-21T22:56:29.794+0000,2019-01-27T19:49:59.878+0000,Fixed,Major
TEZ-808,Handle task attempts that are not making progress,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12446107'>, <JIRA IssueLink: id='12455298'>]",If a task attempt is not making progress then it may cause the job to hang. We may want to kill and restart the attempt. With speculation support and free resources we may want to run another version in parallel.,2014-02-06T23:26:22.701+0000,2016-05-18T04:58:03.085+0000,Fixed,Major
SPARK-19912,String literals are not escaped while performing Hive metastore level partition pruning,SPARK,Bug,Resolved,[],1,[<JIRA IssueLink: id='12497858'>],"{{Shim_v0_13.convertFilters()}} doesn't escape string literals while generating Hive style partition predicates.

The following SQL-injection-like test case illustrates this issue:
{code}
  test(""SPARK-19912"") {
    withTable(""spark_19912"") {
      Seq(
        (1, ""p1"", ""q1""),
        (2, ""p1\"" and q=\""q1"", ""q2"")
      ).toDF(""a"", ""p"", ""q"").write.partitionBy(""p"", ""q"").saveAsTable(""spark_19912"")

      checkAnswer(
        spark.table(""foo"").filter($""p"" === ""p1\"" and q = \""q1"").select($""a""),
        Row(2)
      )
    }
  }
{code}
The above test case fails like this:
{noformat}
[info] - spark_19912 *** FAILED *** (13 seconds, 74 milliseconds)
[info]   Results do not match for query:
[info]   Timezone: sun.util.calendar.ZoneInfo[id=""America/Los_Angeles"",offset=-28800000,dstSavings=3600000,useDaylight=true,transitions=185,lastRule=java.util.SimpleTimeZone[id=America/Los_Angeles,offset=-28800000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]
[info]   Timezone Env:
[info]
[info]   == Parsed Logical Plan ==
[info]   'Project [unresolvedalias('a, None)]
[info]   +- Filter (p#27 = p1"" and q = ""q1)
[info]      +- SubqueryAlias spark_19912
[info]         +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Analyzed Logical Plan ==
[info]   a: int
[info]   Project [a#26]
[info]   +- Filter (p#27 = p1"" and q = ""q1)
[info]      +- SubqueryAlias spark_19912
[info]         +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Optimized Logical Plan ==
[info]   Project [a#26]
[info]   +- Filter (isnotnull(p#27) && (p#27 = p1"" and q = ""q1))
[info]      +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Physical Plan ==
[info]   *Project [a#26]
[info]   +- *FileScan parquet default.spark_19912[a#26,p#27,q#28] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(p#27), (p#27 = p1"" and q = ""q1)], PushedFilters: [], ReadSchema: struct<a:int>
[info]   == Results ==
[info]
[info]   == Results ==
[info]   !== Correct Answer - 1 ==   == Spark Answer - 0 ==
[info]    struct<>                   struct<>
[info]   ![2]
{noformat}",2017-03-10T23:17:20.764+0000,2017-03-21T04:18:24.009+0000,Fixed,Major
ORC-1,Import code from Hive,ORC,Task,Closed,[],2,"[<JIRA IssueLink: id='12432200'>, <JIRA IssueLink: id='12446929'>]",,2015-04-27T23:34:43.092+0000,2020-10-12T04:05:53.770+0000,Fixed,Major
CALCITE-1695,Not all RexUtil.simplifyXxx code paths carry the provided executor,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12497947'>, <JIRA IssueLink: id='12558798'>, <JIRA IssueLink: id='12497949'>]","CALCITE-1653 resolved some of the issues encountered with different semantics in Hive executor vs. Calcite default executor. When investigating a workaround for CALCITE-1690 I found that there are other code paths that can this the same issue, eg:
{noformat}
	at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1239)
	at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1236)
	at org.apache.calcite.rex.RexExecutable.reduce(RexExecutable.java:86)
	at org.apache.calcite.rex.RexExecutorImpl.reduce(RexExecutorImpl.java:128)
	at org.apache.calcite.rex.RexUtil.simplifyCast(RexUtil.java:2450)
	at org.apache.calcite.rex.RexUtil.simplify(RexUtil.java:1633)
	at org.apache.calcite.rex.RexUtil.simplify(RexUtil.java:1587)
	at org.apache.calcite.rex.RexUtil.simplifyList(RexUtil.java:1747)
	at org.apache.calcite.rex.RexUtil.simplifyComparison(RexUtil.java:1658)
	at org.apache.calcite.rex.RexUtil.simplify(RexUtil.java:1648)
	at org.apache.calcite.rex.RexUtil$ExprSimplifier.visitCall(RexUtil.java:3051)
	at org.apache.calcite.rex.RexUtil$ExprSimplifier.visitCall(RexUtil.java:3016)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:104)
	at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:279)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:473)
{noformat}
Int he stack above neither {{RexUtil.simplifyComparison}} nor {{RexUtil.simplifyList}} accept an executor, and thus the executor info present at the {{RexUtil.simplify:1648}} frame is lost and the default {{EXECUTOR}} is used instead. The result is incorrect for Hive.
",2017-03-14T13:07:22.106+0000,2019-04-11T17:47:17.184+0000,Fixed,Major
KNOX-2321,HDFS UI rewrite rules should handle LogLevel,KNOX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12585544'>],"/LogLevel link is added to hadoop webpages like NN, DN by [HDFS-14463|https://issues.apache.org/jira/browse/HDFS-14463]

When Knox is enabled for HDFS web UI endpoints, this is not working. This require some Knox rewrite URL changes to make this work.

Reported by [~nmaheshwari]",2020-04-10T18:51:39.012+0000,2020-04-14T05:01:42.610+0000,Fixed,Major
SPARK-30019,Add the owner property to v2 table,SPARK,Sub-task,Resolved,[],1,[<JIRA IssueLink: id='12574910'>],"[https://jira.apache.org/jira/browse/HIVE-18762]

To support syntax like below to support change the ownership of a table

 
{code:java}
alter table tb1 set owner user user1;
alter table tb1 set owner role role1;
{code}
 ",2019-11-25T08:28:02.762+0000,2020-01-22T08:39:17.036+0000,Fixed,Major
LEGAL-359,Is it possible to include a file in source code licensed under CDDL / GPLv2 ?,LEGAL,Question,Resolved,[],1,[<JIRA IssueLink: id='12524168'>],"I would like to ask for an advice for the following situation:
Due to technical reasons (lack of extensibility option) I had to copy a class from Jersey which is the implementation of choice for JAX-RS, see webpage: https://jersey.github.io/documentation/1.19.1/index.html
The class is slightly modified by me so it is not an exact copy of the original.

The class in question: 
https://github.com/jersey/jersey-1.x/blob/b73951e9186fb540675eac62aed622946be781ff/jersey-core/src/main/java/com/sun/jersey/spi/service/ServiceFinder.java

According to their documentation and the header of the above file, Jersey is dual licensed under CDDL 1.1 and GPL v2, see: https://jersey.github.io/license.html

Checking the Apache license page (https://www.apache.org/legal/resolved.html#category-b), it says the following about CDDL 1: ""Software under the following licenses may be included in binary form within an Apache product if the inclusion is appropriately labeled (see below):""

Question 1:
Given that, is it possible to include one class from Jersey to our codebase? 

Question 2: 
Since we have automatic license checking turned on for all files, somehow I would need to include the ASF license text on the top of the file. May I legally do that?

Question 3: 
The original license text on ServiceFinder says: ""DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS HEADER.""
Is it possible to include the ASF license on top of the file and keep the original license as well, or if it's not a good practice, how may I handle this situation?

Thanks in advance!",2018-01-12T09:59:43.066+0000,2018-05-24T06:27:39.845+0000,Won't Fix,Major
CALCITE-1338,JoinProjectTransposeRule makes wrong transformation when the right child of left outer join has a RexLiteral project expression,CALCITE,Bug,Closed,[],3,"[<JIRA IssueLink: id='12495672'>, <JIRA IssueLink: id='12478312'>, <JIRA IssueLink: id='12558719'>]","h3. SQL: 

{code:borderStyle=solid}
SELECT * 
FROM dept a 
LEFT JOIN (
  SELECT b.name, 1 FROM dept b
) AS b 
ON a.name=b.name
{code}

h3. Selected rule set:

{code:borderStyle=solid}
  SubQueryRemoveRule.JOIN
  JoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER
{code}

h3. Optimized logical plan:

{code:borderStyle=solid}
LogicalProject(DEPTNO=[$0], NAME=[$1], NAME0=[$2], EXPR$1=[$3])
  LogicalProject(DEPTNO=[$0], NAME=[$1], NAME0=[$2], EXPR$1=[CAST($3):INTEGER])
    LogicalProject(DEPTNO=[$0], NAME=[$1], NAME0=[$3], EXPR$1=[1])
      LogicalJoin(condition=[=($3, $1)], joinType=[left])
        LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
        LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
{code}

h3. The right logical plan should be

{code:borderStyle=solid}
LogicalProject(DEPTNO=[$0], NAME=[$1], NAME0=[$2], EXPR$1=[$3])
  LogicalJoin(condition=[=($2, $1)], joinType=[left])
    LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
    LogicalProject(NAME=[$1], EXPR$1=[1])
      LogicalTableScan(table=[[CATALOG, SALES, DEPT]])
{code}

h3. Summary

The RexLiteral project expression will make logical plan get different results when it's right child or parent node of left outer join.",2016-08-02T02:57:17.184+0000,2019-07-22T09:42:13.757+0000,Fixed,Major
APEXCORE-45,Certain HDFS calls from Apex give NPE with Hadoop 2.7.x,APEXCORE,Bug,Closed,[],1,[<JIRA IssueLink: id='12458078'>],"How to reproduce:
- install apache hadoop 2.7.x
- install RTS 3.0.0 community edition as root
- On the hadoop installation screen in the install wizard, for the DFS root directory field, enter a directory that does not exist, whose parent dtadmin does not have write access to.  For example: /user/root/datatorrent.

Exception is thrown on the namenode:
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkOwner(FSPermissionChecker.java:247)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:227)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1698)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1682)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkOwner(FSDirectory.java:1651)
        at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setPermission(FSDirAttrOp.java:61)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setPermission(FSNamesystem.java:1653)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setPermission(NameNodeRpcServer.java:693)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setPermission(ClientNamenodeProtocolServerSideTranslatorPB.java:453)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
{noformat}

binhnv80@gmail.com on the apex-dev mailing list also mentioned there is another exception in STRAM when FSStorageAgent tries to create a file in HDFS:
{noformat}
ERROR com.datatorrent.stram.StreamingAppMaster: Exiting Application Master
java.lang.NullPointerException
        at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:551)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)
        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
        at org.apache.hadoop.fs.FileContext.create(FileContext.java:682)
        at com.datatorrent.common.util.FSStorageAgent.save(FSStorageAgent.java:92)
        at com.datatorrent.stram.plan.physical.PhysicalPlan.initCheckpoint(PhysicalPlan.java:944)
        at com.datatorrent.stram.plan.physical.PhysicalPlan.<init>(PhysicalPlan.java:363)
        at com.datatorrent.stram.StreamingContainerManager.<init>(StreamingContainerManager.java:330)
        at com.datatorrent.stram.StreamingContainerManager.getInstance(StreamingContainerManager.java:2828)
        at com.datatorrent.stram.StreamingAppMasterService.serviceInit(StreamingAppMasterService.java:516)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at com.datatorrent.stram.StreamingAppMaster.main(StreamingAppMaster.java:98)
{noformat}

I am not able to reproduce the second exception.

Here's the link to the google group email thread:
https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!topic/apex-dev/CxZN-QtR5BE",2015-08-11T21:39:27.000+0000,2017-04-17T23:36:26.344+0000,Fixed,Major
TEZ-1563,TezClient.submitDAGSession alters DAG local resources regardless of DAG submission,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12396419'>, <JIRA IssueLink: id='12396418'>]","In {{TezClient#submitDAGSesssion(Dag)}}, a {{DAGPlan}} is created from the {{DAG}} before the {{DAGClientAMProtocolBlockingPB}} is instantiated. When the application isn't running, {{waitForProxy()}} will throw a {{SessionNotRunning}} Exception.

The problem is that the internal state of the {{DAG}} is modified, regardless of whether the DAG is actually run or not. 

{code}
DAGPlan dagPlan = dag.createDag(amConfig.getTezConfiguration());
{code}

The {{createDag}} method will ultimately call {{addTaskLocalFiles}} for each {{Vertex}} in the {{DAG}}

{code}
// add common task files for this DAG
vertex.addTaskLocalFiles(commonTaskLocalFiles);
{code}

Because the {{DAG}}'s state is modified, {{Vertex#addTaskLocalFiles(Map)}} will fail if any resources are added multiple times. As such, if the application is not running and {{SessionNotRunning}} is thrown, that same DAG cannot be passed in to run the DAG after the application is started again.

Additionally, {{DAG}} is missing a getTaskLocalFiles method as compared to {{Vertex}} which would be good to add to make the two classes more uniform.",2014-09-10T04:28:58.437+0000,2014-10-02T21:41:02.906+0000,Fixed,Major
TEZ-269,Fix ResourceMgrDelegate#getDelegationToken after YARN-868 is fixed,TEZ,Bug,Open,[],1,[<JIRA IssueLink: id='12370941'>],,2013-06-21T00:20:25.834+0000,2014-08-13T22:49:20.538+0000,,Major
INFRA-8147,check on open file limit on jenkins test slaves,INFRA,Bug,Closed,[],3,"[<JIRA IssueLink: id='12399697'>, <JIRA IssueLink: id='12396134'>, <JIRA IssueLink: id='12393427'>]","A test failure (TestPipelinesFailover.testPipelineRecoveryStress) is reported in HDFS-6694, it's strongly suspected that the open file limit on the jenkins test machine is not enough. 

Would anyone from the infra side please help look into? This test fails pretty frequently and we tend to ignore it, which may let real problem get in, so I'm making this jira critical. Thanks.

BTW, Thanks [~vinayrpet] and [~ajisakaa] for looking into HDFS-6694.
",2014-08-03T22:27:22.267+0000,2015-12-07T05:22:14.072+0000,Duplicate,Critical
ORC-42,Advance Hybrid Cloud Architecture (AHA) - Advance ORC (Umbrella) JIRA,ORC,New Feature,Open,[],2,"[<JIRA IssueLink: id='12457288'>, <JIRA IssueLink: id='12457329'>]","Link to Umbrella JIRA
 https://issues.apache.org/jira/browse/HADOOP-12620
 See https://issues.apache.org/jira/browse/HADOOP-12620?focusedCommentId=15046300&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15046300 for more details. 
 This JIRA is an umbrella (parent/master) JIRA for advancing ORC through ORC update capability given https://issues.apache.org/jira/browse/HDFS-9607.

A number of capabilities that can be added to ORC once ORC update (HDFS update) is supported may include: 
 JSON_ORC – native processing of JSON (add MongoDB/CouchDB type capabilities in Hadoop)
 XML_ORC – add native XML processing capability to ORC.
 RDF_ORC – native processing of RDF documents
 MVCC_ORC – Add Multi Version Concurrency Control (MVCC) support to ORC
 INDEX_ORC – Create a variety of Indexes such as B-Tree, Bitmap etc. to other files in Hadoop.

 

This JIRA is within the broad initiative:

Advance Hadoop Architecture (AHA) / Advance Hadoop Adaptabilities (AHA) /

Advance Hybrid Cloud Architecture (AHA)",2016-02-15T04:53:16.832+0000,2020-11-07T05:58:33.923+0000,,Major
SPARK-1828,Created forked version of hive-exec that doesn't bundle other dependencies,SPARK,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12388032'>, <JIRA IssueLink: id='12388031'>]","The hive-exec jar includes a bunch of Hive's dependencies in addition to hive itself (protobuf, guava, etc). See HIVE-5733. This breaks any attempt in Spark to manage those dependencies.

The only solution to this problem is to publish our own version of hive-exec 0.12.0 that behaves correctly. While we are doing this, we might as well re-write the protobuf dependency to use the shaded version of protobuf 2.4.1 that we already have for Akka.",2014-05-14T07:52:57.244+0000,2014-08-15T17:20:23.212+0000,Fixed,Blocker
SENTRY-609,"[Unit Test] Many tests failing with error ""The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x""",SENTRY,Bug,Resolved,"[<JIRA Issue: key='SENTRY-610', id='12767474'>]",1,[<JIRA IssueLink: id='12407973'>],"26 tests are failing in this run: https://builds.apache.org/job/Sentry-jdk-1.7/196/org.apache.sentry$sentry-binding-hive/testReport/ with the following error:

{code}
Error Message

java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x
Stacktrace

java.lang.RuntimeException: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x
	at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:535)
	at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:484)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:436)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:392)
	at org.apache.sentry.binding.hive.TestSentryHiveAuthorizationTaskFactory.setup(TestSentryHiveAuthorizationTaskFactory.java:80)
{code}

",2015-01-14T17:04:17.630+0000,2015-02-12T02:45:28.296+0000,Fixed,Blocker
PHOENIX-5250,The accumulated wal files can't be cleaned,PHOENIX,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12608530'>, <JIRA IssueLink: id='12560133'>]","Because of the modification of HBASE-20781,  the faked WALEdits won't be filtered, all WALEdits will be saved into Memstore with a status that inMemstore is true(uses WAL->append method).

!image-2019-04-19-21-31-27-888.png|width=755,height=310!

The family array of IndexedKeyValue is WALEdit.METAFAMILY that is used to describe a fake WALEdit, and it will put into Memstore with WALedits of data table during sync global index.

WAL files can't be cleaned, except for restarting RS, Many WAL files will decrease the percent of disk free.  

!https://gw.alicdn.com/tfscom/TB1n3cDQVzqK1RjSZFCXXbbxVXa.png|width=422,height=159!

 

 

 ",2019-04-19T13:32:04.349+0000,2021-02-13T18:13:36.698+0000,Fixed,Blocker
BIGTOP-3298,Bump Phoenix to 4.15.0-HBase-1.5,BIGTOP,Sub-task,Resolved,[],4,"[<JIRA IssueLink: id='12579629'>, <JIRA IssueLink: id='12579632'>, <JIRA IssueLink: id='12579648'>, <JIRA IssueLink: id='12579406'>]",,2020-01-30T13:20:05.437+0000,2020-02-14T05:10:40.666+0000,Fixed,Major
TEZ-2917,change some logs from info to debug,TEZ,Bug,Resolved,[],1,[<JIRA IssueLink: id='12447417'>],"I've done a highly unscientific summarization of the logs from some random queries, and will now change some log statements that are the most prevalent and not extremely useful from info to debug.",2015-10-29T21:53:22.377+0000,2015-11-02T20:52:29.034+0000,Won't Fix,Major
TEZ-3537,ArrayIndexOutOfBoundsException with empty environment variables/Port YARN-3768 to Tez,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12486367'>],YARN-3768 fixes the ArrayIndexOutOfBoundsException problem when you serve up environment variables that were previously unexpected.,2016-11-11T21:00:45.233+0000,2017-03-14T03:49:55.844+0000,Fixed,Major
ACCUMULO-3036,"1.5 MiniCluster fails to start, forces clients to wait indefinitely",ACCUMULO,Improvement,Resolved,[],1,[<JIRA IssueLink: id='12393101'>],"Over in Pig land, a user was complaining about a test which used MiniAccumuloCluster that hung until the JUnit timeout was hit.

Eventually, the problem was diagnosed as a bad classpath (old version of Thrift was included and used), which was causing the TServer and Master to immediately bail out. However, the client sat indefinitely trying to connect unsuccessfully.

MAC#start should not return before we're sure that the processes are actually up and running (a very quick smoke test).

It looks like ACCUMULO-1537 introduced a call to SetGoalState on the Master before MAC#start returned which would (I assume) fail and then throw a RTE if the Master decided to die. Including this fix in 1.5 may be sufficient to fix the underlying issue the user was seeing.",2014-08-01T15:40:27.794+0000,2015-05-21T23:11:50.694+0000,Won't Fix,Major
AMBARI-3029,Ambari clusters HBASE_CLASSPATH setting is resulting in different configs not getting used.,AMBARI,Bug,Resolved,[],1,[<JIRA IssueLink: id='12374364'>],HBASE_CLASSPATH is being set to hbase configuration directory in hbase-env.sh and thus being prefixed to CLASSPATH variable in hbase file. This doesn't let other hbase configs to take effect. ,2013-08-26T22:40:55.406+0000,2013-08-26T23:21:41.377+0000,Fixed,Major
NIFI-2623,SelectHiveQL with Avro output doesn't handle binary types,NIFI,Bug,Resolved,[],2,"[<JIRA IssueLink: id='12478266'>, <JIRA IssueLink: id='12478189'>]","SelectHiveQL, when specifying Avro output, and having columns of binary types, throws an error when run.

The error is a SQLException with text ""Method not supported"". This is coming from the Hive driver because HiveJdbcCommon is calling getBytes() on the ResultSet, and that method has not been implemented in the driver (HIVE-7134).

It may be possible to use getObject() and convert it into a byte array or ByteBuffer (if it is not already) for storage into an Avro record.",2016-08-22T17:58:07.515+0000,2016-08-23T15:45:39.968+0000,Fixed,Major
ATLAS-4639,remove dependency on commons-configuration 1.10,ATLAS,Improvement,Open,[],1,[<JIRA IssueLink: id='12644168'>],"commons-configuration v1 is end of life and has some dependencies that are insecure.
https://mvnrepository.com/artifact/commons-configuration/commons-configuration/1.10

atlas-common 2.2.0 has commons-configuration v1.10 dependency:
https://mvnrepository.com/artifact/org.apache.atlas/atlas-common/2.2.0

commons-configuration2 is supported still and is worth uptaking if hive still needs config support.
",2022-07-20T02:22:57.257+0000,2022-07-20T02:23:21.412+0000,,Major
TEZ-800,One-one edge with parallelism -1 fails if source vertex parallelism is not -1 as well,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12388948'>, <JIRA IssueLink: id='12382508'>]","// no input initializers. At this moment, only other case is 1-1 edge
          // with uninitialized sources
          boolean hasOneToOneUninitedSource = false;
          for (Map.Entry<Vertex, Edge> entry : vertex.sourceVertices.entrySet()) {
            if (entry.getValue().getEdgeProperty().getDataMovementType() == 
                DataMovementType.ONE_TO_ONE) {
              if (entry.getKey().getTotalTasks() == -1) {
                hasOneToOneUninitedSource = true;
                break;
              }
            }
          }

This checks for the source vertex which has the 1-1 edge to also have -1 parallelism.",2014-02-06T07:08:48.074+0000,2018-01-28T08:15:49.889+0000,Fixed,Major
THRIFT-1625,Null handling in TBinaryProtocol,THRIFT,Bug,Closed,[],1,[<JIRA IssueLink: id='12353362'>],As pointed out here: https://issues.apache.org/jira/browse/HIVE-2800?focusedCommentId=13291334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13291334 thrift doesn't check for nulls in payload and will throw NPE if data has null in it.,2012-06-07T21:52:18.479+0000,2012-06-22T04:19:59.797+0000,Won't Fix,Major
SLIDER-552,remove root registry path setup in AM,SLIDER,Sub-task,Open,[],1,[<JIRA IssueLink: id='12399494'>],"once YARN-2571 is in, downgrade the {{RegistryOperations}} instance instantied in the slider AM from a {{RegistryRMOperationsService}} —which creates the root paths on startup— to a normal client.",2014-10-22T08:05:26.547+0000,2015-04-29T17:01:29.992+0000,,Critical
CASSANDRA-2735,Timestamp Based Compaction Strategy,CASSANDRA,New Feature,Resolved,[],3,"[<JIRA IssueLink: id='12339545'>, <JIRA IssueLink: id='12339412'>, <JIRA IssueLink: id='12354996'>]","Compaction strategy implementation based on max timestamp ordering of the sstables while satisfying max sstable size, min and max compaction thresholds. It also handles expiration of sstables based on a timestamp.",2011-06-02T03:11:34.709+0000,2019-04-16T09:32:58.126+0000,Duplicate,Low
PHOENIX-4658,IllegalStateException: requestSeek cannot be called on ReversedKeyValueHeap,PHOENIX,Bug,Resolved,[],1,[<JIRA IssueLink: id='12529827'>],"Steps to reproduce are as follows:

1. Create a table with multiple column families
{code}
CREATE TABLE TBL (
  COL1 VARCHAR NOT NULL,
  COL2 VARCHAR NOT NULL,
  COL3 VARCHAR,
  FAM.COL4 VARCHAR,
  CONSTRAINT TRADE_EVENT_PK PRIMARY KEY (COL1, COL2)
)
{code}

2. Upsert a row
{code}
UPSERT INTO TBL (COL1, COL2) values ('AAA', 'BBB')
{code}

3. Query with DESC for the table
{code}
SELECT * FROM TBL WHERE COL2 = 'BBB' ORDER BY COL1 DESC
{code}

By following the above steps, we face the following exception.
{code}
java.util.concurrent.ExecutionException: org.apache.phoenix.exception.PhoenixIOException: org.apache.hadoop.hbase.DoNotRetryIOException: TBL,,1521251842845.153781990c0fb4bc34e3f2c721a6f415.: requestSeek cannot be called on ReversedKeyValueHeap
	at org.apache.phoenix.util.ServerUtil.createIOException(ServerUtil.java:96)
	at org.apache.phoenix.util.ServerUtil.throwIOException(ServerUtil.java:62)
	at org.apache.phoenix.iterate.RegionScannerFactory$1.nextRaw(RegionScannerFactory.java:212)
	at org.apache.phoenix.coprocessor.DelegateRegionScanner.nextRaw(DelegateRegionScanner.java:82)
	at org.apache.phoenix.coprocessor.DelegateRegionScanner.nextRaw(DelegateRegionScanner.java:82)
	at org.apache.phoenix.coprocessor.BaseScannerRegionObserver$RegionScannerHolder.nextRaw(BaseScannerRegionObserver.java:294)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2808)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:3045)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36613)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2352)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:297)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:277)
Caused by: java.lang.IllegalStateException: requestSeek cannot be called on ReversedKeyValueHeap
	at org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.requestSeek(ReversedKeyValueHeap.java:65)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.joinedHeapMayHaveData(HRegion.java:6485)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:6412)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:6126)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:6112)
	at org.apache.phoenix.iterate.RegionScannerFactory$1.nextRaw(RegionScannerFactory.java:175)
	... 10 more
{code}
",2018-03-17T02:00:05.179+0000,2018-07-26T01:15:37.339+0000,Fixed,Major
TEZ-4236,DAGClientServer is not really needed to be started/used in local mode,TEZ,Improvement,Resolved,[],5,"[<JIRA IssueLink: id='12607961'>, <JIRA IssueLink: id='12621091'>, <JIRA IssueLink: id='12647215'>, <JIRA IssueLink: id='12644890'>, <JIRA IssueLink: id='12647304'>]","For a customer, we found that they try to run hive on tez local mode tests, and due to strict firewall rules and security policy, the DagClientServer struggles to start or respond to connections (think about corporate laptops here). Instead of investigating the network-related workarounds, let's find another way and decide if DAGAppMaster really needs this server. 
LocalClient has an embedded [DAGAppMaster|https://github.com/apache/tez/blob/master/tez-dag/src/main/java/org/apache/tez/client/LocalClient.java#L65], so at the first sight, it doesn't make sense to rely on the network environment in terms of AM usage. Hopefully, AM can become more lightweight in the long term if we try to eliminate pointless network usage. ",2020-10-01T11:07:19.252+0000,2022-09-09T09:43:54.062+0000,Fixed,Major
CALCITE-1016,"""GROUP BY constant"" on empty relation should return 0 rows",CALCITE,Bug,Closed,[],1,[<JIRA IssueLink: id='12451517'>],"What should ""GROUP BY 1"" return on an empty table? Calcite currently returns 0 rows. Does that comply with the SQL standard?

Here is what Oracle 11.2.0.2.0 does.

{noformat}
SQL> select count(*) from emp where 1 = 0;

  COUNT(*)
----------
	 0

SQL> select count(*) from emp where 1 = 0 group by ();

no rows selected

SQL> select count(*) from emp where 1 = 0 group by 1;

no rows selected

SQL> select count(*) from emp;

  COUNT(*)
----------
	14

SQL> select count(*) from emp group by ();

  COUNT(*)
----------
	14

SQL> select count(*) from emp group by 1;

  COUNT(*)
----------
	14
{noformat}

I had expected {code}select count(*) from emp where 1 = 0 group by (){code} would return 1 row, but it returns 0, like {code}group by 1{code}.",2015-12-10T19:49:18.621+0000,2016-01-21T22:20:24.652+0000,Fixed,Major
TEZ-2171,Remove unused metrics code,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12409640'>],"DefaultMetricsSystem.shutdown() is called in TezChild. IIUC, shutdown is meant to be called in daemon context (datanode/namenode). LLAP daemon initializes the metrics system, after running the first query TezChild shuts down the metrics system which results in closing of all metrics endpoints/sinks. ",2015-03-04T08:31:42.382+0000,2015-06-30T04:52:57.469+0000,Fixed,Major
HCATALOG-276,After merging in HCATALOG-237 related changes Pig scripts with more than one store fail,HCATALOG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12349106'>, <JIRA IssueLink: id='12348626'>]",e2e tests Pig_Checkin_4 and Pig_Checkin_5 are failing.,2012-02-26T00:31:35.350+0000,2012-05-17T01:20:33.510+0000,Fixed,Major
TEZ-3244,Allow overlap of input and output memory when they are not concurrent,TEZ,Bug,Closed,[],1,[<JIRA IssueLink: id='12492726'>],For cases when memory for inputs and outputs are not needed simultaneously it would be more efficient to allow inputs to use the memory normally set aside for outputs and vice-versa.,2016-05-06T15:30:11.598+0000,2017-03-14T03:50:05.648+0000,Fixed,Major
DRILL-8240,Revisit clone of log4j Strings class,DRILL,Improvement,Closed,[],2,"[<JIRA IssueLink: id='12640839'>, <JIRA IssueLink: id='12640838'>]","See https://issues.apache.org/jira/browse/DRILL-8044 for background.

The code added there is now out of date. After the log4j panic late last year, 5 commits were made to modify the real log4j class and these are missing from the Drill copy.

Compare https://github.com/apache/logging-log4j2/commits/rel/2.17.2/log4j-api/src/main/java/org/apache/logging/log4j/util/Strings.java to https://github.com/apache/logging-log4j2/commits/rel/2.14.1/log4j-api/src/main/java/org/apache/logging/log4j/util/Strings.java

The Drill copy is based on Log4J 2.14.1. Every commit in 2021 and 2022 is missing from the Drill copy.",2022-05-29T08:59:59.074+0000,2022-07-13T15:53:04.105+0000,Fixed,Major
SLIDER-382,Uber Jira: Slider to work on secure ZK quora,SLIDER,New Feature,Resolved,"[<JIRA Issue: key='SLIDER-344', id='12735843'>]",1,[<JIRA IssueLink: id='12399117'>],"Uber JIRA to cover all issues of secure JIRA clusters, where permissions restrict access

# SLIDER-149/YARN-913 will cover core setup of a registry
# Agent bindings need to adapt",2014-09-01T16:09:52.567+0000,2014-10-16T17:24:55.150+0000,Fixed,Major
REEF-337,Support REEF on YARN Federation,REEF,New Feature,Open,[],3,"[<JIRA IssueLink: id='12425230'>, <JIRA IssueLink: id='12433498'>, <JIRA IssueLink: id='12434236'>]","In [YARN-2915], the YARN team is working on supporting scale-out of the RM via federation of multiple RMs. We should track this and make sure REEF applications can work in that space.",2015-05-19T20:40:26.040+0000,2015-09-03T04:09:20.672+0000,,Major
MGPG-59,"GPG Plugin: ""gpg: signing failed: Inappropriate ioctl for device""",MGPG,Bug,Closed,[],2,"[<JIRA IssueLink: id='12579574'>, <JIRA IssueLink: id='12511859'>]","Some versions and configurations of GPG want to pop an interactive UI, which the maven-gpg-plugin cannot handle.  This appears in the log, before Maven quits without much advice:

{{*gpg: WARNING: ""--no-use-agent"" is an obsolete option - it has no effect}}
{{""gpg: signing failed: Inappropriate ioctl for device""}}

!screenshot-1.png!

*The remedy is for the user to do something with gpg on the command line before going into mvn-release:prepare*. Something that will allow gpg to remember (for a period of time) the passphrase entered in that interactive UI, that the maven invocation of gpg can take advantage of afterwards.

Historically there was a gpg option '--no-use-agent', but not all gpg implementations support that today, not will going forwards as it is being removed.

Suggestion: On command error, System.out.println the remedy above, perhaps even suggesting the command to run:

{{gpg --use-agent --armor --detach-sign  --output $(mktemp) pom.xml}}",2017-06-18T13:59:19.016+0000,2020-02-01T17:28:50.828+0000,Fixed,Major
CALCITE-953,Improve RelMdPredicates to deal with RexLiteral,CALCITE,Improvement,Closed,[],1,[<JIRA IssueLink: id='12447826'>],"Right now, Calcite will treat a RexLiteral's ColumnOrigin as an empty set. Rather than dropping the information of RexLiteral, this patch proposes to keep this information in the RelColumnOrigin itself.",2015-11-04T21:01:54.818+0000,2015-11-10T08:02:50.311+0000,Fixed,Major
PHOENIX-3103,Remove xom:xom dependency,PHOENIX,Sub-task,Closed,[],1,[<JIRA IssueLink: id='12475566'>],"{noformat}
[INFO] |  +- org.apache.hbase:hbase-server:jar:1.2.0:provided
[INFO] |  |  +- org.apache.hbase:hbase-procedure:jar:1.2.0:provided
[INFO] |  |  |  \- org.apache.hbase:hbase-common:jar:tests:1.2.0:provided
[INFO] |  |  +- org.apache.hbase:hbase-prefix-tree:jar:1.2.0:provided
[INFO] |  |  +- org.apache.hbase:hbase-hadoop2-compat:jar:1.2.0:test
[INFO] |  |  +- org.apache.commons:commons-math:jar:2.2:provided
[INFO] |  |  +- org.mortbay.jetty:jetty-sslengine:jar:6.1.26:provided
[INFO] |  |  +- org.mortbay.jetty:jsp-2.1:jar:6.1.14:provided
[INFO] |  |  +- org.mortbay.jetty:jsp-api-2.1:jar:6.1.14:provided
[INFO] |  |  +- org.mortbay.jetty:servlet-api-2.5:jar:6.1.14:provided
[INFO] |  |  +- tomcat:jasper-compiler:jar:5.5.23:provided
[INFO] |  |  +- tomcat:jasper-runtime:jar:5.5.23:provided
[INFO] |  |  |  \- commons-el:commons-el:jar:1.0:provided
[INFO] |  |  +- org.jamon:jamon-runtime:jar:2.4.1:provided
[INFO] |  |  +- com.lmax:disruptor:jar:3.3.0:provided
[INFO] |  |  +- org.owasp.esapi:esapi:jar:2.1.0:provided
[INFO] |  |  |  +- commons-fileupload:commons-fileupload:jar:1.2:provided
[INFO] |  |  |  +- xom:xom:jar:1.2.5:provided
{noformat}

Looks like this is actually coming in via hbase-server which is surprising. Probably the issue is that we're bundling it in. Hopefully we can just exclude the dependency.",2016-07-20T15:29:40.568+0000,2016-08-11T08:27:43.157+0000,Fixed,Major
PARQUET-382,Add a way to append encoded blocks in ParquetFileWriter,PARQUET,New Feature,Resolved,[],2,"[<JIRA IssueLink: id='12548983'>, <JIRA IssueLink: id='12445619'>]","Concatenating two files together currently requires reading the source files and rewriting the content from scratch. This ends up taking a lot of memory, even if the data is already encoded correctly and blocks just need to be appended and have their metadata updated. Merging two files should be fast and not take much memory.",2015-09-24T17:03:12.576+0000,2018-11-29T09:26:59.015+0000,Fixed,Major
LENS-703,"ClassNotFound while doing ""list partition"" after ""update partition""",LENS,Bug,Closed,[],2,"[<JIRA IssueLink: id='12432542'>, <JIRA IssueLink: id='12432544'>]","{noformat}
29 Jul 2015 06:26:49,748 [616fb893-c46c-41a5-be69-f9325be67052] [Grizzly-worker(6)] ERROR org.apache.lens.server.LensRequestListener  - Encountered HTTP exception
org.glassfish.jersey.server.internal.process.MappableException: org.apache.lens.server.api.error.LensException
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.mapTargetToRuntimeEx(AbstractJavaResourceMethodDispatcher.java:190)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.access$200(AbstractJavaResourceMethodDispatcher.java:74)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:160)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
	at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
	at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lens.server.api.error.LensException
	at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllPartitionsOfFactStorage(CubeMetastoreServiceImpl.java:715)
	at org.apache.lens.server.metastore.MetastoreResource.getAllPartitionsOfFactStorageByFilter(MetastoreResource.java:1046)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)
	... 20 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Class not found: class org.apache.hadoop.hive.ql.io.RCFileOutputFormat
	at org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass(Partition.java:331)
	at org.apache.lens.server.metastore.JAXBUtils.xpartitionFromPartition(JAXBUtils.java:795)
	at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllPartitionsOfFactStorage(CubeMetastoreServiceImpl.java:707)
	... 27 more
Caused by: java.lang.ClassNotFoundException: class org.apache.hadoop.hive.ql.io.RCFileOutputFormat
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass(Partition.java:322)
	... 29 more
29 Jul 2015 06:26:49,749 [616fb893-c46c-41a5-be69-f9325be67052] [Grizzly-worker(6)] WARN  org.glassfish.jersey.server.ServerRuntime$Responder  - WebApplicationException cause:
org.apache.lens.server.api.error.LensException
	at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllPartitionsOfFactStorage(CubeMetastoreServiceImpl.java:715)
	at org.apache.lens.server.metastore.MetastoreResource.getAllPartitionsOfFactStorageByFilter(MetastoreResource.java:1046)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
	at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
	at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Class not found: class org.apache.hadoop.hive.ql.io.RCFileOutputFormat
	at org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass(Partition.java:331)
	at org.apache.lens.server.metastore.JAXBUtils.xpartitionFromPartition(JAXBUtils.java:795)
	at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllPartitionsOfFactStorage(CubeMetastoreServiceImpl.java:707)
	... 27 more
Caused by: java.lang.ClassNotFoundException: class org.apache.hadoop.hive.ql.io.RCFileOutputFormat
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass(Partition.java:322)
	... 29 more
29 Jul 2015 06:26:49,751 [616fb893-c46c-41a5-be69-f9325be67052] [Grizzly-worker(6)] ERROR org.glassfish.jersey.server.ServerRuntime$Responder  - An exception has been thrown from an exception mapper class org.apache.lens.server.error.LensExceptionMapper.
java.lang.NullPointerException
	at org.apache.lens.server.error.LensExceptionMapper.toResponse(LensExceptionMapper.java:35)
	at org.apache.lens.server.error.LensExceptionMapper.toResponse(LensExceptionMapper.java:28)
	at org.glassfish.jersey.server.ServerRuntime$Responder.mapException(ServerRuntime.java:443)
	at org.glassfish.jersey.server.ServerRuntime$Responder.process(ServerRuntime.java:371)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:261)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
	at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
	at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
	at java.lang.Thread.run(Thread.java:745)
29 Jul 2015 06:26:49,751 [616fb893-c46c-41a5-be69-f9325be67052] [Grizzly-worker(6)] ERROR org.glassfish.jersey.server.ServerRuntime$Responder  - An exception was not mapped due to exception mapper failure. The HTTP 500 response will be returned.
org.apache.lens.server.api.error.LensException
	at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllPartitionsOfFactStorage(CubeMetastoreServiceImpl.java:715)
	at org.apache.lens.server.metastore.MetastoreResource.getAllPartitionsOfFactStorageByFilter(MetastoreResource.java:1046)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:353)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:343)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:318)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:983)
	at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:330)
	at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:212)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)
	at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Class not found: class org.apache.hadoop.hive.ql.io.RCFileOutputFormat
	at org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass(Partition.java:331)
	at org.apache.lens.server.metastore.JAXBUtils.xpartitionFromPartition(JAXBUtils.java:795)
	at org.apache.lens.server.metastore.CubeMetastoreServiceImpl.getAllPartitionsOfFactStorage(CubeMetastoreServiceImpl.java:707)
	... 27 more
Caused by: java.lang.ClassNotFoundException: class org.apache.hadoop.hive.ql.io.RCFileOutputFormat
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass(Partition.java:322)
	... 29 more
{noformat}",2015-07-29T06:36:49.030+0000,2015-08-25T04:00:08.257+0000,Fixed,Major
MESOS-1203,Shade protobuf dependency in Mesos Java library,MESOS,Improvement,Resolved,[],4,"[<JIRA IssueLink: id='12386344'>, <JIRA IssueLink: id='12481919'>, <JIRA IssueLink: id='12386340'>, <JIRA IssueLink: id='12451305'>]","Mesos's Java library uses the protobuf library which is also used by Hadoop. Unfortunately the protobuf library does not provide binary compatiblity between minor versions (for code compiled against 2.4.1 and 2.5.0 cannot run together in a single JVM classlaoder) .

This makes use of Mesos via it's Java API, something that is required for Spark and I'm assuming other frameworks, fundamentally incompatible for certain Hadoop versions.

Mesos could shade this jar using the maven shade plug-in. Take a look at the Parquet project for an example of shading:

https://github.com/Parquet/parquet-format/blob/master/pom.xml#L140

Without this fix Java users won't be able to use Mesos (< 0.17) with newer versions of Hadoop. Or Mesos 0.17+ with older versions of Hadoop.",2014-04-10T20:38:33.399+0000,2019-04-29T09:26:55.623+0000,Fixed,Major
BIGTOP-683,remove hacks for datanode init.d now that bin/hadoop has HDFS-1943 in it,BIGTOP,Improvement,Open,[],2,"[<JIRA IssueLink: id='12355493'>, <JIRA IssueLink: id='12363086'>]","Now that HDFS-1943 is in Hadoop 1.0+, the workaround in bigtop-packages/src/common/hadoop/hadoop-hdfs-datanode.svc is obsolete and can be dropped",2012-07-25T17:33:20.680+0000,2013-01-14T18:05:38.541+0000,,Minor
TEZ-2097,TEZ-UI Add dag logs backend support,TEZ,Bug,Closed,[],2,"[<JIRA IssueLink: id='12411128'>, <JIRA IssueLink: id='12442578'>]","If dag fails due to AM error, there's no way to check the dag logs on tez-ui. Users have to grab the app logs. 

",2015-02-13T07:26:59.509+0000,2015-10-12T20:46:30.563+0000,Fixed,Critical
SENTRY-1387,Add HDFS sync tests for drop partition for external/implicit locations,SENTRY,Test,Resolved,[],1,[<JIRA IssueLink: id='12474009'>],"One thing I noted is: When a partition is added to a implicit location (with out using 'location' keyword), the location is stored as table location rather than partition location. So, I think we need to double check if we have coverage for these cases and make sure we handle this correctly.",2016-07-05T21:58:56.204+0000,2017-07-24T15:36:52.679+0000,Fixed,Major
SPARK-2636,Expose job ID in JobWaiter API,SPARK,New Feature,Resolved,[],5,"[<JIRA IssueLink: id='12393428'>, <JIRA IssueLink: id='12394570'>, <JIRA IssueLink: id='12394421'>, <JIRA IssueLink: id='12392234'>, <JIRA IssueLink: id='12395282'>]","In Hive on Spark, we want to track spark job status through Spark API, the basic idea is as following:
# create an hive-specified spark listener and register it to spark listener bus.
# hive-specified spark listener generate job status by spark listener events.
# hive driver track job status through hive-specified spark listener. 

the current problem is that hive driver need job identifier to track specified job status through spark listener, but there is no spark API to get job identifier(like job id) while submit spark job.

I think other project whoever try to track job status with spark API would suffer from this as well.",2014-07-23T03:01:07.285+0000,2014-09-02T06:28:35.308+0000,Fixed,Major
CHUKWA-342,Static Swimlanes Visualization Widget,CHUKWA,New Feature,Resolved,[],5,"[<JIRA IssueLink: id='12325617'>, <JIRA IssueLink: id='12325585'>, <JIRA IssueLink: id='12325584'>, <JIRA IssueLink: id='12325841'>, <JIRA IssueLink: id='12325649'>]","Online JavaScript-based Swimlanes widget causes browser stalls for large jobs with large numbers of maps/reduces being plotted. New static Swimlanes widget generates images offline and returns the image for on-screen viewing, and has a dynamically zoomable tiled viewer.",2009-07-09T23:17:52.712+0000,2009-07-21T07:47:58.409+0000,Fixed,Major
SENTRY-1371,Rework fetching Hive Paths state ,SENTRY,Sub-task,Resolved,[],2,"[<JIRA IssueLink: id='12472208'>, <JIRA IssueLink: id='12492904'>]",Rework fetching Hive paths state. Retrieving HMS paths state from HMS.,2016-06-28T00:10:28.859+0000,2017-07-24T15:36:58.195+0000,Fixed,Major
INFRA-22861,Migrate the hbase1-10 nodes from ci-hadoop to ci-hbase,INFRA,Task,Closed,[],1,[<JIRA IssueLink: id='12633159'>],So we have enough resources to also migrate the pre commit job to ci-hbase.,2022-02-09T09:14:13.977+0000,2022-02-09T15:14:44.614+0000,Fixed,Major
TEZ-696,Remove implicit copying of processor payload to input and output,TEZ,Sub-task,Closed,[],2,"[<JIRA IssueLink: id='12383415'>, <JIRA IssueLink: id='12383416'>]","In the task, the processor payload is implicitly copied to the input and output if they dont have one. While this may have been convenient in some cases, its very confusing when things dont work or when they surprisingly work. Also, its probably clearer to specify payload where appropriate so that its clear which one is for which. While writing the word count example I had to spend a lot of time debugging why my outputs had stopped working when I removed the map reduce confs from my non-MR processors.",2013-12-29T13:03:31.198+0000,2014-09-06T01:35:13.084+0000,Fixed,Major
SUREFIRE-1426,Fork crash doesn't fail build with -Dmaven.test.failure.ignore=true,SUREFIRE,Bug,Closed,[],2,"[<JIRA IssueLink: id='12636402'>, <JIRA IssueLink: id='12642172'>]","We have a reactor with many modules, and also a few random test failures. We want to run the tests of all the modules even if a test failed in the core module, so we use {{mvn -Dmaven.test.failure.ignore=true}}. {{mvn --fail-at-end}} only builds the modules that do not depend on the module with a failed test.

The problem is that fork crashes like this one are written to the surefire reports, so Jenkins ignores them:

{noformat}
org.apache.maven.surefire.booter.SurefireBooterForkException: There was an error in the forked process
Unable to load category: stress
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:665)
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:533)
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:279)
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:243)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1077)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:907)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:785)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356){noformat}


",2017-09-29T09:48:43.079+0000,2022-06-20T14:42:52.657+0000,Fixed,Major
TEZ-3865,A new vertex manager to partition data for STORE,TEZ,New Feature,Open,[],2,"[<JIRA IssueLink: id='12520120'>, <JIRA IssueLink: id='12521781'>]","Restricting number of files in output is a very common use case. In Pig, currently users add a ORDER BY, GROUP BY or DISTINCT with the required parallelism before STORE to achieve it. All of the above operations create unnecessary overhead in processing. It would be ideal if STORE clause supported the PARALLEL statement and the partitioning of data was handled in a more simple and efficient manner.

Partitioning of the data can be achieved using a very efficient vertex manager as described below. Going to call it PartitionVertexManager (PVM) for now till someone proposes a better name. Will be explaining using Pig examples, but the logic is same for hive as well.

There are multiple cases to consider when storing
1) No partitions
       - Data is stored into a single directory using FileOutputFormat implementations
2) Partitions
      - Data is stored into multiple partitions. Case of static or dynamic partitioning with HCat
3) HBase
    I have kind of forgotten what exactly my thoughts were on this when storing to multiple regions. Will update once I remember.

Let us consider below script with pig.exec.bytes.per.reducer (this setting is usually translated to tez.shuffle-vertex-manager.desired-task-input-size with ShuffleVertexManager) set to 1G.
{code}
A = LOAD 'data' ....;
B = GROUP A BY $0 PARALLEL 1000;
C = FOREACH B GENERATE group, COUNT(A.a), SUM(A.b), ..;
D = STORE C into 'output' using SomeStoreFunc() PARALLEL 20;
{code}

The implementation will have 3 vertices.
v1 - LOAD vertex
v2 - GROUP BY vertex
v3 - STORE vertex

PVM will be used on v3. It is going to be similar to ShuffleVertexManager but with some differences. The main difference is that the source vertex does not care about the parallelism of destination vertex and the number of partitioned outputs it produces does not depend on that.

1) Case of no partitions
   Each task in vertex v2 will produce a single partition output (no Partitioner is required). The PVM will bucket this single partition data from 1000 source tasks into multiple destination tasks of v3 trying to keep 1G per task but max of 20 tasks (auto parallelism).
   
2) Partitions
   Let us say the table has 2 partition keys (dt and region). Since there could be any number of regions for a given date, we will use store parallelism as the upper limit on the number of partitions. i.e a HashPartitioner with numReduceTasks as 20 and (dt, region) as the partition key. If there are only 5 regions then each task of v2 will produce 5 partitions (with rest 15 being empty) if there is no hash collision. If there are 30 regions, then each task of v2 will produce 20 partitions.
   
   The PVM when it groups will try to group all Partition0 segments as much as possible into one v3 task. Based on skew it could end up in more tasks. i.e there is no restriction on one partition going to same reducer task. Doing this will avoid having to open multiple ORC files in one task when doing dynamic partitioning and will be very efficient reducing namespace usage even further while keeping file sizes more uniform.",2017-11-14T20:24:56.641+0000,2017-12-07T16:59:50.005+0000,,Major
